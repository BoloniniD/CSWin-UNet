[23:56:11.276] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes_old=9, num_classes_new=4, output_dir='./debug_simple_tpgm', max_iterations=10000, max_epochs=5, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.35, kd_temperature=3.0, kd_weight=0.2, freeze_old_classes=False, auto_tune='none', gradient_batches=5, tpgm_norm_mode='l2', tpgm_lr=0.01, tpgm_iters=200, tpgm_exclude=[], tpgm_frequency=5, tpgm_start_epoch=10, disable_tpgm=False, tpgm_data_fraction=0.1, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[23:56:11.297] Using 33327/95221 samples (35.0%) for continual learning
[23:56:11.297] Old classes: 9, New classes: 4, Total: 12
[23:56:11.297] TPGM enabled: True
[23:56:11.297] Surgical fine-tuning method: none
[23:58:29.193] Combined Continual Learning + Surgical + TPGM Configuration:
[23:58:29.194] KD Temperature: 3.0
[23:58:29.194] KD Weight: 0.2
[23:58:29.194] Auto-tune method: none
[23:58:29.194] TPGM start epoch: 10
[23:58:29.194] TPGM frequency: 5
[23:58:29.194] 1042 iterations per epoch. 5210 max iterations 
[23:58:46.973] iteration 10 : loss : 6301.992676, loss_ce: 0.198181, loss_kd: 31507.916016
[23:58:52.446] iteration 20 : loss : 5235.099121, loss_ce: 0.222409, loss_kd: 26172.578125
[23:58:57.946] iteration 30 : loss : 3640.593750, loss_ce: 0.151602, loss_kd: 18200.001953
[23:59:03.437] iteration 40 : loss : 2744.828369, loss_ce: 0.209289, loss_kd: 13721.267578
[23:59:08.946] iteration 50 : loss : 2040.166992, loss_ce: 0.166130, loss_kd: 10197.995117
[23:59:14.448] iteration 60 : loss : 2317.333984, loss_ce: 0.163122, loss_kd: 11583.814453
[23:59:19.958] iteration 70 : loss : 1679.260620, loss_ce: 0.213664, loss_kd: 8393.434570
[23:59:25.466] iteration 80 : loss : 1606.224243, loss_ce: 0.095745, loss_kd: 8028.332031
[23:59:30.984] iteration 90 : loss : 1351.611938, loss_ce: 0.118228, loss_kd: 6755.125000
[23:59:36.491] iteration 100 : loss : 1473.906616, loss_ce: 0.089193, loss_kd: 7366.684082
[23:59:42.023] iteration 110 : loss : 1208.943604, loss_ce: 0.121265, loss_kd: 6041.935547
[23:59:47.534] iteration 120 : loss : 1323.377808, loss_ce: 0.070310, loss_kd: 6614.161621
[23:59:53.060] iteration 130 : loss : 1045.191650, loss_ce: 0.075926, loss_kd: 5223.224609
[23:59:58.573] iteration 140 : loss : 1376.516357, loss_ce: 0.038102, loss_kd: 6879.866211
[00:00:04.101] iteration 150 : loss : 1371.044434, loss_ce: 0.069519, loss_kd: 6852.440918
[00:00:09.621] iteration 160 : loss : 1154.909424, loss_ce: 0.030118, loss_kd: 5771.775879
[00:00:15.150] iteration 170 : loss : 1162.571411, loss_ce: 0.027913, loss_kd: 5810.291016
[00:00:20.673] iteration 180 : loss : 1432.007080, loss_ce: 0.018618, loss_kd: 7157.342285
[00:00:26.210] iteration 190 : loss : 960.016418, loss_ce: 0.026000, loss_kd: 4797.350586
[00:00:31.767] iteration 200 : loss : 958.004944, loss_ce: 0.022786, loss_kd: 4787.314453
[00:00:37.314] iteration 210 : loss : 863.154846, loss_ce: 0.027904, loss_kd: 4313.101562
[00:00:42.840] iteration 220 : loss : 934.114563, loss_ce: 0.015804, loss_kd: 4667.893555
[00:00:48.383] iteration 230 : loss : 1122.044067, loss_ce: 0.020089, loss_kd: 5607.571289
[00:00:53.911] iteration 240 : loss : 930.701538, loss_ce: 0.018013, loss_kd: 4650.843262
[00:00:59.455] iteration 250 : loss : 826.670898, loss_ce: 0.026110, loss_kd: 4130.701172
[00:01:04.983] iteration 260 : loss : 953.940918, loss_ce: 0.021844, loss_kd: 4767.205078
[00:01:10.524] iteration 270 : loss : 720.633057, loss_ce: 0.015098, loss_kd: 3600.542480
[00:01:16.057] iteration 280 : loss : 790.581604, loss_ce: 0.030073, loss_kd: 3950.347168
[00:01:21.598] iteration 290 : loss : 850.850769, loss_ce: 0.013022, loss_kd: 4251.703613
[00:01:27.133] iteration 300 : loss : 913.787720, loss_ce: 0.013461, loss_kd: 4566.443359
[00:01:32.681] iteration 310 : loss : 769.306274, loss_ce: 0.016772, loss_kd: 3843.960449
[00:01:38.218] iteration 320 : loss : 765.508057, loss_ce: 0.014307, loss_kd: 3825.035156
[00:01:43.766] iteration 330 : loss : 994.420837, loss_ce: 0.019897, loss_kd: 4969.536621
[00:01:49.303] iteration 340 : loss : 1008.881348, loss_ce: 0.013098, loss_kd: 5041.923828
[00:01:54.852] iteration 350 : loss : 969.707825, loss_ce: 0.025274, loss_kd: 4845.980957
[00:02:00.390] iteration 360 : loss : 887.628845, loss_ce: 0.027959, loss_kd: 4435.604492
[00:02:05.946] iteration 370 : loss : 779.592896, loss_ce: 0.019202, loss_kd: 3895.408691
[00:02:11.484] iteration 380 : loss : 748.214905, loss_ce: 0.029567, loss_kd: 3738.551270
[00:02:17.033] iteration 390 : loss : 835.180786, loss_ce: 0.016141, loss_kd: 4173.386230
[00:02:22.574] iteration 400 : loss : 1190.558350, loss_ce: 0.014407, loss_kd: 5950.205566
[00:02:28.125] iteration 410 : loss : 778.773804, loss_ce: 0.026246, loss_kd: 3891.378418
[00:02:33.662] iteration 420 : loss : 1097.909424, loss_ce: 0.020971, loss_kd: 5487.067383
[00:02:39.218] iteration 430 : loss : 531.415161, loss_ce: 0.017017, loss_kd: 2654.605957
[00:02:44.771] iteration 440 : loss : 621.984192, loss_ce: 0.018610, loss_kd: 3107.417480
[00:02:50.331] iteration 450 : loss : 761.383728, loss_ce: 0.021549, loss_kd: 3804.373047
[00:02:55.881] iteration 460 : loss : 591.545288, loss_ce: 0.027909, loss_kd: 2955.218994
[00:03:01.434] iteration 470 : loss : 677.736755, loss_ce: 0.018926, loss_kd: 3385.984619
[00:03:06.981] iteration 480 : loss : 958.180237, loss_ce: 0.034763, loss_kd: 4788.374023
[00:03:12.544] iteration 490 : loss : 656.168579, loss_ce: 0.015372, loss_kd: 3278.200684
[00:03:18.092] iteration 500 : loss : 692.132568, loss_ce: 0.019971, loss_kd: 3458.120850
[00:03:23.657] iteration 510 : loss : 648.700378, loss_ce: 0.026182, loss_kd: 3241.033203
[00:03:29.205] iteration 520 : loss : 566.115723, loss_ce: 0.024851, loss_kd: 2827.968750
[00:03:34.762] iteration 530 : loss : 620.572083, loss_ce: 0.015074, loss_kd: 3100.262207
[00:03:40.311] iteration 540 : loss : 691.550232, loss_ce: 0.021775, loss_kd: 3455.215820
[00:03:45.877] iteration 550 : loss : 953.938354, loss_ce: 0.019887, loss_kd: 4767.178223
[00:03:51.427] iteration 560 : loss : 619.584290, loss_ce: 0.020442, loss_kd: 3095.398193
[00:03:56.986] iteration 570 : loss : 788.535156, loss_ce: 0.036427, loss_kd: 3940.176270
[00:04:02.535] iteration 580 : loss : 784.847656, loss_ce: 0.038108, loss_kd: 3921.742188
[00:04:08.097] iteration 590 : loss : 707.251709, loss_ce: 0.014106, loss_kd: 3533.747559
[00:04:13.649] iteration 600 : loss : 754.522888, loss_ce: 0.022197, loss_kd: 3770.125977
[00:04:19.219] iteration 610 : loss : 570.347473, loss_ce: 0.020464, loss_kd: 2849.277100
[00:04:24.773] iteration 620 : loss : 703.846252, loss_ce: 0.020361, loss_kd: 3516.687500
[00:04:30.334] iteration 630 : loss : 632.352051, loss_ce: 0.029300, loss_kd: 3159.184082
[00:04:35.886] iteration 640 : loss : 543.905701, loss_ce: 0.029212, loss_kd: 2717.027832
[00:04:41.451] iteration 650 : loss : 513.276062, loss_ce: 0.022687, loss_kd: 2563.876953
[00:04:47.010] iteration 660 : loss : 557.487305, loss_ce: 0.029769, loss_kd: 2784.951172
[00:04:52.575] iteration 670 : loss : 745.483337, loss_ce: 0.029091, loss_kd: 3724.935059
[00:04:58.129] iteration 680 : loss : 535.916870, loss_ce: 0.033475, loss_kd: 2677.021729
[00:05:03.698] iteration 690 : loss : 700.436768, loss_ce: 0.011043, loss_kd: 3499.606934
[00:05:09.250] iteration 700 : loss : 549.448547, loss_ce: 0.029009, loss_kd: 2744.710449
[00:05:14.818] iteration 710 : loss : 589.189758, loss_ce: 0.036965, loss_kd: 2943.442383
[00:05:20.375] iteration 720 : loss : 717.150330, loss_ce: 0.026489, loss_kd: 3583.287598
[00:05:25.951] iteration 730 : loss : 571.733704, loss_ce: 0.035390, loss_kd: 2856.148926
[00:05:31.509] iteration 740 : loss : 479.064758, loss_ce: 0.026550, loss_kd: 2392.803223
[00:05:37.080] iteration 750 : loss : 643.248901, loss_ce: 0.022442, loss_kd: 3213.753662
[00:05:42.636] iteration 760 : loss : 701.796448, loss_ce: 0.018848, loss_kd: 3506.369873
[00:05:48.209] iteration 770 : loss : 720.217529, loss_ce: 0.036457, loss_kd: 3598.575195
[00:05:53.773] iteration 780 : loss : 610.198914, loss_ce: 0.019531, loss_kd: 3048.498779
[00:05:59.337] iteration 790 : loss : 564.452209, loss_ce: 0.020891, loss_kd: 2819.811523
[00:06:04.891] iteration 800 : loss : 573.205627, loss_ce: 0.034533, loss_kd: 2863.507812
[00:06:10.464] iteration 810 : loss : 495.657562, loss_ce: 0.021416, loss_kd: 2475.662109
[00:06:16.018] iteration 820 : loss : 633.644531, loss_ce: 0.030229, loss_kd: 3165.649414
[00:06:21.593] iteration 830 : loss : 623.760437, loss_ce: 0.026686, loss_kd: 3116.258301
[00:06:27.151] iteration 840 : loss : 587.900940, loss_ce: 0.021398, loss_kd: 2937.020020
[00:06:32.717] iteration 850 : loss : 696.813965, loss_ce: 0.030022, loss_kd: 3481.505371
[00:06:38.279] iteration 860 : loss : 576.290833, loss_ce: 0.025735, loss_kd: 2878.976074
[00:06:43.852] iteration 870 : loss : 515.943787, loss_ce: 0.017495, loss_kd: 2577.158691
[00:06:49.407] iteration 880 : loss : 585.622498, loss_ce: 0.027904, loss_kd: 2925.573486
[00:06:54.997] iteration 890 : loss : 518.195007, loss_ce: 0.020933, loss_kd: 2588.544922
[00:07:00.557] iteration 900 : loss : 525.036743, loss_ce: 0.028447, loss_kd: 2622.633789
[00:07:06.130] iteration 910 : loss : 542.884277, loss_ce: 0.018154, loss_kd: 2711.834229
[00:07:11.690] iteration 920 : loss : 544.115784, loss_ce: 0.016323, loss_kd: 2718.077393
[00:07:17.260] iteration 930 : loss : 641.668274, loss_ce: 0.023712, loss_kd: 3205.781494
[00:07:22.818] iteration 940 : loss : 461.072479, loss_ce: 0.020248, loss_kd: 2302.854980
[00:07:28.392] iteration 950 : loss : 517.990662, loss_ce: 0.022621, loss_kd: 2587.437744
[00:07:33.956] iteration 960 : loss : 618.519409, loss_ce: 0.024079, loss_kd: 3090.014404
[00:07:39.529] iteration 970 : loss : 511.119965, loss_ce: 0.038566, loss_kd: 2553.060547
[00:07:45.093] iteration 980 : loss : 787.889465, loss_ce: 0.039365, loss_kd: 3936.873291
[00:07:50.662] iteration 990 : loss : 462.969727, loss_ce: 0.017592, loss_kd: 2312.307373
[00:07:56.221] iteration 1000 : loss : 619.528259, loss_ce: 0.026151, loss_kd: 3095.132080
[00:08:01.793] iteration 1010 : loss : 433.944885, loss_ce: 0.029308, loss_kd: 2167.174561
[00:08:07.349] iteration 1020 : loss : 514.401794, loss_ce: 0.029307, loss_kd: 2569.464844
[00:08:12.932] iteration 1030 : loss : 581.826599, loss_ce: 0.024048, loss_kd: 2906.628662
[00:08:18.505] iteration 1040 : loss : 535.856750, loss_ce: 0.022480, loss_kd: 2676.767090
[00:08:35.946] iteration 1050 : loss : 635.783997, loss_ce: 0.025395, loss_kd: 3176.476807
[00:08:41.479] iteration 1060 : loss : 423.892548, loss_ce: 0.029434, loss_kd: 2116.964844
[00:08:47.027] iteration 1070 : loss : 449.220764, loss_ce: 0.017780, loss_kd: 2243.608398
[00:08:52.563] iteration 1080 : loss : 550.868896, loss_ce: 0.028217, loss_kd: 2751.829102
[00:08:58.114] iteration 1090 : loss : 512.531006, loss_ce: 0.019163, loss_kd: 2560.176514
[00:09:03.656] iteration 1100 : loss : 467.794037, loss_ce: 0.015092, loss_kd: 2336.479736
[00:09:09.210] iteration 1110 : loss : 453.728302, loss_ce: 0.028621, loss_kd: 2266.103516
[00:09:14.753] iteration 1120 : loss : 471.560364, loss_ce: 0.018981, loss_kd: 2355.314453
[00:09:20.308] iteration 1130 : loss : 487.993317, loss_ce: 0.022299, loss_kd: 2437.389648
[00:09:25.851] iteration 1140 : loss : 738.332458, loss_ce: 0.021823, loss_kd: 3689.132568
[00:09:31.414] iteration 1150 : loss : 448.443817, loss_ce: 0.021047, loss_kd: 2239.754883
[00:09:36.968] iteration 1160 : loss : 450.558105, loss_ce: 0.024057, loss_kd: 2250.229736
[00:09:42.534] iteration 1170 : loss : 548.731750, loss_ce: 0.019277, loss_kd: 2741.202637
[00:09:48.086] iteration 1180 : loss : 540.248962, loss_ce: 0.019116, loss_kd: 2698.794189
[00:09:53.660] iteration 1190 : loss : 441.378082, loss_ce: 0.040137, loss_kd: 2204.337646
[00:09:59.218] iteration 1200 : loss : 321.755890, loss_ce: 0.038455, loss_kd: 1606.268799
[00:10:04.788] iteration 1210 : loss : 432.658844, loss_ce: 0.024373, loss_kd: 2160.844482
[00:10:10.338] iteration 1220 : loss : 438.516541, loss_ce: 0.022485, loss_kd: 2190.110596
[00:10:15.906] iteration 1230 : loss : 584.516235, loss_ce: 0.024645, loss_kd: 2920.125488
[00:10:21.457] iteration 1240 : loss : 530.109558, loss_ce: 0.031574, loss_kd: 2648.040527
[00:10:27.024] iteration 1250 : loss : 430.051575, loss_ce: 0.025281, loss_kd: 2147.746338
[00:10:32.584] iteration 1260 : loss : 517.493225, loss_ce: 0.019082, loss_kd: 2585.011230
[00:10:38.155] iteration 1270 : loss : 406.935059, loss_ce: 0.021362, loss_kd: 2032.204346
[00:10:43.715] iteration 1280 : loss : 559.701782, loss_ce: 0.024150, loss_kd: 2796.048584
[00:10:49.283] iteration 1290 : loss : 428.804840, loss_ce: 0.034221, loss_kd: 2141.521240
[00:10:54.850] iteration 1300 : loss : 518.063232, loss_ce: 0.028014, loss_kd: 2587.838379
[00:11:00.427] iteration 1310 : loss : 613.050781, loss_ce: 0.023588, loss_kd: 3062.781738
[00:11:05.993] iteration 1320 : loss : 445.646240, loss_ce: 0.030407, loss_kd: 2225.764648
[00:11:11.571] iteration 1330 : loss : 445.818115, loss_ce: 0.028189, loss_kd: 2226.591064
[00:11:17.135] iteration 1340 : loss : 383.245453, loss_ce: 0.015485, loss_kd: 1913.705200
[00:11:22.702] iteration 1350 : loss : 369.486755, loss_ce: 0.026482, loss_kd: 1844.942017
[00:11:28.266] iteration 1360 : loss : 457.637421, loss_ce: 0.022822, loss_kd: 2285.695801
[00:11:33.847] iteration 1370 : loss : 343.290955, loss_ce: 0.032200, loss_kd: 1713.946655
[00:11:39.408] iteration 1380 : loss : 355.735748, loss_ce: 0.032947, loss_kd: 1776.165894
[00:11:44.987] iteration 1390 : loss : 525.627808, loss_ce: 0.028832, loss_kd: 2625.659424
[00:11:50.555] iteration 1400 : loss : 456.037506, loss_ce: 0.029856, loss_kd: 2277.498047
[00:11:56.133] iteration 1410 : loss : 557.233459, loss_ce: 0.024732, loss_kd: 2783.666748
[00:12:01.699] iteration 1420 : loss : 681.178284, loss_ce: 0.023084, loss_kd: 3403.379395
[00:12:07.274] iteration 1430 : loss : 642.531311, loss_ce: 0.019977, loss_kd: 3210.222656
[00:12:12.838] iteration 1440 : loss : 478.641754, loss_ce: 0.025695, loss_kd: 2390.690430
[00:12:18.432] iteration 1450 : loss : 386.804077, loss_ce: 0.030744, loss_kd: 1931.508057
[00:12:23.998] iteration 1460 : loss : 452.928833, loss_ce: 0.020798, loss_kd: 2262.177979
[00:12:29.578] iteration 1470 : loss : 353.576416, loss_ce: 0.020438, loss_kd: 1765.423950
[00:12:35.146] iteration 1480 : loss : 598.186646, loss_ce: 0.017057, loss_kd: 2988.472412
[00:12:40.726] iteration 1490 : loss : 692.279297, loss_ce: 0.028658, loss_kd: 3458.897461
[00:12:46.292] iteration 1500 : loss : 391.676636, loss_ce: 0.026385, loss_kd: 1955.898071
[00:12:51.878] iteration 1510 : loss : 431.762451, loss_ce: 0.041179, loss_kd: 2156.281494
[00:12:57.445] iteration 1520 : loss : 574.267212, loss_ce: 0.017082, loss_kd: 2868.866455
[00:13:03.018] iteration 1530 : loss : 434.229858, loss_ce: 0.018970, loss_kd: 2168.701416
[00:13:08.580] iteration 1540 : loss : 391.223053, loss_ce: 0.029759, loss_kd: 1953.567383
[00:13:14.157] iteration 1550 : loss : 458.018799, loss_ce: 0.024796, loss_kd: 2287.643311
[00:13:19.727] iteration 1560 : loss : 384.358765, loss_ce: 0.024880, loss_kd: 1919.296143
[00:13:25.305] iteration 1570 : loss : 571.269348, loss_ce: 0.031334, loss_kd: 2853.877197
[00:13:30.871] iteration 1580 : loss : 569.232605, loss_ce: 0.015601, loss_kd: 2843.672852
[00:13:36.451] iteration 1590 : loss : 400.777740, loss_ce: 0.043427, loss_kd: 2001.372803
[00:13:42.012] iteration 1600 : loss : 603.065308, loss_ce: 0.028015, loss_kd: 3012.834473
[00:13:47.588] iteration 1610 : loss : 367.624847, loss_ce: 0.025409, loss_kd: 1835.687866
[00:13:53.155] iteration 1620 : loss : 387.346252, loss_ce: 0.021963, loss_kd: 1934.241943
[00:13:58.735] iteration 1630 : loss : 391.888824, loss_ce: 0.029500, loss_kd: 1956.961304
[00:14:04.301] iteration 1640 : loss : 375.737427, loss_ce: 0.026536, loss_kd: 1876.148682
[00:14:09.880] iteration 1650 : loss : 406.337494, loss_ce: 0.041914, loss_kd: 2029.212646
[00:14:15.446] iteration 1660 : loss : 430.482880, loss_ce: 0.034264, loss_kd: 2149.962402
[00:14:21.026] iteration 1670 : loss : 313.462738, loss_ce: 0.030208, loss_kd: 1564.819214
[00:14:26.598] iteration 1680 : loss : 436.488892, loss_ce: 0.026846, loss_kd: 2179.943604
[00:14:32.172] iteration 1690 : loss : 549.635681, loss_ce: 0.036216, loss_kd: 2745.688232
[00:14:37.738] iteration 1700 : loss : 553.660522, loss_ce: 0.017854, loss_kd: 2765.791016
[00:14:43.319] iteration 1710 : loss : 406.700806, loss_ce: 0.028153, loss_kd: 2030.999390
[00:14:48.887] iteration 1720 : loss : 334.824738, loss_ce: 0.030112, loss_kd: 1671.631348
[00:14:54.469] iteration 1730 : loss : 406.280853, loss_ce: 0.036908, loss_kd: 2028.879639
[00:15:00.032] iteration 1740 : loss : 478.344910, loss_ce: 0.026388, loss_kd: 2389.281982
[00:15:05.614] iteration 1750 : loss : 430.366577, loss_ce: 0.016022, loss_kd: 2149.345947
[00:15:11.191] iteration 1760 : loss : 372.643250, loss_ce: 0.011796, loss_kd: 1860.739258
[00:15:16.771] iteration 1770 : loss : 349.610840, loss_ce: 0.023210, loss_kd: 1745.604736
[00:15:22.344] iteration 1780 : loss : 427.590179, loss_ce: 0.027846, loss_kd: 2135.493652
[00:15:27.922] iteration 1790 : loss : 398.811066, loss_ce: 0.024026, loss_kd: 1991.582764
[00:15:33.496] iteration 1800 : loss : 421.760040, loss_ce: 0.020543, loss_kd: 2106.341309
[00:15:39.080] iteration 1810 : loss : 424.449249, loss_ce: 0.024276, loss_kd: 2119.772705
[00:15:44.648] iteration 1820 : loss : 457.338348, loss_ce: 0.036907, loss_kd: 2284.191406
[00:15:50.233] iteration 1830 : loss : 487.135803, loss_ce: 0.041985, loss_kd: 2433.188232
[00:15:55.795] iteration 1840 : loss : 439.184082, loss_ce: 0.025105, loss_kd: 2193.402100
[00:16:01.379] iteration 1850 : loss : 421.466492, loss_ce: 0.029212, loss_kd: 2104.779053
[00:16:06.951] iteration 1860 : loss : 338.039520, loss_ce: 0.028851, loss_kd: 1687.723633
[00:16:12.530] iteration 1870 : loss : 425.687775, loss_ce: 0.027134, loss_kd: 2125.958496
[00:16:18.106] iteration 1880 : loss : 364.021881, loss_ce: 0.020060, loss_kd: 1817.619141
[00:16:23.681] iteration 1890 : loss : 388.849915, loss_ce: 0.039755, loss_kd: 1941.776611
[00:16:29.253] iteration 1900 : loss : 443.206268, loss_ce: 0.024806, loss_kd: 2213.566895
[00:16:34.840] iteration 1910 : loss : 445.167236, loss_ce: 0.016422, loss_kd: 2223.349365
[00:16:40.418] iteration 1920 : loss : 390.773285, loss_ce: 0.037467, loss_kd: 1951.359009
[00:16:45.996] iteration 1930 : loss : 350.177155, loss_ce: 0.020940, loss_kd: 1748.410767
[00:16:51.566] iteration 1940 : loss : 305.791107, loss_ce: 0.028550, loss_kd: 1526.466431
[00:16:57.144] iteration 1950 : loss : 427.749084, loss_ce: 0.016209, loss_kd: 2136.283203
[00:17:02.713] iteration 1960 : loss : 446.828857, loss_ce: 0.017244, loss_kd: 2231.667725
[00:17:08.294] iteration 1970 : loss : 320.904816, loss_ce: 0.020688, loss_kd: 1602.066772
[00:17:13.866] iteration 1980 : loss : 359.741669, loss_ce: 0.024428, loss_kd: 1796.202637
[00:17:19.452] iteration 1990 : loss : 517.939209, loss_ce: 0.024687, loss_kd: 2587.218018
[00:17:25.022] iteration 2000 : loss : 339.122223, loss_ce: 0.024440, loss_kd: 1693.133301
[00:17:30.608] iteration 2010 : loss : 467.796417, loss_ce: 0.026631, loss_kd: 2336.521729
[00:17:36.175] iteration 2020 : loss : 413.756989, loss_ce: 0.035980, loss_kd: 2066.306641
[00:17:41.759] iteration 2030 : loss : 400.358185, loss_ce: 0.018064, loss_kd: 1999.300537
[00:17:47.325] iteration 2040 : loss : 293.949860, loss_ce: 0.021608, loss_kd: 1467.259399
[00:17:52.915] iteration 2050 : loss : 314.191803, loss_ce: 0.017572, loss_kd: 1568.461792
[00:17:58.488] iteration 2060 : loss : 398.115356, loss_ce: 0.024575, loss_kd: 1988.108154
[00:18:04.064] iteration 2070 : loss : 465.217651, loss_ce: 0.026928, loss_kd: 2323.613281
[00:18:09.666] iteration 2080 : loss : 459.620758, loss_ce: 0.030342, loss_kd: 2295.616699
[00:18:27.459] iteration 2090 : loss : 416.502472, loss_ce: 0.026209, loss_kd: 2080.079346
[00:18:32.991] iteration 2100 : loss : 413.295258, loss_ce: 0.025751, loss_kd: 2063.966064
[00:18:38.539] iteration 2110 : loss : 467.959778, loss_ce: 0.020440, loss_kd: 2337.345215
[00:18:44.076] iteration 2120 : loss : 317.176544, loss_ce: 0.032060, loss_kd: 1583.388672
[00:18:49.627] iteration 2130 : loss : 363.715088, loss_ce: 0.020914, loss_kd: 1816.149536
[00:18:55.167] iteration 2140 : loss : 327.673981, loss_ce: 0.024746, loss_kd: 1635.895874
[00:19:00.726] iteration 2150 : loss : 416.263611, loss_ce: 0.018628, loss_kd: 2078.794189
[00:19:06.285] iteration 2160 : loss : 353.719940, loss_ce: 0.021555, loss_kd: 1766.138062
[00:19:11.847] iteration 2170 : loss : 464.830048, loss_ce: 0.015764, loss_kd: 2321.705322
[00:19:17.402] iteration 2180 : loss : 400.882965, loss_ce: 0.039616, loss_kd: 2001.951172
[00:19:22.957] iteration 2190 : loss : 454.210724, loss_ce: 0.023904, loss_kd: 2268.609375
[00:19:28.510] iteration 2200 : loss : 271.961945, loss_ce: 0.027861, loss_kd: 1357.289062
[00:19:34.081] iteration 2210 : loss : 349.900574, loss_ce: 0.026374, loss_kd: 1747.032837
[00:19:39.641] iteration 2220 : loss : 373.686859, loss_ce: 0.026055, loss_kd: 1865.980225
[00:19:45.219] iteration 2230 : loss : 420.923492, loss_ce: 0.013693, loss_kd: 2102.122070
[00:19:50.779] iteration 2240 : loss : 347.230377, loss_ce: 0.022184, loss_kd: 1733.661011
[00:19:56.353] iteration 2250 : loss : 340.165771, loss_ce: 0.020997, loss_kd: 1698.391113
[00:20:01.914] iteration 2260 : loss : 291.599579, loss_ce: 0.030226, loss_kd: 1455.512695
[00:20:07.484] iteration 2270 : loss : 268.326813, loss_ce: 0.023645, loss_kd: 1339.161377
[00:20:13.042] iteration 2280 : loss : 396.715851, loss_ce: 0.025161, loss_kd: 1981.102783
[00:20:18.616] iteration 2290 : loss : 276.291840, loss_ce: 0.022625, loss_kd: 1379.009399
[00:20:24.172] iteration 2300 : loss : 339.051697, loss_ce: 0.025072, loss_kd: 1692.754761
[00:20:29.753] iteration 2310 : loss : 352.221588, loss_ce: 0.032145, loss_kd: 1758.662720
[00:20:35.311] iteration 2320 : loss : 487.952820, loss_ce: 0.031265, loss_kd: 2437.316895
[00:20:40.884] iteration 2330 : loss : 395.183441, loss_ce: 0.028519, loss_kd: 1973.451904
[00:20:46.453] iteration 2340 : loss : 464.775970, loss_ce: 0.023382, loss_kd: 2321.428223
[00:20:52.027] iteration 2350 : loss : 399.060242, loss_ce: 0.016718, loss_kd: 1992.836304
[00:20:57.591] iteration 2360 : loss : 292.930634, loss_ce: 0.025378, loss_kd: 1462.197510
[00:21:03.165] iteration 2370 : loss : 350.187744, loss_ce: 0.030046, loss_kd: 1748.444336
[00:21:08.726] iteration 2380 : loss : 359.528595, loss_ce: 0.016140, loss_kd: 1795.208008
[00:21:14.301] iteration 2390 : loss : 508.654602, loss_ce: 0.021922, loss_kd: 2540.786621
[00:21:19.867] iteration 2400 : loss : 396.384674, loss_ce: 0.018326, loss_kd: 1979.444336
[00:21:25.457] iteration 2410 : loss : 404.430573, loss_ce: 0.028036, loss_kd: 2019.701294
[00:21:31.027] iteration 2420 : loss : 476.417816, loss_ce: 0.031846, loss_kd: 2379.558594
[00:21:36.604] iteration 2430 : loss : 355.326721, loss_ce: 0.025051, loss_kd: 1774.168823
[00:21:42.179] iteration 2440 : loss : 328.783752, loss_ce: 0.014405, loss_kd: 1641.501587
[00:21:47.761] iteration 2450 : loss : 291.553070, loss_ce: 0.026861, loss_kd: 1455.291138
[00:21:53.329] iteration 2460 : loss : 311.269592, loss_ce: 0.018534, loss_kd: 1553.871948
[00:21:58.912] iteration 2470 : loss : 347.574982, loss_ce: 0.014971, loss_kd: 1735.413330
[00:22:04.474] iteration 2480 : loss : 334.333771, loss_ce: 0.024607, loss_kd: 1669.208740
[00:22:10.062] iteration 2490 : loss : 360.523132, loss_ce: 0.045457, loss_kd: 1800.102783
[00:22:15.629] iteration 2500 : loss : 372.371490, loss_ce: 0.026011, loss_kd: 1859.407959
[00:22:21.234] iteration 2510 : loss : 372.530914, loss_ce: 0.026579, loss_kd: 1860.182251
[00:22:26.802] iteration 2520 : loss : 343.114502, loss_ce: 0.024061, loss_kd: 1713.092041
[00:22:32.380] iteration 2530 : loss : 515.373291, loss_ce: 0.022132, loss_kd: 2574.414062
[00:22:37.949] iteration 2540 : loss : 348.253357, loss_ce: 0.026135, loss_kd: 1738.771973
[00:22:43.540] iteration 2550 : loss : 323.953644, loss_ce: 0.029062, loss_kd: 1617.264526
[00:22:49.113] iteration 2560 : loss : 342.981598, loss_ce: 0.029774, loss_kd: 1712.444214
[00:22:54.695] iteration 2570 : loss : 266.003510, loss_ce: 0.029361, loss_kd: 1327.569092
[00:23:00.263] iteration 2580 : loss : 324.268066, loss_ce: 0.029201, loss_kd: 1618.854980
[00:23:05.851] iteration 2590 : loss : 326.462250, loss_ce: 0.027378, loss_kd: 1629.870972
[00:23:11.419] iteration 2600 : loss : 394.762634, loss_ce: 0.031736, loss_kd: 1971.323242
[00:23:17.003] iteration 2610 : loss : 292.031616, loss_ce: 0.020911, loss_kd: 1457.630127
[00:23:22.570] iteration 2620 : loss : 349.396759, loss_ce: 0.031853, loss_kd: 1744.523682
[00:23:28.149] iteration 2630 : loss : 436.663757, loss_ce: 0.022538, loss_kd: 2180.876221
[00:23:33.713] iteration 2640 : loss : 314.263214, loss_ce: 0.027546, loss_kd: 1568.799194
[00:23:39.299] iteration 2650 : loss : 396.254578, loss_ce: 0.021563, loss_kd: 1978.780029
[00:23:44.871] iteration 2660 : loss : 473.517242, loss_ce: 0.023910, loss_kd: 2365.113037
[00:23:50.446] iteration 2670 : loss : 404.442871, loss_ce: 0.031390, loss_kd: 2019.749023
[00:23:56.016] iteration 2680 : loss : 338.175232, loss_ce: 0.015291, loss_kd: 1688.387207
[00:24:01.613] iteration 2690 : loss : 287.968292, loss_ce: 0.018130, loss_kd: 1437.417114
[00:24:07.177] iteration 2700 : loss : 298.392517, loss_ce: 0.023530, loss_kd: 1489.521118
[00:24:12.759] iteration 2710 : loss : 369.647552, loss_ce: 0.036636, loss_kd: 1845.736938
[00:24:18.328] iteration 2720 : loss : 313.839355, loss_ce: 0.015863, loss_kd: 1566.749023
[00:24:23.911] iteration 2730 : loss : 320.421844, loss_ce: 0.024529, loss_kd: 1599.605835
[00:24:29.472] iteration 2740 : loss : 291.199829, loss_ce: 0.014872, loss_kd: 1453.506470
[00:24:35.054] iteration 2750 : loss : 350.490875, loss_ce: 0.035764, loss_kd: 1749.953003
[00:24:40.626] iteration 2760 : loss : 346.353363, loss_ce: 0.031344, loss_kd: 1729.287231
[00:24:46.210] iteration 2770 : loss : 339.299469, loss_ce: 0.034901, loss_kd: 1693.984375
[00:24:51.776] iteration 2780 : loss : 300.774658, loss_ce: 0.018785, loss_kd: 1501.437622
[00:24:57.356] iteration 2790 : loss : 342.226044, loss_ce: 0.027478, loss_kd: 1708.644775
[00:25:02.927] iteration 2800 : loss : 284.467133, loss_ce: 0.023215, loss_kd: 1419.886108
[00:25:08.504] iteration 2810 : loss : 268.350708, loss_ce: 0.030765, loss_kd: 1339.245850
[00:25:14.075] iteration 2820 : loss : 360.916412, loss_ce: 0.035537, loss_kd: 1802.116455
[00:25:19.655] iteration 2830 : loss : 284.387085, loss_ce: 0.020079, loss_kd: 1419.492065
[00:25:25.227] iteration 2840 : loss : 282.411102, loss_ce: 0.025336, loss_kd: 1409.572266
[00:25:30.799] iteration 2850 : loss : 270.515472, loss_ce: 0.022509, loss_kd: 1350.140747
[00:25:36.378] iteration 2860 : loss : 321.365540, loss_ce: 0.035787, loss_kd: 1604.324829
[00:25:41.966] iteration 2870 : loss : 303.833191, loss_ce: 0.027194, loss_kd: 1516.619873
[00:25:47.525] iteration 2880 : loss : 338.829712, loss_ce: 0.027217, loss_kd: 1691.670776
[00:25:53.112] iteration 2890 : loss : 422.730164, loss_ce: 0.032271, loss_kd: 2111.159668
[00:25:58.676] iteration 2900 : loss : 319.481873, loss_ce: 0.029362, loss_kd: 1594.923584
[00:26:04.263] iteration 2910 : loss : 254.388107, loss_ce: 0.037686, loss_kd: 1269.478760
[00:26:09.836] iteration 2920 : loss : 312.888306, loss_ce: 0.025442, loss_kd: 1561.965698
[00:26:15.415] iteration 2930 : loss : 291.316956, loss_ce: 0.026276, loss_kd: 1454.093994
[00:26:20.987] iteration 2940 : loss : 298.408539, loss_ce: 0.021357, loss_kd: 1489.563965
[00:26:26.570] iteration 2950 : loss : 295.327576, loss_ce: 0.018614, loss_kd: 1474.161743
[00:26:32.145] iteration 2960 : loss : 297.802307, loss_ce: 0.022192, loss_kd: 1486.596069
[00:26:37.729] iteration 2970 : loss : 293.202759, loss_ce: 0.013176, loss_kd: 1463.581787
[00:26:43.295] iteration 2980 : loss : 343.983795, loss_ce: 0.019851, loss_kd: 1717.439697
[00:26:48.880] iteration 2990 : loss : 299.831024, loss_ce: 0.023466, loss_kd: 1496.699097
[00:26:54.446] iteration 3000 : loss : 445.706696, loss_ce: 0.024706, loss_kd: 2226.096436
[00:27:00.024] iteration 3010 : loss : 289.173065, loss_ce: 0.023785, loss_kd: 1443.361328
[00:27:05.590] iteration 3020 : loss : 498.124969, loss_ce: 0.020583, loss_kd: 2488.156494
[00:27:11.175] iteration 3030 : loss : 371.613251, loss_ce: 0.024800, loss_kd: 1855.674438
[00:27:16.747] iteration 3040 : loss : 352.950500, loss_ce: 0.017763, loss_kd: 1762.288086
[00:27:22.327] iteration 3050 : loss : 445.726685, loss_ce: 0.028616, loss_kd: 2226.145996
[00:27:27.901] iteration 3060 : loss : 406.523163, loss_ce: 0.022140, loss_kd: 2030.151611
[00:27:33.478] iteration 3070 : loss : 375.356903, loss_ce: 0.028099, loss_kd: 1874.327881
[00:27:39.054] iteration 3080 : loss : 312.005035, loss_ce: 0.026694, loss_kd: 1557.544922
[00:27:44.638] iteration 3090 : loss : 311.940704, loss_ce: 0.023133, loss_kd: 1557.187622
[00:27:50.204] iteration 3100 : loss : 260.915710, loss_ce: 0.019701, loss_kd: 1302.122681
[00:27:55.788] iteration 3110 : loss : 287.918823, loss_ce: 0.023458, loss_kd: 1437.107056
[00:28:01.354] iteration 3120 : loss : 220.580475, loss_ce: 0.023084, loss_kd: 1100.478027
[00:28:18.775] iteration 3130 : loss : 343.047516, loss_ce: 0.027098, loss_kd: 1712.710205
[00:28:24.307] iteration 3140 : loss : 324.585266, loss_ce: 0.026133, loss_kd: 1620.439453
[00:28:29.864] iteration 3150 : loss : 300.282715, loss_ce: 0.033878, loss_kd: 1498.929321
[00:28:35.400] iteration 3160 : loss : 295.034027, loss_ce: 0.038040, loss_kd: 1472.718140
[00:28:40.952] iteration 3170 : loss : 287.318726, loss_ce: 0.023605, loss_kd: 1434.104614
[00:28:46.491] iteration 3180 : loss : 296.351990, loss_ce: 0.018783, loss_kd: 1479.311035
[00:28:52.051] iteration 3190 : loss : 323.653656, loss_ce: 0.027196, loss_kd: 1615.779297
[00:28:57.595] iteration 3200 : loss : 325.996765, loss_ce: 0.021231, loss_kd: 1627.522095
[00:29:03.159] iteration 3210 : loss : 358.412781, loss_ce: 0.033092, loss_kd: 1789.606079
[00:29:08.707] iteration 3220 : loss : 388.316864, loss_ce: 0.021158, loss_kd: 1939.111206
[00:29:14.268] iteration 3230 : loss : 367.724274, loss_ce: 0.030320, loss_kd: 1836.149658
[00:29:19.819] iteration 3240 : loss : 380.927551, loss_ce: 0.028288, loss_kd: 1902.170166
[00:29:25.381] iteration 3250 : loss : 307.929199, loss_ce: 0.024147, loss_kd: 1537.165405
[00:29:30.933] iteration 3260 : loss : 241.882217, loss_ce: 0.027345, loss_kd: 1206.897949
[00:29:36.503] iteration 3270 : loss : 254.789230, loss_ce: 0.012999, loss_kd: 1271.487305
[00:29:42.059] iteration 3280 : loss : 402.090546, loss_ce: 0.023202, loss_kd: 2007.975586
[00:29:47.629] iteration 3290 : loss : 229.072357, loss_ce: 0.027484, loss_kd: 1142.914673
[00:29:53.195] iteration 3300 : loss : 274.198822, loss_ce: 0.024087, loss_kd: 1368.519043
[00:29:58.769] iteration 3310 : loss : 334.108521, loss_ce: 0.031935, loss_kd: 1668.090698
[00:30:04.328] iteration 3320 : loss : 264.402100, loss_ce: 0.022744, loss_kd: 1319.543457
[00:30:09.905] iteration 3330 : loss : 304.354614, loss_ce: 0.018193, loss_kd: 1519.278442
[00:30:15.469] iteration 3340 : loss : 314.759033, loss_ce: 0.028446, loss_kd: 1571.349243
[00:30:21.048] iteration 3350 : loss : 388.011292, loss_ce: 0.027576, loss_kd: 1937.602783
[00:30:26.614] iteration 3360 : loss : 294.730225, loss_ce: 0.021810, loss_kd: 1471.258545
[00:30:32.197] iteration 3370 : loss : 267.510803, loss_ce: 0.018304, loss_kd: 1335.102295
[00:30:37.766] iteration 3380 : loss : 288.526886, loss_ce: 0.028973, loss_kd: 1440.152832
[00:30:43.350] iteration 3390 : loss : 354.393158, loss_ce: 0.018517, loss_kd: 1769.489258
[00:30:48.915] iteration 3400 : loss : 252.885376, loss_ce: 0.019561, loss_kd: 1261.982422
[00:30:54.505] iteration 3410 : loss : 319.593536, loss_ce: 0.020732, loss_kd: 1595.516357
[00:31:00.080] iteration 3420 : loss : 264.079834, loss_ce: 0.019779, loss_kd: 1317.940186
[00:31:05.659] iteration 3430 : loss : 277.691254, loss_ce: 0.029549, loss_kd: 1385.957520
[00:31:11.235] iteration 3440 : loss : 370.827515, loss_ce: 0.027009, loss_kd: 1851.631348
[00:31:16.821] iteration 3450 : loss : 292.441284, loss_ce: 0.021593, loss_kd: 1459.779053
[00:31:22.397] iteration 3460 : loss : 308.807953, loss_ce: 0.020473, loss_kd: 1541.556885
[00:31:27.976] iteration 3470 : loss : 326.863495, loss_ce: 0.024637, loss_kd: 1631.815552
[00:31:33.555] iteration 3480 : loss : 302.334869, loss_ce: 0.021602, loss_kd: 1509.220215
[00:31:39.149] iteration 3490 : loss : 374.998199, loss_ce: 0.028796, loss_kd: 1872.531250
[00:31:44.726] iteration 3500 : loss : 310.545563, loss_ce: 0.014780, loss_kd: 1550.248901
[00:31:50.320] iteration 3510 : loss : 288.464966, loss_ce: 0.034714, loss_kd: 1439.859253
[00:31:55.894] iteration 3520 : loss : 326.574951, loss_ce: 0.023089, loss_kd: 1630.454102
[00:32:01.486] iteration 3530 : loss : 309.995392, loss_ce: 0.020580, loss_kd: 1547.513184
[00:32:07.061] iteration 3540 : loss : 325.419922, loss_ce: 0.025704, loss_kd: 1624.623047
[00:32:12.648] iteration 3550 : loss : 222.923401, loss_ce: 0.025104, loss_kd: 1112.136719
[00:32:18.227] iteration 3560 : loss : 384.129486, loss_ce: 0.018799, loss_kd: 1918.194824
[00:32:23.814] iteration 3570 : loss : 281.325470, loss_ce: 0.021501, loss_kd: 1404.186035
[00:32:29.387] iteration 3580 : loss : 384.050049, loss_ce: 0.027283, loss_kd: 1917.769165
[00:32:34.971] iteration 3590 : loss : 238.532089, loss_ce: 0.027187, loss_kd: 1190.238037
[00:32:40.541] iteration 3600 : loss : 410.935425, loss_ce: 0.030571, loss_kd: 2052.201172
[00:32:46.138] iteration 3610 : loss : 271.448639, loss_ce: 0.018681, loss_kd: 1354.778320
[00:32:51.723] iteration 3620 : loss : 276.566315, loss_ce: 0.016471, loss_kd: 1380.395630
[00:32:57.309] iteration 3630 : loss : 425.490845, loss_ce: 0.022270, loss_kd: 2124.985107
[00:33:02.885] iteration 3640 : loss : 208.993866, loss_ce: 0.016279, loss_kd: 1042.555542
[00:33:08.471] iteration 3650 : loss : 211.363556, loss_ce: 0.031293, loss_kd: 1054.337036
[00:33:14.057] iteration 3660 : loss : 330.183563, loss_ce: 0.023985, loss_kd: 1648.454834
[00:33:19.650] iteration 3670 : loss : 321.324493, loss_ce: 0.014073, loss_kd: 1604.190186
[00:33:25.225] iteration 3680 : loss : 250.274719, loss_ce: 0.032482, loss_kd: 1248.926880
[00:33:30.807] iteration 3690 : loss : 342.398468, loss_ce: 0.025434, loss_kd: 1709.521240
[00:33:36.383] iteration 3700 : loss : 267.126373, loss_ce: 0.023802, loss_kd: 1333.169800
[00:33:41.976] iteration 3710 : loss : 336.767426, loss_ce: 0.028718, loss_kd: 1681.389893
[00:33:47.555] iteration 3720 : loss : 315.072144, loss_ce: 0.022104, loss_kd: 1572.906860
[00:33:53.141] iteration 3730 : loss : 235.245499, loss_ce: 0.020756, loss_kd: 1173.784180
[00:33:58.710] iteration 3740 : loss : 294.484589, loss_ce: 0.025305, loss_kd: 1469.950928
[00:34:04.298] iteration 3750 : loss : 229.916046, loss_ce: 0.018122, loss_kd: 1147.124756
[00:34:09.872] iteration 3760 : loss : 301.410126, loss_ce: 0.028810, loss_kd: 1504.621948
[00:34:15.452] iteration 3770 : loss : 349.795135, loss_ce: 0.029980, loss_kd: 1746.505859
[00:34:21.030] iteration 3780 : loss : 306.548218, loss_ce: 0.021801, loss_kd: 1530.295898
[00:34:26.614] iteration 3790 : loss : 383.624817, loss_ce: 0.023378, loss_kd: 1915.694580
[00:34:32.182] iteration 3800 : loss : 298.834534, loss_ce: 0.019789, loss_kd: 1491.700317
[00:34:37.768] iteration 3810 : loss : 276.114563, loss_ce: 0.022319, loss_kd: 1378.095459
[00:34:43.348] iteration 3820 : loss : 274.564362, loss_ce: 0.020455, loss_kd: 1370.369751
[00:34:48.936] iteration 3830 : loss : 209.639053, loss_ce: 0.022609, loss_kd: 1045.726196
[00:34:54.508] iteration 3840 : loss : 296.195160, loss_ce: 0.022597, loss_kd: 1478.520508
[00:35:00.090] iteration 3850 : loss : 236.240204, loss_ce: 0.026091, loss_kd: 1178.694336
[00:35:05.673] iteration 3860 : loss : 287.789337, loss_ce: 0.033771, loss_kd: 1436.493652
[00:35:11.259] iteration 3870 : loss : 423.164368, loss_ce: 0.020452, loss_kd: 2113.350098
[00:35:16.833] iteration 3880 : loss : 271.566620, loss_ce: 0.022895, loss_kd: 1355.391968
[00:35:22.409] iteration 3890 : loss : 284.192505, loss_ce: 0.027099, loss_kd: 1418.517822
[00:35:27.987] iteration 3900 : loss : 323.637177, loss_ce: 0.017851, loss_kd: 1615.777832
[00:35:33.576] iteration 3910 : loss : 241.518219, loss_ce: 0.018282, loss_kd: 1205.129028
[00:35:39.149] iteration 3920 : loss : 337.369904, loss_ce: 0.013517, loss_kd: 1684.431763
[00:35:44.737] iteration 3930 : loss : 246.209991, loss_ce: 0.018388, loss_kd: 1228.595337
[00:35:50.326] iteration 3940 : loss : 279.046326, loss_ce: 0.019804, loss_kd: 1392.752686
[00:35:55.910] iteration 3950 : loss : 337.002228, loss_ce: 0.029737, loss_kd: 1682.583374
[00:36:01.484] iteration 3960 : loss : 341.123322, loss_ce: 0.020317, loss_kd: 1703.182007
[00:36:07.065] iteration 3970 : loss : 323.004303, loss_ce: 0.030608, loss_kd: 1612.549194
[00:36:12.645] iteration 3980 : loss : 284.081604, loss_ce: 0.023040, loss_kd: 1417.908325
[00:36:18.229] iteration 3990 : loss : 290.627228, loss_ce: 0.023674, loss_kd: 1450.695923
[00:36:23.798] iteration 4000 : loss : 218.569458, loss_ce: 0.017201, loss_kd: 1090.379883
[00:36:29.382] iteration 4010 : loss : 278.395386, loss_ce: 0.029812, loss_kd: 1389.496704
[00:36:34.959] iteration 4020 : loss : 250.683929, loss_ce: 0.020168, loss_kd: 1250.961548
[00:36:40.545] iteration 4030 : loss : 281.028168, loss_ce: 0.021819, loss_kd: 1402.678223
[00:36:46.116] iteration 4040 : loss : 274.353912, loss_ce: 0.024695, loss_kd: 1369.297852
[00:36:51.705] iteration 4050 : loss : 367.232361, loss_ce: 0.016249, loss_kd: 1833.715210
[00:36:57.287] iteration 4060 : loss : 299.906128, loss_ce: 0.028760, loss_kd: 1497.051636
[00:37:02.876] iteration 4070 : loss : 404.941803, loss_ce: 0.025506, loss_kd: 2022.262207
[00:37:08.448] iteration 4080 : loss : 282.349304, loss_ce: 0.029383, loss_kd: 1409.284424
[00:37:14.034] iteration 4090 : loss : 208.002777, loss_ce: 0.022643, loss_kd: 1037.504883
[00:37:19.614] iteration 4100 : loss : 281.034515, loss_ce: 0.023869, loss_kd: 1402.737793
[00:37:25.209] iteration 4110 : loss : 231.712845, loss_ce: 0.017748, loss_kd: 1156.119629
[00:37:30.782] iteration 4120 : loss : 214.982391, loss_ce: 0.025945, loss_kd: 1072.457397
[00:37:36.364] iteration 4130 : loss : 242.471069, loss_ce: 0.024678, loss_kd: 1209.872437
[00:37:41.944] iteration 4140 : loss : 368.757019, loss_ce: 0.025953, loss_kd: 1841.343750
[00:37:47.533] iteration 4150 : loss : 327.292603, loss_ce: 0.025444, loss_kd: 1634.026367
[00:37:53.116] iteration 4160 : loss : 228.561218, loss_ce: 0.043500, loss_kd: 1140.321289
[00:38:10.514] iteration 4170 : loss : 245.930771, loss_ce: 0.031068, loss_kd: 1227.203247
[00:38:16.040] iteration 4180 : loss : 357.764374, loss_ce: 0.020407, loss_kd: 1786.392456
[00:38:21.587] iteration 4190 : loss : 334.832031, loss_ce: 0.010734, loss_kd: 1671.723389
[00:38:27.129] iteration 4200 : loss : 241.226517, loss_ce: 0.034248, loss_kd: 1203.635986
[00:38:32.683] iteration 4210 : loss : 259.386017, loss_ce: 0.018766, loss_kd: 1294.509521
[00:38:38.230] iteration 4220 : loss : 359.433868, loss_ce: 0.022439, loss_kd: 1794.708130
[00:38:43.788] iteration 4230 : loss : 225.995483, loss_ce: 0.027463, loss_kd: 1127.505737
[00:38:49.332] iteration 4240 : loss : 316.643158, loss_ce: 0.036638, loss_kd: 1580.773315
[00:38:54.895] iteration 4250 : loss : 254.513885, loss_ce: 0.022540, loss_kd: 1270.098389
[00:39:00.452] iteration 4260 : loss : 296.222168, loss_ce: 0.018900, loss_kd: 1478.675903
[00:39:06.016] iteration 4270 : loss : 298.854675, loss_ce: 0.024220, loss_kd: 1491.768433
[00:39:11.576] iteration 4280 : loss : 251.865555, loss_ce: 0.031502, loss_kd: 1256.903564
[00:39:17.140] iteration 4290 : loss : 249.640564, loss_ce: 0.024433, loss_kd: 1245.778931
[00:39:22.700] iteration 4300 : loss : 241.679443, loss_ce: 0.020216, loss_kd: 1205.968262
[00:39:28.278] iteration 4310 : loss : 249.542969, loss_ce: 0.021354, loss_kd: 1245.219849
[00:39:33.845] iteration 4320 : loss : 299.484283, loss_ce: 0.027582, loss_kd: 1495.000854
[00:39:39.419] iteration 4330 : loss : 354.097992, loss_ce: 0.047334, loss_kd: 1768.041260
[00:39:44.983] iteration 4340 : loss : 285.450989, loss_ce: 0.020436, loss_kd: 1424.827393
[00:39:50.550] iteration 4350 : loss : 365.450012, loss_ce: 0.025906, loss_kd: 1824.764526
[00:39:56.115] iteration 4360 : loss : 258.607697, loss_ce: 0.020240, loss_kd: 1290.622192
[00:40:01.690] iteration 4370 : loss : 360.562683, loss_ce: 0.023618, loss_kd: 1800.355835
[00:40:07.260] iteration 4380 : loss : 268.621094, loss_ce: 0.026681, loss_kd: 1340.653198
[00:40:12.840] iteration 4390 : loss : 254.258957, loss_ce: 0.020533, loss_kd: 1268.822876
[00:40:18.407] iteration 4400 : loss : 260.932312, loss_ce: 0.016073, loss_kd: 1302.238037
[00:40:23.989] iteration 4410 : loss : 247.073380, loss_ce: 0.020462, loss_kd: 1232.910889
[00:40:29.552] iteration 4420 : loss : 272.199432, loss_ce: 0.016060, loss_kd: 1358.568237
[00:40:35.130] iteration 4430 : loss : 251.774811, loss_ce: 0.012585, loss_kd: 1256.394043
[00:40:40.698] iteration 4440 : loss : 263.181366, loss_ce: 0.020045, loss_kd: 1313.457764
[00:40:46.275] iteration 4450 : loss : 277.962311, loss_ce: 0.020216, loss_kd: 1387.375854
[00:40:51.851] iteration 4460 : loss : 225.452560, loss_ce: 0.029207, loss_kd: 1124.807251
[00:40:57.440] iteration 4470 : loss : 255.925079, loss_ce: 0.021477, loss_kd: 1277.212402
[00:41:03.011] iteration 4480 : loss : 215.860504, loss_ce: 0.029865, loss_kd: 1076.841797
[00:41:08.589] iteration 4490 : loss : 254.909821, loss_ce: 0.022559, loss_kd: 1272.082886
[00:41:14.163] iteration 4500 : loss : 301.987396, loss_ce: 0.022823, loss_kd: 1507.492554
[00:41:19.747] iteration 4510 : loss : 315.995148, loss_ce: 0.023043, loss_kd: 1577.549072
[00:41:25.319] iteration 4520 : loss : 378.488861, loss_ce: 0.032395, loss_kd: 1889.959717
[00:41:30.908] iteration 4530 : loss : 270.159821, loss_ce: 0.033332, loss_kd: 1348.336670
[00:41:36.474] iteration 4540 : loss : 273.421173, loss_ce: 0.015240, loss_kd: 1364.674438
[00:41:42.060] iteration 4550 : loss : 254.698196, loss_ce: 0.024167, loss_kd: 1271.036987
[00:41:47.632] iteration 4560 : loss : 299.862915, loss_ce: 0.039176, loss_kd: 1496.856812
[00:41:53.232] iteration 4570 : loss : 382.816223, loss_ce: 0.023150, loss_kd: 1911.625732
[00:41:58.808] iteration 4580 : loss : 288.110809, loss_ce: 0.015610, loss_kd: 1438.105835
[00:42:04.393] iteration 4590 : loss : 259.201202, loss_ce: 0.017403, loss_kd: 1293.592163
[00:42:09.968] iteration 4600 : loss : 357.382050, loss_ce: 0.017884, loss_kd: 1784.393066
[00:42:15.560] iteration 4610 : loss : 314.749878, loss_ce: 0.022484, loss_kd: 1571.294922
[00:42:21.137] iteration 4620 : loss : 265.298737, loss_ce: 0.038233, loss_kd: 1324.002075
[00:42:26.720] iteration 4630 : loss : 255.023651, loss_ce: 0.031364, loss_kd: 1272.648315
[00:42:32.292] iteration 4640 : loss : 294.159668, loss_ce: 0.020975, loss_kd: 1468.340088
[00:42:37.882] iteration 4650 : loss : 246.801422, loss_ce: 0.020829, loss_kd: 1231.554565
[00:42:43.463] iteration 4660 : loss : 276.463745, loss_ce: 0.019527, loss_kd: 1379.841431
[00:42:49.055] iteration 4670 : loss : 257.088379, loss_ce: 0.028684, loss_kd: 1282.977783
[00:42:54.625] iteration 4680 : loss : 291.629944, loss_ce: 0.034134, loss_kd: 1455.693970
[00:43:00.207] iteration 4690 : loss : 320.422272, loss_ce: 0.024060, loss_kd: 1599.672119
[00:43:05.783] iteration 4700 : loss : 186.296219, loss_ce: 0.013537, loss_kd: 929.041016
[00:43:11.374] iteration 4710 : loss : 203.973099, loss_ce: 0.022168, loss_kd: 1017.430481
[00:43:16.948] iteration 4720 : loss : 304.980347, loss_ce: 0.014251, loss_kd: 1522.439697
[00:43:22.533] iteration 4730 : loss : 260.867065, loss_ce: 0.031196, loss_kd: 1301.857910
[00:43:28.112] iteration 4740 : loss : 227.574478, loss_ce: 0.018174, loss_kd: 1135.426514
[00:43:33.708] iteration 4750 : loss : 310.993073, loss_ce: 0.020841, loss_kd: 1552.530518
[00:43:39.287] iteration 4760 : loss : 298.939087, loss_ce: 0.030010, loss_kd: 1492.258911
[00:43:44.883] iteration 4770 : loss : 262.819916, loss_ce: 0.021016, loss_kd: 1311.630127
[00:43:50.457] iteration 4780 : loss : 294.367249, loss_ce: 0.019547, loss_kd: 1469.399536
[00:43:56.045] iteration 4790 : loss : 326.855469, loss_ce: 0.029016, loss_kd: 1631.790771
[00:44:01.623] iteration 4800 : loss : 239.220291, loss_ce: 0.039910, loss_kd: 1193.621094
[00:44:07.207] iteration 4810 : loss : 289.825470, loss_ce: 0.022518, loss_kd: 1446.662354
[00:44:12.785] iteration 4820 : loss : 255.004883, loss_ce: 0.017343, loss_kd: 1272.595825
[00:44:18.372] iteration 4830 : loss : 220.178467, loss_ce: 0.019261, loss_kd: 1098.466064
[00:44:23.944] iteration 4840 : loss : 224.494110, loss_ce: 0.019423, loss_kd: 1120.036377
[00:44:29.533] iteration 4850 : loss : 302.177826, loss_ce: 0.022727, loss_kd: 1508.439697
[00:44:35.111] iteration 4860 : loss : 274.545715, loss_ce: 0.029152, loss_kd: 1370.260742
[00:44:40.697] iteration 4870 : loss : 253.376450, loss_ce: 0.008212, loss_kd: 1264.439575
[00:44:46.282] iteration 4880 : loss : 265.542023, loss_ce: 0.023670, loss_kd: 1325.249634
[00:44:51.873] iteration 4890 : loss : 280.765045, loss_ce: 0.018022, loss_kd: 1401.394897
[00:44:57.455] iteration 4900 : loss : 237.412277, loss_ce: 0.023196, loss_kd: 1184.590942
[00:45:03.038] iteration 4910 : loss : 257.235626, loss_ce: 0.025203, loss_kd: 1283.791870
[00:45:08.617] iteration 4920 : loss : 234.611038, loss_ce: 0.042674, loss_kd: 1170.444092
[00:45:14.208] iteration 4930 : loss : 226.797089, loss_ce: 0.023983, loss_kd: 1131.430664
[00:45:19.786] iteration 4940 : loss : 320.324554, loss_ce: 0.019531, loss_kd: 1599.201904
[00:45:25.373] iteration 4950 : loss : 239.413651, loss_ce: 0.027741, loss_kd: 1194.627441
[00:45:30.944] iteration 4960 : loss : 307.709351, loss_ce: 0.016856, loss_kd: 1536.080933
[00:45:36.536] iteration 4970 : loss : 282.825745, loss_ce: 0.020181, loss_kd: 1411.677490
[00:45:42.118] iteration 4980 : loss : 245.325211, loss_ce: 0.020489, loss_kd: 1224.168091
[00:45:47.704] iteration 4990 : loss : 256.124939, loss_ce: 0.020666, loss_kd: 1278.189209
[00:45:53.277] iteration 5000 : loss : 217.689590, loss_ce: 0.019342, loss_kd: 1085.991577
[00:45:58.868] iteration 5010 : loss : 241.058533, loss_ce: 0.028391, loss_kd: 1202.839478
[00:46:04.449] iteration 5020 : loss : 237.687149, loss_ce: 0.017285, loss_kd: 1186.027344
[00:46:10.033] iteration 5030 : loss : 204.505630, loss_ce: 0.029153, loss_kd: 1020.050842
[00:46:15.597] iteration 5040 : loss : 285.370758, loss_ce: 0.020134, loss_kd: 1424.415649
[00:46:21.183] iteration 5050 : loss : 226.564941, loss_ce: 0.032465, loss_kd: 1130.377441
[00:46:26.766] iteration 5060 : loss : 306.960480, loss_ce: 0.031461, loss_kd: 1532.349243
[00:46:32.350] iteration 5070 : loss : 294.713623, loss_ce: 0.016068, loss_kd: 1471.143066
[00:46:37.920] iteration 5080 : loss : 257.895813, loss_ce: 0.019658, loss_kd: 1287.019897
[00:46:43.505] iteration 5090 : loss : 248.412979, loss_ce: 0.011924, loss_kd: 1239.663818
[00:46:49.088] iteration 5100 : loss : 175.950623, loss_ce: 0.023268, loss_kd: 877.298706
[00:46:54.680] iteration 5110 : loss : 251.057373, loss_ce: 0.011905, loss_kd: 1252.829834
[00:47:00.257] iteration 5120 : loss : 241.834030, loss_ce: 0.032963, loss_kd: 1206.714600
[00:47:05.845] iteration 5130 : loss : 261.143890, loss_ce: 0.014862, loss_kd: 1303.291870
[00:47:11.418] iteration 5140 : loss : 241.043365, loss_ce: 0.022744, loss_kd: 1202.784058
[00:47:16.994] iteration 5150 : loss : 281.096313, loss_ce: 0.013943, loss_kd: 1403.000366
[00:47:22.563] iteration 5160 : loss : 296.573792, loss_ce: 0.016080, loss_kd: 1480.432739
[00:47:28.150] iteration 5170 : loss : 260.735596, loss_ce: 0.023679, loss_kd: 1301.083984
[00:47:33.745] iteration 5180 : loss : 198.272781, loss_ce: 0.036304, loss_kd: 988.902588
[00:47:39.330] iteration 5190 : loss : 421.213379, loss_ce: 0.015293, loss_kd: 2103.647461
[00:47:44.902] iteration 5200 : loss : 261.857452, loss_ce: 0.022095, loss_kd: 1306.837891
[00:47:50.194] iteration 5210 : loss : 283.676361, loss_ce: 0.032625, loss_kd: 1415.878784
[00:47:50.963] save model to ./debug_simple_tpgm\continual_surgical_tpgm_epoch_4.pth
[00:47:50.963] Applying final TPGM projection
[00:47:51.153] save final model to ./debug_simple_tpgm\continual_surgical_tpgm_final.pth
