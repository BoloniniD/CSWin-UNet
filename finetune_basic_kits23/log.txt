[16:16:46.990] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./finetune_basic_kits23', max_iterations=10000, max_epochs=30, batch_size=8, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.05, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[16:17:16.444] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./finetune_basic_kits23', max_iterations=10000, max_epochs=30, batch_size=8, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.05, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[16:17:16.452] Using 4761/95221 samples (5.0%) for finetuning
[16:17:16.457] 596 iterations per epoch. 17880 max iterations 
[16:17:28.181] iteration 10 : loss : 0.454631, loss_ce: 0.030800
[16:17:29.890] iteration 20 : loss : 0.450594, loss_ce: 0.043470
[16:17:31.650] iteration 30 : loss : 0.445908, loss_ce: 0.066281
[16:17:33.387] iteration 40 : loss : 0.441934, loss_ce: 0.041138
[16:21:40.197] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./finetune_basic_kits23', max_iterations=10000, max_epochs=30, batch_size=8, n_gpu=2, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.05, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[16:21:40.204] Using 4761/95221 samples (5.0%) for finetuning
[16:21:40.209] 298 iterations per epoch. 8940 max iterations 
[16:21:53.057] iteration 10 : loss : 0.460286, loss_ce: 0.036247
[16:21:55.586] iteration 20 : loss : 0.434879, loss_ce: 0.039642
[16:21:58.010] iteration 30 : loss : 0.431880, loss_ce: 0.045215
[16:22:00.438] iteration 40 : loss : 0.395068, loss_ce: 0.060426
[16:22:25.089] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./finetune_basic_kits23', max_iterations=10000, max_epochs=30, batch_size=48, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.05, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[16:22:25.099] Using 4761/95221 samples (5.0%) for finetuning
[16:22:25.104] 100 iterations per epoch. 3000 max iterations 
[16:22:42.097] iteration 10 : loss : 0.463923, loss_ce: 0.063346
[16:22:48.919] iteration 20 : loss : 0.443282, loss_ce: 0.040783
[16:22:55.732] iteration 30 : loss : 0.439645, loss_ce: 0.062922
[16:23:02.679] iteration 40 : loss : 0.419810, loss_ce: 0.046976
[16:23:09.660] iteration 50 : loss : 0.410854, loss_ce: 0.048756
[16:23:16.505] iteration 60 : loss : 0.395558, loss_ce: 0.048571
[16:23:22.979] iteration 70 : loss : 0.376279, loss_ce: 0.058078
[16:23:29.847] iteration 80 : loss : 0.383405, loss_ce: 0.040960
[16:23:36.504] iteration 90 : loss : 0.395476, loss_ce: 0.055606
[16:23:42.382] iteration 100 : loss : 0.400505, loss_ce: 0.043539
[16:24:00.491] iteration 110 : loss : 0.299471, loss_ce: 0.022115
[16:24:07.413] iteration 120 : loss : 0.337374, loss_ce: 0.029831
[16:24:14.296] iteration 130 : loss : 0.309897, loss_ce: 0.037572
[16:24:21.058] iteration 140 : loss : 0.295560, loss_ce: 0.017083
[16:24:27.875] iteration 150 : loss : 0.276345, loss_ce: 0.023717
[16:24:34.344] iteration 160 : loss : 0.324490, loss_ce: 0.023059
[16:24:40.729] iteration 170 : loss : 0.309603, loss_ce: 0.008992
[16:24:47.098] iteration 180 : loss : 0.242783, loss_ce: 0.015069
[16:24:53.518] iteration 190 : loss : 0.329188, loss_ce: 0.025802
[16:24:59.480] iteration 200 : loss : 0.360898, loss_ce: 0.010231
[16:25:16.335] iteration 210 : loss : 0.332247, loss_ce: 0.012480
[16:25:22.723] iteration 220 : loss : 0.276483, loss_ce: 0.016161
[16:25:29.158] iteration 230 : loss : 0.225556, loss_ce: 0.016794
[16:25:35.530] iteration 240 : loss : 0.322686, loss_ce: 0.021387
[16:25:41.945] iteration 250 : loss : 0.187177, loss_ce: 0.010599
[16:25:48.340] iteration 260 : loss : 0.296153, loss_ce: 0.033741
[16:25:54.720] iteration 270 : loss : 0.280316, loss_ce: 0.028516
[16:26:01.090] iteration 280 : loss : 0.252566, loss_ce: 0.010315
[16:26:07.465] iteration 290 : loss : 0.232181, loss_ce: 0.032246
[16:26:13.326] iteration 300 : loss : 0.348223, loss_ce: 0.014116
[16:26:30.233] iteration 310 : loss : 0.197612, loss_ce: 0.016281
[16:26:36.692] iteration 320 : loss : 0.207004, loss_ce: 0.014545
[16:26:43.184] iteration 330 : loss : 0.322629, loss_ce: 0.008688
[16:26:49.583] iteration 340 : loss : 0.330581, loss_ce: 0.012184
[16:26:56.097] iteration 350 : loss : 0.281094, loss_ce: 0.025917
[16:27:02.665] iteration 360 : loss : 0.256780, loss_ce: 0.020513
[16:27:09.215] iteration 370 : loss : 0.148647, loss_ce: 0.009114
[16:27:15.653] iteration 380 : loss : 0.186209, loss_ce: 0.005418
[16:27:22.034] iteration 390 : loss : 0.223949, loss_ce: 0.021270
[16:27:27.975] iteration 400 : loss : 0.290313, loss_ce: 0.020366
[16:27:45.151] iteration 410 : loss : 0.275512, loss_ce: 0.044177
[16:27:51.572] iteration 420 : loss : 0.205119, loss_ce: 0.017194
[16:27:58.018] iteration 430 : loss : 0.138309, loss_ce: 0.009167
[16:28:04.386] iteration 440 : loss : 0.119365, loss_ce: 0.009739
[16:28:10.769] iteration 450 : loss : 0.202445, loss_ce: 0.011566
[16:28:17.139] iteration 460 : loss : 0.248859, loss_ce: 0.015454
[16:28:23.523] iteration 470 : loss : 0.296153, loss_ce: 0.008077
[16:28:29.895] iteration 480 : loss : 0.247607, loss_ce: 0.011828
[16:28:36.273] iteration 490 : loss : 0.223170, loss_ce: 0.014359
[16:28:42.124] iteration 500 : loss : 0.334509, loss_ce: 0.006759
[16:28:58.866] iteration 510 : loss : 0.219370, loss_ce: 0.015144
[16:29:05.239] iteration 520 : loss : 0.196112, loss_ce: 0.020003
[16:29:11.625] iteration 530 : loss : 0.166764, loss_ce: 0.014958
[16:29:17.999] iteration 540 : loss : 0.176011, loss_ce: 0.013022
[16:29:24.372] iteration 550 : loss : 0.265688, loss_ce: 0.021009
[16:29:30.792] iteration 560 : loss : 0.206071, loss_ce: 0.013301
[16:29:37.217] iteration 570 : loss : 0.295509, loss_ce: 0.007269
[16:29:43.643] iteration 580 : loss : 0.128833, loss_ce: 0.003599
[16:29:50.075] iteration 590 : loss : 0.326914, loss_ce: 0.016566
[16:29:56.083] iteration 600 : loss : 0.116665, loss_ce: 0.012094
[16:30:12.837] iteration 610 : loss : 0.179703, loss_ce: 0.007442
[16:30:19.210] iteration 620 : loss : 0.142351, loss_ce: 0.005247
[16:30:25.597] iteration 630 : loss : 0.223707, loss_ce: 0.008191
[16:30:31.977] iteration 640 : loss : 0.177398, loss_ce: 0.015298
[16:30:38.364] iteration 650 : loss : 0.099255, loss_ce: 0.005167
[16:30:44.749] iteration 660 : loss : 0.108665, loss_ce: 0.011081
[16:30:51.148] iteration 670 : loss : 0.238352, loss_ce: 0.015845
[16:30:57.534] iteration 680 : loss : 0.257803, loss_ce: 0.010464
[16:31:03.935] iteration 690 : loss : 0.127366, loss_ce: 0.013486
[16:31:09.806] iteration 700 : loss : 0.339480, loss_ce: 0.023981
[16:31:26.620] iteration 710 : loss : 0.179491, loss_ce: 0.010654
[16:31:32.999] iteration 720 : loss : 0.133844, loss_ce: 0.015039
[16:31:39.396] iteration 730 : loss : 0.234593, loss_ce: 0.007168
[16:31:45.772] iteration 740 : loss : 0.268722, loss_ce: 0.009948
[16:31:52.163] iteration 750 : loss : 0.181123, loss_ce: 0.005395
[16:31:58.544] iteration 760 : loss : 0.283139, loss_ce: 0.011759
[16:32:04.932] iteration 770 : loss : 0.234677, loss_ce: 0.003572
[16:32:11.322] iteration 780 : loss : 0.174762, loss_ce: 0.009537
[16:32:17.767] iteration 790 : loss : 0.239190, loss_ce: 0.009583
[16:32:23.972] iteration 800 : loss : 0.340760, loss_ce: 0.007877
[16:32:40.704] iteration 810 : loss : 0.187714, loss_ce: 0.017583
[16:32:47.226] iteration 820 : loss : 0.209716, loss_ce: 0.005039
[16:32:53.623] iteration 830 : loss : 0.185022, loss_ce: 0.006729
[16:33:00.011] iteration 840 : loss : 0.235376, loss_ce: 0.016127
[16:33:06.485] iteration 850 : loss : 0.116271, loss_ce: 0.009814
[16:33:12.917] iteration 860 : loss : 0.262949, loss_ce: 0.013337
[16:33:19.299] iteration 870 : loss : 0.192650, loss_ce: 0.019397
[16:33:25.675] iteration 880 : loss : 0.147710, loss_ce: 0.007266
[16:33:32.044] iteration 890 : loss : 0.187535, loss_ce: 0.037357
[16:33:37.906] iteration 900 : loss : 0.202246, loss_ce: 0.019594
[16:33:55.009] iteration 910 : loss : 0.101834, loss_ce: 0.006013
[16:34:01.418] iteration 920 : loss : 0.208958, loss_ce: 0.016410
[16:34:07.797] iteration 930 : loss : 0.149255, loss_ce: 0.016989
[16:34:14.175] iteration 940 : loss : 0.103412, loss_ce: 0.011886
[16:34:20.562] iteration 950 : loss : 0.186125, loss_ce: 0.007281
[16:34:26.928] iteration 960 : loss : 0.206910, loss_ce: 0.011946
[16:34:33.303] iteration 970 : loss : 0.195756, loss_ce: 0.006487
[16:34:39.669] iteration 980 : loss : 0.159193, loss_ce: 0.007507
[16:34:46.059] iteration 990 : loss : 0.141120, loss_ce: 0.008492
[16:34:51.925] iteration 1000 : loss : 0.207341, loss_ce: 0.008969
[16:34:52.613] save model to ./finetune_basic_kits23\finetuned_epoch_9.pth
[16:35:09.308] iteration 1010 : loss : 0.127881, loss_ce: 0.008888
[16:35:15.697] iteration 1020 : loss : 0.157314, loss_ce: 0.008774
[16:35:22.104] iteration 1030 : loss : 0.225670, loss_ce: 0.019603
[16:35:28.505] iteration 1040 : loss : 0.187747, loss_ce: 0.020493
[16:35:34.966] iteration 1050 : loss : 0.138557, loss_ce: 0.006660
[16:35:41.430] iteration 1060 : loss : 0.162275, loss_ce: 0.008042
[16:35:47.896] iteration 1070 : loss : 0.165753, loss_ce: 0.005620
[16:35:54.345] iteration 1080 : loss : 0.192457, loss_ce: 0.008552
[16:36:00.812] iteration 1090 : loss : 0.183718, loss_ce: 0.007005
[16:36:06.669] iteration 1100 : loss : 0.320553, loss_ce: 0.007637
[16:36:23.529] iteration 1110 : loss : 0.280670, loss_ce: 0.004329
[16:36:29.958] iteration 1120 : loss : 0.113629, loss_ce: 0.006419
[16:36:36.368] iteration 1130 : loss : 0.198630, loss_ce: 0.004611
[16:36:42.736] iteration 1140 : loss : 0.087221, loss_ce: 0.010350
[16:36:49.114] iteration 1150 : loss : 0.083083, loss_ce: 0.007395
[16:36:55.471] iteration 1160 : loss : 0.300506, loss_ce: 0.003888
[16:37:01.864] iteration 1170 : loss : 0.159666, loss_ce: 0.005926
[16:37:08.534] iteration 1180 : loss : 0.118599, loss_ce: 0.005729
[16:37:15.004] iteration 1190 : loss : 0.122331, loss_ce: 0.009953
[16:37:21.152] iteration 1200 : loss : 0.313918, loss_ce: 0.007344
[16:37:38.288] iteration 1210 : loss : 0.217954, loss_ce: 0.007434
[16:37:44.592] iteration 1220 : loss : 0.118273, loss_ce: 0.005660
[16:37:50.937] iteration 1230 : loss : 0.198941, loss_ce: 0.008671
[16:37:57.277] iteration 1240 : loss : 0.207966, loss_ce: 0.005048
[16:38:03.625] iteration 1250 : loss : 0.201946, loss_ce: 0.005174
[16:38:09.971] iteration 1260 : loss : 0.260162, loss_ce: 0.002527
[16:38:16.323] iteration 1270 : loss : 0.234355, loss_ce: 0.008077
[16:38:22.656] iteration 1280 : loss : 0.158619, loss_ce: 0.005163
[16:38:28.980] iteration 1290 : loss : 0.068357, loss_ce: 0.006755
[16:38:34.787] iteration 1300 : loss : 0.354279, loss_ce: 0.005467
[16:38:51.442] iteration 1310 : loss : 0.154555, loss_ce: 0.006647
[16:38:57.751] iteration 1320 : loss : 0.134741, loss_ce: 0.003736
[16:39:04.072] iteration 1330 : loss : 0.233678, loss_ce: 0.007234
[16:39:10.379] iteration 1340 : loss : 0.177886, loss_ce: 0.002112
[16:39:16.696] iteration 1350 : loss : 0.090407, loss_ce: 0.007498
[16:39:23.008] iteration 1360 : loss : 0.221254, loss_ce: 0.010768
[16:39:29.331] iteration 1370 : loss : 0.206584, loss_ce: 0.007329
[16:39:35.640] iteration 1380 : loss : 0.148446, loss_ce: 0.011810
[16:39:41.960] iteration 1390 : loss : 0.082986, loss_ce: 0.009013
[16:39:47.768] iteration 1400 : loss : 0.179161, loss_ce: 0.005578
[16:40:04.387] iteration 1410 : loss : 0.110484, loss_ce: 0.007488
[16:40:10.693] iteration 1420 : loss : 0.163663, loss_ce: 0.011558
[16:40:17.008] iteration 1430 : loss : 0.070178, loss_ce: 0.010946
[16:40:23.317] iteration 1440 : loss : 0.165416, loss_ce: 0.008426
[16:40:29.637] iteration 1450 : loss : 0.210797, loss_ce: 0.005797
[16:40:35.952] iteration 1460 : loss : 0.138539, loss_ce: 0.007690
[16:40:42.266] iteration 1470 : loss : 0.295044, loss_ce: 0.004531
[16:40:48.574] iteration 1480 : loss : 0.103622, loss_ce: 0.006270
[16:40:54.897] iteration 1490 : loss : 0.227793, loss_ce: 0.013600
[16:41:00.707] iteration 1500 : loss : 0.334690, loss_ce: 0.011456
[16:41:17.369] iteration 1510 : loss : 0.085784, loss_ce: 0.004159
[16:41:23.673] iteration 1520 : loss : 0.197351, loss_ce: 0.008787
[16:41:29.988] iteration 1530 : loss : 0.105093, loss_ce: 0.004593
[16:41:36.289] iteration 1540 : loss : 0.171259, loss_ce: 0.002589
[16:41:42.613] iteration 1550 : loss : 0.080553, loss_ce: 0.006809
[16:41:48.926] iteration 1560 : loss : 0.086706, loss_ce: 0.003751
[16:41:55.245] iteration 1570 : loss : 0.295891, loss_ce: 0.008372
[16:42:01.552] iteration 1580 : loss : 0.198443, loss_ce: 0.006081
[16:42:07.876] iteration 1590 : loss : 0.064251, loss_ce: 0.006157
[16:42:13.682] iteration 1600 : loss : 0.305185, loss_ce: 0.004398
[16:42:30.346] iteration 1610 : loss : 0.282127, loss_ce: 0.010023
[16:42:36.655] iteration 1620 : loss : 0.188916, loss_ce: 0.007649
[16:42:42.971] iteration 1630 : loss : 0.163014, loss_ce: 0.007964
[16:42:49.276] iteration 1640 : loss : 0.174250, loss_ce: 0.005358
[16:42:55.597] iteration 1650 : loss : 0.079644, loss_ce: 0.006393
[16:43:01.910] iteration 1660 : loss : 0.132760, loss_ce: 0.011597
[16:43:08.232] iteration 1670 : loss : 0.214144, loss_ce: 0.004165
[16:43:14.538] iteration 1680 : loss : 0.184494, loss_ce: 0.008665
[16:43:20.857] iteration 1690 : loss : 0.214024, loss_ce: 0.012665
[16:43:26.662] iteration 1700 : loss : 0.267734, loss_ce: 0.009704
[16:43:43.346] iteration 1710 : loss : 0.112804, loss_ce: 0.004967
[16:43:49.650] iteration 1720 : loss : 0.097921, loss_ce: 0.007307
[16:43:55.966] iteration 1730 : loss : 0.125012, loss_ce: 0.005393
[16:44:02.269] iteration 1740 : loss : 0.207907, loss_ce: 0.009011
[16:44:08.590] iteration 1750 : loss : 0.153246, loss_ce: 0.006694
[16:44:14.904] iteration 1760 : loss : 0.098046, loss_ce: 0.005652
[16:44:21.222] iteration 1770 : loss : 0.070731, loss_ce: 0.006311
[16:44:27.532] iteration 1780 : loss : 0.039577, loss_ce: 0.003912
[16:44:33.859] iteration 1790 : loss : 0.163341, loss_ce: 0.005372
[16:44:39.668] iteration 1800 : loss : 0.043105, loss_ce: 0.011018
[16:44:56.317] iteration 1810 : loss : 0.121368, loss_ce: 0.007119
[16:45:02.627] iteration 1820 : loss : 0.076406, loss_ce: 0.007216
[16:45:08.947] iteration 1830 : loss : 0.183456, loss_ce: 0.004998
[16:45:15.254] iteration 1840 : loss : 0.195446, loss_ce: 0.004792
[16:45:21.575] iteration 1850 : loss : 0.189070, loss_ce: 0.010466
[16:45:27.887] iteration 1860 : loss : 0.202145, loss_ce: 0.004290
[16:45:34.210] iteration 1870 : loss : 0.081644, loss_ce: 0.009864
[16:45:40.516] iteration 1880 : loss : 0.092995, loss_ce: 0.006170
[16:45:46.840] iteration 1890 : loss : 0.066914, loss_ce: 0.007388
[16:45:52.648] iteration 1900 : loss : 0.445950, loss_ce: 0.002434
[16:46:09.309] iteration 1910 : loss : 0.132165, loss_ce: 0.005052
[16:46:15.615] iteration 1920 : loss : 0.177193, loss_ce: 0.008828
[16:46:21.932] iteration 1930 : loss : 0.186036, loss_ce: 0.005656
[16:46:28.239] iteration 1940 : loss : 0.070175, loss_ce: 0.006627
[16:46:34.557] iteration 1950 : loss : 0.087055, loss_ce: 0.005108
[16:46:40.868] iteration 1960 : loss : 0.179942, loss_ce: 0.005606
[16:46:47.191] iteration 1970 : loss : 0.187307, loss_ce: 0.005511
[16:46:53.500] iteration 1980 : loss : 0.096316, loss_ce: 0.005587
[16:46:59.819] iteration 1990 : loss : 0.195210, loss_ce: 0.003601
[16:47:05.629] iteration 2000 : loss : 0.090280, loss_ce: 0.007015
[16:47:06.295] save model to ./finetune_basic_kits23\finetuned_epoch_19.pth
[16:47:22.260] iteration 2010 : loss : 0.097622, loss_ce: 0.005875
[16:47:28.561] iteration 2020 : loss : 0.103768, loss_ce: 0.004284
[16:47:34.872] iteration 2030 : loss : 0.063723, loss_ce: 0.004484
[16:47:41.183] iteration 2040 : loss : 0.211801, loss_ce: 0.007113
[16:47:47.507] iteration 2050 : loss : 0.135448, loss_ce: 0.005187
[16:47:53.818] iteration 2060 : loss : 0.302780, loss_ce: 0.003069
[16:48:00.137] iteration 2070 : loss : 0.089108, loss_ce: 0.008092
[16:48:06.453] iteration 2080 : loss : 0.067689, loss_ce: 0.004448
[16:48:12.777] iteration 2090 : loss : 0.086644, loss_ce: 0.005221
[16:48:18.578] iteration 2100 : loss : 0.175810, loss_ce: 0.006562
[16:48:35.232] iteration 2110 : loss : 0.117418, loss_ce: 0.003914
[16:48:41.535] iteration 2120 : loss : 0.131660, loss_ce: 0.008468
[16:48:47.849] iteration 2130 : loss : 0.152562, loss_ce: 0.005002
[16:48:54.157] iteration 2140 : loss : 0.174948, loss_ce: 0.002796
[16:49:00.479] iteration 2150 : loss : 0.177780, loss_ce: 0.004337
[16:49:06.795] iteration 2160 : loss : 0.088914, loss_ce: 0.002684
[16:49:13.115] iteration 2170 : loss : 0.163866, loss_ce: 0.004966
[16:49:19.423] iteration 2180 : loss : 0.075463, loss_ce: 0.005280
[16:49:25.747] iteration 2190 : loss : 0.215858, loss_ce: 0.004054
[16:49:31.556] iteration 2200 : loss : 0.200880, loss_ce: 0.004679
[16:49:48.211] iteration 2210 : loss : 0.176258, loss_ce: 0.004376
[16:49:54.521] iteration 2220 : loss : 0.202682, loss_ce: 0.005142
[16:50:00.835] iteration 2230 : loss : 0.154618, loss_ce: 0.003781
[16:50:07.142] iteration 2240 : loss : 0.203931, loss_ce: 0.002732
[16:50:13.467] iteration 2250 : loss : 0.090993, loss_ce: 0.004966
[16:50:19.782] iteration 2260 : loss : 0.165872, loss_ce: 0.002726
[16:50:26.101] iteration 2270 : loss : 0.203962, loss_ce: 0.006523
[16:50:32.408] iteration 2280 : loss : 0.188819, loss_ce: 0.006940
[16:50:38.735] iteration 2290 : loss : 0.134770, loss_ce: 0.004650
[16:50:44.541] iteration 2300 : loss : 0.173796, loss_ce: 0.003803
[16:51:01.001] iteration 2310 : loss : 0.165012, loss_ce: 0.005006
[16:51:07.303] iteration 2320 : loss : 0.186388, loss_ce: 0.008644
[16:51:13.617] iteration 2330 : loss : 0.172032, loss_ce: 0.002281
[16:51:19.924] iteration 2340 : loss : 0.125584, loss_ce: 0.004345
[16:51:26.248] iteration 2350 : loss : 0.169697, loss_ce: 0.004384
[16:51:32.563] iteration 2360 : loss : 0.175253, loss_ce: 0.005023
[16:51:38.880] iteration 2370 : loss : 0.236464, loss_ce: 0.005288
[16:51:45.188] iteration 2380 : loss : 0.036311, loss_ce: 0.003467
[16:51:51.514] iteration 2390 : loss : 0.177909, loss_ce: 0.006458
[16:51:57.320] iteration 2400 : loss : 0.042743, loss_ce: 0.008464
[16:52:13.994] iteration 2410 : loss : 0.199298, loss_ce: 0.008446
[16:52:20.298] iteration 2420 : loss : 0.077611, loss_ce: 0.003303
[16:52:26.611] iteration 2430 : loss : 0.181485, loss_ce: 0.003972
[16:52:32.920] iteration 2440 : loss : 0.097162, loss_ce: 0.007976
[16:52:39.243] iteration 2450 : loss : 0.185306, loss_ce: 0.005296
[16:52:45.557] iteration 2460 : loss : 0.045526, loss_ce: 0.003510
[16:52:51.876] iteration 2470 : loss : 0.169643, loss_ce: 0.004870
[16:52:58.187] iteration 2480 : loss : 0.182825, loss_ce: 0.005711
[16:53:04.514] iteration 2490 : loss : 0.036254, loss_ce: 0.003730
[16:53:10.319] iteration 2500 : loss : 0.303010, loss_ce: 0.000370
[16:53:26.964] iteration 2510 : loss : 0.178753, loss_ce: 0.006352
[16:53:33.274] iteration 2520 : loss : 0.247616, loss_ce: 0.003384
[16:53:39.592] iteration 2530 : loss : 0.172506, loss_ce: 0.005093
[16:53:45.897] iteration 2540 : loss : 0.198874, loss_ce: 0.006458
[16:53:52.219] iteration 2550 : loss : 0.175883, loss_ce: 0.007487
[16:53:58.532] iteration 2560 : loss : 0.220408, loss_ce: 0.004696
[16:54:04.853] iteration 2570 : loss : 0.062402, loss_ce: 0.005566
[16:54:11.162] iteration 2580 : loss : 0.049750, loss_ce: 0.004868
[16:54:17.488] iteration 2590 : loss : 0.223668, loss_ce: 0.004497
[16:54:23.296] iteration 2600 : loss : 0.307148, loss_ce: 0.001598
[16:54:39.952] iteration 2610 : loss : 0.076109, loss_ce: 0.004987
[16:54:46.265] iteration 2620 : loss : 0.093238, loss_ce: 0.005319
[16:54:52.579] iteration 2630 : loss : 0.198080, loss_ce: 0.008911
[16:54:58.884] iteration 2640 : loss : 0.195640, loss_ce: 0.003219
[16:55:05.205] iteration 2650 : loss : 0.076354, loss_ce: 0.004497
[16:55:11.517] iteration 2660 : loss : 0.154222, loss_ce: 0.009968
[16:55:17.837] iteration 2670 : loss : 0.113894, loss_ce: 0.005065
[16:55:24.144] iteration 2680 : loss : 0.163117, loss_ce: 0.006344
[16:55:30.468] iteration 2690 : loss : 0.174948, loss_ce: 0.003279
[16:55:36.274] iteration 2700 : loss : 0.280249, loss_ce: 0.001983
[16:55:52.908] iteration 2710 : loss : 0.179450, loss_ce: 0.002665
[16:55:59.207] iteration 2720 : loss : 0.100156, loss_ce: 0.005161
[16:56:05.527] iteration 2730 : loss : 0.077720, loss_ce: 0.005691
[16:56:11.838] iteration 2740 : loss : 0.173064, loss_ce: 0.006173
[16:56:18.157] iteration 2750 : loss : 0.179893, loss_ce: 0.006258
[16:56:24.462] iteration 2760 : loss : 0.136227, loss_ce: 0.006054
[16:56:30.781] iteration 2770 : loss : 0.078526, loss_ce: 0.003984
[16:56:37.096] iteration 2780 : loss : 0.062987, loss_ce: 0.012578
[16:56:43.419] iteration 2790 : loss : 0.096517, loss_ce: 0.010156
[16:56:49.239] iteration 2800 : loss : 0.303816, loss_ce: 0.001387
[16:57:05.911] iteration 2810 : loss : 0.099510, loss_ce: 0.004359
[16:57:12.223] iteration 2820 : loss : 0.183864, loss_ce: 0.003137
[16:57:18.547] iteration 2830 : loss : 0.180233, loss_ce: 0.005693
[16:57:24.857] iteration 2840 : loss : 0.055162, loss_ce: 0.003336
[16:57:31.173] iteration 2850 : loss : 0.060404, loss_ce: 0.004897
[16:57:37.485] iteration 2860 : loss : 0.079606, loss_ce: 0.003763
[16:57:43.807] iteration 2870 : loss : 0.039586, loss_ce: 0.003482
[16:57:50.116] iteration 2880 : loss : 0.200291, loss_ce: 0.009090
[16:57:56.434] iteration 2890 : loss : 0.250229, loss_ce: 0.002797
[16:58:02.240] iteration 2900 : loss : 0.236178, loss_ce: 0.004281
[16:58:18.870] iteration 2910 : loss : 0.080560, loss_ce: 0.007483
[16:58:25.173] iteration 2920 : loss : 0.050687, loss_ce: 0.002891
[16:58:31.488] iteration 2930 : loss : 0.058703, loss_ce: 0.006117
[16:58:37.799] iteration 2940 : loss : 0.181714, loss_ce: 0.007257
[16:58:44.122] iteration 2950 : loss : 0.177967, loss_ce: 0.004799
[16:58:50.433] iteration 2960 : loss : 0.108170, loss_ce: 0.003749
[16:58:56.753] iteration 2970 : loss : 0.120070, loss_ce: 0.004025
[16:59:03.069] iteration 2980 : loss : 0.184975, loss_ce: 0.005092
[16:59:09.393] iteration 2990 : loss : 0.238515, loss_ce: 0.006574
[16:59:15.194] iteration 3000 : loss : 0.183232, loss_ce: 0.003976
[16:59:15.873] save model to ./finetune_basic_kits23\finetuned_epoch_29.pth
[15:26:30.399] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./finetune_basic_kits23', max_iterations=10000, max_epochs=30, batch_size=48, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.05, freeze_layers=0, auto_tune='none', tune_layers='all', gradient_batches=5, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[15:26:30.408] Using 4761/95221 samples (5.0%) for finetuning
[15:26:30.412] Auto-tune method: none
[15:26:30.412] Tune layers: all
[15:26:30.412] 100 iterations per epoch. 3000 max iterations 
[15:26:48.263] iteration 10 : loss : 0.463923, loss_ce: 0.063346
[15:26:55.198] iteration 20 : loss : 0.443282, loss_ce: 0.040783
[15:27:01.873] iteration 30 : loss : 0.439645, loss_ce: 0.062922
[15:27:08.429] iteration 40 : loss : 0.419810, loss_ce: 0.046976
[15:27:15.230] iteration 50 : loss : 0.410854, loss_ce: 0.048756
[15:27:22.016] iteration 60 : loss : 0.395558, loss_ce: 0.048571
[15:27:28.387] iteration 70 : loss : 0.376279, loss_ce: 0.058078
[15:27:34.754] iteration 80 : loss : 0.383405, loss_ce: 0.040960
[15:27:41.159] iteration 90 : loss : 0.395476, loss_ce: 0.055606
[15:27:47.023] iteration 100 : loss : 0.400505, loss_ce: 0.043539
[15:28:03.870] iteration 110 : loss : 0.299455, loss_ce: 0.022096
[15:28:10.325] iteration 120 : loss : 0.337571, loss_ce: 0.029856
[15:28:17.307] iteration 130 : loss : 0.308938, loss_ce: 0.038281
[15:28:24.315] iteration 140 : loss : 0.292030, loss_ce: 0.016837
[15:28:31.305] iteration 150 : loss : 0.274808, loss_ce: 0.024989
[15:28:38.295] iteration 160 : loss : 0.317031, loss_ce: 0.022451
[15:28:45.277] iteration 170 : loss : 0.325550, loss_ce: 0.011061
[15:28:52.168] iteration 180 : loss : 0.246757, loss_ce: 0.016586
[15:28:59.093] iteration 190 : loss : 0.288643, loss_ce: 0.018763
[15:29:04.992] iteration 200 : loss : 0.272141, loss_ce: 0.006503
[15:29:22.456] iteration 210 : loss : 0.352113, loss_ce: 0.018308
[15:29:28.843] iteration 220 : loss : 0.264809, loss_ce: 0.020621
[15:29:36.780] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./finetune_basic_kits23', max_iterations=10000, max_epochs=30, batch_size=48, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.05, freeze_layers=0, auto_tune='none', tune_layers='all', gradient_batches=5, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[15:29:36.788] Using 4761/95221 samples (5.0%) for finetuning
[15:29:36.792] Auto-tune method: none
[15:29:36.792] Tune layers: all
[15:29:36.792] 100 iterations per epoch. 3000 max iterations 
[15:29:54.328] iteration 10 : loss : 0.463923, loss_ce: 0.063346
[15:30:01.182] iteration 20 : loss : 0.443282, loss_ce: 0.040783
[15:30:08.100] iteration 30 : loss : 0.439645, loss_ce: 0.062922
[15:30:14.952] iteration 40 : loss : 0.419810, loss_ce: 0.046976
[15:30:21.831] iteration 50 : loss : 0.410854, loss_ce: 0.048756
[15:30:28.688] iteration 60 : loss : 0.395558, loss_ce: 0.048571
[15:30:35.555] iteration 70 : loss : 0.376279, loss_ce: 0.058078
[15:30:42.462] iteration 80 : loss : 0.383405, loss_ce: 0.040960
[15:30:49.356] iteration 90 : loss : 0.395476, loss_ce: 0.055606
[15:30:55.390] iteration 100 : loss : 0.400505, loss_ce: 0.043539
[15:31:12.616] iteration 110 : loss : 0.299455, loss_ce: 0.022096
[15:31:19.044] iteration 120 : loss : 0.337570, loss_ce: 0.029856
[15:31:25.889] iteration 130 : loss : 0.308938, loss_ce: 0.038281
[15:31:32.656] iteration 140 : loss : 0.292030, loss_ce: 0.016837
