[02:32:43.393] Namespace(root_path='./datasets/lits17/train_npz', dataset='lits17', list_dir='./lists/lits17', num_classes=3, output_dir='./finetune_basic_lits17', max_iterations=10000, max_epochs=30, batch_size=48, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.05, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[02:32:43.396] Using 742/14843 samples (5.0%) for finetuning
[02:32:43.401] 16 iterations per epoch. 480 max iterations 
[02:33:45.027] Namespace(root_path='./datasets/lits17/train_npz', dataset='lits17', list_dir='./lists/lits17', num_classes=3, output_dir='./finetune_basic_lits17', max_iterations=10000, max_epochs=30, batch_size=48, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.2, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[02:33:45.029] Using 2968/14843 samples (20.0%) for finetuning
[02:33:45.033] 62 iterations per epoch. 1860 max iterations 
[02:34:59.489] Namespace(root_path='./datasets/lits17/train_npz', dataset='lits17', list_dir='./lists/lits17', num_classes=3, output_dir='./finetune_basic_lits17', max_iterations=10000, max_epochs=30, batch_size=48, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.2, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[02:34:59.491] Using 2968/14843 samples (20.0%) for finetuning
[02:34:59.496] 62 iterations per epoch. 1860 max iterations 
[02:35:18.110] iteration 10 : loss : 0.400785, loss_ce: 0.101822
[02:35:24.349] iteration 20 : loss : 0.273149, loss_ce: 0.034821
[02:35:30.670] iteration 30 : loss : 0.253421, loss_ce: 0.026683
[02:35:36.928] iteration 40 : loss : 0.231663, loss_ce: 0.032890
[02:35:43.200] iteration 50 : loss : 0.237312, loss_ce: 0.039651
[02:35:49.461] iteration 60 : loss : 0.240850, loss_ce: 0.035849
[02:36:07.256] iteration 70 : loss : 0.209524, loss_ce: 0.052738
[02:36:13.513] iteration 80 : loss : 0.109256, loss_ce: 0.035064
[02:36:19.792] iteration 90 : loss : 0.093489, loss_ce: 0.016804
[02:36:26.056] iteration 100 : loss : 0.236364, loss_ce: 0.025514
[02:36:32.340] iteration 110 : loss : 0.166396, loss_ce: 0.021405
[02:36:38.611] iteration 120 : loss : 0.171701, loss_ce: 0.016080
[02:36:56.390] iteration 130 : loss : 0.106148, loss_ce: 0.020261
[02:37:02.658] iteration 140 : loss : 0.104310, loss_ce: 0.028736
[02:37:08.937] iteration 150 : loss : 0.217506, loss_ce: 0.027003
[02:37:15.209] iteration 160 : loss : 0.239754, loss_ce: 0.023611
[02:37:21.487] iteration 170 : loss : 0.181052, loss_ce: 0.022560
[02:37:27.763] iteration 180 : loss : 0.206411, loss_ce: 0.017749
[02:37:45.533] iteration 190 : loss : 0.207626, loss_ce: 0.019521
[02:37:51.802] iteration 200 : loss : 0.173064, loss_ce: 0.035341
[02:37:58.084] iteration 210 : loss : 0.125776, loss_ce: 0.033895
[02:38:04.360] iteration 220 : loss : 0.104639, loss_ce: 0.027184
[02:38:10.652] iteration 230 : loss : 0.098642, loss_ce: 0.019193
[02:38:16.927] iteration 240 : loss : 0.195154, loss_ce: 0.024943
[02:38:34.848] iteration 250 : loss : 0.045419, loss_ce: 0.011105
[02:38:41.120] iteration 260 : loss : 0.052097, loss_ce: 0.013131
[02:38:47.406] iteration 270 : loss : 0.087619, loss_ce: 0.017815
[02:38:53.686] iteration 280 : loss : 0.074638, loss_ce: 0.020693
[02:38:59.975] iteration 290 : loss : 0.170451, loss_ce: 0.016356
[02:39:06.251] iteration 300 : loss : 0.081605, loss_ce: 0.020666
[02:39:12.428] iteration 310 : loss : 0.213852, loss_ce: 0.017539
[02:39:30.470] iteration 320 : loss : 0.094531, loss_ce: 0.015053
[02:39:36.759] iteration 330 : loss : 0.061898, loss_ce: 0.017669
[02:39:43.037] iteration 340 : loss : 0.095296, loss_ce: 0.023371
[02:39:49.327] iteration 350 : loss : 0.112841, loss_ce: 0.019595
[02:39:55.609] iteration 360 : loss : 0.065815, loss_ce: 0.014178
[02:40:01.906] iteration 370 : loss : 0.052487, loss_ce: 0.011474
[02:40:19.808] iteration 380 : loss : 0.122955, loss_ce: 0.013823
[02:40:26.090] iteration 390 : loss : 0.167127, loss_ce: 0.023885
[02:40:32.374] iteration 400 : loss : 0.087690, loss_ce: 0.016896
[02:40:38.676] iteration 410 : loss : 0.050000, loss_ce: 0.014353
[02:40:44.963] iteration 420 : loss : 0.133755, loss_ce: 0.011336
[02:40:51.263] iteration 430 : loss : 0.215793, loss_ce: 0.014163
[02:41:09.125] iteration 440 : loss : 0.226827, loss_ce: 0.010846
[02:41:15.411] iteration 450 : loss : 0.046274, loss_ce: 0.014026
[02:41:21.694] iteration 460 : loss : 0.061875, loss_ce: 0.015975
[02:41:27.989] iteration 470 : loss : 0.071522, loss_ce: 0.022286
[02:41:34.269] iteration 480 : loss : 0.060597, loss_ce: 0.020785
[02:41:40.557] iteration 490 : loss : 0.044285, loss_ce: 0.009923
[02:41:58.456] iteration 500 : loss : 0.081178, loss_ce: 0.015569
[02:42:04.740] iteration 510 : loss : 0.117991, loss_ce: 0.017068
[02:42:11.024] iteration 520 : loss : 0.041536, loss_ce: 0.011792
[02:42:17.326] iteration 530 : loss : 0.065565, loss_ce: 0.024333
[02:42:23.620] iteration 540 : loss : 0.147625, loss_ce: 0.014040
[02:42:29.927] iteration 550 : loss : 0.099002, loss_ce: 0.014342
[02:42:47.781] iteration 560 : loss : 0.063679, loss_ce: 0.020507
[02:42:54.070] iteration 570 : loss : 0.044392, loss_ce: 0.010972
[02:43:00.349] iteration 580 : loss : 0.162426, loss_ce: 0.018470
[02:43:06.636] iteration 590 : loss : 0.062992, loss_ce: 0.011252
[02:43:12.918] iteration 600 : loss : 0.062708, loss_ce: 0.013894
[02:43:19.214] iteration 610 : loss : 0.101211, loss_ce: 0.015254
[02:43:25.385] iteration 620 : loss : 0.059399, loss_ce: 0.019289
[02:43:26.144] save model to ./finetune_basic_lits17\finetuned_epoch_9.pth
[02:43:43.579] iteration 630 : loss : 0.129104, loss_ce: 0.011002
[02:43:49.861] iteration 640 : loss : 0.211677, loss_ce: 0.021119
[02:43:56.158] iteration 650 : loss : 0.073931, loss_ce: 0.015694
[02:44:02.448] iteration 660 : loss : 0.043390, loss_ce: 0.012736
[02:44:08.747] iteration 670 : loss : 0.154565, loss_ce: 0.010194
[02:44:15.039] iteration 680 : loss : 0.077085, loss_ce: 0.009867
[02:44:32.914] iteration 690 : loss : 0.146074, loss_ce: 0.010411
[02:44:39.195] iteration 700 : loss : 0.104393, loss_ce: 0.008452
[02:44:45.488] iteration 710 : loss : 0.039752, loss_ce: 0.011511
[02:44:51.778] iteration 720 : loss : 0.208348, loss_ce: 0.015695
[02:44:58.085] iteration 730 : loss : 0.079889, loss_ce: 0.011083
[02:45:04.379] iteration 740 : loss : 0.048079, loss_ce: 0.015220
[02:45:22.248] iteration 750 : loss : 0.042299, loss_ce: 0.015107
[02:45:28.529] iteration 760 : loss : 0.156371, loss_ce: 0.014610
[02:45:34.831] iteration 770 : loss : 0.053816, loss_ce: 0.010578
[02:45:41.123] iteration 780 : loss : 0.080545, loss_ce: 0.012399
[02:45:47.424] iteration 790 : loss : 0.042021, loss_ce: 0.014569
[02:45:53.715] iteration 800 : loss : 0.077718, loss_ce: 0.019016
[02:46:11.783] iteration 810 : loss : 0.072015, loss_ce: 0.016066
[02:46:18.065] iteration 820 : loss : 0.055093, loss_ce: 0.011678
[02:46:24.363] iteration 830 : loss : 0.063427, loss_ce: 0.006913
[02:46:30.653] iteration 840 : loss : 0.117386, loss_ce: 0.013077
[02:46:36.959] iteration 850 : loss : 0.077594, loss_ce: 0.008685
[02:46:43.253] iteration 860 : loss : 0.048064, loss_ce: 0.013846
[02:47:01.297] iteration 870 : loss : 0.099365, loss_ce: 0.012961
[02:47:07.579] iteration 880 : loss : 0.135692, loss_ce: 0.011020
[02:47:13.869] iteration 890 : loss : 0.166177, loss_ce: 0.017795
[02:47:20.158] iteration 900 : loss : 0.045680, loss_ce: 0.006469
[02:47:26.458] iteration 910 : loss : 0.064918, loss_ce: 0.009497
[02:47:32.752] iteration 920 : loss : 0.081451, loss_ce: 0.010310
[02:47:38.941] iteration 930 : loss : 0.138051, loss_ce: 0.028593
[02:47:56.901] iteration 940 : loss : 0.053239, loss_ce: 0.012757
[02:48:03.194] iteration 950 : loss : 0.065977, loss_ce: 0.012609
[02:48:09.481] iteration 960 : loss : 0.050297, loss_ce: 0.009331
[02:48:15.778] iteration 970 : loss : 0.060477, loss_ce: 0.011352
[02:48:22.071] iteration 980 : loss : 0.046033, loss_ce: 0.011788
[02:48:28.376] iteration 990 : loss : 0.192790, loss_ce: 0.021627
[02:48:46.443] iteration 1000 : loss : 0.083584, loss_ce: 0.020995
[02:48:52.732] iteration 1010 : loss : 0.024041, loss_ce: 0.008109
[02:48:59.020] iteration 1020 : loss : 0.044035, loss_ce: 0.007420
[02:49:05.331] iteration 1030 : loss : 0.030761, loss_ce: 0.007913
[02:49:11.627] iteration 1040 : loss : 0.056169, loss_ce: 0.012542
[02:49:17.929] iteration 1050 : loss : 0.033264, loss_ce: 0.007880
[02:49:35.973] iteration 1060 : loss : 0.035190, loss_ce: 0.013489
[02:49:42.265] iteration 1070 : loss : 0.051539, loss_ce: 0.016462
[02:49:48.546] iteration 1080 : loss : 0.169863, loss_ce: 0.007891
[02:49:54.842] iteration 1090 : loss : 0.128007, loss_ce: 0.013215
[02:50:01.123] iteration 1100 : loss : 0.055812, loss_ce: 0.016381
[02:50:07.417] iteration 1110 : loss : 0.073431, loss_ce: 0.016010
[02:50:25.700] iteration 1120 : loss : 0.035757, loss_ce: 0.012389
[02:50:31.981] iteration 1130 : loss : 0.052671, loss_ce: 0.013333
[02:50:38.265] iteration 1140 : loss : 0.051368, loss_ce: 0.011773
[02:50:44.558] iteration 1150 : loss : 0.040694, loss_ce: 0.006821
[02:50:50.849] iteration 1160 : loss : 0.041239, loss_ce: 0.007938
[02:50:57.151] iteration 1170 : loss : 0.099815, loss_ce: 0.013898
[02:51:15.047] iteration 1180 : loss : 0.033259, loss_ce: 0.008996
[02:51:21.333] iteration 1190 : loss : 0.041117, loss_ce: 0.008048
[02:51:27.610] iteration 1200 : loss : 0.152504, loss_ce: 0.013029
[02:51:33.900] iteration 1210 : loss : 0.046389, loss_ce: 0.010324
[02:51:40.183] iteration 1220 : loss : 0.051861, loss_ce: 0.013846
[02:51:46.484] iteration 1230 : loss : 0.079418, loss_ce: 0.006627
[02:51:52.658] iteration 1240 : loss : 0.079639, loss_ce: 0.010633
[02:51:53.485] save model to ./finetune_basic_lits17\finetuned_epoch_19.pth
[02:52:11.229] iteration 1250 : loss : 0.046988, loss_ce: 0.011808
[02:52:17.509] iteration 1260 : loss : 0.062932, loss_ce: 0.008708
[02:52:23.805] iteration 1270 : loss : 0.033654, loss_ce: 0.008514
[02:52:30.092] iteration 1280 : loss : 0.040739, loss_ce: 0.016968
[02:52:36.387] iteration 1290 : loss : 0.100031, loss_ce: 0.012624
[02:52:42.670] iteration 1300 : loss : 0.053579, loss_ce: 0.012785
[02:53:07.958] iteration 1310 : loss : 0.056646, loss_ce: 0.004950
[02:53:14.234] iteration 1320 : loss : 0.049627, loss_ce: 0.007483
[02:53:20.523] iteration 1330 : loss : 0.026797, loss_ce: 0.007094
[02:53:26.799] iteration 1340 : loss : 0.040957, loss_ce: 0.012838
[02:53:33.091] iteration 1350 : loss : 0.040679, loss_ce: 0.010167
[02:53:39.376] iteration 1360 : loss : 0.042306, loss_ce: 0.008489
[02:53:57.074] iteration 1370 : loss : 0.033312, loss_ce: 0.007073
[02:54:03.352] iteration 1380 : loss : 0.040856, loss_ce: 0.010298
[02:54:09.641] iteration 1390 : loss : 0.050827, loss_ce: 0.006963
[02:54:15.924] iteration 1400 : loss : 0.087565, loss_ce: 0.009994
[02:54:22.208] iteration 1410 : loss : 0.031556, loss_ce: 0.006479
[02:54:28.487] iteration 1420 : loss : 0.045808, loss_ce: 0.012132
[02:54:46.033] iteration 1430 : loss : 0.080613, loss_ce: 0.007869
[02:54:52.310] iteration 1440 : loss : 0.038683, loss_ce: 0.005777
[02:54:58.601] iteration 1450 : loss : 0.039702, loss_ce: 0.012813
[02:55:04.888] iteration 1460 : loss : 0.034964, loss_ce: 0.011962
[02:55:11.182] iteration 1470 : loss : 0.046492, loss_ce: 0.006624
[02:55:17.474] iteration 1480 : loss : 0.038761, loss_ce: 0.009730
[02:55:34.950] iteration 1490 : loss : 0.042609, loss_ce: 0.008892
[02:55:41.224] iteration 1500 : loss : 0.028800, loss_ce: 0.008886
[02:55:47.515] iteration 1510 : loss : 0.032984, loss_ce: 0.006349
[02:55:53.800] iteration 1520 : loss : 0.034511, loss_ce: 0.008758
[02:56:00.097] iteration 1530 : loss : 0.036290, loss_ce: 0.012546
[02:56:06.387] iteration 1540 : loss : 0.019174, loss_ce: 0.005637
[02:56:12.571] iteration 1550 : loss : 0.050548, loss_ce: 0.008585
[02:56:29.965] iteration 1560 : loss : 0.026394, loss_ce: 0.008625
[02:56:36.260] iteration 1570 : loss : 0.040552, loss_ce: 0.006619
[02:56:42.546] iteration 1580 : loss : 0.046206, loss_ce: 0.008826
[02:56:48.846] iteration 1590 : loss : 0.034870, loss_ce: 0.010826
[02:56:55.131] iteration 1600 : loss : 0.042865, loss_ce: 0.009975
[02:57:01.427] iteration 1610 : loss : 0.033739, loss_ce: 0.010981
[02:57:18.684] iteration 1620 : loss : 0.059726, loss_ce: 0.010523
[02:57:24.977] iteration 1630 : loss : 0.027275, loss_ce: 0.009149
[02:57:31.265] iteration 1640 : loss : 0.039555, loss_ce: 0.009370
[02:57:37.564] iteration 1650 : loss : 0.047429, loss_ce: 0.010493
[02:57:43.851] iteration 1660 : loss : 0.148382, loss_ce: 0.005434
[02:57:50.156] iteration 1670 : loss : 0.030880, loss_ce: 0.009619
[02:58:07.399] iteration 1680 : loss : 0.087564, loss_ce: 0.012421
[02:58:13.691] iteration 1690 : loss : 0.031387, loss_ce: 0.006467
[02:58:19.979] iteration 1700 : loss : 0.037131, loss_ce: 0.010099
[02:58:26.281] iteration 1710 : loss : 0.063269, loss_ce: 0.010326
[02:58:32.575] iteration 1720 : loss : 0.035238, loss_ce: 0.008559
[02:58:38.873] iteration 1730 : loss : 0.030041, loss_ce: 0.009594
[02:58:56.135] iteration 1740 : loss : 0.027945, loss_ce: 0.009167
[02:59:02.424] iteration 1750 : loss : 0.029590, loss_ce: 0.007476
[02:59:08.703] iteration 1760 : loss : 0.033751, loss_ce: 0.008140
[02:59:14.994] iteration 1770 : loss : 0.041591, loss_ce: 0.012168
[02:59:21.282] iteration 1780 : loss : 0.041252, loss_ce: 0.005197
[02:59:27.584] iteration 1790 : loss : 0.036493, loss_ce: 0.011282
[02:59:44.473] iteration 1800 : loss : 0.036262, loss_ce: 0.009134
[02:59:50.755] iteration 1810 : loss : 0.041439, loss_ce: 0.009374
[02:59:57.034] iteration 1820 : loss : 0.050848, loss_ce: 0.007372
[03:00:03.326] iteration 1830 : loss : 0.040101, loss_ce: 0.010190
[03:00:09.619] iteration 1840 : loss : 0.035420, loss_ce: 0.007322
[03:00:15.920] iteration 1850 : loss : 0.028415, loss_ce: 0.008414
[03:00:22.098] iteration 1860 : loss : 0.042295, loss_ce: 0.008891
[03:00:22.771] save model to ./finetune_basic_lits17\finetuned_epoch_29.pth
[14:05:00.553] Namespace(root_path='./datasets/lits17/train_npz', dataset='lits17', list_dir='./lists/lits17', num_classes=3, output_dir='./finetune_basic_lits17', max_iterations=10000, max_epochs=30, batch_size=48, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.2, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[14:05:00.557] Using 2968/14843 samples (20.0%) for finetuning
[14:05:00.561] 62 iterations per epoch. 1860 max iterations 
