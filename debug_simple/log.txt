[14:58:52.066] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes_old=9, num_classes_new=4, output_dir='./debug_simple', max_iterations=10000, max_epochs=5, batch_size=24, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.25, kd_temperature=3.0, kd_weight=0.2, freeze_old_classes=False, auto_tune='none', gradient_batches=5, tpgm_norm_mode='l2', tpgm_lr=0.01, tpgm_iters=200, tpgm_exclude=[], tpgm_frequency=5, tpgm_start_epoch=10, disable_tpgm=True, tpgm_data_fraction=0.1, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[14:58:52.081] Using 23805/95221 samples (25.0%) for continual learning
[14:58:52.082] Old classes: 9, New classes: 4, Total: 12
[14:58:52.082] TPGM enabled: False
[14:58:52.082] Surgical fine-tuning method: none
[14:58:52.110] Combined Continual Learning + Surgical + TPGM Configuration:
[14:58:52.110] KD Temperature: 3.0
[14:58:52.110] KD Weight: 0.2
[14:58:52.110] Auto-tune method: none
[14:58:52.110] TPGM start epoch: 10
[14:58:52.110] TPGM frequency: 5
[14:58:52.110] 992 iterations per epoch. 4960 max iterations 
[14:59:06.949] iteration 10 : loss : 5303.358398, loss_ce: 0.058857, loss_kd: 26515.482422
[14:59:10.958] iteration 20 : loss : 3468.957764, loss_ce: 0.096001, loss_kd: 17342.634766
[14:59:30.531] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes_old=9, num_classes_new=4, output_dir='./debug_simple', max_iterations=10000, max_epochs=5, batch_size=64, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.25, kd_temperature=3.0, kd_weight=0.2, freeze_old_classes=False, auto_tune='none', gradient_batches=5, tpgm_norm_mode='l2', tpgm_lr=0.01, tpgm_iters=200, tpgm_exclude=[], tpgm_frequency=5, tpgm_start_epoch=10, disable_tpgm=True, tpgm_data_fraction=0.1, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[14:59:30.543] Using 23805/95221 samples (25.0%) for continual learning
[14:59:30.543] Old classes: 9, New classes: 4, Total: 12
[14:59:30.543] TPGM enabled: False
[14:59:30.543] Surgical fine-tuning method: none
[14:59:30.572] Combined Continual Learning + Surgical + TPGM Configuration:
[14:59:30.572] KD Temperature: 3.0
[14:59:30.572] KD Weight: 0.2
[14:59:30.572] Auto-tune method: none
[14:59:30.572] TPGM start epoch: 10
[14:59:30.572] TPGM frequency: 5
[14:59:30.573] 372 iterations per epoch. 1860 max iterations 
[14:59:56.511] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes_old=9, num_classes_new=4, output_dir='./debug_simple', max_iterations=10000, max_epochs=5, batch_size=48, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.25, kd_temperature=3.0, kd_weight=0.2, freeze_old_classes=False, auto_tune='none', gradient_batches=5, tpgm_norm_mode='l2', tpgm_lr=0.01, tpgm_iters=200, tpgm_exclude=[], tpgm_frequency=5, tpgm_start_epoch=10, disable_tpgm=True, tpgm_data_fraction=0.1, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[14:59:56.524] Using 23805/95221 samples (25.0%) for continual learning
[14:59:56.524] Old classes: 9, New classes: 4, Total: 12
[14:59:56.524] TPGM enabled: False
[14:59:56.524] Surgical fine-tuning method: none
[14:59:56.552] Combined Continual Learning + Surgical + TPGM Configuration:
[14:59:56.553] KD Temperature: 3.0
[14:59:56.553] KD Weight: 0.2
[14:59:56.553] Auto-tune method: none
[14:59:56.553] TPGM start epoch: 10
[14:59:56.553] TPGM frequency: 5
[14:59:56.553] 496 iterations per epoch. 2480 max iterations 
[15:00:15.905] iteration 10 : loss : 5070.241211, loss_ce: 0.083813, loss_kd: 25349.515625
[15:00:24.417] iteration 20 : loss : 2918.345703, loss_ce: 0.047814, loss_kd: 14589.658203
[15:00:32.963] iteration 30 : loss : 3206.457520, loss_ce: 0.082801, loss_kd: 16030.105469
[15:00:41.487] iteration 40 : loss : 1817.131104, loss_ce: 0.103118, loss_kd: 9083.437500
[15:00:50.026] iteration 50 : loss : 2308.335205, loss_ce: 0.096960, loss_kd: 11539.437500
[15:00:58.564] iteration 60 : loss : 1892.898804, loss_ce: 0.057365, loss_kd: 9462.220703
[15:01:07.114] iteration 70 : loss : 1258.282715, loss_ce: 0.071551, loss_kd: 6289.263184
[15:01:15.660] iteration 80 : loss : 1851.504150, loss_ce: 0.098565, loss_kd: 9255.259766
[15:01:24.224] iteration 90 : loss : 1049.540649, loss_ce: 0.053595, loss_kd: 5245.577637
[15:01:32.780] iteration 100 : loss : 1614.971802, loss_ce: 0.101015, loss_kd: 8072.541992
[15:01:41.348] iteration 110 : loss : 1258.558105, loss_ce: 0.082888, loss_kd: 6290.516113
[15:01:49.909] iteration 120 : loss : 1106.059937, loss_ce: 0.075219, loss_kd: 5528.161133
[15:01:58.482] iteration 130 : loss : 789.102295, loss_ce: 0.049001, loss_kd: 3943.450928
[15:02:07.048] iteration 140 : loss : 913.503174, loss_ce: 0.073130, loss_kd: 4565.327148
[15:02:15.623] iteration 150 : loss : 1133.982056, loss_ce: 0.072876, loss_kd: 5667.746582
[15:02:24.192] iteration 160 : loss : 1021.080505, loss_ce: 0.059813, loss_kd: 5103.297363
[15:02:32.777] iteration 170 : loss : 961.233154, loss_ce: 0.050129, loss_kd: 4804.140137
[15:02:41.351] iteration 180 : loss : 920.760742, loss_ce: 0.038143, loss_kd: 4601.776855
[15:02:49.931] iteration 190 : loss : 721.781799, loss_ce: 0.059452, loss_kd: 3606.803467
[15:02:58.505] iteration 200 : loss : 698.865967, loss_ce: 0.076770, loss_kd: 3492.183594
[15:03:07.094] iteration 210 : loss : 629.404480, loss_ce: 0.064217, loss_kd: 3144.937012
[15:03:15.677] iteration 220 : loss : 632.027954, loss_ce: 0.065872, loss_kd: 3158.003174
[15:03:24.276] iteration 230 : loss : 851.024475, loss_ce: 0.081760, loss_kd: 4252.996094
[15:03:32.861] iteration 240 : loss : 818.318054, loss_ce: 0.076469, loss_kd: 4089.445801
[15:03:41.461] iteration 250 : loss : 701.998596, loss_ce: 0.060523, loss_kd: 3507.853760
[15:03:50.049] iteration 260 : loss : 764.988037, loss_ce: 0.059001, loss_kd: 3822.854980
[15:03:58.648] iteration 270 : loss : 827.907227, loss_ce: 0.061989, loss_kd: 4137.480469
[15:04:07.252] iteration 280 : loss : 796.051819, loss_ce: 0.051544, loss_kd: 3978.180176
[15:04:15.856] iteration 290 : loss : 830.857788, loss_ce: 0.077554, loss_kd: 4152.106445
[15:04:24.452] iteration 300 : loss : 663.317566, loss_ce: 0.079462, loss_kd: 3314.444092
[15:04:33.061] iteration 310 : loss : 460.414215, loss_ce: 0.034300, loss_kd: 2300.033691
[15:04:41.665] iteration 320 : loss : 585.764404, loss_ce: 0.057497, loss_kd: 2926.741943
[15:04:50.288] iteration 330 : loss : 652.781982, loss_ce: 0.079197, loss_kd: 3261.798828
[15:04:58.896] iteration 340 : loss : 560.065308, loss_ce: 0.076856, loss_kd: 2798.213867
[15:05:07.523] iteration 350 : loss : 398.245514, loss_ce: 0.054388, loss_kd: 1989.099731
[15:05:16.136] iteration 360 : loss : 891.014709, loss_ce: 0.088664, loss_kd: 4452.931641
[15:05:24.767] iteration 370 : loss : 646.781433, loss_ce: 0.047095, loss_kd: 3231.857910
[15:05:33.385] iteration 380 : loss : 522.778687, loss_ce: 0.053469, loss_kd: 2611.797852
[15:05:42.020] iteration 390 : loss : 922.281433, loss_ce: 0.060959, loss_kd: 4609.334961
[15:05:50.644] iteration 400 : loss : 506.897003, loss_ce: 0.054612, loss_kd: 2532.408203
[15:05:59.284] iteration 410 : loss : 642.888428, loss_ce: 0.062974, loss_kd: 3212.313965
[15:06:07.913] iteration 420 : loss : 638.418274, loss_ce: 0.051589, loss_kd: 3190.066650
[15:06:16.557] iteration 430 : loss : 901.030762, loss_ce: 0.069144, loss_kd: 4503.024902
[15:06:25.187] iteration 440 : loss : 574.124268, loss_ce: 0.057785, loss_kd: 2868.494385
[15:06:33.827] iteration 450 : loss : 498.784912, loss_ce: 0.062321, loss_kd: 2491.781738
[15:06:42.458] iteration 460 : loss : 390.121277, loss_ce: 0.068177, loss_kd: 1948.457520
[15:06:51.106] iteration 470 : loss : 546.615417, loss_ce: 0.091025, loss_kd: 2730.954102
[15:06:59.745] iteration 480 : loss : 532.740967, loss_ce: 0.066182, loss_kd: 2661.560791
[15:07:08.390] iteration 490 : loss : 599.582092, loss_ce: 0.078478, loss_kd: 2995.745850
[15:07:28.833] iteration 500 : loss : 459.211426, loss_ce: 0.064591, loss_kd: 2293.977539
[15:07:37.422] iteration 510 : loss : 531.510132, loss_ce: 0.065781, loss_kd: 2655.432861
[15:07:46.019] iteration 520 : loss : 385.006531, loss_ce: 0.057324, loss_kd: 1922.936890
[15:07:54.624] iteration 530 : loss : 511.150208, loss_ce: 0.070943, loss_kd: 2553.691406
[15:08:03.225] iteration 540 : loss : 672.989746, loss_ce: 0.049214, loss_kd: 3362.924561
[15:08:11.845] iteration 550 : loss : 623.074768, loss_ce: 0.074139, loss_kd: 3113.259033
[15:08:20.458] iteration 560 : loss : 371.429535, loss_ce: 0.056212, loss_kd: 1855.054688
[15:08:29.095] iteration 570 : loss : 455.190613, loss_ce: 0.053113, loss_kd: 2273.889160
[15:08:37.725] iteration 580 : loss : 522.561157, loss_ce: 0.094578, loss_kd: 2610.620361
[15:08:46.361] iteration 590 : loss : 589.964294, loss_ce: 0.076026, loss_kd: 2947.641113
[15:08:54.998] iteration 600 : loss : 469.067413, loss_ce: 0.072597, loss_kd: 2343.244141
[15:09:03.642] iteration 610 : loss : 517.627014, loss_ce: 0.077710, loss_kd: 2585.998779
[15:09:12.282] iteration 620 : loss : 478.267700, loss_ce: 0.069469, loss_kd: 2389.217285
[15:09:20.936] iteration 630 : loss : 459.248444, loss_ce: 0.068779, loss_kd: 2294.108643
[15:09:29.585] iteration 640 : loss : 489.760956, loss_ce: 0.040317, loss_kd: 2446.783691
[15:09:38.246] iteration 650 : loss : 416.220276, loss_ce: 0.087578, loss_kd: 2078.969238
[15:09:46.892] iteration 660 : loss : 423.504547, loss_ce: 0.076938, loss_kd: 2115.387695
[15:09:55.551] iteration 670 : loss : 419.948761, loss_ce: 0.062000, loss_kd: 2097.743408
[15:10:04.203] iteration 680 : loss : 491.573303, loss_ce: 0.089999, loss_kd: 2455.818359
[15:10:12.869] iteration 690 : loss : 495.320984, loss_ce: 0.089899, loss_kd: 2474.529297
[15:10:21.517] iteration 700 : loss : 471.375854, loss_ce: 0.079631, loss_kd: 2354.780518
[15:10:30.183] iteration 710 : loss : 520.876465, loss_ce: 0.082126, loss_kd: 2602.283691
[15:10:38.841] iteration 720 : loss : 495.564514, loss_ce: 0.096026, loss_kd: 2475.743164
[15:10:47.512] iteration 730 : loss : 457.221832, loss_ce: 0.084858, loss_kd: 2284.035645
[15:10:56.174] iteration 740 : loss : 556.410645, loss_ce: 0.108015, loss_kd: 2779.913086
[15:11:04.846] iteration 750 : loss : 639.875488, loss_ce: 0.112756, loss_kd: 3197.300293
[15:11:13.507] iteration 760 : loss : 741.373047, loss_ce: 0.054467, loss_kd: 3704.829102
[15:11:22.171] iteration 770 : loss : 420.186798, loss_ce: 0.080424, loss_kd: 2098.829834
[15:11:30.844] iteration 780 : loss : 410.853821, loss_ce: 0.075724, loss_kd: 2052.213379
[15:11:39.513] iteration 790 : loss : 451.523743, loss_ce: 0.066980, loss_kd: 2255.604492
[15:11:48.171] iteration 800 : loss : 624.715027, loss_ce: 0.057113, loss_kd: 3121.511963
[15:11:56.841] iteration 810 : loss : 402.788513, loss_ce: 0.100137, loss_kd: 2011.818604
[15:12:05.508] iteration 820 : loss : 335.046326, loss_ce: 0.060446, loss_kd: 1673.196655
[15:12:14.179] iteration 830 : loss : 489.218750, loss_ce: 0.074019, loss_kd: 2444.106445
[15:12:22.849] iteration 840 : loss : 424.153259, loss_ce: 0.087769, loss_kd: 2118.696289
[15:12:31.525] iteration 850 : loss : 308.675079, loss_ce: 0.077864, loss_kd: 1541.277344
[15:12:40.202] iteration 860 : loss : 390.983704, loss_ce: 0.089717, loss_kd: 1952.766235
[15:12:48.876] iteration 870 : loss : 447.628204, loss_ce: 0.057056, loss_kd: 2236.118652
[15:12:57.543] iteration 880 : loss : 499.980865, loss_ce: 0.099572, loss_kd: 2497.858887
[15:13:06.227] iteration 890 : loss : 333.681305, loss_ce: 0.075933, loss_kd: 1666.353882
[15:13:14.908] iteration 900 : loss : 424.231293, loss_ce: 0.090844, loss_kd: 2119.085205
[15:13:23.596] iteration 910 : loss : 520.289978, loss_ce: 0.050436, loss_kd: 2599.414062
[15:13:32.273] iteration 920 : loss : 338.152863, loss_ce: 0.066148, loss_kd: 1688.760742
[15:13:40.951] iteration 930 : loss : 604.010193, loss_ce: 0.084115, loss_kd: 3018.011963
[15:13:49.625] iteration 940 : loss : 545.217896, loss_ce: 0.094444, loss_kd: 2723.980469
[15:13:58.309] iteration 950 : loss : 412.418610, loss_ce: 0.091523, loss_kd: 2059.972412
[15:14:06.991] iteration 960 : loss : 417.281830, loss_ce: 0.074015, loss_kd: 2084.373291
[15:14:15.684] iteration 970 : loss : 304.954803, loss_ce: 0.058361, loss_kd: 1522.751465
[15:14:24.368] iteration 980 : loss : 499.925781, loss_ce: 0.056298, loss_kd: 2497.647705
[15:14:33.068] iteration 990 : loss : 420.733887, loss_ce: 0.107802, loss_kd: 2101.605469
[15:14:52.948] iteration 1000 : loss : 353.576141, loss_ce: 0.088025, loss_kd: 1765.831177
[15:15:01.561] iteration 1010 : loss : 375.537964, loss_ce: 0.087433, loss_kd: 1875.592896
[15:15:10.171] iteration 1020 : loss : 409.540802, loss_ce: 0.082654, loss_kd: 2045.643677
[15:15:18.798] iteration 1030 : loss : 445.017914, loss_ce: 0.101126, loss_kd: 2222.955322
[15:15:27.425] iteration 1040 : loss : 476.079742, loss_ce: 0.065624, loss_kd: 2378.415283
[15:15:36.067] iteration 1050 : loss : 443.243835, loss_ce: 0.089287, loss_kd: 2214.145752
[15:15:44.702] iteration 1060 : loss : 332.556519, loss_ce: 0.088992, loss_kd: 1660.707886
[15:15:53.356] iteration 1070 : loss : 631.919495, loss_ce: 0.068996, loss_kd: 3157.548828
[15:16:02.008] iteration 1080 : loss : 436.606110, loss_ce: 0.082956, loss_kd: 2181.012939
[15:16:10.665] iteration 1090 : loss : 465.198975, loss_ce: 0.100326, loss_kd: 2323.891357
[15:16:19.318] iteration 1100 : loss : 478.145508, loss_ce: 0.082135, loss_kd: 2388.678223
[15:16:28.000] iteration 1110 : loss : 434.007935, loss_ce: 0.094365, loss_kd: 2167.987793
[15:16:36.651] iteration 1120 : loss : 337.109253, loss_ce: 0.093010, loss_kd: 1683.475464
[15:16:45.316] iteration 1130 : loss : 358.055756, loss_ce: 0.066927, loss_kd: 1788.263672
[15:16:53.977] iteration 1140 : loss : 459.079773, loss_ce: 0.083078, loss_kd: 2293.333008
[15:17:02.654] iteration 1150 : loss : 467.682556, loss_ce: 0.068977, loss_kd: 2336.406006
[15:17:11.329] iteration 1160 : loss : 379.684387, loss_ce: 0.077429, loss_kd: 1896.364380
[15:17:20.012] iteration 1170 : loss : 321.876801, loss_ce: 0.073177, loss_kd: 1607.380005
[15:17:28.685] iteration 1180 : loss : 393.097748, loss_ce: 0.100547, loss_kd: 1963.366577
[15:17:37.372] iteration 1190 : loss : 330.351807, loss_ce: 0.056931, loss_kd: 1649.744873
[15:17:46.049] iteration 1200 : loss : 339.817169, loss_ce: 0.072929, loss_kd: 1697.080322
[15:17:54.753] iteration 1210 : loss : 405.669769, loss_ce: 0.093023, loss_kd: 2026.261230
[15:18:03.441] iteration 1220 : loss : 343.796448, loss_ce: 0.067533, loss_kd: 1716.943726
[15:18:12.138] iteration 1230 : loss : 333.561920, loss_ce: 0.098802, loss_kd: 1665.729248
[15:18:20.815] iteration 1240 : loss : 352.262604, loss_ce: 0.066501, loss_kd: 1759.294922
[15:18:29.526] iteration 1250 : loss : 341.703979, loss_ce: 0.060197, loss_kd: 1706.493408
[15:18:38.204] iteration 1260 : loss : 353.917480, loss_ce: 0.077219, loss_kd: 1767.591431
[15:18:46.892] iteration 1270 : loss : 553.114502, loss_ce: 0.089757, loss_kd: 2763.504883
[15:18:55.586] iteration 1280 : loss : 346.995911, loss_ce: 0.069727, loss_kd: 1732.946899
[15:19:04.277] iteration 1290 : loss : 336.731842, loss_ce: 0.054553, loss_kd: 1681.608887
[15:19:12.952] iteration 1300 : loss : 437.300537, loss_ce: 0.093215, loss_kd: 2184.408691
[15:19:21.649] iteration 1310 : loss : 339.160217, loss_ce: 0.068703, loss_kd: 1693.815796
[15:19:30.351] iteration 1320 : loss : 321.866791, loss_ce: 0.072034, loss_kd: 1607.251343
[15:19:39.050] iteration 1330 : loss : 312.892120, loss_ce: 0.075456, loss_kd: 1562.408813
[15:19:47.750] iteration 1340 : loss : 410.422119, loss_ce: 0.070233, loss_kd: 2050.089355
[15:19:56.455] iteration 1350 : loss : 415.924927, loss_ce: 0.112385, loss_kd: 2077.509766
[15:20:05.165] iteration 1360 : loss : 291.349335, loss_ce: 0.081311, loss_kd: 1454.681885
[15:20:13.872] iteration 1370 : loss : 315.062103, loss_ce: 0.092046, loss_kd: 1573.249146
[15:20:22.577] iteration 1380 : loss : 379.929565, loss_ce: 0.100669, loss_kd: 1897.557251
[15:20:31.286] iteration 1390 : loss : 355.709137, loss_ce: 0.071463, loss_kd: 1776.481934
[15:20:39.981] iteration 1400 : loss : 313.883362, loss_ce: 0.087593, loss_kd: 1567.324707
[15:20:48.681] iteration 1410 : loss : 271.551575, loss_ce: 0.104790, loss_kd: 1355.648926
[15:20:57.373] iteration 1420 : loss : 322.211487, loss_ce: 0.088808, loss_kd: 1609.002197
[15:21:06.061] iteration 1430 : loss : 443.587891, loss_ce: 0.078155, loss_kd: 2215.928955
[15:21:14.752] iteration 1440 : loss : 585.151794, loss_ce: 0.061513, loss_kd: 2923.784912
[15:21:23.457] iteration 1450 : loss : 361.263458, loss_ce: 0.092723, loss_kd: 1804.273438
[15:21:32.165] iteration 1460 : loss : 358.692963, loss_ce: 0.079626, loss_kd: 1791.355469
[15:21:40.856] iteration 1470 : loss : 369.603271, loss_ce: 0.075670, loss_kd: 1845.975830
[15:21:49.576] iteration 1480 : loss : 286.062256, loss_ce: 0.083398, loss_kd: 1428.258057
[15:22:11.662] iteration 1490 : loss : 375.439575, loss_ce: 0.069513, loss_kd: 1875.127808
[15:22:20.253] iteration 1500 : loss : 447.147949, loss_ce: 0.080382, loss_kd: 2233.705322
[15:22:28.870] iteration 1510 : loss : 347.241302, loss_ce: 0.080288, loss_kd: 1734.136230
[15:22:37.481] iteration 1520 : loss : 393.782684, loss_ce: 0.079799, loss_kd: 1966.836060
[15:22:46.109] iteration 1530 : loss : 403.701172, loss_ce: 0.052226, loss_kd: 2016.464722
[15:22:54.739] iteration 1540 : loss : 349.028900, loss_ce: 0.088863, loss_kd: 1743.062012
[15:23:03.392] iteration 1550 : loss : 337.813721, loss_ce: 0.060825, loss_kd: 1687.060913
[15:23:12.037] iteration 1560 : loss : 327.960815, loss_ce: 0.082732, loss_kd: 1637.720825
[15:23:20.696] iteration 1570 : loss : 368.379791, loss_ce: 0.089064, loss_kd: 1839.801025
[15:23:29.343] iteration 1580 : loss : 291.358551, loss_ce: 0.066095, loss_kd: 1454.796387
[15:23:38.008] iteration 1590 : loss : 320.303680, loss_ce: 0.070756, loss_kd: 1599.424316
[15:23:46.665] iteration 1600 : loss : 313.691803, loss_ce: 0.092004, loss_kd: 1566.403564
[15:23:55.350] iteration 1610 : loss : 340.672974, loss_ce: 0.086439, loss_kd: 1701.336426
[15:24:04.019] iteration 1620 : loss : 627.432861, loss_ce: 0.119429, loss_kd: 3135.021973
[15:24:12.708] iteration 1630 : loss : 475.264954, loss_ce: 0.085058, loss_kd: 2374.297607
[15:24:21.378] iteration 1640 : loss : 225.001434, loss_ce: 0.065229, loss_kd: 1123.026611
[15:24:30.061] iteration 1650 : loss : 302.741119, loss_ce: 0.065925, loss_kd: 1511.691528
[15:24:38.736] iteration 1660 : loss : 291.491211, loss_ce: 0.080407, loss_kd: 1455.366577
[15:24:47.433] iteration 1670 : loss : 433.219391, loss_ce: 0.106639, loss_kd: 2163.949219
[15:24:56.131] iteration 1680 : loss : 344.852020, loss_ce: 0.098289, loss_kd: 1722.146851
[15:25:04.828] iteration 1690 : loss : 316.802399, loss_ce: 0.062770, loss_kd: 1581.966675
[15:25:13.509] iteration 1700 : loss : 247.717743, loss_ce: 0.049497, loss_kd: 1236.587891
[15:25:22.206] iteration 1710 : loss : 649.957886, loss_ce: 0.058504, loss_kd: 3247.782715
[15:25:30.901] iteration 1720 : loss : 406.129425, loss_ce: 0.065702, loss_kd: 2028.657227
[15:25:39.612] iteration 1730 : loss : 272.261719, loss_ce: 0.075534, loss_kd: 1359.376831
[15:25:48.311] iteration 1740 : loss : 319.747589, loss_ce: 0.061578, loss_kd: 1596.726807
[15:25:57.023] iteration 1750 : loss : 271.382355, loss_ce: 0.057152, loss_kd: 1354.891113
[15:26:05.731] iteration 1760 : loss : 265.007324, loss_ce: 0.067913, loss_kd: 1323.006104
[15:26:14.433] iteration 1770 : loss : 272.179626, loss_ce: 0.083886, loss_kd: 1358.839355
[15:26:23.132] iteration 1780 : loss : 323.613281, loss_ce: 0.085578, loss_kd: 1616.029297
[15:26:31.859] iteration 1790 : loss : 388.348145, loss_ce: 0.091099, loss_kd: 1939.641602
[15:26:40.545] iteration 1800 : loss : 297.498962, loss_ce: 0.100299, loss_kd: 1485.429565
[15:26:49.256] iteration 1810 : loss : 268.870087, loss_ce: 0.067573, loss_kd: 1342.370850
[15:26:57.952] iteration 1820 : loss : 293.936218, loss_ce: 0.083483, loss_kd: 1467.690186
[15:27:06.654] iteration 1830 : loss : 319.907837, loss_ce: 0.074192, loss_kd: 1597.485474
[15:27:15.364] iteration 1840 : loss : 541.521729, loss_ce: 0.100512, loss_kd: 2705.573242
[15:27:24.074] iteration 1850 : loss : 359.369537, loss_ce: 0.089842, loss_kd: 1794.775879
[15:27:32.771] iteration 1860 : loss : 328.924927, loss_ce: 0.101285, loss_kd: 1642.516724
[15:27:41.488] iteration 1870 : loss : 297.359802, loss_ce: 0.052123, loss_kd: 1484.778076
[15:27:50.184] iteration 1880 : loss : 344.113831, loss_ce: 0.095227, loss_kd: 1718.460571
[15:27:58.888] iteration 1890 : loss : 300.633057, loss_ce: 0.097094, loss_kd: 1501.067383
[15:28:07.588] iteration 1900 : loss : 294.689087, loss_ce: 0.092858, loss_kd: 1471.401123
[15:28:16.298] iteration 1910 : loss : 388.335663, loss_ce: 0.104942, loss_kd: 1939.606567
[15:28:24.999] iteration 1920 : loss : 332.436005, loss_ce: 0.073575, loss_kd: 1660.157471
[15:28:33.710] iteration 1930 : loss : 304.653625, loss_ce: 0.077626, loss_kd: 1521.238403
[15:28:42.406] iteration 1940 : loss : 277.664948, loss_ce: 0.076971, loss_kd: 1386.291992
[15:28:51.107] iteration 1950 : loss : 297.098236, loss_ce: 0.094703, loss_kd: 1483.403931
[15:28:59.815] iteration 1960 : loss : 256.460419, loss_ce: 0.097560, loss_kd: 1280.238525
[15:29:08.524] iteration 1970 : loss : 261.818726, loss_ce: 0.058012, loss_kd: 1307.007202
[15:29:17.221] iteration 1980 : loss : 308.082336, loss_ce: 0.097762, loss_kd: 1538.369629
[15:29:38.130] iteration 1990 : loss : 336.107635, loss_ce: 0.090378, loss_kd: 1678.435181
[15:29:46.727] iteration 2000 : loss : 336.954010, loss_ce: 0.104555, loss_kd: 1682.685669
[15:29:55.350] iteration 2010 : loss : 281.471008, loss_ce: 0.095080, loss_kd: 1405.283447
[15:30:03.977] iteration 2020 : loss : 236.091858, loss_ce: 0.068222, loss_kd: 1178.463379
[15:30:12.624] iteration 2030 : loss : 492.487549, loss_ce: 0.108793, loss_kd: 2460.373291
[15:30:21.261] iteration 2040 : loss : 308.733521, loss_ce: 0.085821, loss_kd: 1541.573853
[15:30:29.922] iteration 2050 : loss : 329.388611, loss_ce: 0.066601, loss_kd: 1644.895874
[15:30:38.621] iteration 2060 : loss : 293.268677, loss_ce: 0.094267, loss_kd: 1464.286865
[15:30:47.285] iteration 2070 : loss : 356.804657, loss_ce: 0.037443, loss_kd: 1782.100830
[15:30:55.943] iteration 2080 : loss : 222.094330, loss_ce: 0.070524, loss_kd: 1108.468262
[15:31:04.610] iteration 2090 : loss : 217.353638, loss_ce: 0.074355, loss_kd: 1084.723633
[15:31:13.270] iteration 2100 : loss : 366.041809, loss_ce: 0.109569, loss_kd: 1828.123657
[15:31:21.949] iteration 2110 : loss : 278.928406, loss_ce: 0.083020, loss_kd: 1392.624634
[15:31:30.624] iteration 2120 : loss : 262.755341, loss_ce: 0.074348, loss_kd: 1311.706543
[15:31:39.301] iteration 2130 : loss : 367.969788, loss_ce: 0.126855, loss_kd: 1837.707764
[15:31:47.964] iteration 2140 : loss : 305.001343, loss_ce: 0.076518, loss_kd: 1522.993286
[15:31:56.662] iteration 2150 : loss : 188.670837, loss_ce: 0.062040, loss_kd: 941.369995
[15:32:05.332] iteration 2160 : loss : 329.300507, loss_ce: 0.071874, loss_kd: 1644.422363
[15:32:14.024] iteration 2170 : loss : 408.761688, loss_ce: 0.094403, loss_kd: 2041.709961
[15:32:22.702] iteration 2180 : loss : 316.821259, loss_ce: 0.089040, loss_kd: 1582.069702
[15:32:31.379] iteration 2190 : loss : 312.604401, loss_ce: 0.101615, loss_kd: 1560.941895
[15:32:40.045] iteration 2200 : loss : 324.902496, loss_ce: 0.104886, loss_kd: 1622.445557
[15:32:48.728] iteration 2210 : loss : 301.784454, loss_ce: 0.091573, loss_kd: 1506.880127
[15:32:57.413] iteration 2220 : loss : 334.507172, loss_ce: 0.083838, loss_kd: 1670.475098
[15:33:06.143] iteration 2230 : loss : 220.103989, loss_ce: 0.066158, loss_kd: 1098.487793
[15:33:14.818] iteration 2240 : loss : 376.798889, loss_ce: 0.066813, loss_kd: 1882.005493
[15:33:23.497] iteration 2250 : loss : 219.608337, loss_ce: 0.070610, loss_kd: 1095.989746
[15:33:32.162] iteration 2260 : loss : 221.695099, loss_ce: 0.059872, loss_kd: 1106.477295
[15:33:40.840] iteration 2270 : loss : 273.904266, loss_ce: 0.091778, loss_kd: 1367.451782
[15:33:49.505] iteration 2280 : loss : 294.486603, loss_ce: 0.085318, loss_kd: 1470.390747
[15:33:58.188] iteration 2290 : loss : 257.222626, loss_ce: 0.084823, loss_kd: 1284.066650
[15:34:06.856] iteration 2300 : loss : 350.025024, loss_ce: 0.096288, loss_kd: 1748.029053
[15:34:15.526] iteration 2310 : loss : 222.503708, loss_ce: 0.058563, loss_kd: 1110.528687
[15:34:24.190] iteration 2320 : loss : 261.823914, loss_ce: 0.067030, loss_kd: 1307.064697
[15:34:32.865] iteration 2330 : loss : 160.744720, loss_ce: 0.063338, loss_kd: 801.709045
[15:34:41.524] iteration 2340 : loss : 351.296295, loss_ce: 0.097529, loss_kd: 1754.381226
[15:34:50.191] iteration 2350 : loss : 259.046539, loss_ce: 0.091382, loss_kd: 1293.192627
[15:34:58.847] iteration 2360 : loss : 285.918518, loss_ce: 0.081030, loss_kd: 1427.511597
[15:35:08.133] iteration 2370 : loss : 278.553864, loss_ce: 0.084304, loss_kd: 1390.723877
[15:35:16.786] iteration 2380 : loss : 254.893478, loss_ce: 0.084307, loss_kd: 1272.363892
[15:35:25.448] iteration 2390 : loss : 207.523056, loss_ce: 0.074592, loss_kd: 1035.564941
[15:35:34.101] iteration 2400 : loss : 258.577515, loss_ce: 0.089089, loss_kd: 1290.848267
[15:35:42.771] iteration 2410 : loss : 300.883881, loss_ce: 0.068023, loss_kd: 1502.372437
[15:35:51.421] iteration 2420 : loss : 263.426758, loss_ce: 0.109185, loss_kd: 1315.022583
[15:36:00.087] iteration 2430 : loss : 357.654053, loss_ce: 0.096456, loss_kd: 1786.198975
[15:36:08.740] iteration 2440 : loss : 289.684662, loss_ce: 0.079382, loss_kd: 1446.341187
[15:36:17.397] iteration 2450 : loss : 298.125000, loss_ce: 0.082456, loss_kd: 1488.565674
[15:36:26.050] iteration 2460 : loss : 295.058014, loss_ce: 0.061632, loss_kd: 1473.284668
[15:36:34.721] iteration 2470 : loss : 233.128616, loss_ce: 0.095071, loss_kd: 1163.577148
[15:36:43.321] iteration 2480 : loss : 239.633148, loss_ce: 0.075876, loss_kd: 1196.152954
[15:36:44.116] save model to ./debug_simple\continual_surgical_tpgm_epoch_4.pth
[15:36:44.210] save final model to ./debug_simple\continual_surgical_tpgm_final.pth
[16:14:47.431] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes_old=9, num_classes_new=4, output_dir='./debug_simple', max_iterations=10000, max_epochs=5, batch_size=48, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.25, kd_temperature=3.0, kd_weight=0.2, freeze_old_classes=False, auto_tune='none', gradient_batches=5, tpgm_norm_mode='l2', tpgm_lr=0.01, tpgm_iters=200, tpgm_exclude=[], tpgm_frequency=5, tpgm_start_epoch=10, disable_tpgm=True, tpgm_data_fraction=0.1, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[16:14:47.447] Using 23805/95221 samples (25.0%) for continual learning
[16:14:47.447] Old classes: 9, New classes: 4, Total: 12
[16:14:47.447] TPGM enabled: False
[16:14:47.447] Surgical fine-tuning method: none
[16:15:20.791] Combined Continual Learning + Surgical + TPGM Configuration:
[16:15:20.791] KD Temperature: 3.0
[16:15:20.791] KD Weight: 0.2
[16:15:20.791] Auto-tune method: none
[16:15:20.791] TPGM start epoch: 10
[16:15:20.791] TPGM frequency: 5
[16:15:20.791] 496 iterations per epoch. 2480 max iterations 
[16:19:45.605] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes_old=9, num_classes_new=4, output_dir='./debug_simple', max_iterations=10000, max_epochs=5, batch_size=48, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.25, kd_temperature=3.0, kd_weight=0.2, freeze_old_classes=False, auto_tune='none', gradient_batches=5, tpgm_norm_mode='l2', tpgm_lr=0.01, tpgm_iters=200, tpgm_exclude=[], tpgm_frequency=5, tpgm_start_epoch=10, disable_tpgm=True, tpgm_data_fraction=0.1, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[16:19:45.619] Using 23805/95221 samples (25.0%) for continual learning
[16:19:45.620] Old classes: 9, New classes: 4, Total: 12
[16:19:45.620] TPGM enabled: False
[16:19:45.620] Surgical fine-tuning method: none
[16:31:48.886] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes_old=9, num_classes_new=4, output_dir='./debug_simple', max_iterations=10000, max_epochs=5, batch_size=48, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.25, kd_temperature=3.0, kd_weight=0.2, freeze_old_classes=False, auto_tune='none', gradient_batches=5, tpgm_norm_mode='l2', tpgm_lr=0.01, tpgm_iters=200, tpgm_exclude=[], tpgm_frequency=5, tpgm_start_epoch=10, disable_tpgm=True, tpgm_data_fraction=0.1, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[16:31:48.901] Using 23805/95221 samples (25.0%) for continual learning
[16:31:48.901] Old classes: 9, New classes: 4, Total: 12
[16:31:48.901] TPGM enabled: False
[16:31:48.901] Surgical fine-tuning method: none
[16:33:19.067] Combined Continual Learning + Surgical + TPGM Configuration:
[16:33:19.067] KD Temperature: 3.0
[16:33:19.067] KD Weight: 0.2
[16:33:19.067] Auto-tune method: none
[16:33:19.068] TPGM start epoch: 10
[16:33:19.068] TPGM frequency: 5
[16:33:19.068] 496 iterations per epoch. 2480 max iterations 
[16:33:39.676] iteration 10 : loss : 6435.014648, loss_ce: 0.000000, loss_kd: 32172.695312
[16:33:48.192] iteration 20 : loss : 5909.512207, loss_ce: 0.000000, loss_kd: 29544.855469
[16:33:56.772] iteration 30 : loss : 4245.554688, loss_ce: 0.000000, loss_kd: 21225.070312
[16:34:05.317] iteration 40 : loss : 2702.032715, loss_ce: 0.000000, loss_kd: 13507.400391
[16:34:13.875] iteration 50 : loss : 2271.317383, loss_ce: 0.000000, loss_kd: 11353.887695
[16:34:22.429] iteration 60 : loss : 2135.937012, loss_ce: 0.000000, loss_kd: 10676.968750
[16:34:30.996] iteration 70 : loss : 2045.015259, loss_ce: 0.000000, loss_kd: 10222.173828
[16:34:39.558] iteration 80 : loss : 1418.099365, loss_ce: 0.000000, loss_kd: 7087.667969
[16:34:48.138] iteration 90 : loss : 1782.403564, loss_ce: 0.000000, loss_kd: 8909.202148
[16:34:56.708] iteration 100 : loss : 1452.099121, loss_ce: 0.000000, loss_kd: 7257.620117
[16:35:05.288] iteration 110 : loss : 1212.645630, loss_ce: 0.000000, loss_kd: 6060.345215
[16:35:13.864] iteration 120 : loss : 1407.898315, loss_ce: 0.000000, loss_kd: 7036.654785
[16:35:22.453] iteration 130 : loss : 1223.818970, loss_ce: 0.000000, loss_kd: 6116.277344
[16:35:31.036] iteration 140 : loss : 1016.993713, loss_ce: 0.000000, loss_kd: 5082.224121
[16:35:39.628] iteration 150 : loss : 1575.264526, loss_ce: 0.000000, loss_kd: 7873.597168
[16:35:48.211] iteration 160 : loss : 1023.873840, loss_ce: 0.000000, loss_kd: 5116.713867
[16:35:56.807] iteration 170 : loss : 1452.605469, loss_ce: 0.000000, loss_kd: 7260.296387
[16:36:05.395] iteration 180 : loss : 811.715637, loss_ce: 0.000000, loss_kd: 4055.844238
[16:36:13.995] iteration 190 : loss : 1190.122437, loss_ce: 0.000000, loss_kd: 5947.868164
[16:36:22.584] iteration 200 : loss : 789.481934, loss_ce: 0.000000, loss_kd: 3944.657715
[16:36:31.186] iteration 210 : loss : 895.201965, loss_ce: 0.000000, loss_kd: 4473.275391
[16:36:39.775] iteration 220 : loss : 915.421387, loss_ce: 0.000000, loss_kd: 4574.423828
[16:36:48.376] iteration 230 : loss : 892.718384, loss_ce: 0.000000, loss_kd: 4460.874512
[16:36:56.972] iteration 240 : loss : 1014.630493, loss_ce: 0.000000, loss_kd: 5070.429199
[16:37:05.586] iteration 250 : loss : 835.750061, loss_ce: 0.000000, loss_kd: 4176.040527
[16:37:14.185] iteration 260 : loss : 853.072144, loss_ce: 0.000000, loss_kd: 4262.672852
[16:37:22.794] iteration 270 : loss : 918.083923, loss_ce: 0.000000, loss_kd: 4587.722168
[16:37:31.392] iteration 280 : loss : 671.256958, loss_ce: 0.000000, loss_kd: 3353.600342
[16:37:40.000] iteration 290 : loss : 508.545319, loss_ce: 0.000000, loss_kd: 2540.019043
[16:37:48.601] iteration 300 : loss : 1078.240723, loss_ce: 0.000000, loss_kd: 5388.575684
[16:37:57.216] iteration 310 : loss : 728.600525, loss_ce: 0.000000, loss_kd: 3640.335693
[16:38:05.817] iteration 320 : loss : 574.480713, loss_ce: 0.000000, loss_kd: 2869.739502
[16:38:14.433] iteration 330 : loss : 794.255676, loss_ce: 0.000000, loss_kd: 3968.585938
[16:38:23.034] iteration 340 : loss : 621.919800, loss_ce: 0.000000, loss_kd: 3107.052246
[16:38:31.648] iteration 350 : loss : 679.853577, loss_ce: 0.000000, loss_kd: 3396.563721
[16:38:40.259] iteration 360 : loss : 694.169312, loss_ce: 0.000000, loss_kd: 3468.269043
[16:38:48.880] iteration 370 : loss : 577.358215, loss_ce: 0.000000, loss_kd: 2884.138916
[16:38:57.489] iteration 380 : loss : 593.502625, loss_ce: 0.000000, loss_kd: 2964.900635
[16:39:06.110] iteration 390 : loss : 895.408203, loss_ce: 0.000000, loss_kd: 4474.474609
[16:39:14.722] iteration 400 : loss : 534.078735, loss_ce: 0.000000, loss_kd: 2667.863525
[16:39:23.344] iteration 410 : loss : 648.071838, loss_ce: 0.000000, loss_kd: 3237.778564
[16:39:31.964] iteration 420 : loss : 583.601990, loss_ce: 0.000000, loss_kd: 2915.411621
[16:39:40.586] iteration 430 : loss : 840.402100, loss_ce: 0.000000, loss_kd: 4199.354492
[16:39:49.197] iteration 440 : loss : 639.992920, loss_ce: 0.000000, loss_kd: 3197.345215
[16:39:57.825] iteration 450 : loss : 712.191772, loss_ce: 0.000000, loss_kd: 3558.369629
[16:40:06.440] iteration 460 : loss : 676.946533, loss_ce: 0.000000, loss_kd: 3382.109863
[16:40:15.068] iteration 470 : loss : 725.358276, loss_ce: 0.000000, loss_kd: 3624.208984
[16:40:23.685] iteration 480 : loss : 527.525391, loss_ce: 0.000000, loss_kd: 2635.031250
[16:40:32.313] iteration 490 : loss : 739.216980, loss_ce: 0.000000, loss_kd: 3693.502930
[16:40:52.817] iteration 500 : loss : 495.817291, loss_ce: 0.000000, loss_kd: 2476.508301
[16:41:01.409] iteration 510 : loss : 565.310547, loss_ce: 0.000000, loss_kd: 2823.977539
[16:41:09.998] iteration 520 : loss : 541.539124, loss_ce: 0.000000, loss_kd: 2705.089355
[16:41:18.599] iteration 530 : loss : 451.286499, loss_ce: 0.000000, loss_kd: 2253.919189
[16:41:27.193] iteration 540 : loss : 828.409241, loss_ce: 0.000000, loss_kd: 4139.524902
[16:41:35.804] iteration 550 : loss : 721.586975, loss_ce: 0.000000, loss_kd: 3605.422852
[16:41:44.403] iteration 560 : loss : 508.058929, loss_ce: 0.000000, loss_kd: 2537.769531
[16:41:53.018] iteration 570 : loss : 563.774536, loss_ce: 0.000000, loss_kd: 2816.365234
[16:42:01.623] iteration 580 : loss : 552.191528, loss_ce: 0.000000, loss_kd: 2758.453613
[16:42:10.237] iteration 590 : loss : 615.845337, loss_ce: 0.000000, loss_kd: 3076.747070
[16:42:18.844] iteration 600 : loss : 674.110840, loss_ce: 0.000000, loss_kd: 3367.984863
[16:42:27.464] iteration 610 : loss : 613.708191, loss_ce: 0.000000, loss_kd: 3065.889160
[16:42:36.075] iteration 620 : loss : 626.688660, loss_ce: 0.000000, loss_kd: 3130.969238
[16:42:44.702] iteration 630 : loss : 581.826477, loss_ce: 0.000000, loss_kd: 2906.627441
[16:42:53.313] iteration 640 : loss : 493.064972, loss_ce: 0.000000, loss_kd: 2462.828857
[16:43:01.943] iteration 650 : loss : 609.752441, loss_ce: 0.000000, loss_kd: 3046.250000
[16:43:10.562] iteration 660 : loss : 499.277466, loss_ce: 0.000000, loss_kd: 2493.839844
[16:43:19.187] iteration 670 : loss : 426.497345, loss_ce: 0.000000, loss_kd: 2130.032227
[16:43:27.813] iteration 680 : loss : 474.766266, loss_ce: 0.000000, loss_kd: 2371.357178
[16:43:36.443] iteration 690 : loss : 498.328583, loss_ce: 0.000000, loss_kd: 2489.144287
[16:43:45.069] iteration 700 : loss : 540.202087, loss_ce: 0.000000, loss_kd: 2698.402588
[16:43:53.704] iteration 710 : loss : 266.744873, loss_ce: 0.000000, loss_kd: 1331.189453
[16:44:02.336] iteration 720 : loss : 570.895935, loss_ce: 0.000000, loss_kd: 2852.005371
[16:44:10.974] iteration 730 : loss : 785.410400, loss_ce: 0.000000, loss_kd: 3924.540527
[16:44:19.607] iteration 740 : loss : 592.122375, loss_ce: 0.000000, loss_kd: 2958.155762
[16:44:28.247] iteration 750 : loss : 466.450684, loss_ce: 0.000000, loss_kd: 2329.770264
[16:44:36.875] iteration 760 : loss : 376.783325, loss_ce: 0.000000, loss_kd: 1881.461670
[16:44:45.513] iteration 770 : loss : 425.256073, loss_ce: 0.000000, loss_kd: 2123.698730
[16:44:54.149] iteration 780 : loss : 467.641510, loss_ce: 0.000000, loss_kd: 2335.663574
[16:45:02.792] iteration 790 : loss : 575.902466, loss_ce: 0.000000, loss_kd: 2877.072998
[16:45:11.430] iteration 800 : loss : 406.280579, loss_ce: 0.000000, loss_kd: 2028.944458
[16:45:20.073] iteration 810 : loss : 387.331207, loss_ce: 0.000000, loss_kd: 1934.141968
[16:45:28.705] iteration 820 : loss : 532.258911, loss_ce: 0.000000, loss_kd: 2658.766602
[16:45:37.352] iteration 830 : loss : 529.637756, loss_ce: 0.000000, loss_kd: 2645.643799
[16:45:45.988] iteration 840 : loss : 682.530151, loss_ce: 0.000000, loss_kd: 3410.165771
[16:45:54.638] iteration 850 : loss : 513.109741, loss_ce: 0.000000, loss_kd: 2563.079346
[16:46:03.279] iteration 860 : loss : 502.719269, loss_ce: 0.000000, loss_kd: 2511.151611
[16:46:11.931] iteration 870 : loss : 392.085266, loss_ce: 0.000000, loss_kd: 1957.939209
[16:46:20.575] iteration 880 : loss : 483.926758, loss_ce: 0.000000, loss_kd: 2417.168701
[16:46:29.231] iteration 890 : loss : 486.291443, loss_ce: 0.000000, loss_kd: 2428.992920
[16:46:37.867] iteration 900 : loss : 346.997742, loss_ce: 0.000000, loss_kd: 1732.481689
[16:46:46.518] iteration 910 : loss : 445.199310, loss_ce: 0.000000, loss_kd: 2223.491943
[16:46:55.161] iteration 920 : loss : 489.887695, loss_ce: 0.000000, loss_kd: 2446.978271
[16:47:03.818] iteration 930 : loss : 385.397644, loss_ce: 0.000000, loss_kd: 1924.515991
[16:47:12.463] iteration 940 : loss : 408.977997, loss_ce: 0.000000, loss_kd: 2042.401123
[16:47:21.115] iteration 950 : loss : 420.042633, loss_ce: 0.000000, loss_kd: 2097.673340
[16:47:29.763] iteration 960 : loss : 455.862946, loss_ce: 0.000000, loss_kd: 2276.780273
[16:47:38.418] iteration 970 : loss : 400.575104, loss_ce: 0.000000, loss_kd: 2000.336548
[16:47:47.064] iteration 980 : loss : 444.800140, loss_ce: 0.000000, loss_kd: 2221.518799
[16:47:55.726] iteration 990 : loss : 488.226990, loss_ce: 0.000000, loss_kd: 2438.660889
[16:48:16.123] iteration 1000 : loss : 496.125946, loss_ce: 0.000000, loss_kd: 2478.133301
[16:48:24.727] iteration 1010 : loss : 542.536377, loss_ce: 0.000000, loss_kd: 2710.189209
[16:48:33.323] iteration 1020 : loss : 516.346619, loss_ce: 0.000000, loss_kd: 2579.276855
[16:48:41.939] iteration 1030 : loss : 342.782837, loss_ce: 0.000000, loss_kd: 1711.468018
[16:48:50.543] iteration 1040 : loss : 416.173157, loss_ce: 0.000000, loss_kd: 2078.361084
[16:48:59.169] iteration 1050 : loss : 421.752991, loss_ce: 0.000000, loss_kd: 2106.273438
[16:49:07.792] iteration 1060 : loss : 352.464905, loss_ce: 0.000000, loss_kd: 1759.848267
[16:49:16.424] iteration 1070 : loss : 441.629486, loss_ce: 0.000000, loss_kd: 2205.647461
[16:49:25.045] iteration 1080 : loss : 411.102173, loss_ce: 0.000000, loss_kd: 2052.959961
[16:49:33.679] iteration 1090 : loss : 463.931396, loss_ce: 0.000000, loss_kd: 2317.149414
[16:49:42.308] iteration 1100 : loss : 322.174377, loss_ce: 0.000000, loss_kd: 1608.364624
[16:49:50.951] iteration 1110 : loss : 423.316650, loss_ce: 0.000000, loss_kd: 2114.085205
[16:49:59.587] iteration 1120 : loss : 311.929779, loss_ce: 0.000000, loss_kd: 1557.156006
[16:50:08.228] iteration 1130 : loss : 369.373718, loss_ce: 0.000000, loss_kd: 1844.296021
[16:50:16.865] iteration 1140 : loss : 412.876831, loss_ce: 0.000000, loss_kd: 2061.880859
[16:50:25.506] iteration 1150 : loss : 365.714355, loss_ce: 0.000000, loss_kd: 1826.029175
[16:50:34.147] iteration 1160 : loss : 405.602142, loss_ce: 0.000000, loss_kd: 2025.476562
[16:50:42.795] iteration 1170 : loss : 356.450745, loss_ce: 0.000000, loss_kd: 1779.777832
[16:50:51.432] iteration 1180 : loss : 390.984314, loss_ce: 0.000000, loss_kd: 1952.449951
[16:51:00.084] iteration 1190 : loss : 458.398560, loss_ce: 0.000000, loss_kd: 2289.479492
[16:51:08.722] iteration 1200 : loss : 436.710266, loss_ce: 0.000000, loss_kd: 2181.073975
[16:51:17.383] iteration 1210 : loss : 296.640747, loss_ce: 0.000000, loss_kd: 1480.700806
[16:51:26.028] iteration 1220 : loss : 364.430664, loss_ce: 0.000000, loss_kd: 1819.655640
[16:51:34.685] iteration 1230 : loss : 303.646515, loss_ce: 0.000000, loss_kd: 1515.748169
[16:51:43.335] iteration 1240 : loss : 458.177460, loss_ce: 0.000000, loss_kd: 2288.396484
[16:51:51.994] iteration 1250 : loss : 370.148560, loss_ce: 0.000000, loss_kd: 1848.148926
[16:52:00.639] iteration 1260 : loss : 377.569244, loss_ce: 0.000000, loss_kd: 1885.431396
[16:52:09.298] iteration 1270 : loss : 378.812958, loss_ce: 0.000000, loss_kd: 1891.555420
[16:52:17.940] iteration 1280 : loss : 345.590424, loss_ce: 0.000000, loss_kd: 1725.505615
[16:52:26.593] iteration 1290 : loss : 370.223724, loss_ce: 0.000000, loss_kd: 1848.620361
[16:52:35.239] iteration 1300 : loss : 365.516876, loss_ce: 0.000000, loss_kd: 1825.085449
[16:52:43.892] iteration 1310 : loss : 369.682037, loss_ce: 0.000000, loss_kd: 1845.939697
[16:52:52.535] iteration 1320 : loss : 415.391998, loss_ce: 0.000000, loss_kd: 2074.453857
[16:53:01.186] iteration 1330 : loss : 406.475891, loss_ce: 0.000000, loss_kd: 2029.899902
[16:53:09.835] iteration 1340 : loss : 375.044281, loss_ce: 0.000000, loss_kd: 1872.714966
[16:53:18.494] iteration 1350 : loss : 579.798950, loss_ce: 0.000000, loss_kd: 2896.509277
[16:53:27.143] iteration 1360 : loss : 472.296326, loss_ce: 0.000000, loss_kd: 2359.050293
[16:53:35.802] iteration 1370 : loss : 301.714691, loss_ce: 0.000000, loss_kd: 1506.086060
[16:53:44.447] iteration 1380 : loss : 345.405487, loss_ce: 0.000000, loss_kd: 1724.515625
[16:53:53.100] iteration 1390 : loss : 309.454437, loss_ce: 0.000000, loss_kd: 1544.805054
[16:54:01.744] iteration 1400 : loss : 609.437927, loss_ce: 0.000000, loss_kd: 3044.692871
[16:54:10.405] iteration 1410 : loss : 524.958801, loss_ce: 0.000000, loss_kd: 2622.264648
[16:54:19.053] iteration 1420 : loss : 341.005524, loss_ce: 0.000000, loss_kd: 1702.564697
[16:54:27.705] iteration 1430 : loss : 480.457703, loss_ce: 0.000000, loss_kd: 2399.765869
[16:54:36.348] iteration 1440 : loss : 408.692596, loss_ce: 0.000000, loss_kd: 2040.993774
[16:54:44.999] iteration 1450 : loss : 397.274567, loss_ce: 0.000000, loss_kd: 1983.871582
[16:54:53.645] iteration 1460 : loss : 311.711945, loss_ce: 0.000000, loss_kd: 1556.049072
[16:55:02.303] iteration 1470 : loss : 282.271698, loss_ce: 0.000000, loss_kd: 1408.875977
[16:55:10.948] iteration 1480 : loss : 254.695526, loss_ce: 0.000000, loss_kd: 1270.963501
[16:55:31.233] iteration 1490 : loss : 336.900909, loss_ce: 0.000000, loss_kd: 1681.986938
[16:55:39.819] iteration 1500 : loss : 353.783447, loss_ce: 0.000000, loss_kd: 1766.417969
[16:55:48.425] iteration 1510 : loss : 334.763794, loss_ce: 0.000000, loss_kd: 1671.357666
[16:55:57.027] iteration 1520 : loss : 387.637756, loss_ce: 0.000000, loss_kd: 1935.716064
[16:56:05.642] iteration 1530 : loss : 261.151611, loss_ce: 0.000000, loss_kd: 1303.270752
[16:56:14.250] iteration 1540 : loss : 309.670288, loss_ce: 0.000000, loss_kd: 1545.878906
[16:56:22.878] iteration 1550 : loss : 374.533386, loss_ce: 0.000000, loss_kd: 1870.135620
[16:56:31.490] iteration 1560 : loss : 377.486786, loss_ce: 0.000000, loss_kd: 1884.922974
[16:56:40.120] iteration 1570 : loss : 319.479218, loss_ce: 0.000000, loss_kd: 1594.907715
[16:56:48.744] iteration 1580 : loss : 356.146454, loss_ce: 0.000000, loss_kd: 1778.266724
[16:56:57.378] iteration 1590 : loss : 383.922638, loss_ce: 0.000000, loss_kd: 1917.130737
[16:57:06.014] iteration 1600 : loss : 482.853394, loss_ce: 0.000000, loss_kd: 2411.795410
[16:57:14.654] iteration 1610 : loss : 323.634064, loss_ce: 0.000000, loss_kd: 1615.738770
[16:57:23.285] iteration 1620 : loss : 292.349365, loss_ce: 0.000000, loss_kd: 1459.257812
[16:57:31.928] iteration 1630 : loss : 358.443451, loss_ce: 0.000000, loss_kd: 1789.746826
[16:57:40.561] iteration 1640 : loss : 326.118530, loss_ce: 0.000000, loss_kd: 1628.075806
[16:57:49.201] iteration 1650 : loss : 389.723297, loss_ce: 0.000000, loss_kd: 1946.142090
[16:57:57.834] iteration 1660 : loss : 288.463379, loss_ce: 0.000000, loss_kd: 1439.846069
[16:58:06.479] iteration 1670 : loss : 502.254944, loss_ce: 0.000000, loss_kd: 2508.841309
[16:58:15.114] iteration 1680 : loss : 285.025360, loss_ce: 0.000000, loss_kd: 1422.607422
[16:58:23.762] iteration 1690 : loss : 321.656219, loss_ce: 0.000000, loss_kd: 1605.781494
[16:58:32.392] iteration 1700 : loss : 402.580139, loss_ce: 0.000000, loss_kd: 2010.430786
[16:58:41.034] iteration 1710 : loss : 343.325195, loss_ce: 0.000000, loss_kd: 1714.120728
[16:58:49.668] iteration 1720 : loss : 399.107605, loss_ce: 0.000000, loss_kd: 1993.054321
[16:58:58.309] iteration 1730 : loss : 273.581940, loss_ce: 0.000000, loss_kd: 1365.441528
[16:59:06.949] iteration 1740 : loss : 411.594818, loss_ce: 0.000000, loss_kd: 2055.501953
[16:59:15.596] iteration 1750 : loss : 354.029114, loss_ce: 0.000000, loss_kd: 1767.665039
[16:59:24.231] iteration 1760 : loss : 405.326172, loss_ce: 0.000000, loss_kd: 2024.124634
[16:59:32.874] iteration 1770 : loss : 362.788361, loss_ce: 0.000000, loss_kd: 1811.501953
[16:59:41.509] iteration 1780 : loss : 267.539917, loss_ce: 0.000000, loss_kd: 1335.225830
[16:59:50.161] iteration 1790 : loss : 383.784210, loss_ce: 0.000000, loss_kd: 1916.451782
[16:59:58.801] iteration 1800 : loss : 330.246185, loss_ce: 0.000000, loss_kd: 1648.696533
[17:00:07.443] iteration 1810 : loss : 309.193390, loss_ce: 0.000000, loss_kd: 1543.537109
[17:00:16.079] iteration 1820 : loss : 432.541168, loss_ce: 0.000000, loss_kd: 2160.226318
[17:00:24.721] iteration 1830 : loss : 307.088226, loss_ce: 0.000000, loss_kd: 1532.980591
[17:00:33.358] iteration 1840 : loss : 360.753906, loss_ce: 0.000000, loss_kd: 1801.249390
[17:00:42.006] iteration 1850 : loss : 407.579163, loss_ce: 0.000000, loss_kd: 2035.384766
[17:00:50.641] iteration 1860 : loss : 318.797729, loss_ce: 0.000000, loss_kd: 1591.470947
[17:00:59.290] iteration 1870 : loss : 298.703156, loss_ce: 0.000000, loss_kd: 1491.057495
[17:01:07.924] iteration 1880 : loss : 287.802155, loss_ce: 0.000000, loss_kd: 1436.531738
[17:01:16.571] iteration 1890 : loss : 438.237610, loss_ce: 0.000000, loss_kd: 2188.712891
[17:01:25.199] iteration 1900 : loss : 347.987762, loss_ce: 0.000000, loss_kd: 1737.476074
[17:01:33.845] iteration 1910 : loss : 344.613953, loss_ce: 0.000000, loss_kd: 1720.580078
[17:01:42.476] iteration 1920 : loss : 313.047424, loss_ce: 0.000000, loss_kd: 1562.709839
[17:01:51.118] iteration 1930 : loss : 318.277740, loss_ce: 0.000000, loss_kd: 1588.888428
[17:01:59.747] iteration 1940 : loss : 321.708191, loss_ce: 0.000000, loss_kd: 1606.091309
[17:02:08.391] iteration 1950 : loss : 268.209869, loss_ce: 0.000000, loss_kd: 1338.595337
[17:02:17.022] iteration 1960 : loss : 368.912323, loss_ce: 0.000000, loss_kd: 1842.110962
[17:02:25.664] iteration 1970 : loss : 325.086121, loss_ce: 0.000000, loss_kd: 1622.966064
[17:02:34.299] iteration 1980 : loss : 278.561920, loss_ce: 0.000000, loss_kd: 1390.371338
[17:02:54.929] iteration 1990 : loss : 432.833069, loss_ce: 0.000000, loss_kd: 2161.693359
[17:03:03.512] iteration 2000 : loss : 281.737885, loss_ce: 0.000000, loss_kd: 1406.232056
[17:03:12.115] iteration 2010 : loss : 278.106689, loss_ce: 0.000000, loss_kd: 1388.085449
[17:03:20.709] iteration 2020 : loss : 354.105164, loss_ce: 0.000000, loss_kd: 1768.040039
[17:03:29.314] iteration 2030 : loss : 417.911346, loss_ce: 0.000000, loss_kd: 2087.096924
[17:03:37.911] iteration 2040 : loss : 331.686035, loss_ce: 0.000000, loss_kd: 1655.965942
[17:03:46.526] iteration 2050 : loss : 270.737885, loss_ce: 0.000000, loss_kd: 1351.259399
[17:03:55.136] iteration 2060 : loss : 312.295593, loss_ce: 0.000000, loss_kd: 1558.974243
[17:04:03.751] iteration 2070 : loss : 345.513336, loss_ce: 0.000000, loss_kd: 1725.091797
[17:04:12.363] iteration 2080 : loss : 384.674713, loss_ce: 0.000000, loss_kd: 1920.940063
[17:04:20.985] iteration 2090 : loss : 274.338013, loss_ce: 0.000000, loss_kd: 1369.254883
[17:04:29.593] iteration 2100 : loss : 206.799011, loss_ce: 0.000000, loss_kd: 1031.526611
[17:04:38.219] iteration 2110 : loss : 342.579620, loss_ce: 0.000000, loss_kd: 1710.463989
[17:04:46.834] iteration 2120 : loss : 313.992767, loss_ce: 0.000000, loss_kd: 1567.501831
[17:04:55.461] iteration 2130 : loss : 316.504852, loss_ce: 0.000000, loss_kd: 1580.080933
[17:05:04.080] iteration 2140 : loss : 330.664917, loss_ce: 0.000000, loss_kd: 1650.841187
[17:05:12.713] iteration 2150 : loss : 348.778076, loss_ce: 0.000000, loss_kd: 1741.439331
[17:05:21.332] iteration 2160 : loss : 326.347839, loss_ce: 0.000000, loss_kd: 1629.284302
[17:05:29.970] iteration 2170 : loss : 296.327515, loss_ce: 0.000000, loss_kd: 1479.184814
[17:05:38.586] iteration 2180 : loss : 261.933167, loss_ce: 0.000000, loss_kd: 1307.236450
[17:05:47.219] iteration 2190 : loss : 250.743027, loss_ce: 0.000000, loss_kd: 1251.080444
[17:05:55.841] iteration 2200 : loss : 260.170502, loss_ce: 0.000000, loss_kd: 1298.394897
[17:06:04.476] iteration 2210 : loss : 343.767303, loss_ce: 0.000000, loss_kd: 1716.395996
[17:06:13.098] iteration 2220 : loss : 369.688873, loss_ce: 0.000000, loss_kd: 1845.966797
[17:06:21.734] iteration 2230 : loss : 346.336365, loss_ce: 0.000000, loss_kd: 1729.193115
[17:06:30.360] iteration 2240 : loss : 356.430573, loss_ce: 0.000000, loss_kd: 1779.684937
[17:06:39.000] iteration 2250 : loss : 253.014664, loss_ce: 0.000000, loss_kd: 1262.618286
[17:06:47.624] iteration 2260 : loss : 216.766891, loss_ce: 0.000000, loss_kd: 1081.355469
[17:06:56.262] iteration 2270 : loss : 218.139984, loss_ce: 0.000000, loss_kd: 1088.235352
[17:07:04.889] iteration 2280 : loss : 299.076813, loss_ce: 0.000000, loss_kd: 1492.894287
[17:07:13.527] iteration 2290 : loss : 373.762360, loss_ce: 0.000000, loss_kd: 1866.358887
[17:07:22.156] iteration 2300 : loss : 349.627655, loss_ce: 0.000000, loss_kd: 1745.660278
[17:07:30.795] iteration 2310 : loss : 248.204758, loss_ce: 0.000000, loss_kd: 1238.581299
[17:07:39.426] iteration 2320 : loss : 286.537323, loss_ce: 0.000000, loss_kd: 1430.211426
[17:07:48.068] iteration 2330 : loss : 266.197052, loss_ce: 0.000000, loss_kd: 1328.508667
[17:07:56.701] iteration 2340 : loss : 297.930481, loss_ce: 0.000000, loss_kd: 1487.193848
[17:08:05.339] iteration 2350 : loss : 224.415146, loss_ce: 0.000000, loss_kd: 1119.639771
[17:08:13.961] iteration 2360 : loss : 395.777496, loss_ce: 0.000000, loss_kd: 1976.415527
[17:08:22.604] iteration 2370 : loss : 205.384201, loss_ce: 0.000000, loss_kd: 1024.498413
[17:08:31.238] iteration 2380 : loss : 314.983795, loss_ce: 0.000000, loss_kd: 1572.432861
[17:08:39.880] iteration 2390 : loss : 367.386597, loss_ce: 0.000000, loss_kd: 1834.489136
[17:08:48.510] iteration 2400 : loss : 331.018585, loss_ce: 0.000000, loss_kd: 1652.610596
[17:08:57.156] iteration 2410 : loss : 465.587494, loss_ce: 0.000000, loss_kd: 2325.490234
[17:09:05.788] iteration 2420 : loss : 245.194031, loss_ce: 0.000000, loss_kd: 1223.479492
[17:09:14.439] iteration 2430 : loss : 296.050537, loss_ce: 0.000000, loss_kd: 1477.801636
[17:09:23.074] iteration 2440 : loss : 301.696991, loss_ce: 0.000000, loss_kd: 1506.050903
[17:09:31.718] iteration 2450 : loss : 245.788300, loss_ce: 0.000000, loss_kd: 1226.527344
[17:09:40.356] iteration 2460 : loss : 318.994843, loss_ce: 0.000000, loss_kd: 1592.518799
[17:09:49.004] iteration 2470 : loss : 271.407135, loss_ce: 0.000000, loss_kd: 1354.601562
[17:09:57.580] iteration 2480 : loss : 238.418839, loss_ce: 0.000000, loss_kd: 1189.633667
[17:09:58.390] save model to ./debug_simple\continual_surgical_tpgm_epoch_4.pth
[17:09:58.479] save final model to ./debug_simple\continual_surgical_tpgm_final.pth
[17:46:27.332] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes_old=9, num_classes_new=4, output_dir='./debug_simple', max_iterations=10000, max_epochs=5, batch_size=48, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.25, kd_temperature=3.0, kd_weight=0.2, freeze_old_classes=False, auto_tune='none', gradient_batches=5, tpgm_norm_mode='l2', tpgm_lr=0.01, tpgm_iters=200, tpgm_exclude=[], tpgm_frequency=5, tpgm_start_epoch=10, disable_tpgm=True, tpgm_data_fraction=0.1, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[17:46:27.346] Using 23805/95221 samples (25.0%) for continual learning
[17:46:27.346] Old classes: 9, New classes: 4, Total: 12
[17:46:27.347] TPGM enabled: False
[17:46:27.347] Surgical fine-tuning method: none
[19:14:30.106] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes_old=9, num_classes_new=4, output_dir='./debug_simple', max_iterations=10000, max_epochs=5, batch_size=64, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.25, kd_temperature=3.0, kd_weight=0.2, freeze_old_classes=False, auto_tune='none', gradient_batches=5, tpgm_norm_mode='l2', tpgm_lr=0.01, tpgm_iters=200, tpgm_exclude=[], tpgm_frequency=5, tpgm_start_epoch=10, disable_tpgm=True, tpgm_data_fraction=0.1, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[19:14:30.120] Using 23805/95221 samples (25.0%) for continual learning
[19:14:30.120] Old classes: 9, New classes: 4, Total: 12
[19:14:30.121] TPGM enabled: False
[19:14:30.121] Surgical fine-tuning method: none
[19:16:06.265] Combined Continual Learning + Surgical + TPGM Configuration:
[19:16:06.265] KD Temperature: 3.0
[19:16:06.265] KD Weight: 0.2
[19:16:06.265] Auto-tune method: none
[19:16:06.266] TPGM start epoch: 10
[19:16:06.266] TPGM frequency: 5
[19:16:06.266] 372 iterations per epoch. 1860 max iterations 
[19:37:37.234] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes_old=9, num_classes_new=4, output_dir='./debug_simple', max_iterations=10000, max_epochs=5, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.25, kd_temperature=3.0, kd_weight=0.2, freeze_old_classes=False, auto_tune='none', gradient_batches=5, tpgm_norm_mode='l2', tpgm_lr=0.01, tpgm_iters=200, tpgm_exclude=[], tpgm_frequency=5, tpgm_start_epoch=10, disable_tpgm=True, tpgm_data_fraction=0.1, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[19:37:37.248] Using 23805/95221 samples (25.0%) for continual learning
[19:37:37.248] Old classes: 9, New classes: 4, Total: 12
[19:37:37.248] TPGM enabled: False
[19:37:37.248] Surgical fine-tuning method: none
[19:39:11.047] Combined Continual Learning + Surgical + TPGM Configuration:
[19:39:11.047] KD Temperature: 3.0
[19:39:11.047] KD Weight: 0.2
[19:39:11.047] Auto-tune method: none
[19:39:11.047] TPGM start epoch: 10
[19:39:11.047] TPGM frequency: 5
[19:39:11.047] 744 iterations per epoch. 3720 max iterations 
[19:39:27.937] iteration 10 : loss : 7015.471191, loss_ce: 0.000000, loss_kd: 35075.000000
[19:39:33.407] iteration 20 : loss : 4159.143066, loss_ce: 0.000000, loss_kd: 20793.044922
[19:39:38.911] iteration 30 : loss : 2988.801514, loss_ce: 0.000000, loss_kd: 14941.113281
[19:39:44.395] iteration 40 : loss : 2167.731201, loss_ce: 0.000000, loss_kd: 10835.940430
[19:39:49.906] iteration 50 : loss : 1745.312256, loss_ce: 0.000000, loss_kd: 8723.870117
[19:39:55.398] iteration 60 : loss : 2133.284668, loss_ce: 0.000000, loss_kd: 10663.720703
[19:40:00.912] iteration 70 : loss : 1706.437866, loss_ce: 0.000000, loss_kd: 8529.489258
[19:40:06.413] iteration 80 : loss : 2194.049561, loss_ce: 0.000000, loss_kd: 10967.543945
[19:40:11.925] iteration 90 : loss : 1547.873413, loss_ce: 0.000000, loss_kd: 7736.676270
[19:40:17.429] iteration 100 : loss : 1449.585083, loss_ce: 0.000000, loss_kd: 7245.209473
[19:40:22.948] iteration 110 : loss : 1450.015381, loss_ce: 0.000000, loss_kd: 7247.395020
[19:40:28.452] iteration 120 : loss : 1357.229248, loss_ce: 0.000000, loss_kd: 6783.457031
[19:40:33.973] iteration 130 : loss : 1206.776123, loss_ce: 0.000000, loss_kd: 6031.166992
[19:40:39.482] iteration 140 : loss : 1545.958130, loss_ce: 0.000000, loss_kd: 7727.034668
[19:40:45.005] iteration 150 : loss : 1266.109009, loss_ce: 0.000000, loss_kd: 6327.901367
[19:40:50.517] iteration 160 : loss : 1026.711792, loss_ce: 0.000000, loss_kd: 5131.000000
[19:40:56.046] iteration 170 : loss : 1943.685669, loss_ce: 0.000000, loss_kd: 9715.824219
[19:41:01.559] iteration 180 : loss : 1024.047119, loss_ce: 0.000000, loss_kd: 5117.693359
[19:41:07.084] iteration 190 : loss : 1070.793335, loss_ce: 0.000000, loss_kd: 5351.402344
[19:41:12.603] iteration 200 : loss : 1584.625610, loss_ce: 0.000000, loss_kd: 7920.552734
[19:41:18.135] iteration 210 : loss : 1130.968628, loss_ce: 0.000000, loss_kd: 5652.241211
[19:41:23.658] iteration 220 : loss : 1075.979614, loss_ce: 0.000000, loss_kd: 5377.284180
[19:41:29.189] iteration 230 : loss : 990.980469, loss_ce: 0.000000, loss_kd: 4952.339844
[19:41:34.734] iteration 240 : loss : 1069.126953, loss_ce: 0.000000, loss_kd: 5343.133301
[19:41:40.280] iteration 250 : loss : 996.790344, loss_ce: 0.000000, loss_kd: 4981.285156
[19:41:45.806] iteration 260 : loss : 1265.849487, loss_ce: 0.000000, loss_kd: 6326.656250
[19:41:51.344] iteration 270 : loss : 982.627441, loss_ce: 0.000000, loss_kd: 4910.457031
[19:41:56.872] iteration 280 : loss : 861.924438, loss_ce: 0.000000, loss_kd: 4307.052246
[19:42:02.415] iteration 290 : loss : 928.732056, loss_ce: 0.000000, loss_kd: 4641.134766
[19:42:07.944] iteration 300 : loss : 862.098938, loss_ce: 0.000000, loss_kd: 4307.928711
[19:42:13.489] iteration 310 : loss : 900.544800, loss_ce: 0.000000, loss_kd: 4500.178223
[19:42:19.032] iteration 320 : loss : 720.891602, loss_ce: 0.000000, loss_kd: 3601.951416
[19:42:24.578] iteration 330 : loss : 735.732788, loss_ce: 0.000000, loss_kd: 3676.156250
[19:42:30.115] iteration 340 : loss : 883.256897, loss_ce: 0.000000, loss_kd: 4413.752441
[19:42:35.661] iteration 350 : loss : 913.324097, loss_ce: 0.000000, loss_kd: 4564.091797
[19:42:41.195] iteration 360 : loss : 594.543457, loss_ce: 0.000000, loss_kd: 2970.268311
[19:42:46.739] iteration 370 : loss : 809.342285, loss_ce: 0.000000, loss_kd: 4044.209961
[19:42:52.282] iteration 380 : loss : 826.889709, loss_ce: 0.000000, loss_kd: 4131.907227
[19:42:57.830] iteration 390 : loss : 933.844482, loss_ce: 0.000000, loss_kd: 4666.750488
[19:43:03.365] iteration 400 : loss : 644.493652, loss_ce: 0.000000, loss_kd: 3220.030762
[19:43:08.924] iteration 410 : loss : 663.570557, loss_ce: 0.000000, loss_kd: 3315.369141
[19:43:14.463] iteration 420 : loss : 461.696930, loss_ce: 0.000000, loss_kd: 2305.975342
[19:43:20.012] iteration 430 : loss : 606.949951, loss_ce: 0.000000, loss_kd: 3032.272217
[19:43:25.556] iteration 440 : loss : 901.013733, loss_ce: 0.000000, loss_kd: 4502.567871
[19:43:31.109] iteration 450 : loss : 718.596802, loss_ce: 0.000000, loss_kd: 3590.501221
[19:43:36.652] iteration 460 : loss : 699.324890, loss_ce: 0.000000, loss_kd: 3494.177002
[19:43:42.208] iteration 470 : loss : 700.145081, loss_ce: 0.000000, loss_kd: 3498.236328
[19:43:47.747] iteration 480 : loss : 765.663208, loss_ce: 0.000000, loss_kd: 3825.812012
[19:43:53.308] iteration 490 : loss : 618.800293, loss_ce: 0.000000, loss_kd: 3091.479980
[19:43:58.852] iteration 500 : loss : 754.578003, loss_ce: 0.000000, loss_kd: 3770.381836
[19:44:04.408] iteration 510 : loss : 790.454407, loss_ce: 0.000000, loss_kd: 3949.773926
[19:44:09.952] iteration 520 : loss : 608.944397, loss_ce: 0.000000, loss_kd: 3042.255859
[19:44:15.509] iteration 530 : loss : 682.032837, loss_ce: 0.000000, loss_kd: 3407.722412
[19:44:21.053] iteration 540 : loss : 697.250793, loss_ce: 0.000000, loss_kd: 3483.781250
[19:44:26.613] iteration 550 : loss : 645.938354, loss_ce: 0.000000, loss_kd: 3227.209961
[19:44:32.157] iteration 560 : loss : 620.264099, loss_ce: 0.000000, loss_kd: 3098.857178
[19:44:37.712] iteration 570 : loss : 559.707642, loss_ce: 0.000000, loss_kd: 2796.093018
[19:44:43.257] iteration 580 : loss : 933.841675, loss_ce: 0.000000, loss_kd: 4666.707031
[19:44:48.815] iteration 590 : loss : 576.804626, loss_ce: 0.000000, loss_kd: 2881.469727
[19:44:54.359] iteration 600 : loss : 608.319824, loss_ce: 0.000000, loss_kd: 3039.118896
[19:44:59.924] iteration 610 : loss : 571.871277, loss_ce: 0.000000, loss_kd: 2856.897705
[19:45:05.476] iteration 620 : loss : 552.759888, loss_ce: 0.000000, loss_kd: 2761.310791
[19:45:11.039] iteration 630 : loss : 489.289185, loss_ce: 0.000000, loss_kd: 2443.978027
[19:45:16.584] iteration 640 : loss : 656.411499, loss_ce: 0.000000, loss_kd: 3279.632568
[19:45:22.150] iteration 650 : loss : 777.404541, loss_ce: 0.000000, loss_kd: 3884.574463
[19:45:27.702] iteration 660 : loss : 548.591492, loss_ce: 0.000000, loss_kd: 2740.493652
[19:45:33.264] iteration 670 : loss : 536.725098, loss_ce: 0.000000, loss_kd: 2681.076660
[19:45:38.818] iteration 680 : loss : 533.861572, loss_ce: 0.000000, loss_kd: 2666.805908
[19:45:44.381] iteration 690 : loss : 604.289612, loss_ce: 0.000000, loss_kd: 3018.976074
[19:45:49.933] iteration 700 : loss : 727.142212, loss_ce: 0.000000, loss_kd: 3633.249756
[19:45:55.502] iteration 710 : loss : 747.572693, loss_ce: 0.000000, loss_kd: 3735.415283
[19:46:01.055] iteration 720 : loss : 457.330658, loss_ce: 0.000000, loss_kd: 2284.207764
[19:46:06.619] iteration 730 : loss : 612.349243, loss_ce: 0.000000, loss_kd: 3059.321533
[19:46:12.175] iteration 740 : loss : 789.240662, loss_ce: 0.000000, loss_kd: 3943.643555
[19:46:29.560] iteration 750 : loss : 471.024841, loss_ce: 0.000000, loss_kd: 2352.647949
[19:46:35.086] iteration 760 : loss : 428.314514, loss_ce: 0.000000, loss_kd: 2139.085693
[19:46:40.625] iteration 770 : loss : 785.056519, loss_ce: 0.000000, loss_kd: 3922.776611
[19:46:46.155] iteration 780 : loss : 537.388733, loss_ce: 0.000000, loss_kd: 2684.462891
[19:46:51.698] iteration 790 : loss : 661.683838, loss_ce: 0.000000, loss_kd: 3305.932129
[19:46:57.227] iteration 800 : loss : 585.477966, loss_ce: 0.000000, loss_kd: 2924.861084
[19:47:02.775] iteration 810 : loss : 553.201843, loss_ce: 0.000000, loss_kd: 2763.583496
[19:47:08.312] iteration 820 : loss : 537.717896, loss_ce: 0.000000, loss_kd: 2686.105957
[19:47:13.863] iteration 830 : loss : 548.894958, loss_ce: 0.000000, loss_kd: 2742.012695
[19:47:19.401] iteration 840 : loss : 901.685242, loss_ce: 0.000000, loss_kd: 4505.928711
[19:47:24.956] iteration 850 : loss : 591.220459, loss_ce: 0.000000, loss_kd: 2953.657471
[19:47:30.500] iteration 860 : loss : 487.081696, loss_ce: 0.000000, loss_kd: 2432.925781
[19:47:36.063] iteration 870 : loss : 565.903870, loss_ce: 0.000000, loss_kd: 2827.042480
[19:47:41.614] iteration 880 : loss : 492.921906, loss_ce: 0.000000, loss_kd: 2462.155762
[19:47:47.183] iteration 890 : loss : 498.973969, loss_ce: 0.000000, loss_kd: 2492.381348
[19:47:52.729] iteration 900 : loss : 379.070801, loss_ce: 0.000000, loss_kd: 1892.906860
[19:47:58.303] iteration 910 : loss : 526.148499, loss_ce: 0.000000, loss_kd: 2628.267578
[19:48:03.864] iteration 920 : loss : 580.097656, loss_ce: 0.000000, loss_kd: 2898.009277
[19:48:09.433] iteration 930 : loss : 808.741455, loss_ce: 0.000000, loss_kd: 4041.203613
[19:48:14.985] iteration 940 : loss : 818.078735, loss_ce: 0.000000, loss_kd: 4087.935303
[19:48:20.545] iteration 950 : loss : 683.273071, loss_ce: 0.000000, loss_kd: 3413.907471
[19:48:26.100] iteration 960 : loss : 477.163605, loss_ce: 0.000000, loss_kd: 2383.371094
[19:48:31.661] iteration 970 : loss : 542.767029, loss_ce: 0.000000, loss_kd: 2711.334229
[19:48:37.217] iteration 980 : loss : 574.150879, loss_ce: 0.000000, loss_kd: 2868.288574
[19:48:42.785] iteration 990 : loss : 546.568542, loss_ce: 0.000000, loss_kd: 2730.355713
[19:48:48.341] iteration 1000 : loss : 524.518677, loss_ce: 0.000000, loss_kd: 2620.107910
[19:48:53.908] iteration 1010 : loss : 542.217957, loss_ce: 0.000000, loss_kd: 2708.637939
[19:48:59.461] iteration 1020 : loss : 577.906311, loss_ce: 0.000000, loss_kd: 2887.055664
[19:49:05.027] iteration 1030 : loss : 499.482117, loss_ce: 0.000000, loss_kd: 2494.928223
[19:49:10.587] iteration 1040 : loss : 521.041260, loss_ce: 0.000000, loss_kd: 2602.762207
[19:49:16.162] iteration 1050 : loss : 741.181274, loss_ce: 0.000000, loss_kd: 3703.501953
[19:49:21.721] iteration 1060 : loss : 664.987305, loss_ce: 0.000000, loss_kd: 3322.449707
[19:49:27.292] iteration 1070 : loss : 462.440369, loss_ce: 0.000000, loss_kd: 2309.809570
[19:49:32.851] iteration 1080 : loss : 489.242615, loss_ce: 0.000000, loss_kd: 2443.805420
[19:49:38.428] iteration 1090 : loss : 495.920441, loss_ce: 0.000000, loss_kd: 2477.147217
[19:49:43.988] iteration 1100 : loss : 368.949829, loss_ce: 0.000000, loss_kd: 1842.271118
[19:49:49.557] iteration 1110 : loss : 509.454865, loss_ce: 0.000000, loss_kd: 2544.881836
[19:49:55.113] iteration 1120 : loss : 482.255585, loss_ce: 0.000000, loss_kd: 2408.809570
[19:50:00.686] iteration 1130 : loss : 402.392883, loss_ce: 0.000000, loss_kd: 2009.477173
[19:50:06.252] iteration 1140 : loss : 473.602142, loss_ce: 0.000000, loss_kd: 2365.511230
[19:50:11.842] iteration 1150 : loss : 453.678345, loss_ce: 0.000000, loss_kd: 2265.941895
[19:50:17.405] iteration 1160 : loss : 401.800415, loss_ce: 0.000000, loss_kd: 2006.547852
[19:50:22.980] iteration 1170 : loss : 426.144897, loss_ce: 0.000000, loss_kd: 2128.244385
[19:50:28.543] iteration 1180 : loss : 332.304749, loss_ce: 0.000000, loss_kd: 1659.013550
[19:50:34.118] iteration 1190 : loss : 547.114075, loss_ce: 0.000000, loss_kd: 2733.116943
[19:50:39.677] iteration 1200 : loss : 591.208313, loss_ce: 0.000000, loss_kd: 2953.604492
[19:50:45.251] iteration 1210 : loss : 482.244934, loss_ce: 0.000000, loss_kd: 2408.748779
[19:50:50.816] iteration 1220 : loss : 464.018951, loss_ce: 0.000000, loss_kd: 2317.649658
[19:50:56.389] iteration 1230 : loss : 580.517822, loss_ce: 0.000000, loss_kd: 2900.111572
[19:51:01.954] iteration 1240 : loss : 501.407928, loss_ce: 0.000000, loss_kd: 2504.638184
[19:51:07.531] iteration 1250 : loss : 467.918518, loss_ce: 0.000000, loss_kd: 2337.122070
[19:51:13.088] iteration 1260 : loss : 504.864624, loss_ce: 0.000000, loss_kd: 2521.850098
[19:51:18.665] iteration 1270 : loss : 462.750549, loss_ce: 0.000000, loss_kd: 2311.293945
[19:51:24.226] iteration 1280 : loss : 474.483307, loss_ce: 0.000000, loss_kd: 2369.952148
[19:51:29.797] iteration 1290 : loss : 659.283386, loss_ce: 0.000000, loss_kd: 3293.945312
[19:51:35.364] iteration 1300 : loss : 448.695740, loss_ce: 0.000000, loss_kd: 2240.980469
[19:51:40.938] iteration 1310 : loss : 574.106689, loss_ce: 0.000000, loss_kd: 2868.088623
[19:51:46.506] iteration 1320 : loss : 545.162964, loss_ce: 0.000000, loss_kd: 2723.321777
[19:51:52.083] iteration 1330 : loss : 439.420013, loss_ce: 0.000000, loss_kd: 2194.631836
[19:51:57.646] iteration 1340 : loss : 536.911255, loss_ce: 0.000000, loss_kd: 2682.091309
[19:52:03.225] iteration 1350 : loss : 522.453857, loss_ce: 0.000000, loss_kd: 2609.840088
[19:52:08.785] iteration 1360 : loss : 492.898865, loss_ce: 0.000000, loss_kd: 2462.072998
[19:52:14.362] iteration 1370 : loss : 675.263611, loss_ce: 0.000000, loss_kd: 3373.854736
[19:52:19.930] iteration 1380 : loss : 597.760437, loss_ce: 0.000000, loss_kd: 2986.341553
[19:52:25.503] iteration 1390 : loss : 441.148743, loss_ce: 0.000000, loss_kd: 2203.291504
[19:52:31.076] iteration 1400 : loss : 391.828949, loss_ce: 0.000000, loss_kd: 1956.657227
[19:52:36.644] iteration 1410 : loss : 371.674194, loss_ce: 0.000000, loss_kd: 1855.844971
[19:52:42.212] iteration 1420 : loss : 367.355133, loss_ce: 0.000000, loss_kd: 1834.266479
[19:52:47.787] iteration 1430 : loss : 469.349365, loss_ce: 0.000000, loss_kd: 2344.276855
[19:52:53.349] iteration 1440 : loss : 445.625275, loss_ce: 0.000000, loss_kd: 2225.702393
[19:52:58.926] iteration 1450 : loss : 351.322479, loss_ce: 0.000000, loss_kd: 1754.151978
[19:53:04.489] iteration 1460 : loss : 467.468506, loss_ce: 0.000000, loss_kd: 2334.924805
[19:53:10.064] iteration 1470 : loss : 496.417755, loss_ce: 0.000000, loss_kd: 2479.614258
[19:53:15.631] iteration 1480 : loss : 361.790466, loss_ce: 0.000000, loss_kd: 1806.445679
[19:53:32.838] iteration 1490 : loss : 503.733582, loss_ce: 0.000000, loss_kd: 2516.173096
[19:53:38.358] iteration 1500 : loss : 547.089050, loss_ce: 0.000000, loss_kd: 2732.950195
[19:53:43.896] iteration 1510 : loss : 393.385986, loss_ce: 0.000000, loss_kd: 1964.461426
[19:53:49.425] iteration 1520 : loss : 576.487671, loss_ce: 0.000000, loss_kd: 2879.964844
[19:53:54.969] iteration 1530 : loss : 426.575653, loss_ce: 0.000000, loss_kd: 2130.392578
[19:54:00.509] iteration 1540 : loss : 610.605164, loss_ce: 0.000000, loss_kd: 3050.600098
[19:54:06.060] iteration 1550 : loss : 555.178772, loss_ce: 0.000000, loss_kd: 2773.476074
[19:54:11.600] iteration 1560 : loss : 718.795959, loss_ce: 0.000000, loss_kd: 3591.505859
[19:54:17.152] iteration 1570 : loss : 490.777130, loss_ce: 0.000000, loss_kd: 2451.418457
[19:54:22.689] iteration 1580 : loss : 340.343018, loss_ce: 0.000000, loss_kd: 1699.239868
[19:54:28.249] iteration 1590 : loss : 497.537292, loss_ce: 0.000000, loss_kd: 2485.267334
[19:54:33.794] iteration 1600 : loss : 396.671600, loss_ce: 0.000000, loss_kd: 1980.849121
[19:54:39.362] iteration 1610 : loss : 417.637238, loss_ce: 0.000000, loss_kd: 2085.728516
[19:54:44.912] iteration 1620 : loss : 444.148987, loss_ce: 0.000000, loss_kd: 2218.316650
[19:54:50.481] iteration 1630 : loss : 495.250336, loss_ce: 0.000000, loss_kd: 2473.757080
[19:54:56.035] iteration 1640 : loss : 347.572021, loss_ce: 0.000000, loss_kd: 1735.398682
[19:55:01.598] iteration 1650 : loss : 471.737823, loss_ce: 0.000000, loss_kd: 2356.228027
[19:55:07.154] iteration 1660 : loss : 366.876007, loss_ce: 0.000000, loss_kd: 1831.878662
[19:55:12.727] iteration 1670 : loss : 615.480164, loss_ce: 0.000000, loss_kd: 3074.960449
[19:55:18.283] iteration 1680 : loss : 354.563721, loss_ce: 0.000000, loss_kd: 1770.399292
[19:55:23.848] iteration 1690 : loss : 359.609772, loss_ce: 0.000000, loss_kd: 1795.553223
[19:55:29.406] iteration 1700 : loss : 297.865723, loss_ce: 0.000000, loss_kd: 1486.850464
[19:55:34.980] iteration 1710 : loss : 484.722382, loss_ce: 0.000000, loss_kd: 2421.155762
[19:55:40.536] iteration 1720 : loss : 520.599243, loss_ce: 0.000000, loss_kd: 2600.512207
[19:55:46.112] iteration 1730 : loss : 436.161774, loss_ce: 0.000000, loss_kd: 2178.331787
[19:55:51.670] iteration 1740 : loss : 462.833038, loss_ce: 0.000000, loss_kd: 2311.704346
[19:55:57.248] iteration 1750 : loss : 349.288177, loss_ce: 0.000000, loss_kd: 1743.985474
[19:56:02.812] iteration 1760 : loss : 406.609283, loss_ce: 0.000000, loss_kd: 2030.635254
[19:56:08.379] iteration 1770 : loss : 471.923798, loss_ce: 0.000000, loss_kd: 2357.160156
[19:56:13.942] iteration 1780 : loss : 383.453461, loss_ce: 0.000000, loss_kd: 1914.797607
[19:56:19.515] iteration 1790 : loss : 534.809265, loss_ce: 0.000000, loss_kd: 2671.460938
[19:56:25.086] iteration 1800 : loss : 514.774658, loss_ce: 0.000000, loss_kd: 2571.389160
[19:56:30.664] iteration 1810 : loss : 367.572296, loss_ce: 0.000000, loss_kd: 1835.373535
[19:56:36.229] iteration 1820 : loss : 294.074982, loss_ce: 0.000000, loss_kd: 1467.901123
[19:56:41.815] iteration 1830 : loss : 391.199585, loss_ce: 0.000000, loss_kd: 1953.551147
[19:56:47.381] iteration 1840 : loss : 368.277924, loss_ce: 0.000000, loss_kd: 1838.934326
[19:56:52.962] iteration 1850 : loss : 302.776672, loss_ce: 0.000000, loss_kd: 1511.433350
[19:56:58.529] iteration 1860 : loss : 342.406006, loss_ce: 0.000000, loss_kd: 1709.567993
[19:57:04.108] iteration 1870 : loss : 444.763733, loss_ce: 0.000000, loss_kd: 2221.368652
[19:57:09.674] iteration 1880 : loss : 413.847412, loss_ce: 0.000000, loss_kd: 2066.757568
[19:57:15.261] iteration 1890 : loss : 744.998596, loss_ce: 0.000000, loss_kd: 3722.570557
[19:57:20.829] iteration 1900 : loss : 387.320526, loss_ce: 0.000000, loss_kd: 1934.078003
[19:57:26.413] iteration 1910 : loss : 290.653320, loss_ce: 0.000000, loss_kd: 1450.813721
[19:57:31.991] iteration 1920 : loss : 368.942139, loss_ce: 0.000000, loss_kd: 1842.266968
[19:57:37.572] iteration 1930 : loss : 386.723969, loss_ce: 0.000000, loss_kd: 1931.159546
[19:57:43.145] iteration 1940 : loss : 440.916138, loss_ce: 0.000000, loss_kd: 2202.132324
[19:57:48.730] iteration 1950 : loss : 381.799805, loss_ce: 0.000000, loss_kd: 1906.489624
[19:57:54.306] iteration 1960 : loss : 313.214783, loss_ce: 0.000000, loss_kd: 1563.583740
[19:57:59.885] iteration 1970 : loss : 359.956696, loss_ce: 0.000000, loss_kd: 1797.315796
[19:58:05.458] iteration 1980 : loss : 522.264221, loss_ce: 0.000000, loss_kd: 2608.865479
[19:58:11.049] iteration 1990 : loss : 385.266357, loss_ce: 0.000000, loss_kd: 1923.902588
[19:58:16.627] iteration 2000 : loss : 385.467285, loss_ce: 0.000000, loss_kd: 1924.864868
[19:58:22.208] iteration 2010 : loss : 274.359497, loss_ce: 0.000000, loss_kd: 1369.336792
[19:58:27.780] iteration 2020 : loss : 309.129303, loss_ce: 0.000000, loss_kd: 1543.196899
[19:58:33.368] iteration 2030 : loss : 265.356781, loss_ce: 0.000000, loss_kd: 1324.336792
[19:58:38.938] iteration 2040 : loss : 307.236359, loss_ce: 0.000000, loss_kd: 1533.731812
[19:58:44.524] iteration 2050 : loss : 324.956665, loss_ce: 0.000000, loss_kd: 1622.352173
[19:58:50.092] iteration 2060 : loss : 456.214447, loss_ce: 0.000000, loss_kd: 2278.619385
[19:58:55.673] iteration 2070 : loss : 420.082855, loss_ce: 0.000000, loss_kd: 2097.975586
[19:59:01.245] iteration 2080 : loss : 395.679413, loss_ce: 0.000000, loss_kd: 1975.950317
[19:59:06.828] iteration 2090 : loss : 377.438751, loss_ce: 0.000000, loss_kd: 1884.767578
[19:59:12.406] iteration 2100 : loss : 398.409515, loss_ce: 0.000000, loss_kd: 1989.600952
[19:59:17.988] iteration 2110 : loss : 437.159088, loss_ce: 0.000000, loss_kd: 2183.345459
[19:59:23.565] iteration 2120 : loss : 403.146576, loss_ce: 0.000000, loss_kd: 2013.303101
[19:59:29.150] iteration 2130 : loss : 382.928894, loss_ce: 0.000000, loss_kd: 1912.201416
[19:59:34.723] iteration 2140 : loss : 395.609161, loss_ce: 0.000000, loss_kd: 1975.572998
[19:59:40.301] iteration 2150 : loss : 288.401306, loss_ce: 0.000000, loss_kd: 1439.534424
[19:59:45.878] iteration 2160 : loss : 349.270996, loss_ce: 0.000000, loss_kd: 1743.923828
[19:59:51.470] iteration 2170 : loss : 359.697357, loss_ce: 0.000000, loss_kd: 1796.049316
[19:59:57.052] iteration 2180 : loss : 345.501373, loss_ce: 0.000000, loss_kd: 1725.036987
[20:00:02.638] iteration 2190 : loss : 313.581696, loss_ce: 0.000000, loss_kd: 1565.458984
[20:00:08.212] iteration 2200 : loss : 312.740265, loss_ce: 0.000000, loss_kd: 1561.249634
[20:00:13.798] iteration 2210 : loss : 346.607147, loss_ce: 0.000000, loss_kd: 1730.593506
[20:00:19.376] iteration 2220 : loss : 374.012726, loss_ce: 0.000000, loss_kd: 1867.597168
[20:00:24.960] iteration 2230 : loss : 324.063568, loss_ce: 0.000000, loss_kd: 1617.868164
[20:00:42.016] iteration 2240 : loss : 249.248779, loss_ce: 0.000000, loss_kd: 1243.825439
[20:00:47.563] iteration 2250 : loss : 405.807770, loss_ce: 0.000000, loss_kd: 2026.557373
[20:00:53.096] iteration 2260 : loss : 348.036133, loss_ce: 0.000000, loss_kd: 1737.689941
[20:00:58.642] iteration 2270 : loss : 317.894104, loss_ce: 0.000000, loss_kd: 1587.054321
[20:01:04.183] iteration 2280 : loss : 387.399078, loss_ce: 0.000000, loss_kd: 1934.496582
[20:01:09.735] iteration 2290 : loss : 383.794037, loss_ce: 0.000000, loss_kd: 1916.508179
[20:01:15.281] iteration 2300 : loss : 391.544342, loss_ce: 0.000000, loss_kd: 1955.290649
[20:01:20.834] iteration 2310 : loss : 395.672211, loss_ce: 0.000000, loss_kd: 1975.931030
[20:01:26.381] iteration 2320 : loss : 363.304321, loss_ce: 0.000000, loss_kd: 1814.036133
[20:01:31.941] iteration 2330 : loss : 299.139130, loss_ce: 0.000000, loss_kd: 1493.256226
[20:01:37.493] iteration 2340 : loss : 335.741241, loss_ce: 0.000000, loss_kd: 1676.320679
[20:01:43.061] iteration 2350 : loss : 376.327515, loss_ce: 0.000000, loss_kd: 1879.229492
[20:01:48.611] iteration 2360 : loss : 471.568176, loss_ce: 0.000000, loss_kd: 2355.376709
[20:01:54.176] iteration 2370 : loss : 266.498352, loss_ce: 0.000000, loss_kd: 1330.033691
[20:01:59.735] iteration 2380 : loss : 308.498169, loss_ce: 0.000000, loss_kd: 1540.046387
[20:02:05.310] iteration 2390 : loss : 338.963867, loss_ce: 0.000000, loss_kd: 1692.381470
[20:02:10.872] iteration 2400 : loss : 330.040283, loss_ce: 0.000000, loss_kd: 1647.775146
[20:02:16.447] iteration 2410 : loss : 333.013977, loss_ce: 0.000000, loss_kd: 1662.651978
[20:02:22.005] iteration 2420 : loss : 331.858093, loss_ce: 0.000000, loss_kd: 1656.838257
[20:02:27.584] iteration 2430 : loss : 309.504272, loss_ce: 0.000000, loss_kd: 1545.038940
[20:02:33.147] iteration 2440 : loss : 302.298279, loss_ce: 0.000000, loss_kd: 1509.035522
[20:02:38.723] iteration 2450 : loss : 370.604462, loss_ce: 0.000000, loss_kd: 1850.581299
[20:02:44.292] iteration 2460 : loss : 271.610016, loss_ce: 0.000000, loss_kd: 1355.593994
[20:02:49.864] iteration 2470 : loss : 349.661133, loss_ce: 0.000000, loss_kd: 1745.868042
[20:02:55.434] iteration 2480 : loss : 377.882568, loss_ce: 0.000000, loss_kd: 1887.034180
[20:03:01.010] iteration 2490 : loss : 297.180542, loss_ce: 0.000000, loss_kd: 1483.422607
[20:03:06.577] iteration 2500 : loss : 354.726776, loss_ce: 0.000000, loss_kd: 1771.172119
[20:03:12.153] iteration 2510 : loss : 370.682129, loss_ce: 0.000000, loss_kd: 1850.955322
[20:03:17.719] iteration 2520 : loss : 352.384888, loss_ce: 0.000000, loss_kd: 1759.457764
[20:03:23.299] iteration 2530 : loss : 281.791992, loss_ce: 0.000000, loss_kd: 1406.463867
[20:03:28.867] iteration 2540 : loss : 483.770966, loss_ce: 0.000000, loss_kd: 2416.410645
[20:03:34.446] iteration 2550 : loss : 367.540527, loss_ce: 0.000000, loss_kd: 1835.265137
[20:03:40.020] iteration 2560 : loss : 283.607544, loss_ce: 0.000000, loss_kd: 1415.584961
[20:03:45.603] iteration 2570 : loss : 362.170349, loss_ce: 0.000000, loss_kd: 1808.390991
[20:03:51.173] iteration 2580 : loss : 383.807037, loss_ce: 0.000000, loss_kd: 1916.598022
[20:03:56.757] iteration 2590 : loss : 299.781860, loss_ce: 0.000000, loss_kd: 1496.435303
[20:04:02.334] iteration 2600 : loss : 394.470886, loss_ce: 0.000000, loss_kd: 1969.886230
[20:04:07.916] iteration 2610 : loss : 356.746155, loss_ce: 0.000000, loss_kd: 1781.299927
[20:04:13.478] iteration 2620 : loss : 325.770477, loss_ce: 0.000000, loss_kd: 1626.406250
[20:04:19.068] iteration 2630 : loss : 280.120392, loss_ce: 0.000000, loss_kd: 1398.189087
[20:04:24.638] iteration 2640 : loss : 256.270813, loss_ce: 0.000000, loss_kd: 1278.939087
[20:04:30.218] iteration 2650 : loss : 334.846710, loss_ce: 0.000000, loss_kd: 1671.790894
[20:04:35.792] iteration 2660 : loss : 300.610321, loss_ce: 0.000000, loss_kd: 1500.605225
[20:04:41.384] iteration 2670 : loss : 259.251984, loss_ce: 0.000000, loss_kd: 1293.804077
[20:04:46.950] iteration 2680 : loss : 376.625824, loss_ce: 0.000000, loss_kd: 1880.687012
[20:04:52.530] iteration 2690 : loss : 637.406372, loss_ce: 0.000000, loss_kd: 3184.604980
[20:04:58.103] iteration 2700 : loss : 361.356262, loss_ce: 0.000000, loss_kd: 1804.393311
[20:05:03.683] iteration 2710 : loss : 315.811890, loss_ce: 0.000000, loss_kd: 1576.614136
[20:05:09.250] iteration 2720 : loss : 326.487488, loss_ce: 0.000000, loss_kd: 1629.850098
[20:05:14.835] iteration 2730 : loss : 269.540833, loss_ce: 0.000000, loss_kd: 1345.253906
[20:05:20.395] iteration 2740 : loss : 285.178040, loss_ce: 0.000000, loss_kd: 1423.474121
[20:05:25.974] iteration 2750 : loss : 360.191803, loss_ce: 0.000000, loss_kd: 1798.522339
[20:05:31.578] iteration 2760 : loss : 305.753235, loss_ce: 0.000000, loss_kd: 1526.356445
[20:05:37.159] iteration 2770 : loss : 312.560547, loss_ce: 0.000000, loss_kd: 1560.309448
[20:05:42.731] iteration 2780 : loss : 305.004883, loss_ce: 0.000000, loss_kd: 1522.598511
[20:05:48.311] iteration 2790 : loss : 254.578445, loss_ce: 0.000000, loss_kd: 1270.463013
[20:05:53.888] iteration 2800 : loss : 293.118134, loss_ce: 0.000000, loss_kd: 1463.111694
[20:05:59.471] iteration 2810 : loss : 399.607849, loss_ce: 0.000000, loss_kd: 1995.617432
[20:06:05.035] iteration 2820 : loss : 233.562271, loss_ce: 0.000000, loss_kd: 1165.328125
[20:06:10.619] iteration 2830 : loss : 275.396210, loss_ce: 0.000000, loss_kd: 1374.556396
[20:06:16.186] iteration 2840 : loss : 362.226746, loss_ce: 0.000000, loss_kd: 1808.690674
[20:06:21.762] iteration 2850 : loss : 295.960419, loss_ce: 0.000000, loss_kd: 1477.360352
[20:06:27.333] iteration 2860 : loss : 244.215179, loss_ce: 0.000000, loss_kd: 1218.656494
[20:06:32.915] iteration 2870 : loss : 363.319336, loss_ce: 0.000000, loss_kd: 1814.122681
[20:06:38.480] iteration 2880 : loss : 210.160385, loss_ce: 0.000000, loss_kd: 1048.369629
[20:06:44.064] iteration 2890 : loss : 346.456482, loss_ce: 0.000000, loss_kd: 1729.817627
[20:06:49.631] iteration 2900 : loss : 396.864960, loss_ce: 0.000000, loss_kd: 1981.828125
[20:06:55.208] iteration 2910 : loss : 270.862335, loss_ce: 0.000000, loss_kd: 1351.876343
[20:07:00.784] iteration 2920 : loss : 368.299469, loss_ce: 0.000000, loss_kd: 1839.064331
[20:07:06.364] iteration 2930 : loss : 300.198334, loss_ce: 0.000000, loss_kd: 1498.534424
[20:07:11.929] iteration 2940 : loss : 321.306274, loss_ce: 0.000000, loss_kd: 1604.049194
[20:07:17.515] iteration 2950 : loss : 343.630981, loss_ce: 0.000000, loss_kd: 1715.687012
[20:07:23.089] iteration 2960 : loss : 367.930267, loss_ce: 0.000000, loss_kd: 1837.173218
[20:07:28.666] iteration 2970 : loss : 304.097107, loss_ce: 0.000000, loss_kd: 1518.013184
[20:07:46.460] iteration 2980 : loss : 305.754150, loss_ce: 0.000000, loss_kd: 1526.293335
[20:07:52.000] iteration 2990 : loss : 277.682434, loss_ce: 0.000000, loss_kd: 1385.995361
[20:07:57.529] iteration 3000 : loss : 321.339722, loss_ce: 0.000000, loss_kd: 1604.232422
[20:08:03.074] iteration 3010 : loss : 356.523468, loss_ce: 0.000000, loss_kd: 1780.190308
[20:08:08.611] iteration 3020 : loss : 285.440765, loss_ce: 0.000000, loss_kd: 1424.783936
[20:08:14.161] iteration 3030 : loss : 310.065582, loss_ce: 0.000000, loss_kd: 1547.886475
[20:08:19.702] iteration 3040 : loss : 384.848450, loss_ce: 0.000000, loss_kd: 1921.807373
[20:08:25.251] iteration 3050 : loss : 255.177612, loss_ce: 0.000000, loss_kd: 1273.462891
[20:08:30.797] iteration 3060 : loss : 233.078430, loss_ce: 0.000000, loss_kd: 1162.966675
[20:08:36.356] iteration 3070 : loss : 215.325516, loss_ce: 0.000000, loss_kd: 1074.230835
[20:08:41.907] iteration 3080 : loss : 276.617493, loss_ce: 0.000000, loss_kd: 1380.612061
[20:08:47.469] iteration 3090 : loss : 330.898254, loss_ce: 0.000000, loss_kd: 1652.073242
[20:08:53.022] iteration 3100 : loss : 384.724457, loss_ce: 0.000000, loss_kd: 1921.187988
[20:08:58.583] iteration 3110 : loss : 334.825623, loss_ce: 0.000000, loss_kd: 1671.671997
[20:09:04.140] iteration 3120 : loss : 291.023010, loss_ce: 0.000000, loss_kd: 1452.666992
[20:09:09.700] iteration 3130 : loss : 348.528564, loss_ce: 0.000000, loss_kd: 1740.169800
[20:09:15.259] iteration 3140 : loss : 299.182129, loss_ce: 0.000000, loss_kd: 1493.424072
[20:09:20.818] iteration 3150 : loss : 378.950562, loss_ce: 0.000000, loss_kd: 1892.302734
[20:09:26.379] iteration 3160 : loss : 269.729218, loss_ce: 0.000000, loss_kd: 1346.167969
[20:09:31.946] iteration 3170 : loss : 314.037781, loss_ce: 0.000000, loss_kd: 1567.709106
[20:09:37.504] iteration 3180 : loss : 251.474762, loss_ce: 0.000000, loss_kd: 1254.946289
[20:09:43.076] iteration 3190 : loss : 364.105774, loss_ce: 0.000000, loss_kd: 1818.085205
[20:09:48.637] iteration 3200 : loss : 349.571899, loss_ce: 0.000000, loss_kd: 1745.460571
[20:09:54.209] iteration 3210 : loss : 298.741516, loss_ce: 0.000000, loss_kd: 1491.253906
[20:09:59.782] iteration 3220 : loss : 248.427109, loss_ce: 0.000000, loss_kd: 1239.704834
[20:10:05.353] iteration 3230 : loss : 315.562134, loss_ce: 0.000000, loss_kd: 1575.399902
[20:10:10.920] iteration 3240 : loss : 278.730347, loss_ce: 0.000000, loss_kd: 1391.181885
[20:10:16.499] iteration 3250 : loss : 296.555023, loss_ce: 0.000000, loss_kd: 1480.297852
[20:10:22.064] iteration 3260 : loss : 293.238739, loss_ce: 0.000000, loss_kd: 1463.740356
[20:10:27.646] iteration 3270 : loss : 303.964020, loss_ce: 0.000000, loss_kd: 1517.354492
[20:10:33.209] iteration 3280 : loss : 313.420837, loss_ce: 0.000000, loss_kd: 1564.663330
[20:10:38.790] iteration 3290 : loss : 285.164124, loss_ce: 0.000000, loss_kd: 1423.405762
[20:10:44.369] iteration 3300 : loss : 344.656189, loss_ce: 0.000000, loss_kd: 1720.821045
[20:10:49.955] iteration 3310 : loss : 284.767273, loss_ce: 0.000000, loss_kd: 1421.384888
[20:10:55.529] iteration 3320 : loss : 412.426483, loss_ce: 0.000000, loss_kd: 2059.657715
[20:11:01.117] iteration 3330 : loss : 301.500427, loss_ce: 0.000000, loss_kd: 1505.056763
[20:11:06.696] iteration 3340 : loss : 344.582520, loss_ce: 0.000000, loss_kd: 1720.443604
[20:11:12.281] iteration 3350 : loss : 239.119263, loss_ce: 0.000000, loss_kd: 1193.153442
[20:11:17.850] iteration 3360 : loss : 251.670731, loss_ce: 0.000000, loss_kd: 1255.919678
[20:11:23.440] iteration 3370 : loss : 250.002151, loss_ce: 0.000000, loss_kd: 1247.511719
[20:11:29.016] iteration 3380 : loss : 267.546814, loss_ce: 0.000000, loss_kd: 1335.311523
[20:11:34.605] iteration 3390 : loss : 243.741806, loss_ce: 0.000000, loss_kd: 1216.295166
[20:11:40.186] iteration 3400 : loss : 264.354919, loss_ce: 0.000000, loss_kd: 1319.319824
[20:11:45.774] iteration 3410 : loss : 295.032166, loss_ce: 0.000000, loss_kd: 1472.696655
[20:11:51.347] iteration 3420 : loss : 284.246155, loss_ce: 0.000000, loss_kd: 1418.775513
[20:11:56.929] iteration 3430 : loss : 280.323273, loss_ce: 0.000000, loss_kd: 1399.204712
[20:12:02.501] iteration 3440 : loss : 361.061890, loss_ce: 0.000000, loss_kd: 1802.888672
[20:12:08.085] iteration 3450 : loss : 263.065552, loss_ce: 0.000000, loss_kd: 1312.897217
[20:12:13.665] iteration 3460 : loss : 302.612915, loss_ce: 0.000000, loss_kd: 1510.600220
[20:12:19.254] iteration 3470 : loss : 341.350281, loss_ce: 0.000000, loss_kd: 1704.300293
[20:12:24.826] iteration 3480 : loss : 471.820343, loss_ce: 0.000000, loss_kd: 2356.653564
[20:12:30.409] iteration 3490 : loss : 242.099777, loss_ce: 0.000000, loss_kd: 1208.047852
[20:12:35.984] iteration 3500 : loss : 232.296692, loss_ce: 0.000000, loss_kd: 1159.038574
[20:12:41.569] iteration 3510 : loss : 333.125061, loss_ce: 0.000000, loss_kd: 1663.171387
[20:12:47.146] iteration 3520 : loss : 272.869598, loss_ce: 0.000000, loss_kd: 1361.910278
[20:12:52.724] iteration 3530 : loss : 333.432495, loss_ce: 0.000000, loss_kd: 1664.713379
[20:12:58.298] iteration 3540 : loss : 229.787277, loss_ce: 0.000000, loss_kd: 1146.447876
[20:13:03.882] iteration 3550 : loss : 298.872528, loss_ce: 0.000000, loss_kd: 1491.937256
[20:13:09.451] iteration 3560 : loss : 265.601440, loss_ce: 0.000000, loss_kd: 1325.582764
[20:13:15.037] iteration 3570 : loss : 272.597870, loss_ce: 0.000000, loss_kd: 1360.549561
[20:13:20.610] iteration 3580 : loss : 282.331482, loss_ce: 0.000000, loss_kd: 1409.205811
[20:13:26.197] iteration 3590 : loss : 295.177856, loss_ce: 0.000000, loss_kd: 1473.480957
[20:13:31.771] iteration 3600 : loss : 337.506836, loss_ce: 0.000000, loss_kd: 1685.077271
[20:13:37.362] iteration 3610 : loss : 274.533295, loss_ce: 0.000000, loss_kd: 1370.084351
[20:13:42.940] iteration 3620 : loss : 290.941315, loss_ce: 0.000000, loss_kd: 1452.283447
[20:13:48.528] iteration 3630 : loss : 340.039825, loss_ce: 0.000000, loss_kd: 1697.759888
[20:13:54.098] iteration 3640 : loss : 234.848114, loss_ce: 0.000000, loss_kd: 1171.733521
[20:13:59.685] iteration 3650 : loss : 297.552643, loss_ce: 0.000000, loss_kd: 1485.325195
[20:14:05.264] iteration 3660 : loss : 273.122314, loss_ce: 0.000000, loss_kd: 1363.245605
[20:14:10.850] iteration 3670 : loss : 310.625977, loss_ce: 0.000000, loss_kd: 1550.678467
[20:14:16.419] iteration 3680 : loss : 299.200073, loss_ce: 0.000000, loss_kd: 1493.560791
[20:14:22.013] iteration 3690 : loss : 360.003326, loss_ce: 0.000000, loss_kd: 1797.557129
[20:14:27.590] iteration 3700 : loss : 335.977264, loss_ce: 0.000000, loss_kd: 1677.442627
[20:14:33.176] iteration 3710 : loss : 290.847198, loss_ce: 0.000000, loss_kd: 1451.791748
[20:14:38.686] iteration 3720 : loss : 195.686111, loss_ce: 0.000000, loss_kd: 976.003540
[20:14:39.486] save model to ./debug_simple\continual_surgical_tpgm_epoch_4.pth
[20:14:39.578] save final model to ./debug_simple\continual_surgical_tpgm_final.pth
[20:47:06.659] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes_old=9, num_classes_new=4, output_dir='./debug_simple', max_iterations=10000, max_epochs=5, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.25, kd_temperature=3.0, kd_weight=0.2, freeze_old_classes=False, auto_tune='none', gradient_batches=5, tpgm_norm_mode='l2', tpgm_lr=0.01, tpgm_iters=200, tpgm_exclude=[], tpgm_frequency=5, tpgm_start_epoch=10, disable_tpgm=True, tpgm_data_fraction=0.1, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[20:47:06.673] Using 23805/95221 samples (25.0%) for continual learning
[20:47:06.673] Old classes: 9, New classes: 4, Total: 12
[20:47:06.673] TPGM enabled: False
[20:47:06.673] Surgical fine-tuning method: none
[20:47:18.070] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes_old=9, num_classes_new=4, output_dir='./debug_simple', max_iterations=10000, max_epochs=5, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.35, kd_temperature=3.0, kd_weight=0.2, freeze_old_classes=False, auto_tune='none', gradient_batches=5, tpgm_norm_mode='l2', tpgm_lr=0.01, tpgm_iters=200, tpgm_exclude=[], tpgm_frequency=5, tpgm_start_epoch=10, disable_tpgm=True, tpgm_data_fraction=0.1, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[20:47:18.088] Using 33327/95221 samples (35.0%) for continual learning
[20:47:18.088] Old classes: 9, New classes: 4, Total: 12
[20:47:18.088] TPGM enabled: False
[20:47:18.088] Surgical fine-tuning method: none
[20:49:34.564] Combined Continual Learning + Surgical + TPGM Configuration:
[20:49:34.564] KD Temperature: 3.0
[20:49:34.564] KD Weight: 0.2
[20:49:34.564] Auto-tune method: none
[20:49:34.564] TPGM start epoch: 10
[20:49:34.564] TPGM frequency: 5
[20:49:34.564] 1042 iterations per epoch. 5210 max iterations 
[20:49:51.652] iteration 10 : loss : 18078.230469, loss_ce: 0.000000, loss_kd: 90388.210938
[20:49:57.128] iteration 20 : loss : 4202.719727, loss_ce: 0.000000, loss_kd: 21010.882812
[20:50:02.636] iteration 30 : loss : 3286.276855, loss_ce: 0.000000, loss_kd: 16428.453125
[20:50:08.127] iteration 40 : loss : 2315.473877, loss_ce: 0.000000, loss_kd: 11574.624023
[20:50:13.639] iteration 50 : loss : 1886.130005, loss_ce: 0.000000, loss_kd: 9427.867188
[20:50:19.147] iteration 60 : loss : 1979.399780, loss_ce: 0.000000, loss_kd: 9894.293945
[20:50:24.663] iteration 70 : loss : 1767.265503, loss_ce: 0.000000, loss_kd: 8833.622070
[20:50:30.173] iteration 80 : loss : 1761.473999, loss_ce: 0.000000, loss_kd: 8804.683594
[20:50:35.693] iteration 90 : loss : 1479.098511, loss_ce: 0.000000, loss_kd: 7392.774902
[20:50:41.200] iteration 100 : loss : 1339.856812, loss_ce: 0.000000, loss_kd: 6696.516602
[20:50:46.727] iteration 110 : loss : 1437.330811, loss_ce: 0.000000, loss_kd: 7183.879395
[20:50:52.239] iteration 120 : loss : 1275.974121, loss_ce: 0.000000, loss_kd: 6377.185547
[20:50:57.770] iteration 130 : loss : 1449.427368, loss_ce: 0.000000, loss_kd: 7244.554688
[20:51:03.292] iteration 140 : loss : 977.768433, loss_ce: 0.000000, loss_kd: 4886.151367
[20:51:08.823] iteration 150 : loss : 1271.833252, loss_ce: 0.000000, loss_kd: 6356.424316
[20:51:14.341] iteration 160 : loss : 998.000061, loss_ce: 0.000000, loss_kd: 4987.282715
[20:51:19.872] iteration 170 : loss : 1353.617432, loss_ce: 0.000000, loss_kd: 6765.392578
[20:51:25.391] iteration 180 : loss : 877.486023, loss_ce: 0.000000, loss_kd: 4384.691406
[20:51:30.919] iteration 190 : loss : 894.638794, loss_ce: 0.000000, loss_kd: 4470.455078
[20:51:36.440] iteration 200 : loss : 944.751099, loss_ce: 0.000000, loss_kd: 4721.034180
[20:51:41.986] iteration 210 : loss : 962.959717, loss_ce: 0.000000, loss_kd: 4812.179688
[20:51:47.536] iteration 220 : loss : 919.039856, loss_ce: 0.000000, loss_kd: 4592.541016
[20:51:53.075] iteration 230 : loss : 1121.984131, loss_ce: 0.000000, loss_kd: 5607.308594
[20:51:58.608] iteration 240 : loss : 795.394653, loss_ce: 0.000000, loss_kd: 3974.262451
[20:52:04.151] iteration 250 : loss : 672.302856, loss_ce: 0.000000, loss_kd: 3358.875488
[20:52:09.680] iteration 260 : loss : 1070.992188, loss_ce: 0.000000, loss_kd: 5352.409180
[20:52:15.216] iteration 270 : loss : 820.225037, loss_ce: 0.000000, loss_kd: 4098.601562
[20:52:20.749] iteration 280 : loss : 800.067139, loss_ce: 0.000000, loss_kd: 3997.777100
[20:52:26.290] iteration 290 : loss : 798.909058, loss_ce: 0.000000, loss_kd: 3991.996338
[20:52:31.823] iteration 300 : loss : 909.711304, loss_ce: 0.000000, loss_kd: 4546.048828
[20:52:37.368] iteration 310 : loss : 921.927002, loss_ce: 0.000000, loss_kd: 4607.069824
[20:52:42.907] iteration 320 : loss : 782.559570, loss_ce: 0.000000, loss_kd: 3910.240723
[20:52:48.455] iteration 330 : loss : 1072.266602, loss_ce: 0.000000, loss_kd: 5358.718750
[20:52:54.002] iteration 340 : loss : 804.982361, loss_ce: 0.000000, loss_kd: 4022.367676
[20:52:59.559] iteration 350 : loss : 931.176880, loss_ce: 0.000000, loss_kd: 4653.307129
[21:08:25.913] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes_old=9, num_classes_new=4, output_dir='./debug_simple', max_iterations=10000, max_epochs=5, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.35, kd_temperature=3.0, kd_weight=0.2, freeze_old_classes=False, auto_tune='none', gradient_batches=5, tpgm_norm_mode='l2', tpgm_lr=0.01, tpgm_iters=200, tpgm_exclude=[], tpgm_frequency=5, tpgm_start_epoch=10, disable_tpgm=True, tpgm_data_fraction=0.1, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[21:08:25.931] Using 33327/95221 samples (35.0%) for continual learning
[21:08:25.931] Old classes: 9, New classes: 4, Total: 12
[21:08:25.931] TPGM enabled: False
[21:08:25.931] Surgical fine-tuning method: none
[21:15:57.760] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes_old=9, num_classes_new=4, output_dir='./debug_simple', max_iterations=10000, max_epochs=5, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.35, kd_temperature=3.0, kd_weight=0.2, freeze_old_classes=False, auto_tune='none', gradient_batches=5, tpgm_norm_mode='l2', tpgm_lr=0.01, tpgm_iters=200, tpgm_exclude=[], tpgm_frequency=5, tpgm_start_epoch=10, disable_tpgm=True, tpgm_data_fraction=0.1, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[21:15:57.777] Using 33327/95221 samples (35.0%) for continual learning
[21:15:57.777] Old classes: 9, New classes: 4, Total: 12
[21:15:57.777] TPGM enabled: False
[21:15:57.778] Surgical fine-tuning method: none
[21:17:56.466] Combined Continual Learning + Surgical + TPGM Configuration:
[21:17:56.466] KD Temperature: 3.0
[21:17:56.466] KD Weight: 0.2
[21:17:56.466] Auto-tune method: none
[21:17:56.466] TPGM start epoch: 10
[21:17:56.466] TPGM frequency: 5
[21:17:56.466] 1042 iterations per epoch. 5210 max iterations 
[21:18:13.575] iteration 10 : loss : 18079.298828, loss_ce: 0.094866, loss_kd: 90393.476562
[21:18:19.053] iteration 20 : loss : 4202.724609, loss_ce: 0.254477, loss_kd: 21010.705078
[21:18:24.555] iteration 30 : loss : 3285.333984, loss_ce: 0.146452, loss_kd: 16423.623047
[21:18:30.046] iteration 40 : loss : 2318.009521, loss_ce: 0.194886, loss_kd: 11587.148438
[21:18:35.557] iteration 50 : loss : 1861.441040, loss_ce: 0.145094, loss_kd: 9304.314453
[21:18:41.062] iteration 60 : loss : 2011.300659, loss_ce: 0.149955, loss_kd: 10053.681641
[21:18:46.570] iteration 70 : loss : 1804.735229, loss_ce: 0.201140, loss_kd: 9020.814453
[21:18:52.070] iteration 80 : loss : 1896.747314, loss_ce: 0.098234, loss_kd: 9480.979492
[21:18:57.585] iteration 90 : loss : 1605.403564, loss_ce: 0.142419, loss_kd: 8024.204102
[21:19:03.086] iteration 100 : loss : 1349.268555, loss_ce: 0.101674, loss_kd: 6743.548828
[21:19:08.609] iteration 110 : loss : 1574.727661, loss_ce: 0.133486, loss_kd: 7870.878418
[21:19:14.120] iteration 120 : loss : 1503.364502, loss_ce: 0.078837, loss_kd: 7514.081543
[21:19:19.639] iteration 130 : loss : 1235.960449, loss_ce: 0.079231, loss_kd: 6177.041992
[21:19:25.152] iteration 140 : loss : 1012.183899, loss_ce: 0.053532, loss_kd: 5058.255859
[21:19:30.677] iteration 150 : loss : 1165.672852, loss_ce: 0.109775, loss_kd: 5825.558594
[21:19:36.191] iteration 160 : loss : 1145.960571, loss_ce: 0.039001, loss_kd: 5727.000488
[21:19:41.719] iteration 170 : loss : 1000.895508, loss_ce: 0.028015, loss_kd: 5001.901367
[21:19:47.236] iteration 180 : loss : 1065.893921, loss_ce: 0.022063, loss_kd: 5326.750000
[21:19:52.767] iteration 190 : loss : 1156.271484, loss_ce: 0.017472, loss_kd: 5778.671387
[21:19:58.289] iteration 200 : loss : 815.982422, loss_ce: 0.025918, loss_kd: 4077.325928
[21:20:03.828] iteration 210 : loss : 959.926941, loss_ce: 0.037670, loss_kd: 4797.106445
[21:20:09.375] iteration 220 : loss : 1805.067505, loss_ce: 0.010628, loss_kd: 9022.689453
[21:20:14.909] iteration 230 : loss : 1232.513306, loss_ce: 0.016044, loss_kd: 6160.073730
[21:20:20.439] iteration 240 : loss : 935.170532, loss_ce: 0.016693, loss_kd: 4673.308594
[21:20:25.982] iteration 250 : loss : 757.718506, loss_ce: 0.019824, loss_kd: 3786.021484
[21:20:31.510] iteration 260 : loss : 1103.883911, loss_ce: 0.020885, loss_kd: 5516.838867
[21:20:37.049] iteration 270 : loss : 1012.294556, loss_ce: 0.015775, loss_kd: 5058.961426
[21:20:42.577] iteration 280 : loss : 904.219116, loss_ce: 0.029265, loss_kd: 4518.552246
[21:20:48.115] iteration 290 : loss : 683.556702, loss_ce: 0.014866, loss_kd: 3415.217041
[21:20:53.642] iteration 300 : loss : 922.131165, loss_ce: 0.014638, loss_kd: 4608.119629
[21:20:59.186] iteration 310 : loss : 888.272095, loss_ce: 0.017445, loss_kd: 4438.816406
[21:21:04.720] iteration 320 : loss : 751.923157, loss_ce: 0.014779, loss_kd: 3757.124756
[21:21:10.261] iteration 330 : loss : 772.530151, loss_ce: 0.021447, loss_kd: 3860.112061
[21:21:15.796] iteration 340 : loss : 812.631592, loss_ce: 0.013952, loss_kd: 4060.665771
[21:21:21.345] iteration 350 : loss : 889.561951, loss_ce: 0.028152, loss_kd: 4445.153320
[21:21:26.881] iteration 360 : loss : 661.248413, loss_ce: 0.028253, loss_kd: 3303.628418
[21:21:32.425] iteration 370 : loss : 735.765442, loss_ce: 0.016364, loss_kd: 3676.342773
[21:21:37.963] iteration 380 : loss : 982.650879, loss_ce: 0.025946, loss_kd: 4910.734375
[21:21:43.513] iteration 390 : loss : 800.591980, loss_ce: 0.017729, loss_kd: 4000.445068
[21:21:49.051] iteration 400 : loss : 729.426025, loss_ce: 0.015033, loss_kd: 3644.613525
[21:21:54.605] iteration 410 : loss : 752.225708, loss_ce: 0.027180, loss_kd: 3758.642822
[21:22:00.145] iteration 420 : loss : 767.352234, loss_ce: 0.014357, loss_kd: 3834.246582
[21:22:05.697] iteration 430 : loss : 688.375061, loss_ce: 0.015946, loss_kd: 3439.422119
[21:22:11.241] iteration 440 : loss : 820.781555, loss_ce: 0.017725, loss_kd: 4101.391113
[21:22:16.797] iteration 450 : loss : 891.775879, loss_ce: 0.018148, loss_kd: 4456.329590
[21:22:22.341] iteration 460 : loss : 812.002563, loss_ce: 0.025380, loss_kd: 4057.490967
[21:22:27.898] iteration 470 : loss : 532.830444, loss_ce: 0.014576, loss_kd: 2661.566895
[21:22:33.447] iteration 480 : loss : 716.046387, loss_ce: 0.031853, loss_kd: 3577.642822
[21:22:39.012] iteration 490 : loss : 629.661682, loss_ce: 0.014818, loss_kd: 3145.667969
[21:22:44.562] iteration 500 : loss : 551.479126, loss_ce: 0.020371, loss_kd: 2754.847656
[21:22:50.125] iteration 510 : loss : 621.427917, loss_ce: 0.036547, loss_kd: 3104.655273
[21:22:55.674] iteration 520 : loss : 852.005188, loss_ce: 0.025181, loss_kd: 4257.477539
[21:23:01.239] iteration 530 : loss : 994.290039, loss_ce: 0.015437, loss_kd: 4968.762695
[21:23:06.796] iteration 540 : loss : 697.805481, loss_ce: 0.018557, loss_kd: 3486.457520
[21:23:12.362] iteration 550 : loss : 665.506714, loss_ce: 0.016269, loss_kd: 3324.996582
[21:23:17.919] iteration 560 : loss : 711.794006, loss_ce: 0.016142, loss_kd: 3556.364258
[21:23:23.493] iteration 570 : loss : 662.514221, loss_ce: 0.040740, loss_kd: 3310.023438
[21:23:29.053] iteration 580 : loss : 729.172668, loss_ce: 0.031467, loss_kd: 3643.386963
[21:23:34.619] iteration 590 : loss : 680.766296, loss_ce: 0.012217, loss_kd: 3401.248779
[21:23:40.174] iteration 600 : loss : 781.898804, loss_ce: 0.020235, loss_kd: 3906.930176
[21:23:45.759] iteration 610 : loss : 542.916748, loss_ce: 0.024909, loss_kd: 2712.117432
[21:23:51.323] iteration 620 : loss : 612.015564, loss_ce: 0.017954, loss_kd: 3057.548828
[21:23:56.902] iteration 630 : loss : 617.674683, loss_ce: 0.026490, loss_kd: 3085.840820
[21:24:02.469] iteration 640 : loss : 637.380676, loss_ce: 0.023978, loss_kd: 3184.286133
[21:24:08.037] iteration 650 : loss : 608.414307, loss_ce: 0.021506, loss_kd: 3039.551270
[21:24:13.608] iteration 660 : loss : 625.687439, loss_ce: 0.028355, loss_kd: 3125.975830
[21:24:19.189] iteration 670 : loss : 609.898926, loss_ce: 0.029649, loss_kd: 3046.978516
[21:24:24.758] iteration 680 : loss : 473.121124, loss_ce: 0.029306, loss_kd: 2363.020508
[21:24:30.343] iteration 690 : loss : 576.306763, loss_ce: 0.010384, loss_kd: 2878.968262
[21:24:35.916] iteration 700 : loss : 526.750366, loss_ce: 0.026370, loss_kd: 2631.155762
[21:24:41.504] iteration 710 : loss : 584.441956, loss_ce: 0.036167, loss_kd: 2919.721680
[21:24:47.073] iteration 720 : loss : 764.028564, loss_ce: 0.021448, loss_kd: 3817.684082
[21:24:52.666] iteration 730 : loss : 900.618652, loss_ce: 0.031910, loss_kd: 4500.566895
[21:24:58.248] iteration 740 : loss : 480.019318, loss_ce: 0.026861, loss_kd: 2397.585449
[21:25:03.831] iteration 750 : loss : 534.186340, loss_ce: 0.027957, loss_kd: 2668.437012
[21:25:09.418] iteration 760 : loss : 514.026001, loss_ce: 0.018145, loss_kd: 2567.533447
[21:25:15.007] iteration 770 : loss : 597.856079, loss_ce: 0.026316, loss_kd: 2986.812500
[21:25:20.585] iteration 780 : loss : 566.424133, loss_ce: 0.024469, loss_kd: 2829.616211
[21:25:26.179] iteration 790 : loss : 654.215332, loss_ce: 0.017572, loss_kd: 3268.614990
[21:25:31.765] iteration 800 : loss : 585.397827, loss_ce: 0.031141, loss_kd: 2924.473145
[21:25:37.371] iteration 810 : loss : 935.514221, loss_ce: 0.014806, loss_kd: 4675.107910
[21:25:42.952] iteration 820 : loss : 574.166931, loss_ce: 0.031012, loss_kd: 2868.314941
[21:25:48.549] iteration 830 : loss : 485.800568, loss_ce: 0.029553, loss_kd: 2426.451416
[21:25:54.128] iteration 840 : loss : 649.747375, loss_ce: 0.031866, loss_kd: 3246.232422
[21:25:59.735] iteration 850 : loss : 658.810608, loss_ce: 0.024692, loss_kd: 3291.473877
[21:26:05.313] iteration 860 : loss : 566.203979, loss_ce: 0.024484, loss_kd: 2828.531494
[21:26:10.912] iteration 870 : loss : 595.320557, loss_ce: 0.016833, loss_kd: 2974.104248
[21:26:16.505] iteration 880 : loss : 519.242432, loss_ce: 0.028878, loss_kd: 2593.676025
[21:26:22.113] iteration 890 : loss : 470.383026, loss_ce: 0.020163, loss_kd: 2349.469482
[21:26:27.711] iteration 900 : loss : 473.475281, loss_ce: 0.028349, loss_kd: 2364.906494
[21:26:33.314] iteration 910 : loss : 583.796326, loss_ce: 0.023239, loss_kd: 2916.485840
[21:26:38.908] iteration 920 : loss : 478.967621, loss_ce: 0.023053, loss_kd: 2392.376221
[21:26:44.509] iteration 930 : loss : 453.538513, loss_ce: 0.023613, loss_kd: 2265.204346
[21:26:50.102] iteration 940 : loss : 380.113953, loss_ce: 0.025611, loss_kd: 1898.067383
[21:26:55.708] iteration 950 : loss : 484.440552, loss_ce: 0.020849, loss_kd: 2419.705811
[21:27:01.306] iteration 960 : loss : 627.721924, loss_ce: 0.027742, loss_kd: 3136.147949
[21:27:06.907] iteration 970 : loss : 739.701721, loss_ce: 0.045098, loss_kd: 3696.001221
[21:27:12.497] iteration 980 : loss : 599.384460, loss_ce: 0.042690, loss_kd: 2994.401855
[21:27:18.100] iteration 990 : loss : 605.171448, loss_ce: 0.036125, loss_kd: 3023.366211
[21:27:23.693] iteration 1000 : loss : 609.007996, loss_ce: 0.034446, loss_kd: 3042.546631
[21:27:29.295] iteration 1010 : loss : 510.361206, loss_ce: 0.023906, loss_kd: 2549.279053
[21:27:34.900] iteration 1020 : loss : 503.416168, loss_ce: 0.041732, loss_kd: 2514.533447
[21:27:40.509] iteration 1030 : loss : 589.781555, loss_ce: 0.022486, loss_kd: 2946.373535
[21:27:46.111] iteration 1040 : loss : 394.423431, loss_ce: 0.025458, loss_kd: 1969.651855
[21:28:03.034] iteration 1050 : loss : 683.187744, loss_ce: 0.030482, loss_kd: 3413.483887
[21:28:08.580] iteration 1060 : loss : 516.715332, loss_ce: 0.038101, loss_kd: 2581.070801
[21:28:14.139] iteration 1070 : loss : 427.219055, loss_ce: 0.022765, loss_kd: 2133.585449
[21:28:19.696] iteration 1080 : loss : 515.924988, loss_ce: 0.036127, loss_kd: 2577.120605
[21:28:25.262] iteration 1090 : loss : 656.595947, loss_ce: 0.022965, loss_kd: 3280.500000
[21:28:30.822] iteration 1100 : loss : 504.643005, loss_ce: 0.017974, loss_kd: 2520.726074
[21:28:36.405] iteration 1110 : loss : 381.765442, loss_ce: 0.029321, loss_kd: 1906.307251
[21:28:41.977] iteration 1120 : loss : 381.729919, loss_ce: 0.023970, loss_kd: 1906.151001
[21:28:47.558] iteration 1130 : loss : 492.064453, loss_ce: 0.027833, loss_kd: 2457.829590
[21:28:53.130] iteration 1140 : loss : 500.781006, loss_ce: 0.023280, loss_kd: 2501.398682
[21:28:58.710] iteration 1150 : loss : 529.758179, loss_ce: 0.021761, loss_kd: 2646.319092
[21:29:04.296] iteration 1160 : loss : 407.082977, loss_ce: 0.020946, loss_kd: 2032.914307
[21:29:09.897] iteration 1170 : loss : 461.934631, loss_ce: 0.024685, loss_kd: 2307.174561
[21:29:15.484] iteration 1180 : loss : 391.000122, loss_ce: 0.019066, loss_kd: 1952.548218
[21:29:21.079] iteration 1190 : loss : 471.079620, loss_ce: 0.046390, loss_kd: 2352.829834
[21:29:26.668] iteration 1200 : loss : 532.304199, loss_ce: 0.033235, loss_kd: 2659.018311
[21:29:32.267] iteration 1210 : loss : 498.747162, loss_ce: 0.027136, loss_kd: 2491.279297
[21:29:37.853] iteration 1220 : loss : 426.695557, loss_ce: 0.030030, loss_kd: 2130.973633
[21:29:43.459] iteration 1230 : loss : 715.535461, loss_ce: 0.024157, loss_kd: 3575.202148
[21:29:49.048] iteration 1240 : loss : 610.420776, loss_ce: 0.034560, loss_kd: 3049.546875
[21:29:54.646] iteration 1250 : loss : 435.131226, loss_ce: 0.023337, loss_kd: 2173.179443
[21:30:00.247] iteration 1260 : loss : 389.467224, loss_ce: 0.022406, loss_kd: 1944.869751
[21:30:05.850] iteration 1270 : loss : 436.547760, loss_ce: 0.020922, loss_kd: 2180.276367
[21:30:11.445] iteration 1280 : loss : 574.799561, loss_ce: 0.021636, loss_kd: 2871.554688
[21:30:17.055] iteration 1290 : loss : 629.368469, loss_ce: 0.036127, loss_kd: 3144.359375
[21:30:22.648] iteration 1300 : loss : 493.297272, loss_ce: 0.028302, loss_kd: 2464.002686
[21:30:28.258] iteration 1310 : loss : 559.398254, loss_ce: 0.030764, loss_kd: 2794.494629
[21:30:33.853] iteration 1320 : loss : 403.992188, loss_ce: 0.037350, loss_kd: 2017.475830
[21:30:39.462] iteration 1330 : loss : 477.639435, loss_ce: 0.026757, loss_kd: 2385.666260
[21:30:45.075] iteration 1340 : loss : 418.140564, loss_ce: 0.021429, loss_kd: 2088.208740
[21:30:50.694] iteration 1350 : loss : 425.711517, loss_ce: 0.026240, loss_kd: 2126.065674
[21:30:56.290] iteration 1360 : loss : 462.877747, loss_ce: 0.024209, loss_kd: 2311.874023
[21:31:01.898] iteration 1370 : loss : 371.642792, loss_ce: 0.023859, loss_kd: 1855.712769
[21:31:07.497] iteration 1380 : loss : 460.644806, loss_ce: 0.032750, loss_kd: 2300.709961
[21:31:13.117] iteration 1390 : loss : 476.514435, loss_ce: 0.034160, loss_kd: 2380.102051
[21:31:18.721] iteration 1400 : loss : 416.478302, loss_ce: 0.030665, loss_kd: 2079.886230
[21:31:24.350] iteration 1410 : loss : 352.911041, loss_ce: 0.029168, loss_kd: 1762.036133
[21:31:29.967] iteration 1420 : loss : 506.873718, loss_ce: 0.027343, loss_kd: 2531.858154
[21:31:35.580] iteration 1430 : loss : 481.505127, loss_ce: 0.018358, loss_kd: 2405.022217
[21:31:41.191] iteration 1440 : loss : 373.920349, loss_ce: 0.023715, loss_kd: 1867.085571
[21:31:46.813] iteration 1450 : loss : 462.675720, loss_ce: 0.021312, loss_kd: 2310.938477
[21:31:52.428] iteration 1460 : loss : 393.100647, loss_ce: 0.021786, loss_kd: 1963.027222
[21:31:58.048] iteration 1470 : loss : 469.901428, loss_ce: 0.022340, loss_kd: 2347.045410
[21:32:03.661] iteration 1480 : loss : 460.285553, loss_ce: 0.018507, loss_kd: 2298.948486
[21:32:09.313] iteration 1490 : loss : 531.066528, loss_ce: 0.025162, loss_kd: 2652.843750
[21:32:14.938] iteration 1500 : loss : 478.712769, loss_ce: 0.023983, loss_kd: 2391.072754
[21:32:20.560] iteration 1510 : loss : 619.422852, loss_ce: 0.040409, loss_kd: 3094.554688
[21:32:26.169] iteration 1520 : loss : 669.699951, loss_ce: 0.020354, loss_kd: 3346.015625
[21:32:31.792] iteration 1530 : loss : 481.969849, loss_ce: 0.022423, loss_kd: 2407.383057
[21:32:37.416] iteration 1540 : loss : 377.662109, loss_ce: 0.029404, loss_kd: 1885.836182
[21:32:43.032] iteration 1550 : loss : 465.745056, loss_ce: 0.026439, loss_kd: 2326.255615
[21:32:48.645] iteration 1560 : loss : 410.835907, loss_ce: 0.027626, loss_kd: 2051.693848
[21:32:54.275] iteration 1570 : loss : 385.251862, loss_ce: 0.049227, loss_kd: 1923.732544
[21:32:59.884] iteration 1580 : loss : 381.521454, loss_ce: 0.016673, loss_kd: 1905.117554
[21:33:05.502] iteration 1590 : loss : 434.395844, loss_ce: 0.046007, loss_kd: 2169.462158
[21:33:11.106] iteration 1600 : loss : 470.231659, loss_ce: 0.027512, loss_kd: 2348.682129
[21:33:16.741] iteration 1610 : loss : 460.178040, loss_ce: 0.026996, loss_kd: 2298.457520
[21:33:22.356] iteration 1620 : loss : 416.279907, loss_ce: 0.018650, loss_kd: 2078.944580
[21:33:28.006] iteration 1630 : loss : 452.216248, loss_ce: 0.033166, loss_kd: 2258.594238
[21:33:33.622] iteration 1640 : loss : 411.002197, loss_ce: 0.032932, loss_kd: 2052.492432
[21:33:39.274] iteration 1650 : loss : 397.264771, loss_ce: 0.043430, loss_kd: 1983.833984
[21:33:44.886] iteration 1660 : loss : 351.739899, loss_ce: 0.030623, loss_kd: 1756.234009
[21:33:50.515] iteration 1670 : loss : 302.498962, loss_ce: 0.032235, loss_kd: 1509.969849
[21:33:56.150] iteration 1680 : loss : 449.495850, loss_ce: 0.029471, loss_kd: 2245.006104
[21:34:01.773] iteration 1690 : loss : 499.248840, loss_ce: 0.036076, loss_kd: 2493.762451
[21:34:07.383] iteration 1700 : loss : 424.257080, loss_ce: 0.015305, loss_kd: 2118.826660
[21:34:13.021] iteration 1710 : loss : 456.724792, loss_ce: 0.024889, loss_kd: 2281.144287
[21:34:18.642] iteration 1720 : loss : 277.337189, loss_ce: 0.031989, loss_kd: 1384.202759
[21:34:24.276] iteration 1730 : loss : 309.275604, loss_ce: 0.036747, loss_kd: 1543.837524
[21:34:29.897] iteration 1740 : loss : 360.511017, loss_ce: 0.026524, loss_kd: 1800.085205
[21:34:35.523] iteration 1750 : loss : 420.706696, loss_ce: 0.013666, loss_kd: 2101.064697
[21:34:41.150] iteration 1760 : loss : 354.594269, loss_ce: 0.009595, loss_kd: 1770.531616
[21:34:46.773] iteration 1770 : loss : 399.985413, loss_ce: 0.027712, loss_kd: 1997.450928
[21:34:52.405] iteration 1780 : loss : 391.595947, loss_ce: 0.024421, loss_kd: 1955.570557
[21:34:58.039] iteration 1790 : loss : 542.667236, loss_ce: 0.024785, loss_kd: 2710.881104
[21:35:03.666] iteration 1800 : loss : 391.997314, loss_ce: 0.027683, loss_kd: 1957.519287
[21:35:09.317] iteration 1810 : loss : 489.702850, loss_ce: 0.024026, loss_kd: 2446.042969
[21:35:14.943] iteration 1820 : loss : 460.520599, loss_ce: 0.035580, loss_kd: 2300.133545
[21:35:20.571] iteration 1830 : loss : 588.246094, loss_ce: 0.041317, loss_kd: 2938.742188
[21:35:26.184] iteration 1840 : loss : 383.212006, loss_ce: 0.019259, loss_kd: 1913.579102
[21:35:31.826] iteration 1850 : loss : 402.124298, loss_ce: 0.031088, loss_kd: 2008.121948
[21:35:37.452] iteration 1860 : loss : 402.120026, loss_ce: 0.019179, loss_kd: 2008.168091
[21:35:43.089] iteration 1870 : loss : 354.889404, loss_ce: 0.030531, loss_kd: 1771.950439
[21:35:48.714] iteration 1880 : loss : 379.481842, loss_ce: 0.021774, loss_kd: 1894.923218
[21:35:54.347] iteration 1890 : loss : 400.728607, loss_ce: 0.026684, loss_kd: 2001.168091
[21:35:59.981] iteration 1900 : loss : 382.100861, loss_ce: 0.022769, loss_kd: 1908.035400
[21:36:05.608] iteration 1910 : loss : 384.619354, loss_ce: 0.014312, loss_kd: 1920.623657
[21:36:11.231] iteration 1920 : loss : 361.396759, loss_ce: 0.032227, loss_kd: 1804.503662
[21:36:16.874] iteration 1930 : loss : 370.699951, loss_ce: 0.017942, loss_kd: 1851.011230
[21:36:22.494] iteration 1940 : loss : 316.127991, loss_ce: 0.031110, loss_kd: 1578.164429
[21:36:28.132] iteration 1950 : loss : 448.308899, loss_ce: 0.019158, loss_kd: 2239.088379
[21:36:33.751] iteration 1960 : loss : 434.662323, loss_ce: 0.019783, loss_kd: 2170.839600
[21:36:39.396] iteration 1970 : loss : 411.672516, loss_ce: 0.020343, loss_kd: 2055.890869
[21:36:45.021] iteration 1980 : loss : 382.966644, loss_ce: 0.025101, loss_kd: 1912.342163
[21:36:50.660] iteration 1990 : loss : 548.548279, loss_ce: 0.027673, loss_kd: 2740.270020
[21:36:56.276] iteration 2000 : loss : 546.899963, loss_ce: 0.020495, loss_kd: 2732.026855
[21:37:01.923] iteration 2010 : loss : 471.203156, loss_ce: 0.031057, loss_kd: 2353.550537
[21:37:07.543] iteration 2020 : loss : 350.663391, loss_ce: 0.039570, loss_kd: 1750.835938
[21:37:13.200] iteration 2030 : loss : 486.245911, loss_ce: 0.014097, loss_kd: 2428.773682
[21:37:18.814] iteration 2040 : loss : 298.577362, loss_ce: 0.018599, loss_kd: 1490.409790
[21:37:24.456] iteration 2050 : loss : 396.531738, loss_ce: 0.019128, loss_kd: 1980.167725
[21:37:30.086] iteration 2060 : loss : 562.049744, loss_ce: 0.025048, loss_kd: 2807.765625
[21:37:35.724] iteration 2070 : loss : 470.585266, loss_ce: 0.023462, loss_kd: 2350.445801
[21:37:41.347] iteration 2080 : loss : 483.441101, loss_ce: 0.023839, loss_kd: 2414.730713
[21:37:58.354] iteration 2090 : loss : 680.301758, loss_ce: 0.030092, loss_kd: 3399.059570
[21:38:03.906] iteration 2100 : loss : 338.006378, loss_ce: 0.024608, loss_kd: 1687.585327
[21:38:09.475] iteration 2110 : loss : 337.393585, loss_ce: 0.026779, loss_kd: 1684.483032
[21:38:15.037] iteration 2120 : loss : 334.872864, loss_ce: 0.031137, loss_kd: 1671.862793
[21:38:20.616] iteration 2130 : loss : 403.215637, loss_ce: 0.019834, loss_kd: 2013.639160
[21:38:26.186] iteration 2140 : loss : 372.606354, loss_ce: 0.027773, loss_kd: 1860.550903
[21:38:31.773] iteration 2150 : loss : 247.437363, loss_ce: 0.017042, loss_kd: 1234.624146
[21:38:37.353] iteration 2160 : loss : 318.626434, loss_ce: 0.024444, loss_kd: 1590.661987
[21:38:42.967] iteration 2170 : loss : 457.437042, loss_ce: 0.020524, loss_kd: 2284.717529
[21:38:48.551] iteration 2180 : loss : 388.971497, loss_ce: 0.036849, loss_kd: 1942.388550
[21:38:54.158] iteration 2190 : loss : 296.297577, loss_ce: 0.022134, loss_kd: 1479.025146
[21:38:59.756] iteration 2200 : loss : 302.666138, loss_ce: 0.026902, loss_kd: 1510.818237
[21:39:05.366] iteration 2210 : loss : 277.392273, loss_ce: 0.020978, loss_kd: 1384.514526
[21:39:10.968] iteration 2220 : loss : 346.544983, loss_ce: 0.027079, loss_kd: 1730.254883
[21:39:16.578] iteration 2230 : loss : 390.442444, loss_ce: 0.014809, loss_kd: 1949.719482
[21:39:22.172] iteration 2240 : loss : 369.498962, loss_ce: 0.020930, loss_kd: 1845.031616
[21:39:27.780] iteration 2250 : loss : 322.149261, loss_ce: 0.019059, loss_kd: 1608.316650
[21:39:33.373] iteration 2260 : loss : 339.865448, loss_ce: 0.033935, loss_kd: 1696.815918
[21:39:38.986] iteration 2270 : loss : 263.103333, loss_ce: 0.025051, loss_kd: 1313.042847
[21:39:44.586] iteration 2280 : loss : 352.772919, loss_ce: 0.022250, loss_kd: 1761.429321
[21:39:50.218] iteration 2290 : loss : 358.265564, loss_ce: 0.025808, loss_kd: 1788.866455
[21:39:55.829] iteration 2300 : loss : 451.889069, loss_ce: 0.030149, loss_kd: 2256.968262
[21:40:01.451] iteration 2310 : loss : 397.704407, loss_ce: 0.029234, loss_kd: 1986.035034
[21:40:07.059] iteration 2320 : loss : 466.147614, loss_ce: 0.039435, loss_kd: 2328.281250
[21:40:12.693] iteration 2330 : loss : 441.678009, loss_ce: 0.032249, loss_kd: 2205.942139
[21:40:18.308] iteration 2340 : loss : 407.930237, loss_ce: 0.020706, loss_kd: 2037.227295
[21:40:23.926] iteration 2350 : loss : 310.138153, loss_ce: 0.017898, loss_kd: 1548.236206
[21:40:29.544] iteration 2360 : loss : 355.640381, loss_ce: 0.031228, loss_kd: 1775.738037
[21:40:35.162] iteration 2370 : loss : 265.104126, loss_ce: 0.027359, loss_kd: 1323.056030
[21:40:40.780] iteration 2380 : loss : 307.116821, loss_ce: 0.019323, loss_kd: 1533.139404
[21:40:46.397] iteration 2390 : loss : 475.270691, loss_ce: 0.024278, loss_kd: 2373.878906
[21:40:52.028] iteration 2400 : loss : 420.888519, loss_ce: 0.017451, loss_kd: 2101.955078
[21:40:57.665] iteration 2410 : loss : 345.361145, loss_ce: 0.029030, loss_kd: 1724.351440
[21:41:03.289] iteration 2420 : loss : 346.765747, loss_ce: 0.033392, loss_kd: 1731.332397
[21:41:08.914] iteration 2430 : loss : 316.202515, loss_ce: 0.033982, loss_kd: 1578.519653
[21:41:14.557] iteration 2440 : loss : 345.837799, loss_ce: 0.018342, loss_kd: 1726.716064
[21:41:20.186] iteration 2450 : loss : 437.673248, loss_ce: 0.022533, loss_kd: 2185.898926
[21:41:25.814] iteration 2460 : loss : 338.057678, loss_ce: 0.020913, loss_kd: 1687.824951
[21:41:31.433] iteration 2470 : loss : 392.170471, loss_ce: 0.013096, loss_kd: 1958.389648
[21:41:37.054] iteration 2480 : loss : 332.376099, loss_ce: 0.025463, loss_kd: 1659.421265
[21:41:42.692] iteration 2490 : loss : 372.011749, loss_ce: 0.042399, loss_kd: 1857.575928
[21:41:48.317] iteration 2500 : loss : 363.886719, loss_ce: 0.026837, loss_kd: 1816.966797
[21:41:53.958] iteration 2510 : loss : 269.400482, loss_ce: 0.027202, loss_kd: 1344.452148
[21:41:59.579] iteration 2520 : loss : 331.191833, loss_ce: 0.019759, loss_kd: 1653.484131
[21:42:05.224] iteration 2530 : loss : 360.946777, loss_ce: 0.024464, loss_kd: 1802.245361
[21:42:10.843] iteration 2540 : loss : 365.283478, loss_ce: 0.023927, loss_kd: 1823.947144
[21:42:16.490] iteration 2550 : loss : 514.130554, loss_ce: 0.023508, loss_kd: 2568.174805
[21:42:22.108] iteration 2560 : loss : 299.025360, loss_ce: 0.028659, loss_kd: 1492.702881
[21:42:27.717] iteration 2570 : loss : 316.714355, loss_ce: 0.027666, loss_kd: 1581.147705
[21:42:33.351] iteration 2580 : loss : 331.514282, loss_ce: 0.031747, loss_kd: 1655.060669
[21:42:38.988] iteration 2590 : loss : 485.051361, loss_ce: 0.028573, loss_kd: 2422.812256
[21:42:44.623] iteration 2600 : loss : 405.277893, loss_ce: 0.028073, loss_kd: 2023.916748
[21:42:50.257] iteration 2610 : loss : 198.898483, loss_ce: 0.019671, loss_kd: 991.954285
[21:42:55.883] iteration 2620 : loss : 412.388153, loss_ce: 0.035966, loss_kd: 2059.479736
[21:43:01.519] iteration 2630 : loss : 294.088165, loss_ce: 0.026766, loss_kd: 1467.979126
[21:43:07.155] iteration 2640 : loss : 332.279053, loss_ce: 0.027812, loss_kd: 1658.890259
[21:43:12.789] iteration 2650 : loss : 311.542389, loss_ce: 0.021465, loss_kd: 1555.231567
[21:43:18.429] iteration 2660 : loss : 296.622070, loss_ce: 0.026211, loss_kd: 1480.665649
[21:43:24.072] iteration 2670 : loss : 275.562744, loss_ce: 0.033274, loss_kd: 1375.333374
[21:43:29.705] iteration 2680 : loss : 259.124054, loss_ce: 0.014139, loss_kd: 1293.146606
[21:43:35.341] iteration 2690 : loss : 373.126251, loss_ce: 0.020069, loss_kd: 1863.175537
[21:43:40.970] iteration 2700 : loss : 324.240753, loss_ce: 0.023409, loss_kd: 1618.765381
[21:43:46.620] iteration 2710 : loss : 315.455261, loss_ce: 0.033788, loss_kd: 1574.810669
[21:43:52.247] iteration 2720 : loss : 327.277313, loss_ce: 0.015155, loss_kd: 1633.945190
[21:43:57.889] iteration 2730 : loss : 391.776611, loss_ce: 0.026803, loss_kd: 1956.369507
[21:44:03.520] iteration 2740 : loss : 341.741760, loss_ce: 0.017031, loss_kd: 1706.214844
[21:44:09.167] iteration 2750 : loss : 417.183258, loss_ce: 0.037840, loss_kd: 2083.438721
[21:44:14.802] iteration 2760 : loss : 325.420593, loss_ce: 0.030887, loss_kd: 1624.613159
[21:44:20.442] iteration 2770 : loss : 312.534210, loss_ce: 0.036162, loss_kd: 1560.147583
[21:44:26.081] iteration 2780 : loss : 383.063538, loss_ce: 0.014545, loss_kd: 1912.841675
[21:44:31.714] iteration 2790 : loss : 425.465118, loss_ce: 0.024761, loss_kd: 2124.862549
[21:44:37.350] iteration 2800 : loss : 370.405609, loss_ce: 0.022538, loss_kd: 1849.564209
[21:44:42.989] iteration 2810 : loss : 256.879303, loss_ce: 0.028562, loss_kd: 1281.879639
[21:44:48.623] iteration 2820 : loss : 356.062164, loss_ce: 0.033078, loss_kd: 1777.849243
[21:44:54.264] iteration 2830 : loss : 425.736023, loss_ce: 0.022256, loss_kd: 2126.244141
[21:44:59.897] iteration 2840 : loss : 270.042023, loss_ce: 0.022932, loss_kd: 1347.723877
[21:45:05.526] iteration 2850 : loss : 293.557678, loss_ce: 0.023046, loss_kd: 1465.332397
[21:45:11.166] iteration 2860 : loss : 327.280518, loss_ce: 0.036921, loss_kd: 1633.928589
[21:45:16.790] iteration 2870 : loss : 328.373871, loss_ce: 0.018200, loss_kd: 1639.359741
[21:45:22.429] iteration 2880 : loss : 543.818848, loss_ce: 0.027893, loss_kd: 2716.592285
[21:45:28.058] iteration 2890 : loss : 285.295563, loss_ce: 0.030640, loss_kd: 1424.012451
[21:45:33.694] iteration 2900 : loss : 301.591217, loss_ce: 0.018411, loss_kd: 1505.432861
[21:45:39.350] iteration 2910 : loss : 288.093964, loss_ce: 0.045073, loss_kd: 1437.991577
[21:45:44.971] iteration 2920 : loss : 243.412476, loss_ce: 0.025123, loss_kd: 1214.587891
[21:45:50.627] iteration 2930 : loss : 382.141418, loss_ce: 0.026997, loss_kd: 1908.187256
[21:45:56.247] iteration 2940 : loss : 329.743652, loss_ce: 0.018524, loss_kd: 1646.240601
[21:46:01.902] iteration 2950 : loss : 413.361877, loss_ce: 0.018850, loss_kd: 2064.307373
[21:46:07.529] iteration 2960 : loss : 410.839722, loss_ce: 0.026171, loss_kd: 2051.754639
[21:46:13.176] iteration 2970 : loss : 439.483673, loss_ce: 0.013650, loss_kd: 2194.975830
[21:46:18.793] iteration 2980 : loss : 307.213562, loss_ce: 0.018630, loss_kd: 1533.618896
[21:46:24.432] iteration 2990 : loss : 291.056091, loss_ce: 0.019740, loss_kd: 1452.830200
[21:46:30.066] iteration 3000 : loss : 329.384308, loss_ce: 0.026558, loss_kd: 1644.470459
[21:46:35.706] iteration 3010 : loss : 302.725250, loss_ce: 0.020480, loss_kd: 1511.158691
[21:46:41.321] iteration 3020 : loss : 376.118256, loss_ce: 0.020681, loss_kd: 1878.093384
[21:46:46.975] iteration 3030 : loss : 341.163483, loss_ce: 0.023979, loss_kd: 1703.423584
[21:46:52.605] iteration 3040 : loss : 310.795624, loss_ce: 0.018667, loss_kd: 1551.516724
[21:46:58.253] iteration 3050 : loss : 312.014069, loss_ce: 0.030821, loss_kd: 1557.595947
[21:47:03.867] iteration 3060 : loss : 345.616730, loss_ce: 0.021102, loss_kd: 1725.510498
[21:47:09.515] iteration 3070 : loss : 239.969070, loss_ce: 0.032755, loss_kd: 1197.370850
[21:47:15.158] iteration 3080 : loss : 301.858704, loss_ce: 0.025357, loss_kd: 1506.823364
[21:47:20.780] iteration 3090 : loss : 277.824402, loss_ce: 0.020309, loss_kd: 1386.636353
[21:47:26.426] iteration 3100 : loss : 308.825439, loss_ce: 0.019718, loss_kd: 1541.655640
[21:47:32.064] iteration 3110 : loss : 349.353973, loss_ce: 0.020756, loss_kd: 1744.310547
[21:47:37.714] iteration 3120 : loss : 243.832901, loss_ce: 0.022509, loss_kd: 1216.748169
[21:47:54.660] iteration 3130 : loss : 304.587921, loss_ce: 0.028053, loss_kd: 1520.447388
[21:48:00.209] iteration 3140 : loss : 337.030029, loss_ce: 0.025144, loss_kd: 1682.689697
[21:48:05.777] iteration 3150 : loss : 344.945557, loss_ce: 0.028478, loss_kd: 1722.233154
[21:48:11.338] iteration 3160 : loss : 311.248535, loss_ce: 0.035267, loss_kd: 1553.800293
[21:48:16.928] iteration 3170 : loss : 407.513885, loss_ce: 0.022655, loss_kd: 2035.086304
[21:48:22.499] iteration 3180 : loss : 322.693176, loss_ce: 0.017115, loss_kd: 1611.008789
[21:48:28.081] iteration 3190 : loss : 457.515442, loss_ce: 0.021389, loss_kd: 2285.110107
[21:48:33.660] iteration 3200 : loss : 259.916443, loss_ce: 0.021208, loss_kd: 1297.082153
[21:48:39.260] iteration 3210 : loss : 261.446533, loss_ce: 0.028494, loss_kd: 1304.780273
[21:48:44.845] iteration 3220 : loss : 340.908112, loss_ce: 0.018460, loss_kd: 1702.064819
[21:48:50.447] iteration 3230 : loss : 400.789001, loss_ce: 0.036670, loss_kd: 2001.408691
[21:48:56.051] iteration 3240 : loss : 304.186401, loss_ce: 0.027775, loss_kd: 1518.503418
[21:49:01.666] iteration 3250 : loss : 293.729675, loss_ce: 0.019906, loss_kd: 1466.201050
[21:49:07.259] iteration 3260 : loss : 226.336075, loss_ce: 0.026656, loss_kd: 1129.208618
[21:49:12.868] iteration 3270 : loss : 328.969116, loss_ce: 0.013025, loss_kd: 1642.366699
[21:49:18.464] iteration 3280 : loss : 288.646271, loss_ce: 0.021749, loss_kd: 1440.766968
[21:49:24.075] iteration 3290 : loss : 248.775177, loss_ce: 0.024365, loss_kd: 1241.414673
[21:49:29.682] iteration 3300 : loss : 541.258179, loss_ce: 0.021908, loss_kd: 2703.846680
[21:49:35.311] iteration 3310 : loss : 360.673035, loss_ce: 0.028919, loss_kd: 1800.909790
[21:49:40.926] iteration 3320 : loss : 278.977753, loss_ce: 0.025779, loss_kd: 1392.401733
[21:49:46.542] iteration 3330 : loss : 269.024933, loss_ce: 0.016612, loss_kd: 1342.627686
[21:49:52.143] iteration 3340 : loss : 332.327545, loss_ce: 0.029773, loss_kd: 1659.158936
[21:49:57.785] iteration 3350 : loss : 331.240967, loss_ce: 0.027848, loss_kd: 1653.765137
[21:50:03.395] iteration 3360 : loss : 264.648468, loss_ce: 0.025103, loss_kd: 1320.828613
[21:50:09.022] iteration 3370 : loss : 320.252014, loss_ce: 0.019732, loss_kd: 1598.773926
[21:50:14.634] iteration 3380 : loss : 236.389709, loss_ce: 0.031746, loss_kd: 1179.462524
[21:50:20.282] iteration 3390 : loss : 301.394928, loss_ce: 0.016739, loss_kd: 1504.518066
[21:50:25.895] iteration 3400 : loss : 320.540222, loss_ce: 0.021355, loss_kd: 1600.250977
[21:50:31.545] iteration 3410 : loss : 260.710449, loss_ce: 0.019496, loss_kd: 1301.109619
[21:50:37.159] iteration 3420 : loss : 345.550201, loss_ce: 0.018532, loss_kd: 1725.270874
[21:50:42.800] iteration 3430 : loss : 314.716949, loss_ce: 0.028769, loss_kd: 1571.091919
[21:50:48.420] iteration 3440 : loss : 285.260040, loss_ce: 0.024239, loss_kd: 1423.802246
[21:50:54.059] iteration 3450 : loss : 329.720184, loss_ce: 0.022111, loss_kd: 1646.158936
[21:50:59.682] iteration 3460 : loss : 288.607941, loss_ce: 0.020526, loss_kd: 1440.557861
[21:51:05.325] iteration 3470 : loss : 286.560150, loss_ce: 0.022250, loss_kd: 1430.306641
[21:51:10.931] iteration 3480 : loss : 270.248596, loss_ce: 0.024871, loss_kd: 1348.764771
[21:51:16.573] iteration 3490 : loss : 334.710602, loss_ce: 0.027624, loss_kd: 1671.101929
[21:51:22.205] iteration 3500 : loss : 318.119080, loss_ce: 0.016376, loss_kd: 1588.114136
[21:51:27.840] iteration 3510 : loss : 297.207489, loss_ce: 0.034886, loss_kd: 1483.547974
[21:51:33.463] iteration 3520 : loss : 296.482697, loss_ce: 0.022113, loss_kd: 1479.990356
[21:51:39.114] iteration 3530 : loss : 387.393005, loss_ce: 0.021150, loss_kd: 1934.512695
[21:51:44.735] iteration 3540 : loss : 307.583801, loss_ce: 0.019222, loss_kd: 1535.458252
[21:51:50.372] iteration 3550 : loss : 469.278503, loss_ce: 0.025415, loss_kd: 2343.932129
[21:51:56.007] iteration 3560 : loss : 309.053314, loss_ce: 0.021114, loss_kd: 1542.782471
[21:52:01.646] iteration 3570 : loss : 331.044830, loss_ce: 0.021618, loss_kd: 1652.768066
[21:52:07.274] iteration 3580 : loss : 240.153519, loss_ce: 0.025124, loss_kd: 1198.311279
[21:52:12.927] iteration 3590 : loss : 275.580566, loss_ce: 0.028389, loss_kd: 1375.505859
[21:52:18.556] iteration 3600 : loss : 362.801147, loss_ce: 0.026563, loss_kd: 1811.538940
[21:52:24.200] iteration 3610 : loss : 371.994049, loss_ce: 0.019167, loss_kd: 1857.540161
[21:52:29.816] iteration 3620 : loss : 233.259689, loss_ce: 0.015021, loss_kd: 1163.856812
[21:52:35.458] iteration 3630 : loss : 282.542755, loss_ce: 0.021418, loss_kd: 1410.289062
[21:52:41.107] iteration 3640 : loss : 298.829956, loss_ce: 0.021862, loss_kd: 1491.717896
[21:52:46.744] iteration 3650 : loss : 214.141052, loss_ce: 0.031213, loss_kd: 1068.225830
[21:52:52.374] iteration 3660 : loss : 331.433929, loss_ce: 0.025087, loss_kd: 1654.687744
[21:52:58.015] iteration 3670 : loss : 297.485504, loss_ce: 0.013724, loss_kd: 1484.977783
[21:53:03.648] iteration 3680 : loss : 227.708572, loss_ce: 0.032201, loss_kd: 1136.076904
[21:53:09.282] iteration 3690 : loss : 397.999023, loss_ce: 0.031020, loss_kd: 1987.494019
[21:53:14.913] iteration 3700 : loss : 344.548187, loss_ce: 0.028379, loss_kd: 1720.261963
[21:53:20.559] iteration 3710 : loss : 329.723114, loss_ce: 0.029819, loss_kd: 1646.163696
[21:53:26.200] iteration 3720 : loss : 276.037384, loss_ce: 0.020533, loss_kd: 1377.723145
[21:53:31.842] iteration 3730 : loss : 290.124268, loss_ce: 0.019125, loss_kd: 1448.180664
[21:53:37.458] iteration 3740 : loss : 274.764191, loss_ce: 0.024759, loss_kd: 1371.375000
[21:53:43.116] iteration 3750 : loss : 256.855408, loss_ce: 0.020665, loss_kd: 1281.841675
[21:53:48.746] iteration 3760 : loss : 304.071289, loss_ce: 0.029955, loss_kd: 1517.913330
[21:53:54.392] iteration 3770 : loss : 357.057220, loss_ce: 0.023894, loss_kd: 1782.835205
[21:54:00.017] iteration 3780 : loss : 251.474777, loss_ce: 0.021170, loss_kd: 1254.922363
[21:54:05.660] iteration 3790 : loss : 270.295319, loss_ce: 0.025505, loss_kd: 1349.012085
[21:54:11.287] iteration 3800 : loss : 247.016953, loss_ce: 0.018768, loss_kd: 1232.622314
[21:54:16.940] iteration 3810 : loss : 206.774033, loss_ce: 0.019559, loss_kd: 1031.370239
[21:54:22.573] iteration 3820 : loss : 273.789185, loss_ce: 0.022377, loss_kd: 1366.413696
[21:54:28.212] iteration 3830 : loss : 203.070267, loss_ce: 0.027719, loss_kd: 1012.848267
[21:54:33.849] iteration 3840 : loss : 362.654755, loss_ce: 0.022236, loss_kd: 1810.841309
[21:54:39.487] iteration 3850 : loss : 278.855133, loss_ce: 0.027527, loss_kd: 1391.790283
[21:54:45.123] iteration 3860 : loss : 338.062592, loss_ce: 0.032728, loss_kd: 1687.843140
[21:54:50.759] iteration 3870 : loss : 241.991470, loss_ce: 0.017858, loss_kd: 1207.504395
[21:54:56.391] iteration 3880 : loss : 284.431061, loss_ce: 0.019492, loss_kd: 1419.739868
[21:55:02.026] iteration 3890 : loss : 283.491333, loss_ce: 0.027010, loss_kd: 1415.007812
[21:55:07.658] iteration 3900 : loss : 386.913605, loss_ce: 0.021952, loss_kd: 1932.119385
[21:55:13.298] iteration 3910 : loss : 259.314789, loss_ce: 0.017042, loss_kd: 1294.129150
[21:55:18.941] iteration 3920 : loss : 392.848755, loss_ce: 0.016022, loss_kd: 1961.825928
[21:55:24.590] iteration 3930 : loss : 211.831894, loss_ce: 0.020416, loss_kd: 1056.711670
[21:55:30.217] iteration 3940 : loss : 285.554138, loss_ce: 0.017362, loss_kd: 1425.308716
[21:55:35.852] iteration 3950 : loss : 290.441589, loss_ce: 0.033478, loss_kd: 1449.767090
[21:55:41.496] iteration 3960 : loss : 287.694000, loss_ce: 0.018126, loss_kd: 1436.024048
[21:55:47.150] iteration 3970 : loss : 308.596436, loss_ce: 0.031641, loss_kd: 1540.516846
[21:55:52.790] iteration 3980 : loss : 247.360474, loss_ce: 0.018694, loss_kd: 1234.324585
[21:55:58.439] iteration 3990 : loss : 243.903305, loss_ce: 0.022071, loss_kd: 1217.081055
[21:56:04.081] iteration 4000 : loss : 233.326187, loss_ce: 0.016214, loss_kd: 1164.189941
[21:56:09.740] iteration 4010 : loss : 333.576263, loss_ce: 0.037431, loss_kd: 1665.381958
[21:56:15.371] iteration 4020 : loss : 289.902954, loss_ce: 0.019395, loss_kd: 1446.987549
[21:56:21.028] iteration 4030 : loss : 352.865662, loss_ce: 0.017938, loss_kd: 1761.890625
[21:56:26.662] iteration 4040 : loss : 273.244476, loss_ce: 0.030139, loss_kd: 1363.752930
[21:56:32.297] iteration 4050 : loss : 256.430817, loss_ce: 0.020124, loss_kd: 1279.689575
[21:56:37.944] iteration 4060 : loss : 304.511322, loss_ce: 0.034394, loss_kd: 1520.072266
[21:56:43.600] iteration 4070 : loss : 225.423889, loss_ce: 0.023780, loss_kd: 1124.682129
[21:56:49.220] iteration 4080 : loss : 305.716095, loss_ce: 0.033060, loss_kd: 1526.089233
[21:56:54.865] iteration 4090 : loss : 227.767700, loss_ce: 0.019871, loss_kd: 1136.341187
[21:57:00.497] iteration 4100 : loss : 281.075256, loss_ce: 0.027820, loss_kd: 1402.939209
[21:57:06.153] iteration 4110 : loss : 263.387817, loss_ce: 0.019736, loss_kd: 1314.493530
[21:57:11.800] iteration 4120 : loss : 344.252960, loss_ce: 0.026991, loss_kd: 1718.811768
[21:57:17.430] iteration 4130 : loss : 292.880981, loss_ce: 0.022894, loss_kd: 1461.925659
[21:57:23.067] iteration 4140 : loss : 293.420441, loss_ce: 0.026992, loss_kd: 1464.654419
[21:57:28.719] iteration 4150 : loss : 277.405731, loss_ce: 0.025982, loss_kd: 1384.576660
[21:57:34.364] iteration 4160 : loss : 229.847977, loss_ce: 0.036530, loss_kd: 1146.783936
[21:57:51.220] iteration 4170 : loss : 261.692383, loss_ce: 0.030309, loss_kd: 1306.003296
[21:57:56.769] iteration 4180 : loss : 259.720245, loss_ce: 0.016755, loss_kd: 1296.159424
[21:58:02.336] iteration 4190 : loss : 267.440094, loss_ce: 0.015005, loss_kd: 1334.747681
[21:58:07.900] iteration 4200 : loss : 251.079178, loss_ce: 0.034828, loss_kd: 1252.892334
[21:58:13.484] iteration 4210 : loss : 281.182983, loss_ce: 0.019409, loss_kd: 1403.497070
[21:58:19.057] iteration 4220 : loss : 345.013153, loss_ce: 0.023496, loss_kd: 1722.603516
[21:58:24.642] iteration 4230 : loss : 406.068176, loss_ce: 0.027074, loss_kd: 2027.878662
[21:58:30.224] iteration 4240 : loss : 214.308640, loss_ce: 0.028270, loss_kd: 1069.094727
[21:58:35.817] iteration 4250 : loss : 268.454559, loss_ce: 0.020097, loss_kd: 1339.819580
[21:58:41.403] iteration 4260 : loss : 340.727386, loss_ce: 0.020007, loss_kd: 1701.171021
[21:58:47.000] iteration 4270 : loss : 275.302856, loss_ce: 0.023216, loss_kd: 1374.013428
[21:58:52.600] iteration 4280 : loss : 257.568787, loss_ce: 0.041637, loss_kd: 1285.276245
[21:58:58.202] iteration 4290 : loss : 237.850647, loss_ce: 0.018585, loss_kd: 1186.826904
[21:59:03.796] iteration 4300 : loss : 301.919952, loss_ce: 0.019350, loss_kd: 1507.177002
[21:59:09.413] iteration 4310 : loss : 260.247925, loss_ce: 0.024241, loss_kd: 1298.770264
[21:59:15.023] iteration 4320 : loss : 293.694763, loss_ce: 0.024334, loss_kd: 1466.053101
[21:59:20.633] iteration 4330 : loss : 245.201141, loss_ce: 0.039028, loss_kd: 1223.582520
[21:59:26.221] iteration 4340 : loss : 239.517105, loss_ce: 0.027296, loss_kd: 1195.151733
[21:59:31.830] iteration 4350 : loss : 241.432632, loss_ce: 0.029031, loss_kd: 1204.673340
[21:59:37.429] iteration 4360 : loss : 239.320129, loss_ce: 0.021666, loss_kd: 1194.169189
[21:59:43.040] iteration 4370 : loss : 263.645721, loss_ce: 0.025567, loss_kd: 1315.769531
[21:59:48.646] iteration 4380 : loss : 228.546021, loss_ce: 0.023779, loss_kd: 1140.291016
[21:59:54.272] iteration 4390 : loss : 202.963776, loss_ce: 0.019933, loss_kd: 1012.346008
[21:59:59.879] iteration 4400 : loss : 219.227844, loss_ce: 0.015267, loss_kd: 1093.715698
[22:00:05.488] iteration 4410 : loss : 203.354385, loss_ce: 0.020137, loss_kd: 1014.307373
[22:00:11.084] iteration 4420 : loss : 319.247223, loss_ce: 0.014651, loss_kd: 1593.797729
[22:00:16.691] iteration 4430 : loss : 282.767303, loss_ce: 0.011848, loss_kd: 1411.369873
[22:00:22.289] iteration 4440 : loss : 262.512970, loss_ce: 0.021223, loss_kd: 1310.113892
[22:00:27.899] iteration 4450 : loss : 298.838226, loss_ce: 0.023852, loss_kd: 1491.746460
[22:00:33.502] iteration 4460 : loss : 201.165558, loss_ce: 0.028276, loss_kd: 1003.371704
[22:00:39.120] iteration 4470 : loss : 286.640259, loss_ce: 0.017940, loss_kd: 1430.757202
[22:00:44.726] iteration 4480 : loss : 231.654785, loss_ce: 0.037353, loss_kd: 1155.797852
[22:00:50.333] iteration 4490 : loss : 273.133362, loss_ce: 0.023372, loss_kd: 1363.110352
[22:00:55.931] iteration 4500 : loss : 263.664032, loss_ce: 0.023149, loss_kd: 1315.842041
[22:01:01.538] iteration 4510 : loss : 323.761505, loss_ce: 0.022889, loss_kd: 1616.355957
[22:01:07.137] iteration 4520 : loss : 292.954895, loss_ce: 0.033114, loss_kd: 1462.293945
[22:01:12.734] iteration 4530 : loss : 264.597656, loss_ce: 0.032895, loss_kd: 1320.534180
[22:01:18.329] iteration 4540 : loss : 274.880157, loss_ce: 0.019164, loss_kd: 1371.956543
[22:01:23.933] iteration 4550 : loss : 308.103088, loss_ce: 0.025239, loss_kd: 1538.076660
[22:01:29.524] iteration 4560 : loss : 224.011444, loss_ce: 0.043229, loss_kd: 1117.591553
[22:01:35.126] iteration 4570 : loss : 354.026978, loss_ce: 0.019525, loss_kd: 1767.686768
[22:01:40.718] iteration 4580 : loss : 249.809860, loss_ce: 0.016411, loss_kd: 1246.572998
[22:01:46.324] iteration 4590 : loss : 261.709808, loss_ce: 0.021189, loss_kd: 1306.091675
[22:01:51.916] iteration 4600 : loss : 244.451126, loss_ce: 0.016661, loss_kd: 1219.843628
[22:01:57.520] iteration 4610 : loss : 276.370819, loss_ce: 0.020531, loss_kd: 1379.412231
[22:02:03.115] iteration 4620 : loss : 237.637344, loss_ce: 0.038230, loss_kd: 1185.746338
[22:02:08.714] iteration 4630 : loss : 327.454773, loss_ce: 0.024739, loss_kd: 1634.817871
[22:02:14.306] iteration 4640 : loss : 317.987183, loss_ce: 0.019049, loss_kd: 1587.467773
[22:02:19.908] iteration 4650 : loss : 225.876312, loss_ce: 0.021421, loss_kd: 1126.929932
[22:02:25.505] iteration 4660 : loss : 233.450897, loss_ce: 0.016531, loss_kd: 1164.801392
[22:02:31.110] iteration 4670 : loss : 319.711731, loss_ce: 0.024790, loss_kd: 1596.123413
[22:02:36.696] iteration 4680 : loss : 306.897034, loss_ce: 0.028475, loss_kd: 1532.065430
[22:02:42.296] iteration 4690 : loss : 295.991974, loss_ce: 0.031114, loss_kd: 1477.509766
[22:02:47.884] iteration 4700 : loss : 210.114288, loss_ce: 0.012396, loss_kd: 1048.114624
[22:02:53.489] iteration 4710 : loss : 201.405136, loss_ce: 0.018560, loss_kd: 1004.587280
[22:02:59.083] iteration 4720 : loss : 263.477448, loss_ce: 0.015152, loss_kd: 1314.924072
[22:03:04.686] iteration 4730 : loss : 272.582703, loss_ce: 0.026866, loss_kd: 1360.463623
[22:03:10.276] iteration 4740 : loss : 222.254562, loss_ce: 0.019301, loss_kd: 1108.812866
[22:03:15.884] iteration 4750 : loss : 200.585876, loss_ce: 0.021404, loss_kd: 1000.488464
[22:03:21.483] iteration 4760 : loss : 232.605896, loss_ce: 0.030889, loss_kd: 1160.536255
[22:03:27.076] iteration 4770 : loss : 260.984192, loss_ce: 0.022971, loss_kd: 1302.465820
[22:03:32.671] iteration 4780 : loss : 386.479950, loss_ce: 0.016815, loss_kd: 1929.966309
[22:03:38.269] iteration 4790 : loss : 219.768936, loss_ce: 0.026026, loss_kd: 1096.373779
[22:03:43.859] iteration 4800 : loss : 222.579758, loss_ce: 0.042990, loss_kd: 1110.416870
[22:03:49.468] iteration 4810 : loss : 262.683868, loss_ce: 0.023040, loss_kd: 1310.969727
[22:03:55.055] iteration 4820 : loss : 240.587204, loss_ce: 0.016184, loss_kd: 1200.504272
[22:04:00.653] iteration 4830 : loss : 243.312531, loss_ce: 0.018409, loss_kd: 1214.133911
[22:04:06.231] iteration 4840 : loss : 246.591568, loss_ce: 0.022341, loss_kd: 1230.497559
[22:04:11.819] iteration 4850 : loss : 308.790680, loss_ce: 0.021365, loss_kd: 1541.531006
[22:04:17.396] iteration 4860 : loss : 336.021210, loss_ce: 0.032992, loss_kd: 1677.626587
[22:04:22.992] iteration 4870 : loss : 231.299377, loss_ce: 0.007829, loss_kd: 1154.062988
[22:04:28.576] iteration 4880 : loss : 259.621948, loss_ce: 0.016883, loss_kd: 1295.646729
[22:04:34.179] iteration 4890 : loss : 369.400909, loss_ce: 0.019908, loss_kd: 1844.561035
[22:04:39.759] iteration 4900 : loss : 244.228287, loss_ce: 0.024643, loss_kd: 1218.652588
[22:04:45.350] iteration 4910 : loss : 207.127808, loss_ce: 0.026576, loss_kd: 1033.251465
[22:04:50.936] iteration 4920 : loss : 202.400757, loss_ce: 0.036398, loss_kd: 1009.542725
[22:04:56.524] iteration 4930 : loss : 213.445557, loss_ce: 0.020665, loss_kd: 1064.805054
[22:05:02.100] iteration 4940 : loss : 312.530518, loss_ce: 0.020152, loss_kd: 1560.246460
[22:05:07.690] iteration 4950 : loss : 204.175003, loss_ce: 0.030188, loss_kd: 1018.379272
[22:05:13.269] iteration 4960 : loss : 257.789795, loss_ce: 0.016815, loss_kd: 1286.505127
[22:05:18.863] iteration 4970 : loss : 214.574966, loss_ce: 0.022445, loss_kd: 1070.408936
[22:05:24.437] iteration 4980 : loss : 223.444794, loss_ce: 0.022980, loss_kd: 1114.757202
[22:05:30.026] iteration 4990 : loss : 219.285629, loss_ce: 0.017034, loss_kd: 1094.009033
[22:05:35.607] iteration 5000 : loss : 211.480896, loss_ce: 0.019020, loss_kd: 1054.953369
[22:05:41.209] iteration 5010 : loss : 248.236038, loss_ce: 0.029201, loss_kd: 1238.723877
[22:05:46.788] iteration 5020 : loss : 418.718475, loss_ce: 0.018075, loss_kd: 2091.170410
[22:05:52.377] iteration 5030 : loss : 259.209747, loss_ce: 0.028079, loss_kd: 1293.569946
[22:05:57.961] iteration 5040 : loss : 263.055450, loss_ce: 0.023368, loss_kd: 1312.855347
[22:06:03.554] iteration 5050 : loss : 214.393921, loss_ce: 0.035341, loss_kd: 1069.506348
[22:06:09.146] iteration 5060 : loss : 251.469345, loss_ce: 0.034897, loss_kd: 1254.906860
[22:06:14.746] iteration 5070 : loss : 224.375458, loss_ce: 0.017043, loss_kd: 1119.450806
[22:06:20.337] iteration 5080 : loss : 240.179611, loss_ce: 0.018721, loss_kd: 1198.441895
[22:06:25.938] iteration 5090 : loss : 267.411469, loss_ce: 0.014271, loss_kd: 1334.647949
[22:06:31.529] iteration 5100 : loss : 244.199356, loss_ce: 0.024949, loss_kd: 1218.566650
[22:06:37.135] iteration 5110 : loss : 263.407074, loss_ce: 0.012322, loss_kd: 1314.582520
[22:06:42.727] iteration 5120 : loss : 261.988617, loss_ce: 0.036680, loss_kd: 1307.484863
[22:06:48.336] iteration 5130 : loss : 226.623596, loss_ce: 0.012643, loss_kd: 1130.707520
[22:06:53.925] iteration 5140 : loss : 231.480499, loss_ce: 0.025206, loss_kd: 1154.944580
[22:06:59.533] iteration 5150 : loss : 268.771179, loss_ce: 0.019180, loss_kd: 1341.423584
[22:07:05.126] iteration 5160 : loss : 226.630524, loss_ce: 0.015635, loss_kd: 1130.690918
[22:07:10.734] iteration 5170 : loss : 197.552750, loss_ce: 0.020429, loss_kd: 985.325745
[22:07:16.329] iteration 5180 : loss : 202.705292, loss_ce: 0.032103, loss_kd: 1011.089966
[22:07:21.933] iteration 5190 : loss : 277.762024, loss_ce: 0.017826, loss_kd: 1386.391357
[22:07:27.525] iteration 5200 : loss : 209.408249, loss_ce: 0.017726, loss_kd: 1044.607544
[22:07:32.839] iteration 5210 : loss : 194.944077, loss_ce: 0.031165, loss_kd: 972.208069
[22:07:33.664] save model to ./debug_simple\continual_surgical_tpgm_epoch_4.pth
[22:07:33.758] save final model to ./debug_simple\continual_surgical_tpgm_final.pth
[11:58:01.013] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', stage=1, num_classes_old=9, num_classes_new=4, num_classes_lits17=3, output_dir='./debug_simple', max_iterations=10000, max_epochs=30, batch_size=24, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.5, kd_temperature=3.0, kd_weight=0.2, freeze_old_classes=False, auto_tune='none', gradient_batches=5, tpgm_norm_mode='l2', tpgm_lr=0.01, tpgm_iters=200, tpgm_exclude=[], tpgm_frequency=5, tpgm_start_epoch=10, disable_tpgm=True, tpgm_data_fraction=0.1, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[11:58:01.033] Stage 1: Using 47610/95221 samples (50.0%) for continual learning
[11:58:01.033] Old classes: 9, New classes: 4, Total: 12
[11:58:01.033] Dataset: kits23
[11:58:01.033] TPGM enabled: False
[11:58:01.033] Surgical fine-tuning method: none
