[01:40:15.843] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./finetune_tpgm_kits23', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, proj_freq=10, max_iters=5, proj_lr=0.01, norm_mode='mars', pgm_data_fraction=0.05, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[01:40:15.851] Using 9522/95221 samples (10.0%) for finetuning
[01:40:25.508] TPGM enabled with proj_freq=10, max_iters=5
[01:40:25.512] 298 iterations per epoch. 14900 max iterations 
[01:43:05.037] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./finetune_tpgm_kits23', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, proj_freq=10, max_iters=5, proj_lr=0.01, norm_mode='mars', pgm_data_fraction=0.05, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[01:43:05.046] Using 9522/95221 samples (10.0%) for finetuning
[01:43:14.565] TPGM enabled with proj_freq=10, max_iters=5
[01:43:14.568] 298 iterations per epoch. 14900 max iterations 
[01:43:30.915] iteration 10 : loss : 0.449884, loss_ce: 0.053518
[01:43:37.161] iteration 20 : loss : 0.443575, loss_ce: 0.088916
[01:46:48.493] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./finetune_tpgm_kits23', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, proj_freq=10, max_iters=5, proj_lr=0.01, norm_mode='mars', pgm_data_fraction=0.05, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[01:46:48.501] Using 9522/95221 samples (10.0%) for finetuning
[01:46:58.394] TPGM enabled with proj_freq=10, max_iters=5
[01:46:58.398] 298 iterations per epoch. 14900 max iterations 
[01:50:53.601] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./finetune_tpgm_kits23', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, proj_freq=10, max_iters=5, proj_lr=0.01, norm_mode='mars', pgm_data_fraction=0.05, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[01:50:53.609] Using 9522/95221 samples (10.0%) for finetuning
[01:51:03.166] TPGM enabled with proj_freq=10, max_iters=5
[01:51:03.169] 298 iterations per epoch. 14900 max iterations 
[01:51:21.077] iteration 10 : loss : 0.458644, loss_ce: 0.054593
[01:51:28.012] iteration 20 : loss : 0.457548, loss_ce: 0.072289
[01:51:36.363] iteration 30 : loss : 0.463730, loss_ce: 0.071375
[01:51:45.606] iteration 40 : loss : 0.451991, loss_ce: 0.072464
[01:51:55.343] iteration 50 : loss : 0.466299, loss_ce: 0.071731
[01:52:04.450] iteration 60 : loss : 0.471779, loss_ce: 0.098395
[01:52:11.449] iteration 70 : loss : 0.456024, loss_ce: 0.055778
[01:52:18.430] iteration 80 : loss : 0.470256, loss_ce: 0.101310
[01:52:25.891] iteration 90 : loss : 0.441604, loss_ce: 0.057796
[01:52:33.063] iteration 100 : loss : 0.453907, loss_ce: 0.063264
[01:52:40.067] iteration 110 : loss : 0.452250, loss_ce: 0.061187
[01:52:47.060] iteration 120 : loss : 0.465246, loss_ce: 0.069809
[01:52:54.088] iteration 130 : loss : 0.458161, loss_ce: 0.068852
[01:53:01.075] iteration 140 : loss : 0.464130, loss_ce: 0.068455
[01:53:08.213] iteration 150 : loss : 0.459554, loss_ce: 0.068873
[01:53:15.470] iteration 160 : loss : 0.464310, loss_ce: 0.063009
[01:53:22.524] iteration 170 : loss : 0.462789, loss_ce: 0.083197
[01:53:29.642] iteration 180 : loss : 0.469127, loss_ce: 0.086306
[01:53:36.626] iteration 190 : loss : 0.463873, loss_ce: 0.082234
[01:53:43.611] iteration 200 : loss : 0.459385, loss_ce: 0.060957
[01:53:50.598] iteration 210 : loss : 0.467878, loss_ce: 0.084558
[01:53:57.611] iteration 220 : loss : 0.462624, loss_ce: 0.065770
[01:54:04.626] iteration 230 : loss : 0.463104, loss_ce: 0.064019
[01:54:11.633] iteration 240 : loss : 0.456268, loss_ce: 0.067719
[01:54:18.643] iteration 250 : loss : 0.454966, loss_ce: 0.068811
[01:54:25.647] iteration 260 : loss : 0.467646, loss_ce: 0.081066
[01:54:32.674] iteration 270 : loss : 0.460630, loss_ce: 0.065438
[01:54:39.711] iteration 280 : loss : 0.468955, loss_ce: 0.080509
[01:54:46.785] iteration 290 : loss : 0.460605, loss_ce: 0.054890
[01:55:07.060] iteration 300 : loss : 0.458071, loss_ce: 0.064619
[01:55:14.658] iteration 310 : loss : 0.456281, loss_ce: 0.058262
[01:55:22.446] iteration 320 : loss : 0.460842, loss_ce: 0.078933
[01:55:30.310] iteration 330 : loss : 0.462035, loss_ce: 0.080388
[01:55:37.909] iteration 340 : loss : 0.467572, loss_ce: 0.066749
[01:55:45.494] iteration 350 : loss : 0.466659, loss_ce: 0.076023
[01:55:53.038] iteration 360 : loss : 0.464162, loss_ce: 0.073209
[01:56:00.669] iteration 370 : loss : 0.459817, loss_ce: 0.064983
[01:56:08.241] iteration 380 : loss : 0.461575, loss_ce: 0.073633
[01:56:15.838] iteration 390 : loss : 0.455957, loss_ce: 0.054402
[01:56:23.433] iteration 400 : loss : 0.466883, loss_ce: 0.077926
[01:56:31.085] iteration 410 : loss : 0.454807, loss_ce: 0.044831
[01:56:38.590] iteration 420 : loss : 0.469969, loss_ce: 0.088873
[01:56:46.280] iteration 430 : loss : 0.467596, loss_ce: 0.062115
[01:56:53.621] iteration 440 : loss : 0.448173, loss_ce: 0.066506
[01:57:00.647] iteration 450 : loss : 0.454640, loss_ce: 0.068815
[01:57:07.703] iteration 460 : loss : 0.449808, loss_ce: 0.055064
[01:57:14.929] iteration 470 : loss : 0.458962, loss_ce: 0.058351
[01:57:21.932] iteration 480 : loss : 0.455269, loss_ce: 0.058727
[01:57:28.924] iteration 490 : loss : 0.457588, loss_ce: 0.068085
[01:57:35.907] iteration 500 : loss : 0.463933, loss_ce: 0.071303
[01:57:42.878] iteration 510 : loss : 0.458224, loss_ce: 0.065071
[01:57:49.852] iteration 520 : loss : 0.460541, loss_ce: 0.061930
[01:57:56.833] iteration 530 : loss : 0.461369, loss_ce: 0.057060
[01:58:03.808] iteration 540 : loss : 0.465473, loss_ce: 0.078152
[01:58:11.094] iteration 550 : loss : 0.456672, loss_ce: 0.054463
[01:58:18.490] iteration 560 : loss : 0.464324, loss_ce: 0.071304
[01:58:25.967] iteration 570 : loss : 0.472387, loss_ce: 0.093559
[01:58:33.024] iteration 580 : loss : 0.455325, loss_ce: 0.089960
[01:58:39.935] iteration 590 : loss : 0.458976, loss_ce: 0.074097
[01:58:57.290] iteration 600 : loss : 0.463316, loss_ce: 0.075499
[01:59:04.351] iteration 610 : loss : 0.468811, loss_ce: 0.091052
[01:59:11.926] iteration 620 : loss : 0.460014, loss_ce: 0.066243
[01:59:19.249] iteration 630 : loss : 0.454573, loss_ce: 0.052057
[01:59:26.321] iteration 640 : loss : 0.463466, loss_ce: 0.068532
[01:59:33.345] iteration 650 : loss : 0.457241, loss_ce: 0.063051
[01:59:40.341] iteration 660 : loss : 0.457321, loss_ce: 0.062321
[01:59:47.511] iteration 670 : loss : 0.451625, loss_ce: 0.067291
[01:59:55.193] iteration 680 : loss : 0.469930, loss_ce: 0.093965
[02:00:02.669] iteration 690 : loss : 0.467978, loss_ce: 0.080287
[02:00:10.052] iteration 700 : loss : 0.447292, loss_ce: 0.059896
[02:00:17.105] iteration 710 : loss : 0.459432, loss_ce: 0.074568
[02:00:24.418] iteration 720 : loss : 0.464107, loss_ce: 0.075446
[02:00:31.936] iteration 730 : loss : 0.458622, loss_ce: 0.072095
[02:00:39.534] iteration 740 : loss : 0.458118, loss_ce: 0.055085
[02:00:47.090] iteration 750 : loss : 0.460489, loss_ce: 0.071088
[02:00:54.596] iteration 760 : loss : 0.462677, loss_ce: 0.055915
[02:01:02.131] iteration 770 : loss : 0.448163, loss_ce: 0.053715
[02:01:09.677] iteration 780 : loss : 0.454126, loss_ce: 0.065074
[02:01:17.169] iteration 790 : loss : 0.455635, loss_ce: 0.052448
[02:01:24.670] iteration 800 : loss : 0.467627, loss_ce: 0.072227
[02:01:32.196] iteration 810 : loss : 0.468145, loss_ce: 0.090777
[02:01:39.252] iteration 820 : loss : 0.460127, loss_ce: 0.062641
[02:01:46.240] iteration 830 : loss : 0.450662, loss_ce: 0.055852
[02:01:53.237] iteration 840 : loss : 0.462533, loss_ce: 0.068312
[02:02:00.309] iteration 850 : loss : 0.468986, loss_ce: 0.085719
[02:02:09.932] iteration 860 : loss : 0.462040, loss_ce: 0.075921
[02:02:19.611] iteration 870 : loss : 0.467946, loss_ce: 0.083157
[02:02:29.450] iteration 880 : loss : 0.462698, loss_ce: 0.073024
[02:02:39.842] iteration 890 : loss : 0.461119, loss_ce: 0.073024
[02:02:59.754] iteration 900 : loss : 0.457867, loss_ce: 0.067274
[02:03:09.448] iteration 910 : loss : 0.456031, loss_ce: 0.052316
[02:03:19.151] iteration 920 : loss : 0.459875, loss_ce: 0.056016
[02:03:28.822] iteration 930 : loss : 0.463601, loss_ce: 0.080302
[02:03:38.495] iteration 940 : loss : 0.453384, loss_ce: 0.084594
[02:03:48.176] iteration 950 : loss : 0.456124, loss_ce: 0.067523
[02:03:57.841] iteration 960 : loss : 0.453989, loss_ce: 0.051875
[02:04:07.527] iteration 970 : loss : 0.455296, loss_ce: 0.073240
[02:04:17.186] iteration 980 : loss : 0.460441, loss_ce: 0.077614
[02:04:26.904] iteration 990 : loss : 0.456673, loss_ce: 0.083757
[02:04:36.576] iteration 1000 : loss : 0.460095, loss_ce: 0.082873
[02:04:46.394] iteration 1010 : loss : 0.460022, loss_ce: 0.070485
[02:04:56.386] iteration 1020 : loss : 0.456751, loss_ce: 0.058081
[02:05:06.086] iteration 1030 : loss : 0.459015, loss_ce: 0.076297
[02:05:15.742] iteration 1040 : loss : 0.466955, loss_ce: 0.076521
[02:05:25.427] iteration 1050 : loss : 0.462892, loss_ce: 0.070294
[02:05:35.107] iteration 1060 : loss : 0.452736, loss_ce: 0.056539
[02:05:44.793] iteration 1070 : loss : 0.470252, loss_ce: 0.087108
[02:05:54.515] iteration 1080 : loss : 0.452686, loss_ce: 0.060681
[02:06:04.222] iteration 1090 : loss : 0.461993, loss_ce: 0.079176
[02:06:13.895] iteration 1100 : loss : 0.458102, loss_ce: 0.071024
[02:06:24.272] iteration 1110 : loss : 0.463695, loss_ce: 0.069182
[02:06:34.424] iteration 1120 : loss : 0.461324, loss_ce: 0.069042
[02:06:44.138] iteration 1130 : loss : 0.465412, loss_ce: 0.081194
[02:06:53.834] iteration 1140 : loss : 0.464640, loss_ce: 0.076571
[02:07:03.533] iteration 1150 : loss : 0.464788, loss_ce: 0.070564
[02:07:13.198] iteration 1160 : loss : 0.458668, loss_ce: 0.054497
[02:07:22.900] iteration 1170 : loss : 0.461039, loss_ce: 0.066281
[02:07:32.570] iteration 1180 : loss : 0.468278, loss_ce: 0.098404
[02:07:42.178] iteration 1190 : loss : 0.456511, loss_ce: 0.056731
[02:08:01.956] iteration 1200 : loss : 0.462234, loss_ce: 0.068745
[02:08:11.633] iteration 1210 : loss : 0.459912, loss_ce: 0.063899
[02:08:21.301] iteration 1220 : loss : 0.459011, loss_ce: 0.075745
[02:08:31.510] iteration 1230 : loss : 0.456846, loss_ce: 0.059471
[02:08:41.188] iteration 1240 : loss : 0.466004, loss_ce: 0.074968
[02:08:50.874] iteration 1250 : loss : 0.461504, loss_ce: 0.064621
[02:09:00.562] iteration 1260 : loss : 0.462678, loss_ce: 0.067603
[02:09:10.545] iteration 1270 : loss : 0.458102, loss_ce: 0.070831
[02:09:20.707] iteration 1280 : loss : 0.461509, loss_ce: 0.057029
[02:09:31.392] iteration 1290 : loss : 0.473714, loss_ce: 0.094734
[02:09:41.081] iteration 1300 : loss : 0.464311, loss_ce: 0.076828
[02:09:50.770] iteration 1310 : loss : 0.440530, loss_ce: 0.069020
[02:10:00.759] iteration 1320 : loss : 0.464785, loss_ce: 0.061791
[02:10:10.935] iteration 1330 : loss : 0.457234, loss_ce: 0.055610
[02:10:20.780] iteration 1340 : loss : 0.461192, loss_ce: 0.072393
[02:10:30.468] iteration 1350 : loss : 0.454891, loss_ce: 0.060885
[02:10:40.137] iteration 1360 : loss : 0.467009, loss_ce: 0.089975
[02:10:49.830] iteration 1370 : loss : 0.462149, loss_ce: 0.073100
[02:10:59.490] iteration 1380 : loss : 0.461095, loss_ce: 0.065741
[02:11:09.208] iteration 1390 : loss : 0.463226, loss_ce: 0.067653
[02:11:18.852] iteration 1400 : loss : 0.454360, loss_ce: 0.059557
[02:11:28.590] iteration 1410 : loss : 0.463003, loss_ce: 0.072566
[02:11:38.729] iteration 1420 : loss : 0.447614, loss_ce: 0.047566
[02:11:48.394] iteration 1430 : loss : 0.459238, loss_ce: 0.076986
[02:11:58.065] iteration 1440 : loss : 0.460354, loss_ce: 0.064616
[02:12:07.745] iteration 1450 : loss : 0.460147, loss_ce: 0.076026
[02:12:17.400] iteration 1460 : loss : 0.470211, loss_ce: 0.105856
[02:12:27.069] iteration 1470 : loss : 0.468863, loss_ce: 0.087810
[02:12:36.614] iteration 1480 : loss : 0.452242, loss_ce: 0.055636
[02:12:46.077] iteration 1490 : loss : 0.461789, loss_ce: 0.073362
[02:13:06.184] iteration 1500 : loss : 0.456340, loss_ce: 0.063028
[02:13:15.878] iteration 1510 : loss : 0.469209, loss_ce: 0.085666
[02:13:25.561] iteration 1520 : loss : 0.447533, loss_ce: 0.065568
[02:13:35.249] iteration 1530 : loss : 0.468499, loss_ce: 0.085420
[02:13:44.915] iteration 1540 : loss : 0.467460, loss_ce: 0.079535
[02:13:54.604] iteration 1550 : loss : 0.464741, loss_ce: 0.084207
[02:14:04.277] iteration 1560 : loss : 0.463411, loss_ce: 0.081692
[02:14:14.004] iteration 1570 : loss : 0.464757, loss_ce: 0.070733
[02:14:24.731] iteration 1580 : loss : 0.464269, loss_ce: 0.069220
[02:14:34.478] iteration 1590 : loss : 0.455559, loss_ce: 0.066951
[02:14:44.165] iteration 1600 : loss : 0.478057, loss_ce: 0.114844
[02:14:53.854] iteration 1610 : loss : 0.460986, loss_ce: 0.069500
[02:15:03.578] iteration 1620 : loss : 0.462993, loss_ce: 0.077105
[02:15:13.299] iteration 1630 : loss : 0.475228, loss_ce: 0.111260
[02:15:22.989] iteration 1640 : loss : 0.466079, loss_ce: 0.088963
[02:15:32.707] iteration 1650 : loss : 0.464930, loss_ce: 0.074917
[02:15:42.440] iteration 1660 : loss : 0.454061, loss_ce: 0.058639
[02:15:52.140] iteration 1670 : loss : 0.458823, loss_ce: 0.051309
[02:16:01.844] iteration 1680 : loss : 0.466015, loss_ce: 0.068149
[02:16:11.549] iteration 1690 : loss : 0.458304, loss_ce: 0.064001
[02:16:21.228] iteration 1700 : loss : 0.453105, loss_ce: 0.066602
[02:16:31.006] iteration 1710 : loss : 0.461633, loss_ce: 0.070750
[02:16:40.716] iteration 1720 : loss : 0.461334, loss_ce: 0.076318
[02:16:50.430] iteration 1730 : loss : 0.444721, loss_ce: 0.045693
[02:17:00.145] iteration 1740 : loss : 0.464155, loss_ce: 0.070760
[02:17:09.846] iteration 1750 : loss : 0.481033, loss_ce: 0.126149
[02:17:19.531] iteration 1760 : loss : 0.463491, loss_ce: 0.093428
[02:17:29.251] iteration 1770 : loss : 0.458252, loss_ce: 0.065240
[02:17:39.097] iteration 1780 : loss : 0.456537, loss_ce: 0.064781
[02:18:02.280] iteration 1790 : loss : 0.459009, loss_ce: 0.056716
[02:18:11.965] iteration 1800 : loss : 0.463526, loss_ce: 0.072265
[02:18:21.738] iteration 1810 : loss : 0.457287, loss_ce: 0.071290
[02:18:31.416] iteration 1820 : loss : 0.462390, loss_ce: 0.067839
[02:18:41.132] iteration 1830 : loss : 0.451815, loss_ce: 0.061021
[02:18:51.920] iteration 1840 : loss : 0.460647, loss_ce: 0.058660
[02:19:01.646] iteration 1850 : loss : 0.459727, loss_ce: 0.071138
[02:19:11.317] iteration 1860 : loss : 0.457624, loss_ce: 0.063310
[02:19:21.005] iteration 1870 : loss : 0.463407, loss_ce: 0.066731
[02:19:30.651] iteration 1880 : loss : 0.459388, loss_ce: 0.075304
[02:19:40.341] iteration 1890 : loss : 0.452790, loss_ce: 0.055791
[02:19:50.012] iteration 1900 : loss : 0.451993, loss_ce: 0.062710
[02:19:59.693] iteration 1910 : loss : 0.474253, loss_ce: 0.101334
[02:20:09.344] iteration 1920 : loss : 0.458492, loss_ce: 0.065094
[02:20:19.000] iteration 1930 : loss : 0.461521, loss_ce: 0.072880
[02:20:28.654] iteration 1940 : loss : 0.459251, loss_ce: 0.066570
[02:20:38.325] iteration 1950 : loss : 0.452650, loss_ce: 0.065938
[02:20:47.993] iteration 1960 : loss : 0.454953, loss_ce: 0.076681
[02:20:57.658] iteration 1970 : loss : 0.453296, loss_ce: 0.062382
[02:21:07.326] iteration 1980 : loss : 0.459975, loss_ce: 0.070285
[02:21:17.014] iteration 1990 : loss : 0.463808, loss_ce: 0.077003
[02:21:26.919] iteration 2000 : loss : 0.457864, loss_ce: 0.057986
[02:21:37.270] iteration 2010 : loss : 0.451410, loss_ce: 0.061675
[02:21:46.133] iteration 2020 : loss : 0.463892, loss_ce: 0.074744
[02:21:53.193] iteration 2030 : loss : 0.447934, loss_ce: 0.065007
[02:22:00.197] iteration 2040 : loss : 0.466389, loss_ce: 0.076303
[02:22:07.205] iteration 2050 : loss : 0.448976, loss_ce: 0.068937
[02:22:14.228] iteration 2060 : loss : 0.460020, loss_ce: 0.063300
[02:22:21.168] iteration 2070 : loss : 0.456661, loss_ce: 0.059149
[02:22:28.172] iteration 2080 : loss : 0.457348, loss_ce: 0.060894
[02:22:45.322] iteration 2090 : loss : 0.458415, loss_ce: 0.059545
[02:22:52.321] iteration 2100 : loss : 0.451065, loss_ce: 0.070258
[02:22:59.332] iteration 2110 : loss : 0.454236, loss_ce: 0.063791
[02:23:06.351] iteration 2120 : loss : 0.455161, loss_ce: 0.054445
[02:23:13.353] iteration 2130 : loss : 0.463064, loss_ce: 0.078154
[02:23:20.355] iteration 2140 : loss : 0.460392, loss_ce: 0.065832
[02:23:27.364] iteration 2150 : loss : 0.453845, loss_ce: 0.056101
[02:23:34.365] iteration 2160 : loss : 0.460907, loss_ce: 0.062503
[02:23:41.393] iteration 2170 : loss : 0.458986, loss_ce: 0.071915
[02:23:48.423] iteration 2180 : loss : 0.459162, loss_ce: 0.063279
[02:23:55.438] iteration 2190 : loss : 0.453088, loss_ce: 0.063564
[02:24:02.448] iteration 2200 : loss : 0.467516, loss_ce: 0.088050
[02:24:09.462] iteration 2210 : loss : 0.460943, loss_ce: 0.073313
[02:24:16.467] iteration 2220 : loss : 0.444119, loss_ce: 0.053194
[02:24:23.484] iteration 2230 : loss : 0.454801, loss_ce: 0.078385
[02:24:30.498] iteration 2240 : loss : 0.463199, loss_ce: 0.062589
[02:24:37.521] iteration 2250 : loss : 0.446785, loss_ce: 0.071384
[02:24:44.537] iteration 2260 : loss : 0.462153, loss_ce: 0.085141
[02:24:51.553] iteration 2270 : loss : 0.465013, loss_ce: 0.083979
[02:24:58.569] iteration 2280 : loss : 0.450591, loss_ce: 0.060648
[02:25:05.589] iteration 2290 : loss : 0.467759, loss_ce: 0.081146
[02:25:12.601] iteration 2300 : loss : 0.460768, loss_ce: 0.061424
[02:25:19.625] iteration 2310 : loss : 0.468871, loss_ce: 0.083351
[02:25:26.643] iteration 2320 : loss : 0.468041, loss_ce: 0.082787
[02:25:33.660] iteration 2330 : loss : 0.459545, loss_ce: 0.068114
[02:25:40.673] iteration 2340 : loss : 0.464858, loss_ce: 0.073143
[02:25:47.691] iteration 2350 : loss : 0.466164, loss_ce: 0.070198
[02:25:54.703] iteration 2360 : loss : 0.475025, loss_ce: 0.107777
[02:26:01.663] iteration 2370 : loss : 0.467219, loss_ce: 0.076036
[02:26:08.679] iteration 2380 : loss : 0.463788, loss_ce: 0.082942
[02:26:25.805] iteration 2390 : loss : 0.464013, loss_ce: 0.079087
[02:26:32.831] iteration 2400 : loss : 0.458566, loss_ce: 0.077299
[02:26:39.853] iteration 2410 : loss : 0.457947, loss_ce: 0.086417
[02:26:46.847] iteration 2420 : loss : 0.464194, loss_ce: 0.082489
[02:26:53.856] iteration 2430 : loss : 0.465721, loss_ce: 0.083942
[02:27:00.856] iteration 2440 : loss : 0.457190, loss_ce: 0.050178
[02:27:07.866] iteration 2450 : loss : 0.460254, loss_ce: 0.074797
[02:27:14.878] iteration 2460 : loss : 0.468445, loss_ce: 0.066212
[02:27:21.906] iteration 2470 : loss : 0.463616, loss_ce: 0.079182
[02:27:28.913] iteration 2480 : loss : 0.452419, loss_ce: 0.054030
[02:27:35.931] iteration 2490 : loss : 0.456984, loss_ce: 0.063522
[02:27:42.935] iteration 2500 : loss : 0.464596, loss_ce: 0.079992
[02:27:49.975] iteration 2510 : loss : 0.462413, loss_ce: 0.072788
[02:27:56.985] iteration 2520 : loss : 0.476149, loss_ce: 0.099110
[02:28:04.010] iteration 2530 : loss : 0.456385, loss_ce: 0.054703
[02:28:11.034] iteration 2540 : loss : 0.461423, loss_ce: 0.074380
[02:28:18.052] iteration 2550 : loss : 0.461643, loss_ce: 0.067567
[02:28:25.062] iteration 2560 : loss : 0.448776, loss_ce: 0.068645
[02:28:32.090] iteration 2570 : loss : 0.442156, loss_ce: 0.080354
[02:28:39.107] iteration 2580 : loss : 0.451761, loss_ce: 0.069128
[02:28:46.127] iteration 2590 : loss : 0.456504, loss_ce: 0.066888
[02:28:53.138] iteration 2600 : loss : 0.454638, loss_ce: 0.072796
[02:29:00.154] iteration 2610 : loss : 0.457607, loss_ce: 0.073971
[02:29:07.175] iteration 2620 : loss : 0.456806, loss_ce: 0.063500
[02:29:14.196] iteration 2630 : loss : 0.466512, loss_ce: 0.073388
[02:29:21.208] iteration 2640 : loss : 0.449022, loss_ce: 0.063640
[02:29:28.228] iteration 2650 : loss : 0.455646, loss_ce: 0.074297
[02:29:35.228] iteration 2660 : loss : 0.459066, loss_ce: 0.066284
[02:29:42.188] iteration 2670 : loss : 0.470298, loss_ce: 0.089796
[02:29:49.201] iteration 2680 : loss : 0.456895, loss_ce: 0.084844
[02:30:06.280] iteration 2690 : loss : 0.459858, loss_ce: 0.060025
[02:30:13.283] iteration 2700 : loss : 0.456620, loss_ce: 0.062702
[02:30:20.315] iteration 2710 : loss : 0.458868, loss_ce: 0.070899
[02:30:27.335] iteration 2720 : loss : 0.452363, loss_ce: 0.057842
[02:30:34.378] iteration 2730 : loss : 0.455452, loss_ce: 0.075074
[02:30:41.391] iteration 2740 : loss : 0.470536, loss_ce: 0.082444
[02:30:48.402] iteration 2750 : loss : 0.455899, loss_ce: 0.070940
[02:30:55.429] iteration 2760 : loss : 0.463445, loss_ce: 0.074891
[02:31:02.479] iteration 2770 : loss : 0.466653, loss_ce: 0.082111
[02:31:09.515] iteration 2780 : loss : 0.457461, loss_ce: 0.070117
[02:31:16.542] iteration 2790 : loss : 0.463567, loss_ce: 0.073505
[02:31:23.548] iteration 2800 : loss : 0.468406, loss_ce: 0.090336
[02:31:30.564] iteration 2810 : loss : 0.471145, loss_ce: 0.093146
[02:31:37.592] iteration 2820 : loss : 0.467124, loss_ce: 0.073039
[02:31:44.619] iteration 2830 : loss : 0.471659, loss_ce: 0.088624
[02:31:51.626] iteration 2840 : loss : 0.458301, loss_ce: 0.055302
[02:31:58.642] iteration 2850 : loss : 0.442101, loss_ce: 0.051659
[02:32:05.655] iteration 2860 : loss : 0.466454, loss_ce: 0.076857
[02:32:12.681] iteration 2870 : loss : 0.463384, loss_ce: 0.049794
[02:32:19.697] iteration 2880 : loss : 0.453991, loss_ce: 0.064173
[02:32:26.723] iteration 2890 : loss : 0.465173, loss_ce: 0.071893
[02:32:33.739] iteration 2900 : loss : 0.458442, loss_ce: 0.059613
[02:32:40.765] iteration 2910 : loss : 0.458868, loss_ce: 0.077592
[02:32:47.766] iteration 2920 : loss : 0.459768, loss_ce: 0.068803
[02:32:54.786] iteration 2930 : loss : 0.465432, loss_ce: 0.075870
[02:33:01.791] iteration 2940 : loss : 0.460200, loss_ce: 0.078392
[02:33:08.813] iteration 2950 : loss : 0.451658, loss_ce: 0.045463
[02:33:15.719] iteration 2960 : loss : 0.464265, loss_ce: 0.063009
[02:33:22.747] iteration 2970 : loss : 0.462028, loss_ce: 0.075111
[02:33:29.562] iteration 2980 : loss : 0.459662, loss_ce: 0.061639
[02:33:30.219] save model to ./finetune_tpgm_kits23\finetuned_epoch_9.pth
[02:33:46.966] iteration 2990 : loss : 0.488912, loss_ce: 0.125479
[02:33:53.988] iteration 3000 : loss : 0.455336, loss_ce: 0.066953
[02:34:01.004] iteration 3010 : loss : 0.461333, loss_ce: 0.067493
[02:34:08.010] iteration 3020 : loss : 0.449726, loss_ce: 0.071459
[02:34:15.038] iteration 3030 : loss : 0.465023, loss_ce: 0.078377
[02:34:22.043] iteration 3040 : loss : 0.453397, loss_ce: 0.063279
[02:34:29.056] iteration 3050 : loss : 0.453639, loss_ce: 0.070648
[02:34:36.058] iteration 3060 : loss : 0.458262, loss_ce: 0.067240
[02:34:43.077] iteration 3070 : loss : 0.461700, loss_ce: 0.071306
[02:34:50.088] iteration 3080 : loss : 0.463716, loss_ce: 0.079067
[02:34:57.106] iteration 3090 : loss : 0.463862, loss_ce: 0.077502
[02:35:04.121] iteration 3100 : loss : 0.447937, loss_ce: 0.052799
[02:35:11.133] iteration 3110 : loss : 0.485855, loss_ce: 0.127767
[02:35:18.145] iteration 3120 : loss : 0.458273, loss_ce: 0.063886
[02:35:25.179] iteration 3130 : loss : 0.461951, loss_ce: 0.067407
[02:35:32.212] iteration 3140 : loss : 0.456859, loss_ce: 0.063910
[02:35:39.231] iteration 3150 : loss : 0.460056, loss_ce: 0.064244
[02:35:46.247] iteration 3160 : loss : 0.469474, loss_ce: 0.087420
[02:35:53.267] iteration 3170 : loss : 0.460526, loss_ce: 0.060803
[02:36:00.278] iteration 3180 : loss : 0.464985, loss_ce: 0.087149
[02:36:07.301] iteration 3190 : loss : 0.459169, loss_ce: 0.080259
[02:36:14.308] iteration 3200 : loss : 0.456515, loss_ce: 0.064428
[02:36:21.328] iteration 3210 : loss : 0.466749, loss_ce: 0.072369
[02:36:28.337] iteration 3220 : loss : 0.457318, loss_ce: 0.062745
[02:36:35.365] iteration 3230 : loss : 0.459279, loss_ce: 0.071698
[02:36:42.377] iteration 3240 : loss : 0.465059, loss_ce: 0.079849
[02:36:49.392] iteration 3250 : loss : 0.463996, loss_ce: 0.078071
[02:36:56.345] iteration 3260 : loss : 0.452067, loss_ce: 0.062365
[02:37:03.362] iteration 3270 : loss : 0.459727, loss_ce: 0.057890
[02:37:22.710] iteration 3280 : loss : 0.470117, loss_ce: 0.087284
[02:37:29.720] iteration 3290 : loss : 0.464661, loss_ce: 0.074621
[02:37:36.725] iteration 3300 : loss : 0.453290, loss_ce: 0.064804
[02:37:43.738] iteration 3310 : loss : 0.477985, loss_ce: 0.107228
[02:37:50.742] iteration 3320 : loss : 0.453158, loss_ce: 0.068590
[02:37:57.764] iteration 3330 : loss : 0.459729, loss_ce: 0.074795
[02:38:04.767] iteration 3340 : loss : 0.460077, loss_ce: 0.074822
[02:38:11.785] iteration 3350 : loss : 0.451128, loss_ce: 0.071325
[02:38:18.823] iteration 3360 : loss : 0.466606, loss_ce: 0.088259
[02:38:25.872] iteration 3370 : loss : 0.457903, loss_ce: 0.089970
[02:38:32.879] iteration 3380 : loss : 0.462108, loss_ce: 0.080729
[02:38:39.901] iteration 3390 : loss : 0.459485, loss_ce: 0.071138
[02:38:46.906] iteration 3400 : loss : 0.464323, loss_ce: 0.080396
[02:38:53.923] iteration 3410 : loss : 0.464766, loss_ce: 0.074599
[02:39:00.937] iteration 3420 : loss : 0.454260, loss_ce: 0.073928
[02:39:07.952] iteration 3430 : loss : 0.454649, loss_ce: 0.068947
[02:39:14.965] iteration 3440 : loss : 0.466851, loss_ce: 0.069923
[02:39:21.986] iteration 3450 : loss : 0.458490, loss_ce: 0.077386
[02:39:28.997] iteration 3460 : loss : 0.461045, loss_ce: 0.074333
[02:39:36.012] iteration 3470 : loss : 0.457088, loss_ce: 0.073983
[02:39:43.019] iteration 3480 : loss : 0.461713, loss_ce: 0.065654
[02:39:50.037] iteration 3490 : loss : 0.457256, loss_ce: 0.071378
[02:39:57.048] iteration 3500 : loss : 0.469448, loss_ce: 0.087376
[02:40:04.068] iteration 3510 : loss : 0.463272, loss_ce: 0.053382
[02:40:11.070] iteration 3520 : loss : 0.467697, loss_ce: 0.089143
[02:40:18.096] iteration 3530 : loss : 0.463359, loss_ce: 0.079864
[02:40:25.105] iteration 3540 : loss : 0.469479, loss_ce: 0.078642
[02:40:32.055] iteration 3550 : loss : 0.459906, loss_ce: 0.078484
[02:40:39.083] iteration 3560 : loss : 0.457407, loss_ce: 0.077142
[02:40:46.112] iteration 3570 : loss : 0.456962, loss_ce: 0.064646
[02:41:03.193] iteration 3580 : loss : 0.460139, loss_ce: 0.064656
[02:41:10.193] iteration 3590 : loss : 0.468920, loss_ce: 0.090763
[02:41:17.195] iteration 3600 : loss : 0.469431, loss_ce: 0.086635
[02:41:24.244] iteration 3610 : loss : 0.455552, loss_ce: 0.085456
[02:41:31.252] iteration 3620 : loss : 0.457446, loss_ce: 0.066747
[02:41:38.262] iteration 3630 : loss : 0.459396, loss_ce: 0.073190
[02:41:45.258] iteration 3640 : loss : 0.463754, loss_ce: 0.073112
[02:41:52.267] iteration 3650 : loss : 0.456857, loss_ce: 0.060957
[02:41:59.276] iteration 3660 : loss : 0.459864, loss_ce: 0.061600
[02:42:06.297] iteration 3670 : loss : 0.456351, loss_ce: 0.077280
[02:42:13.315] iteration 3680 : loss : 0.454289, loss_ce: 0.074176
[02:42:20.330] iteration 3690 : loss : 0.466287, loss_ce: 0.088165
[02:42:27.347] iteration 3700 : loss : 0.453944, loss_ce: 0.073832
[02:42:34.374] iteration 3710 : loss : 0.456436, loss_ce: 0.067234
[02:42:41.380] iteration 3720 : loss : 0.461105, loss_ce: 0.061834
[02:42:48.404] iteration 3730 : loss : 0.453252, loss_ce: 0.056952
[02:42:55.427] iteration 3740 : loss : 0.457960, loss_ce: 0.051816
[02:43:02.460] iteration 3750 : loss : 0.461595, loss_ce: 0.059776
[02:43:09.473] iteration 3760 : loss : 0.459827, loss_ce: 0.082041
[02:43:16.505] iteration 3770 : loss : 0.459552, loss_ce: 0.069657
[02:43:23.506] iteration 3780 : loss : 0.457173, loss_ce: 0.072848
[02:43:30.524] iteration 3790 : loss : 0.454561, loss_ce: 0.056889
[02:43:37.539] iteration 3800 : loss : 0.454305, loss_ce: 0.057561
[02:43:44.559] iteration 3810 : loss : 0.461137, loss_ce: 0.072409
[02:43:51.563] iteration 3820 : loss : 0.463618, loss_ce: 0.074316
[02:43:58.574] iteration 3830 : loss : 0.460757, loss_ce: 0.072049
[02:44:05.587] iteration 3840 : loss : 0.455043, loss_ce: 0.070235
[02:44:12.551] iteration 3850 : loss : 0.461128, loss_ce: 0.071540
[02:44:19.558] iteration 3860 : loss : 0.453206, loss_ce: 0.055288
[02:44:26.573] iteration 3870 : loss : 0.455100, loss_ce: 0.049860
[02:44:43.669] iteration 3880 : loss : 0.467419, loss_ce: 0.089081
[02:44:50.696] iteration 3890 : loss : 0.464887, loss_ce: 0.080875
[02:44:57.691] iteration 3900 : loss : 0.467632, loss_ce: 0.084465
[02:45:04.721] iteration 3910 : loss : 0.464882, loss_ce: 0.062370
[02:45:11.727] iteration 3920 : loss : 0.462119, loss_ce: 0.073889
[02:45:18.751] iteration 3930 : loss : 0.454231, loss_ce: 0.066884
[02:45:25.774] iteration 3940 : loss : 0.466910, loss_ce: 0.076010
[02:45:32.794] iteration 3950 : loss : 0.462849, loss_ce: 0.072802
[02:45:39.816] iteration 3960 : loss : 0.464599, loss_ce: 0.069744
[02:45:46.826] iteration 3970 : loss : 0.456562, loss_ce: 0.068287
[02:45:53.838] iteration 3980 : loss : 0.456157, loss_ce: 0.074461
[02:46:00.856] iteration 3990 : loss : 0.459234, loss_ce: 0.062679
[02:46:07.877] iteration 4000 : loss : 0.460840, loss_ce: 0.061262
[02:46:14.889] iteration 4010 : loss : 0.470272, loss_ce: 0.085442
[02:46:21.891] iteration 4020 : loss : 0.446625, loss_ce: 0.072874
[02:46:28.907] iteration 4030 : loss : 0.461447, loss_ce: 0.056750
[02:46:35.920] iteration 4040 : loss : 0.457625, loss_ce: 0.067833
[02:46:42.943] iteration 4050 : loss : 0.465103, loss_ce: 0.071290
[02:46:49.963] iteration 4060 : loss : 0.470660, loss_ce: 0.089936
[02:46:56.988] iteration 4070 : loss : 0.452832, loss_ce: 0.046129
[02:47:04.006] iteration 4080 : loss : 0.467798, loss_ce: 0.079650
[02:47:11.027] iteration 4090 : loss : 0.469353, loss_ce: 0.096473
[02:47:18.037] iteration 4100 : loss : 0.453455, loss_ce: 0.054141
[02:47:25.058] iteration 4110 : loss : 0.456895, loss_ce: 0.056850
[02:47:32.072] iteration 4120 : loss : 0.462519, loss_ce: 0.058751
[02:47:39.089] iteration 4130 : loss : 0.448692, loss_ce: 0.060015
[02:47:46.101] iteration 4140 : loss : 0.466258, loss_ce: 0.069730
[02:47:53.040] iteration 4150 : loss : 0.448333, loss_ce: 0.060900
[02:48:00.049] iteration 4160 : loss : 0.461984, loss_ce: 0.081670
[02:48:07.070] iteration 4170 : loss : 0.452230, loss_ce: 0.074544
[02:48:24.162] iteration 4180 : loss : 0.461318, loss_ce: 0.074344
[02:48:31.172] iteration 4190 : loss : 0.462578, loss_ce: 0.066437
[02:48:38.164] iteration 4200 : loss : 0.456873, loss_ce: 0.063807
[02:48:45.176] iteration 4210 : loss : 0.462131, loss_ce: 0.069979
[02:48:52.174] iteration 4220 : loss : 0.464842, loss_ce: 0.082769
[02:48:59.193] iteration 4230 : loss : 0.461402, loss_ce: 0.065657
[02:49:06.210] iteration 4240 : loss : 0.454538, loss_ce: 0.083797
[02:49:13.230] iteration 4250 : loss : 0.455875, loss_ce: 0.068241
[02:49:20.239] iteration 4260 : loss : 0.459976, loss_ce: 0.059659
[02:49:27.257] iteration 4270 : loss : 0.460591, loss_ce: 0.072669
[02:49:34.273] iteration 4280 : loss : 0.462055, loss_ce: 0.058374
[02:49:41.289] iteration 4290 : loss : 0.455729, loss_ce: 0.068030
[02:49:48.295] iteration 4300 : loss : 0.464739, loss_ce: 0.076029
[02:49:55.319] iteration 4310 : loss : 0.460759, loss_ce: 0.051467
[02:50:02.335] iteration 4320 : loss : 0.463251, loss_ce: 0.068979
[02:50:09.369] iteration 4330 : loss : 0.456590, loss_ce: 0.057014
[02:50:16.372] iteration 4340 : loss : 0.463707, loss_ce: 0.067591
[02:50:23.397] iteration 4350 : loss : 0.458277, loss_ce: 0.067164
[02:50:30.408] iteration 4360 : loss : 0.453644, loss_ce: 0.063591
[02:50:37.442] iteration 4370 : loss : 0.456623, loss_ce: 0.064019
[02:50:44.444] iteration 4380 : loss : 0.446843, loss_ce: 0.053897
[02:50:51.457] iteration 4390 : loss : 0.463704, loss_ce: 0.072429
[02:50:58.467] iteration 4400 : loss : 0.449418, loss_ce: 0.071303
[02:51:05.487] iteration 4410 : loss : 0.449134, loss_ce: 0.068048
[02:51:12.493] iteration 4420 : loss : 0.451891, loss_ce: 0.061709
[02:51:19.524] iteration 4430 : loss : 0.459336, loss_ce: 0.064067
[02:51:26.440] iteration 4440 : loss : 0.453991, loss_ce: 0.052999
[02:51:33.466] iteration 4450 : loss : 0.467948, loss_ce: 0.075266
[02:51:40.479] iteration 4460 : loss : 0.463926, loss_ce: 0.087859
[02:51:47.319] iteration 4470 : loss : 0.456488, loss_ce: 0.062923
[02:52:04.633] iteration 4480 : loss : 0.459831, loss_ce: 0.069066
[02:52:11.642] iteration 4490 : loss : 0.462637, loss_ce: 0.065464
[02:52:18.642] iteration 4500 : loss : 0.456950, loss_ce: 0.072124
[02:52:25.659] iteration 4510 : loss : 0.485907, loss_ce: 0.130303
[02:52:32.672] iteration 4520 : loss : 0.450832, loss_ce: 0.059406
[02:52:39.682] iteration 4530 : loss : 0.451430, loss_ce: 0.053143
[02:52:46.698] iteration 4540 : loss : 0.456873, loss_ce: 0.063900
[02:52:53.709] iteration 4550 : loss : 0.462213, loss_ce: 0.080728
[02:53:00.729] iteration 4560 : loss : 0.461786, loss_ce: 0.062845
[02:53:07.757] iteration 4570 : loss : 0.456190, loss_ce: 0.059216
[02:53:14.777] iteration 4580 : loss : 0.454594, loss_ce: 0.055182
[02:53:21.802] iteration 4590 : loss : 0.461858, loss_ce: 0.082648
[02:53:28.806] iteration 4600 : loss : 0.459579, loss_ce: 0.068531
[02:53:35.827] iteration 4610 : loss : 0.462029, loss_ce: 0.077055
[02:53:42.826] iteration 4620 : loss : 0.454961, loss_ce: 0.070043
[02:53:49.842] iteration 4630 : loss : 0.452228, loss_ce: 0.063602
[02:53:56.866] iteration 4640 : loss : 0.467724, loss_ce: 0.089053
[02:54:03.887] iteration 4650 : loss : 0.459908, loss_ce: 0.070891
[02:54:10.897] iteration 4660 : loss : 0.471606, loss_ce: 0.096620
[02:54:17.919] iteration 4670 : loss : 0.456532, loss_ce: 0.065899
[02:54:24.929] iteration 4680 : loss : 0.459498, loss_ce: 0.067762
[02:54:31.958] iteration 4690 : loss : 0.467309, loss_ce: 0.082462
[02:54:38.970] iteration 4700 : loss : 0.466517, loss_ce: 0.073076
[02:54:45.997] iteration 4710 : loss : 0.456557, loss_ce: 0.056427
[02:54:53.003] iteration 4720 : loss : 0.447091, loss_ce: 0.059257
[02:55:00.028] iteration 4730 : loss : 0.463805, loss_ce: 0.104698
[02:55:06.973] iteration 4740 : loss : 0.459363, loss_ce: 0.073298
[02:55:13.986] iteration 4750 : loss : 0.460254, loss_ce: 0.062738
[02:55:20.993] iteration 4760 : loss : 0.470084, loss_ce: 0.089934
[02:55:40.384] iteration 4770 : loss : 0.463298, loss_ce: 0.078719
[02:55:47.375] iteration 4780 : loss : 0.465349, loss_ce: 0.079610
[02:55:54.389] iteration 4790 : loss : 0.461931, loss_ce: 0.058357
[02:56:01.398] iteration 4800 : loss : 0.457202, loss_ce: 0.063172
[02:56:08.412] iteration 4810 : loss : 0.459189, loss_ce: 0.075111
[02:56:15.419] iteration 4820 : loss : 0.457979, loss_ce: 0.072464
[02:56:22.437] iteration 4830 : loss : 0.459935, loss_ce: 0.072993
[02:56:29.442] iteration 4840 : loss : 0.462269, loss_ce: 0.066659
[02:56:36.469] iteration 4850 : loss : 0.469690, loss_ce: 0.087132
[02:56:43.474] iteration 4860 : loss : 0.456805, loss_ce: 0.057015
[02:56:50.487] iteration 4870 : loss : 0.461857, loss_ce: 0.080845
[02:56:57.494] iteration 4880 : loss : 0.464664, loss_ce: 0.078707
[02:57:04.506] iteration 4890 : loss : 0.464158, loss_ce: 0.070774
[02:57:11.522] iteration 4900 : loss : 0.460971, loss_ce: 0.078554
[02:57:18.541] iteration 4910 : loss : 0.470632, loss_ce: 0.091207
[02:57:25.548] iteration 4920 : loss : 0.463236, loss_ce: 0.086445
[02:57:32.564] iteration 4930 : loss : 0.471874, loss_ce: 0.091373
[02:57:39.572] iteration 4940 : loss : 0.457771, loss_ce: 0.066716
[02:57:46.612] iteration 4950 : loss : 0.466781, loss_ce: 0.079278
[02:57:53.615] iteration 4960 : loss : 0.455515, loss_ce: 0.057431
[02:58:00.640] iteration 4970 : loss : 0.470857, loss_ce: 0.085688
[02:58:07.641] iteration 4980 : loss : 0.467787, loss_ce: 0.083840
[02:58:14.672] iteration 4990 : loss : 0.460742, loss_ce: 0.065328
[02:58:21.680] iteration 5000 : loss : 0.460013, loss_ce: 0.060163
[02:58:28.694] iteration 5010 : loss : 0.464856, loss_ce: 0.066074
[02:58:35.721] iteration 5020 : loss : 0.462430, loss_ce: 0.082867
[02:58:42.671] iteration 5030 : loss : 0.454397, loss_ce: 0.068664
[02:58:49.675] iteration 5040 : loss : 0.455225, loss_ce: 0.063257
[02:58:56.694] iteration 5050 : loss : 0.460505, loss_ce: 0.070247
[02:59:03.718] iteration 5060 : loss : 0.462279, loss_ce: 0.079614
[02:59:21.660] iteration 5070 : loss : 0.466752, loss_ce: 0.083216
[02:59:28.663] iteration 5080 : loss : 0.470108, loss_ce: 0.094569
[02:59:35.692] iteration 5090 : loss : 0.460539, loss_ce: 0.074988
[02:59:42.700] iteration 5100 : loss : 0.454110, loss_ce: 0.047641
[02:59:49.720] iteration 5110 : loss : 0.451731, loss_ce: 0.074585
[02:59:56.728] iteration 5120 : loss : 0.457870, loss_ce: 0.065897
[03:00:03.749] iteration 5130 : loss : 0.461713, loss_ce: 0.065523
[03:00:10.757] iteration 5140 : loss : 0.459083, loss_ce: 0.068570
[03:00:17.779] iteration 5150 : loss : 0.456113, loss_ce: 0.051615
[03:00:24.795] iteration 5160 : loss : 0.464494, loss_ce: 0.071729
[03:00:31.823] iteration 5170 : loss : 0.457554, loss_ce: 0.070418
[03:00:38.836] iteration 5180 : loss : 0.458083, loss_ce: 0.070406
[03:00:45.867] iteration 5190 : loss : 0.452240, loss_ce: 0.058570
[03:00:52.897] iteration 5200 : loss : 0.479436, loss_ce: 0.104618
[03:00:59.919] iteration 5210 : loss : 0.459198, loss_ce: 0.082625
[03:01:06.948] iteration 5220 : loss : 0.466717, loss_ce: 0.080432
[03:01:13.966] iteration 5230 : loss : 0.456926, loss_ce: 0.056607
[03:01:20.987] iteration 5240 : loss : 0.455299, loss_ce: 0.057030
[03:01:28.012] iteration 5250 : loss : 0.454300, loss_ce: 0.063928
[03:01:35.036] iteration 5260 : loss : 0.466139, loss_ce: 0.069455
[03:01:42.066] iteration 5270 : loss : 0.467296, loss_ce: 0.077894
[03:01:49.080] iteration 5280 : loss : 0.457909, loss_ce: 0.071906
[03:01:56.104] iteration 5290 : loss : 0.464989, loss_ce: 0.074436
[03:02:03.119] iteration 5300 : loss : 0.461933, loss_ce: 0.063925
[03:02:10.140] iteration 5310 : loss : 0.448487, loss_ce: 0.060129
[03:02:17.163] iteration 5320 : loss : 0.461610, loss_ce: 0.066980
[03:02:24.106] iteration 5330 : loss : 0.471410, loss_ce: 0.079238
[03:02:31.138] iteration 5340 : loss : 0.456176, loss_ce: 0.059786
[03:02:38.167] iteration 5350 : loss : 0.458827, loss_ce: 0.060004
[03:02:45.189] iteration 5360 : loss : 0.458078, loss_ce: 0.044567
[03:03:02.374] iteration 5370 : loss : 0.466256, loss_ce: 0.098890
[03:03:09.381] iteration 5380 : loss : 0.457162, loss_ce: 0.066209
[03:03:16.393] iteration 5390 : loss : 0.459052, loss_ce: 0.068528
[03:03:23.406] iteration 5400 : loss : 0.462570, loss_ce: 0.073791
[03:03:30.436] iteration 5410 : loss : 0.467376, loss_ce: 0.081533
[03:03:37.445] iteration 5420 : loss : 0.462556, loss_ce: 0.084780
[03:03:44.462] iteration 5430 : loss : 0.458330, loss_ce: 0.058627
[03:03:51.483] iteration 5440 : loss : 0.466328, loss_ce: 0.066689
[03:03:58.509] iteration 5450 : loss : 0.456919, loss_ce: 0.066123
[03:04:05.520] iteration 5460 : loss : 0.466195, loss_ce: 0.076998
[03:04:12.545] iteration 5470 : loss : 0.462608, loss_ce: 0.057768
[03:04:19.567] iteration 5480 : loss : 0.459320, loss_ce: 0.061711
[03:04:26.582] iteration 5490 : loss : 0.474097, loss_ce: 0.084301
[03:04:33.601] iteration 5500 : loss : 0.457940, loss_ce: 0.066169
[03:04:40.627] iteration 5510 : loss : 0.454687, loss_ce: 0.061476
[03:04:47.639] iteration 5520 : loss : 0.470199, loss_ce: 0.086099
[03:04:54.656] iteration 5530 : loss : 0.460432, loss_ce: 0.054858
[03:05:01.666] iteration 5540 : loss : 0.453917, loss_ce: 0.058803
[03:05:08.697] iteration 5550 : loss : 0.459198, loss_ce: 0.075919
[03:05:15.725] iteration 5560 : loss : 0.460507, loss_ce: 0.060426
[03:05:22.737] iteration 5570 : loss : 0.454418, loss_ce: 0.052849
[03:05:29.740] iteration 5580 : loss : 0.459143, loss_ce: 0.064092
[03:05:36.759] iteration 5590 : loss : 0.450936, loss_ce: 0.071962
[03:05:43.766] iteration 5600 : loss : 0.461358, loss_ce: 0.063146
[03:05:50.788] iteration 5610 : loss : 0.460503, loss_ce: 0.068073
[03:05:57.804] iteration 5620 : loss : 0.463729, loss_ce: 0.088596
[03:06:04.748] iteration 5630 : loss : 0.460415, loss_ce: 0.078572
[03:06:11.749] iteration 5640 : loss : 0.461260, loss_ce: 0.071123
[03:06:18.764] iteration 5650 : loss : 0.465784, loss_ce: 0.075716
[03:06:25.782] iteration 5660 : loss : 0.461668, loss_ce: 0.070158
[03:06:42.823] iteration 5670 : loss : 0.462044, loss_ce: 0.065744
[03:06:49.827] iteration 5680 : loss : 0.458740, loss_ce: 0.064888
[03:06:56.836] iteration 5690 : loss : 0.454320, loss_ce: 0.066210
[03:07:03.841] iteration 5700 : loss : 0.460076, loss_ce: 0.061284
[03:07:10.854] iteration 5710 : loss : 0.465310, loss_ce: 0.087438
[03:07:17.857] iteration 5720 : loss : 0.459799, loss_ce: 0.067424
[03:07:24.874] iteration 5730 : loss : 0.461041, loss_ce: 0.072409
[03:07:31.881] iteration 5740 : loss : 0.461109, loss_ce: 0.061802
[03:07:38.905] iteration 5750 : loss : 0.461699, loss_ce: 0.064007
[03:07:45.918] iteration 5760 : loss : 0.456306, loss_ce: 0.059647
[03:07:52.939] iteration 5770 : loss : 0.460247, loss_ce: 0.066733
[03:07:59.944] iteration 5780 : loss : 0.461964, loss_ce: 0.078195
[03:08:06.963] iteration 5790 : loss : 0.456861, loss_ce: 0.071209
[03:08:13.977] iteration 5800 : loss : 0.458680, loss_ce: 0.065118
[03:08:21.012] iteration 5810 : loss : 0.467885, loss_ce: 0.081808
[03:08:28.022] iteration 5820 : loss : 0.461418, loss_ce: 0.082786
[03:08:35.043] iteration 5830 : loss : 0.463344, loss_ce: 0.068466
[03:08:42.059] iteration 5840 : loss : 0.463625, loss_ce: 0.067800
[03:08:49.072] iteration 5850 : loss : 0.457614, loss_ce: 0.077671
[03:08:56.081] iteration 5860 : loss : 0.456154, loss_ce: 0.065899
[03:09:03.117] iteration 5870 : loss : 0.455600, loss_ce: 0.050489
[03:09:10.124] iteration 5880 : loss : 0.460804, loss_ce: 0.067289
[03:09:17.154] iteration 5890 : loss : 0.463536, loss_ce: 0.047006
[03:09:24.166] iteration 5900 : loss : 0.457010, loss_ce: 0.072084
[03:09:31.183] iteration 5910 : loss : 0.449299, loss_ce: 0.056174
[03:09:38.090] iteration 5920 : loss : 0.457399, loss_ce: 0.065163
[03:09:45.106] iteration 5930 : loss : 0.467958, loss_ce: 0.073411
[03:09:52.125] iteration 5940 : loss : 0.453667, loss_ce: 0.054171
[03:09:59.139] iteration 5950 : loss : 0.467599, loss_ce: 0.077699
[03:10:05.957] iteration 5960 : loss : 0.449635, loss_ce: 0.075151
[03:10:06.612] save model to ./finetune_tpgm_kits23\finetuned_epoch_19.pth
[03:10:23.306] iteration 5970 : loss : 0.461997, loss_ce: 0.081464
[03:10:30.299] iteration 5980 : loss : 0.461253, loss_ce: 0.064884
[03:10:37.304] iteration 5990 : loss : 0.452184, loss_ce: 0.075424
[03:10:44.317] iteration 6000 : loss : 0.458399, loss_ce: 0.059578
[03:10:51.361] iteration 6010 : loss : 0.471673, loss_ce: 0.097408
[03:10:58.379] iteration 6020 : loss : 0.465279, loss_ce: 0.080744
[03:11:05.394] iteration 6030 : loss : 0.458555, loss_ce: 0.061707
[03:11:12.392] iteration 6040 : loss : 0.452689, loss_ce: 0.065348
[03:11:19.432] iteration 6050 : loss : 0.475563, loss_ce: 0.097958
[03:11:26.436] iteration 6060 : loss : 0.446398, loss_ce: 0.056405
[03:11:33.462] iteration 6070 : loss : 0.456187, loss_ce: 0.071889
[03:11:40.472] iteration 6080 : loss : 0.456864, loss_ce: 0.058031
[03:11:47.493] iteration 6090 : loss : 0.456141, loss_ce: 0.069739
[03:11:54.504] iteration 6100 : loss : 0.463069, loss_ce: 0.073863
[03:12:01.541] iteration 6110 : loss : 0.456491, loss_ce: 0.064135
[03:12:08.547] iteration 6120 : loss : 0.456850, loss_ce: 0.062377
[03:12:15.574] iteration 6130 : loss : 0.449684, loss_ce: 0.053673
[03:12:22.578] iteration 6140 : loss : 0.459186, loss_ce: 0.066357
[03:12:29.602] iteration 6150 : loss : 0.465622, loss_ce: 0.061344
[03:12:36.613] iteration 6160 : loss : 0.457291, loss_ce: 0.074004
[03:12:43.622] iteration 6170 : loss : 0.459791, loss_ce: 0.078263
[03:12:50.626] iteration 6180 : loss : 0.464738, loss_ce: 0.069939
[03:12:57.645] iteration 6190 : loss : 0.441753, loss_ce: 0.064640
[03:13:04.655] iteration 6200 : loss : 0.466667, loss_ce: 0.075016
[03:13:11.687] iteration 6210 : loss : 0.459938, loss_ce: 0.069019
[03:13:18.630] iteration 6220 : loss : 0.468046, loss_ce: 0.081262
[03:13:25.648] iteration 6230 : loss : 0.465022, loss_ce: 0.074128
[03:13:32.658] iteration 6240 : loss : 0.454889, loss_ce: 0.069765
[03:13:39.677] iteration 6250 : loss : 0.464553, loss_ce: 0.074937
[03:13:59.077] iteration 6260 : loss : 0.452720, loss_ce: 0.067699
[03:14:06.145] iteration 6270 : loss : 0.467621, loss_ce: 0.083463
[03:14:13.156] iteration 6280 : loss : 0.457993, loss_ce: 0.081340
[03:14:20.188] iteration 6290 : loss : 0.449921, loss_ce: 0.070878
[03:14:27.205] iteration 6300 : loss : 0.469614, loss_ce: 0.076197
[03:14:34.222] iteration 6310 : loss : 0.464067, loss_ce: 0.085020
[03:14:41.228] iteration 6320 : loss : 0.462123, loss_ce: 0.076809
[03:14:48.250] iteration 6330 : loss : 0.456275, loss_ce: 0.068293
[03:14:55.263] iteration 6340 : loss : 0.456297, loss_ce: 0.068262
[03:15:02.286] iteration 6350 : loss : 0.458330, loss_ce: 0.065957
[03:15:09.292] iteration 6360 : loss : 0.453074, loss_ce: 0.069374
[03:15:16.308] iteration 6370 : loss : 0.463317, loss_ce: 0.084840
[03:15:23.320] iteration 6380 : loss : 0.457163, loss_ce: 0.068143
[03:15:30.346] iteration 6390 : loss : 0.461917, loss_ce: 0.069366
[03:15:37.356] iteration 6400 : loss : 0.460454, loss_ce: 0.068558
[03:15:44.385] iteration 6410 : loss : 0.457667, loss_ce: 0.054499
[03:15:51.395] iteration 6420 : loss : 0.456882, loss_ce: 0.079268
[03:15:58.413] iteration 6430 : loss : 0.460824, loss_ce: 0.062247
[03:16:05.427] iteration 6440 : loss : 0.465793, loss_ce: 0.079839
[03:16:12.449] iteration 6450 : loss : 0.467730, loss_ce: 0.090138
[03:16:19.458] iteration 6460 : loss : 0.458840, loss_ce: 0.072154
[03:16:26.483] iteration 6470 : loss : 0.457132, loss_ce: 0.064881
[03:16:33.493] iteration 6480 : loss : 0.460217, loss_ce: 0.078556
[03:16:40.516] iteration 6490 : loss : 0.466762, loss_ce: 0.078585
[03:16:47.557] iteration 6500 : loss : 0.463132, loss_ce: 0.080752
[03:16:54.511] iteration 6510 : loss : 0.457940, loss_ce: 0.052611
[03:17:01.523] iteration 6520 : loss : 0.469648, loss_ce: 0.094291
[03:17:08.542] iteration 6530 : loss : 0.461091, loss_ce: 0.061178
[03:17:15.555] iteration 6540 : loss : 0.461798, loss_ce: 0.078199
[03:17:22.576] iteration 6550 : loss : 0.457292, loss_ce: 0.066235
[03:17:39.736] iteration 6560 : loss : 0.463436, loss_ce: 0.073087
[03:17:46.741] iteration 6570 : loss : 0.451939, loss_ce: 0.057159
[03:17:53.761] iteration 6580 : loss : 0.456518, loss_ce: 0.060169
[03:18:00.782] iteration 6590 : loss : 0.471292, loss_ce: 0.080063
[03:18:07.792] iteration 6600 : loss : 0.462083, loss_ce: 0.053425
[03:18:14.818] iteration 6610 : loss : 0.458327, loss_ce: 0.058149
[03:18:21.851] iteration 6620 : loss : 0.447124, loss_ce: 0.063369
[03:18:28.873] iteration 6630 : loss : 0.458204, loss_ce: 0.072341
[03:18:35.880] iteration 6640 : loss : 0.455002, loss_ce: 0.060392
[03:18:42.897] iteration 6650 : loss : 0.456697, loss_ce: 0.061905
[03:18:49.901] iteration 6660 : loss : 0.461927, loss_ce: 0.064005
[03:18:56.924] iteration 6670 : loss : 0.453652, loss_ce: 0.061996
[03:19:03.934] iteration 6680 : loss : 0.464080, loss_ce: 0.066896
[03:19:10.953] iteration 6690 : loss : 0.452250, loss_ce: 0.062818
[03:19:17.963] iteration 6700 : loss : 0.463626, loss_ce: 0.085066
[03:19:24.981] iteration 6710 : loss : 0.464820, loss_ce: 0.072880
[03:19:31.993] iteration 6720 : loss : 0.457853, loss_ce: 0.064967
[03:19:39.031] iteration 6730 : loss : 0.459756, loss_ce: 0.071842
[03:19:46.047] iteration 6740 : loss : 0.464097, loss_ce: 0.073152
[03:19:53.065] iteration 6750 : loss : 0.467530, loss_ce: 0.075118
[03:20:00.099] iteration 6760 : loss : 0.463917, loss_ce: 0.079804
[03:20:07.134] iteration 6770 : loss : 0.464547, loss_ce: 0.076358
[03:20:14.148] iteration 6780 : loss : 0.461058, loss_ce: 0.072066
[03:20:21.166] iteration 6790 : loss : 0.459085, loss_ce: 0.057684
[03:20:28.171] iteration 6800 : loss : 0.459153, loss_ce: 0.063535
[03:20:35.115] iteration 6810 : loss : 0.459195, loss_ce: 0.062388
[03:20:42.129] iteration 6820 : loss : 0.463266, loss_ce: 0.059523
[03:20:49.147] iteration 6830 : loss : 0.461397, loss_ce: 0.068394
[03:20:56.167] iteration 6840 : loss : 0.455890, loss_ce: 0.069791
[03:21:03.190] iteration 6850 : loss : 0.466138, loss_ce: 0.076307
[03:21:20.241] iteration 6860 : loss : 0.450626, loss_ce: 0.072351
[03:21:27.259] iteration 6870 : loss : 0.459448, loss_ce: 0.068056
[03:21:34.281] iteration 6880 : loss : 0.464402, loss_ce: 0.083498
[03:21:41.294] iteration 6890 : loss : 0.447093, loss_ce: 0.059199
[03:21:48.302] iteration 6900 : loss : 0.455497, loss_ce: 0.060956
[03:21:55.316] iteration 6910 : loss : 0.462701, loss_ce: 0.066550
[03:22:02.334] iteration 6920 : loss : 0.470753, loss_ce: 0.102863
[03:22:09.353] iteration 6930 : loss : 0.473880, loss_ce: 0.096175
[03:22:16.373] iteration 6940 : loss : 0.468388, loss_ce: 0.086850
[03:22:23.392] iteration 6950 : loss : 0.477540, loss_ce: 0.094764
[03:22:30.400] iteration 6960 : loss : 0.460875, loss_ce: 0.077604
[03:22:37.413] iteration 6970 : loss : 0.454294, loss_ce: 0.069523
[03:22:44.431] iteration 6980 : loss : 0.460526, loss_ce: 0.070629
[03:22:51.454] iteration 6990 : loss : 0.451310, loss_ce: 0.062353
[03:22:58.461] iteration 7000 : loss : 0.451483, loss_ce: 0.055404
[03:23:05.485] iteration 7010 : loss : 0.446501, loss_ce: 0.066061
[03:23:12.520] iteration 7020 : loss : 0.462121, loss_ce: 0.057698
[03:23:19.547] iteration 7030 : loss : 0.452832, loss_ce: 0.071305
[03:23:26.565] iteration 7040 : loss : 0.464122, loss_ce: 0.073807
[03:23:33.593] iteration 7050 : loss : 0.456926, loss_ce: 0.068623
[03:23:40.611] iteration 7060 : loss : 0.465246, loss_ce: 0.087567
[03:23:47.635] iteration 7070 : loss : 0.457708, loss_ce: 0.058324
[03:23:54.651] iteration 7080 : loss : 0.462394, loss_ce: 0.052669
[03:24:01.674] iteration 7090 : loss : 0.465376, loss_ce: 0.075274
[03:24:08.678] iteration 7100 : loss : 0.461994, loss_ce: 0.077090
[03:24:15.610] iteration 7110 : loss : 0.464301, loss_ce: 0.078233
[03:24:22.619] iteration 7120 : loss : 0.466548, loss_ce: 0.071170
[03:24:29.634] iteration 7130 : loss : 0.459312, loss_ce: 0.067860
[03:24:36.641] iteration 7140 : loss : 0.457258, loss_ce: 0.072787
[03:24:43.662] iteration 7150 : loss : 0.463255, loss_ce: 0.080789
[03:25:00.705] iteration 7160 : loss : 0.470313, loss_ce: 0.091060
[03:25:07.719] iteration 7170 : loss : 0.461104, loss_ce: 0.064686
[03:25:14.741] iteration 7180 : loss : 0.461377, loss_ce: 0.075812
[03:25:21.757] iteration 7190 : loss : 0.458108, loss_ce: 0.057094
[03:25:28.758] iteration 7200 : loss : 0.451332, loss_ce: 0.066147
[03:25:35.774] iteration 7210 : loss : 0.455601, loss_ce: 0.068131
[03:25:42.781] iteration 7220 : loss : 0.466140, loss_ce: 0.055505
[03:25:49.801] iteration 7230 : loss : 0.461502, loss_ce: 0.076525
[03:25:56.807] iteration 7240 : loss : 0.449889, loss_ce: 0.051057
[03:26:03.828] iteration 7250 : loss : 0.463592, loss_ce: 0.085349
[03:26:10.834] iteration 7260 : loss : 0.464441, loss_ce: 0.081127
[03:26:17.871] iteration 7270 : loss : 0.465390, loss_ce: 0.064905
[03:26:24.880] iteration 7280 : loss : 0.458125, loss_ce: 0.081755
[03:26:31.906] iteration 7290 : loss : 0.460942, loss_ce: 0.082689
[03:26:38.918] iteration 7300 : loss : 0.459591, loss_ce: 0.057926
[03:26:45.928] iteration 7310 : loss : 0.463546, loss_ce: 0.073936
[03:26:52.956] iteration 7320 : loss : 0.464146, loss_ce: 0.082471
[03:26:59.968] iteration 7330 : loss : 0.463058, loss_ce: 0.083318
[03:27:06.988] iteration 7340 : loss : 0.460547, loss_ce: 0.073628
[03:27:14.011] iteration 7350 : loss : 0.439044, loss_ce: 0.063680
[03:27:21.020] iteration 7360 : loss : 0.457389, loss_ce: 0.068867
[03:27:28.051] iteration 7370 : loss : 0.463282, loss_ce: 0.085875
[03:27:35.055] iteration 7380 : loss : 0.460296, loss_ce: 0.075734
[03:27:42.076] iteration 7390 : loss : 0.459822, loss_ce: 0.061663
[03:27:48.981] iteration 7400 : loss : 0.459688, loss_ce: 0.059721
[03:27:56.022] iteration 7410 : loss : 0.457249, loss_ce: 0.051219
[03:28:03.029] iteration 7420 : loss : 0.461150, loss_ce: 0.069787
[03:28:10.061] iteration 7430 : loss : 0.460650, loss_ce: 0.062119
[03:28:17.075] iteration 7440 : loss : 0.449232, loss_ce: 0.062356
[03:28:23.896] iteration 7450 : loss : 0.453116, loss_ce: 0.052314
[03:28:41.184] iteration 7460 : loss : 0.455510, loss_ce: 0.061476
[03:28:48.213] iteration 7470 : loss : 0.480355, loss_ce: 0.114752
[03:28:55.213] iteration 7480 : loss : 0.461566, loss_ce: 0.071080
[03:29:02.230] iteration 7490 : loss : 0.462129, loss_ce: 0.078613
[03:29:09.240] iteration 7500 : loss : 0.459586, loss_ce: 0.069080
[03:29:16.253] iteration 7510 : loss : 0.467343, loss_ce: 0.083789
[03:29:23.277] iteration 7520 : loss : 0.445563, loss_ce: 0.061744
[03:29:30.296] iteration 7530 : loss : 0.465412, loss_ce: 0.089665
[03:29:37.307] iteration 7540 : loss : 0.453860, loss_ce: 0.058695
[03:29:44.337] iteration 7550 : loss : 0.465137, loss_ce: 0.073443
[03:29:51.344] iteration 7560 : loss : 0.454944, loss_ce: 0.064913
[03:29:58.366] iteration 7570 : loss : 0.462805, loss_ce: 0.069493
[03:30:05.376] iteration 7580 : loss : 0.468298, loss_ce: 0.078764
[03:30:12.398] iteration 7590 : loss : 0.466130, loss_ce: 0.059330
[03:30:19.402] iteration 7600 : loss : 0.460182, loss_ce: 0.067055
[03:30:26.421] iteration 7610 : loss : 0.461575, loss_ce: 0.076885
[03:30:33.442] iteration 7620 : loss : 0.461706, loss_ce: 0.069261
[03:30:40.469] iteration 7630 : loss : 0.457846, loss_ce: 0.067784
[03:30:47.479] iteration 7640 : loss : 0.460414, loss_ce: 0.078510
[03:30:54.513] iteration 7650 : loss : 0.459967, loss_ce: 0.057196
[03:31:01.521] iteration 7660 : loss : 0.462132, loss_ce: 0.061950
[03:31:08.536] iteration 7670 : loss : 0.452753, loss_ce: 0.067751
[03:31:15.540] iteration 7680 : loss : 0.460154, loss_ce: 0.056657
[03:31:22.600] iteration 7690 : loss : 0.455762, loss_ce: 0.064590
[03:31:29.519] iteration 7700 : loss : 0.461701, loss_ce: 0.064681
[03:31:36.539] iteration 7710 : loss : 0.462636, loss_ce: 0.065956
[03:31:43.548] iteration 7720 : loss : 0.469823, loss_ce: 0.084385
[03:31:50.565] iteration 7730 : loss : 0.465960, loss_ce: 0.079625
[03:31:57.578] iteration 7740 : loss : 0.459112, loss_ce: 0.060843
[03:32:16.930] iteration 7750 : loss : 0.463684, loss_ce: 0.075083
[03:32:23.931] iteration 7760 : loss : 0.467812, loss_ce: 0.076111
[03:32:30.943] iteration 7770 : loss : 0.460669, loss_ce: 0.068516
[03:32:37.946] iteration 7780 : loss : 0.452999, loss_ce: 0.064092
[03:32:44.973] iteration 7790 : loss : 0.467893, loss_ce: 0.084474
[03:32:51.974] iteration 7800 : loss : 0.460234, loss_ce: 0.082643
[03:32:58.987] iteration 7810 : loss : 0.465482, loss_ce: 0.076075
[03:33:06.001] iteration 7820 : loss : 0.461120, loss_ce: 0.072733
[03:33:13.021] iteration 7830 : loss : 0.453052, loss_ce: 0.076263
[03:33:20.043] iteration 7840 : loss : 0.461255, loss_ce: 0.066002
[03:33:27.064] iteration 7850 : loss : 0.462053, loss_ce: 0.065682
[03:33:34.078] iteration 7860 : loss : 0.462133, loss_ce: 0.066081
[03:33:41.107] iteration 7870 : loss : 0.461441, loss_ce: 0.061579
[03:33:48.122] iteration 7880 : loss : 0.453457, loss_ce: 0.069277
[03:33:55.149] iteration 7890 : loss : 0.471961, loss_ce: 0.096102
[03:34:02.164] iteration 7900 : loss : 0.462666, loss_ce: 0.068726
[03:34:09.181] iteration 7910 : loss : 0.463234, loss_ce: 0.077033
[03:34:16.186] iteration 7920 : loss : 0.462944, loss_ce: 0.084710
[03:34:23.202] iteration 7930 : loss : 0.468130, loss_ce: 0.083026
[03:34:30.211] iteration 7940 : loss : 0.464353, loss_ce: 0.086472
[03:34:37.229] iteration 7950 : loss : 0.465441, loss_ce: 0.075681
[03:34:44.239] iteration 7960 : loss : 0.459030, loss_ce: 0.064534
[03:34:51.257] iteration 7970 : loss : 0.461034, loss_ce: 0.079518
[03:34:58.263] iteration 7980 : loss : 0.458287, loss_ce: 0.076000
[03:35:05.221] iteration 7990 : loss : 0.438626, loss_ce: 0.057490
[03:35:12.228] iteration 8000 : loss : 0.469230, loss_ce: 0.073344
[03:35:19.246] iteration 8010 : loss : 0.470209, loss_ce: 0.082314
[03:35:26.253] iteration 8020 : loss : 0.463527, loss_ce: 0.074533
[03:35:33.277] iteration 8030 : loss : 0.457475, loss_ce: 0.064726
[03:35:40.314] iteration 8040 : loss : 0.458379, loss_ce: 0.069445
[03:35:57.427] iteration 8050 : loss : 0.471107, loss_ce: 0.081128
[03:36:04.436] iteration 8060 : loss : 0.461533, loss_ce: 0.060435
[03:36:11.462] iteration 8070 : loss : 0.462898, loss_ce: 0.086090
[03:36:18.462] iteration 8080 : loss : 0.461873, loss_ce: 0.059565
[03:36:25.475] iteration 8090 : loss : 0.447731, loss_ce: 0.069681
[03:36:32.483] iteration 8100 : loss : 0.449777, loss_ce: 0.062095
[03:36:39.509] iteration 8110 : loss : 0.459836, loss_ce: 0.076414
[03:36:46.524] iteration 8120 : loss : 0.452361, loss_ce: 0.067357
[03:36:53.545] iteration 8130 : loss : 0.464138, loss_ce: 0.071731
[03:37:00.557] iteration 8140 : loss : 0.462301, loss_ce: 0.055917
[03:37:07.585] iteration 8150 : loss : 0.454375, loss_ce: 0.054067
[03:37:14.589] iteration 8160 : loss : 0.458127, loss_ce: 0.066490
[03:37:21.617] iteration 8170 : loss : 0.453911, loss_ce: 0.062470
[03:37:28.627] iteration 8180 : loss : 0.462864, loss_ce: 0.058329
[03:37:35.639] iteration 8190 : loss : 0.465209, loss_ce: 0.079635
[03:37:42.648] iteration 8200 : loss : 0.452570, loss_ce: 0.058988
[03:37:49.663] iteration 8210 : loss : 0.463355, loss_ce: 0.075181
[03:37:56.671] iteration 8220 : loss : 0.459791, loss_ce: 0.071635
[03:38:03.692] iteration 8230 : loss : 0.458345, loss_ce: 0.075557
[03:38:10.698] iteration 8240 : loss : 0.461569, loss_ce: 0.073694
[03:38:17.710] iteration 8250 : loss : 0.450072, loss_ce: 0.053058
[03:38:24.715] iteration 8260 : loss : 0.469112, loss_ce: 0.094409
[03:38:31.734] iteration 8270 : loss : 0.457978, loss_ce: 0.061204
[03:38:38.745] iteration 8280 : loss : 0.457834, loss_ce: 0.062527
[03:38:45.717] iteration 8290 : loss : 0.461459, loss_ce: 0.074891
[03:38:52.725] iteration 8300 : loss : 0.458838, loss_ce: 0.069139
[03:38:59.741] iteration 8310 : loss : 0.463002, loss_ce: 0.070524
[03:39:06.771] iteration 8320 : loss : 0.457445, loss_ce: 0.070466
[03:39:13.785] iteration 8330 : loss : 0.472161, loss_ce: 0.089437
[03:39:20.792] iteration 8340 : loss : 0.457347, loss_ce: 0.060938
[03:39:37.901] iteration 8350 : loss : 0.467810, loss_ce: 0.092331
[03:39:44.898] iteration 8360 : loss : 0.459039, loss_ce: 0.056193
[03:39:51.940] iteration 8370 : loss : 0.467093, loss_ce: 0.084039
[03:39:58.948] iteration 8380 : loss : 0.453839, loss_ce: 0.077911
[03:40:05.962] iteration 8390 : loss : 0.471959, loss_ce: 0.086526
[03:40:12.983] iteration 8400 : loss : 0.462836, loss_ce: 0.079586
[03:40:19.999] iteration 8410 : loss : 0.464429, loss_ce: 0.089607
[03:40:27.004] iteration 8420 : loss : 0.456656, loss_ce: 0.056296
[03:40:34.019] iteration 8430 : loss : 0.449014, loss_ce: 0.059092
[03:40:41.024] iteration 8440 : loss : 0.464897, loss_ce: 0.076615
[03:40:48.036] iteration 8450 : loss : 0.453963, loss_ce: 0.078871
[03:40:55.052] iteration 8460 : loss : 0.455519, loss_ce: 0.053426
[03:41:02.081] iteration 8470 : loss : 0.458750, loss_ce: 0.065927
[03:41:09.104] iteration 8480 : loss : 0.460137, loss_ce: 0.073648
[03:41:16.128] iteration 8490 : loss : 0.459696, loss_ce: 0.070887
[03:41:23.135] iteration 8500 : loss : 0.461299, loss_ce: 0.075580
[03:41:30.147] iteration 8510 : loss : 0.455492, loss_ce: 0.074507
[03:41:37.155] iteration 8520 : loss : 0.444468, loss_ce: 0.048913
[03:41:44.186] iteration 8530 : loss : 0.466365, loss_ce: 0.079130
[03:41:51.192] iteration 8540 : loss : 0.459726, loss_ce: 0.062246
[03:41:58.212] iteration 8550 : loss : 0.454953, loss_ce: 0.084243
[03:42:05.231] iteration 8560 : loss : 0.458135, loss_ce: 0.058141
[03:42:12.243] iteration 8570 : loss : 0.453080, loss_ce: 0.076631
[03:42:19.246] iteration 8580 : loss : 0.457132, loss_ce: 0.070910
[03:42:26.220] iteration 8590 : loss : 0.462481, loss_ce: 0.083540
[03:42:33.233] iteration 8600 : loss : 0.461592, loss_ce: 0.073330
[03:42:40.259] iteration 8610 : loss : 0.467863, loss_ce: 0.084732
[03:42:47.274] iteration 8620 : loss : 0.457571, loss_ce: 0.064493
[03:42:54.286] iteration 8630 : loss : 0.467544, loss_ce: 0.075394
[03:43:01.303] iteration 8640 : loss : 0.458407, loss_ce: 0.059351
[03:43:18.374] iteration 8650 : loss : 0.452210, loss_ce: 0.053933
[03:43:25.409] iteration 8660 : loss : 0.473475, loss_ce: 0.097311
[03:43:32.419] iteration 8670 : loss : 0.458311, loss_ce: 0.069971
[03:43:39.423] iteration 8680 : loss : 0.457871, loss_ce: 0.070712
[03:43:46.443] iteration 8690 : loss : 0.460188, loss_ce: 0.071184
[03:43:53.454] iteration 8700 : loss : 0.463987, loss_ce: 0.082380
[03:44:00.471] iteration 8710 : loss : 0.462495, loss_ce: 0.078328
[03:44:07.482] iteration 8720 : loss : 0.458351, loss_ce: 0.078381
[03:44:14.503] iteration 8730 : loss : 0.457821, loss_ce: 0.062535
[03:44:21.515] iteration 8740 : loss : 0.463133, loss_ce: 0.077016
[03:44:28.541] iteration 8750 : loss : 0.464463, loss_ce: 0.089026
[03:44:35.555] iteration 8760 : loss : 0.447664, loss_ce: 0.056032
[03:44:42.584] iteration 8770 : loss : 0.458399, loss_ce: 0.068712
[03:44:49.608] iteration 8780 : loss : 0.458940, loss_ce: 0.058379
[03:44:56.632] iteration 8790 : loss : 0.460979, loss_ce: 0.064257
[03:45:03.640] iteration 8800 : loss : 0.463874, loss_ce: 0.073489
[03:45:10.666] iteration 8810 : loss : 0.459388, loss_ce: 0.064922
[03:45:17.678] iteration 8820 : loss : 0.451731, loss_ce: 0.067007
[03:45:24.695] iteration 8830 : loss : 0.462884, loss_ce: 0.065164
[03:45:31.695] iteration 8840 : loss : 0.457121, loss_ce: 0.056147
[03:45:38.722] iteration 8850 : loss : 0.447302, loss_ce: 0.063810
[03:45:45.725] iteration 8860 : loss : 0.458888, loss_ce: 0.072712
[03:45:52.743] iteration 8870 : loss : 0.467774, loss_ce: 0.105361
[03:45:59.664] iteration 8880 : loss : 0.455621, loss_ce: 0.065198
[03:46:06.690] iteration 8890 : loss : 0.460012, loss_ce: 0.061553
[03:46:13.714] iteration 8900 : loss : 0.466217, loss_ce: 0.086043
[03:46:20.740] iteration 8910 : loss : 0.461161, loss_ce: 0.060772
[03:46:27.762] iteration 8920 : loss : 0.460443, loss_ce: 0.064016
[03:46:34.789] iteration 8930 : loss : 0.463558, loss_ce: 0.076243
[03:46:41.613] iteration 8940 : loss : 0.463643, loss_ce: 0.080684
[03:46:42.275] save model to ./finetune_tpgm_kits23\finetuned_epoch_29.pth
[03:46:59.049] iteration 8950 : loss : 0.462093, loss_ce: 0.062395
[03:47:06.051] iteration 8960 : loss : 0.468779, loss_ce: 0.085886
[03:47:13.072] iteration 8970 : loss : 0.463226, loss_ce: 0.085202
[03:47:20.117] iteration 8980 : loss : 0.466454, loss_ce: 0.078272
[03:47:27.132] iteration 8990 : loss : 0.458301, loss_ce: 0.062491
[03:47:34.148] iteration 9000 : loss : 0.463081, loss_ce: 0.060891
[03:47:41.174] iteration 9010 : loss : 0.464249, loss_ce: 0.072462
[03:47:48.181] iteration 9020 : loss : 0.462264, loss_ce: 0.080755
[03:47:55.197] iteration 9030 : loss : 0.449955, loss_ce: 0.056112
[03:48:02.213] iteration 9040 : loss : 0.457137, loss_ce: 0.076255
[03:48:09.242] iteration 9050 : loss : 0.460041, loss_ce: 0.066954
[03:48:16.265] iteration 9060 : loss : 0.467497, loss_ce: 0.082345
[03:48:23.304] iteration 9070 : loss : 0.458832, loss_ce: 0.071771
[03:48:30.347] iteration 9080 : loss : 0.463984, loss_ce: 0.061807
[03:48:37.386] iteration 9090 : loss : 0.473657, loss_ce: 0.090415
[03:48:44.420] iteration 9100 : loss : 0.450145, loss_ce: 0.051429
[03:48:51.450] iteration 9110 : loss : 0.458902, loss_ce: 0.060992
[03:48:58.472] iteration 9120 : loss : 0.459404, loss_ce: 0.065922
[03:49:05.498] iteration 9130 : loss : 0.460817, loss_ce: 0.077408
[03:49:12.529] iteration 9140 : loss : 0.462282, loss_ce: 0.067998
[03:49:19.579] iteration 9150 : loss : 0.464918, loss_ce: 0.076272
[03:49:26.607] iteration 9160 : loss : 0.466713, loss_ce: 0.080127
[03:49:33.642] iteration 9170 : loss : 0.464382, loss_ce: 0.078234
[03:49:40.603] iteration 9180 : loss : 0.454733, loss_ce: 0.065315
[03:49:47.650] iteration 9190 : loss : 0.451374, loss_ce: 0.050023
[03:49:54.693] iteration 9200 : loss : 0.464335, loss_ce: 0.081307
[03:50:01.731] iteration 9210 : loss : 0.467171, loss_ce: 0.081434
[03:50:08.751] iteration 9220 : loss : 0.452708, loss_ce: 0.052628
[03:50:15.768] iteration 9230 : loss : 0.470296, loss_ce: 0.083152
[03:50:35.203] iteration 9240 : loss : 0.454336, loss_ce: 0.071079
[03:50:42.220] iteration 9250 : loss : 0.464582, loss_ce: 0.076396
[03:50:49.268] iteration 9260 : loss : 0.468727, loss_ce: 0.070812
[03:50:56.305] iteration 9270 : loss : 0.453734, loss_ce: 0.049326
[03:51:03.329] iteration 9280 : loss : 0.446875, loss_ce: 0.053706
[03:51:10.365] iteration 9290 : loss : 0.460469, loss_ce: 0.066317
[03:51:17.395] iteration 9300 : loss : 0.456074, loss_ce: 0.065209
[03:51:24.420] iteration 9310 : loss : 0.454917, loss_ce: 0.068761
[03:51:31.428] iteration 9320 : loss : 0.471600, loss_ce: 0.074822
[03:51:38.444] iteration 9330 : loss : 0.467469, loss_ce: 0.078105
[03:51:45.460] iteration 9340 : loss : 0.463504, loss_ce: 0.080945
[03:51:52.480] iteration 9350 : loss : 0.460791, loss_ce: 0.066514
[03:51:59.502] iteration 9360 : loss : 0.459641, loss_ce: 0.058962
[03:52:06.532] iteration 9370 : loss : 0.456803, loss_ce: 0.069315
[03:52:13.537] iteration 9380 : loss : 0.457361, loss_ce: 0.066319
[03:52:20.568] iteration 9390 : loss : 0.460542, loss_ce: 0.069208
[03:52:27.591] iteration 9400 : loss : 0.465807, loss_ce: 0.079861
[03:52:34.624] iteration 9410 : loss : 0.462964, loss_ce: 0.077536
[03:52:41.631] iteration 9420 : loss : 0.453895, loss_ce: 0.060597
[03:52:48.651] iteration 9430 : loss : 0.458878, loss_ce: 0.060079
[03:52:55.656] iteration 9440 : loss : 0.475505, loss_ce: 0.093103
[03:53:02.688] iteration 9450 : loss : 0.462425, loss_ce: 0.074685
[03:53:09.697] iteration 9460 : loss : 0.464091, loss_ce: 0.063265
[03:53:16.657] iteration 9470 : loss : 0.472051, loss_ce: 0.091896
[03:53:23.685] iteration 9480 : loss : 0.463579, loss_ce: 0.079976
[03:53:30.704] iteration 9490 : loss : 0.472743, loss_ce: 0.093606
[03:53:37.722] iteration 9500 : loss : 0.459010, loss_ce: 0.058320
[03:53:44.774] iteration 9510 : loss : 0.467125, loss_ce: 0.090883
[03:53:51.777] iteration 9520 : loss : 0.460323, loss_ce: 0.077401
[03:53:58.788] iteration 9530 : loss : 0.466829, loss_ce: 0.084731
[03:54:15.887] iteration 9540 : loss : 0.470225, loss_ce: 0.093481
[03:54:22.902] iteration 9550 : loss : 0.464020, loss_ce: 0.069965
[03:54:29.926] iteration 9560 : loss : 0.461926, loss_ce: 0.070582
[03:54:36.961] iteration 9570 : loss : 0.453800, loss_ce: 0.075949
[03:54:43.972] iteration 9580 : loss : 0.462425, loss_ce: 0.072159
[03:54:50.986] iteration 9590 : loss : 0.472577, loss_ce: 0.089328
[03:54:58.008] iteration 9600 : loss : 0.457346, loss_ce: 0.064895
[03:55:05.020] iteration 9610 : loss : 0.466279, loss_ce: 0.078352
[03:55:12.027] iteration 9620 : loss : 0.458836, loss_ce: 0.061629
[03:55:19.050] iteration 9630 : loss : 0.455391, loss_ce: 0.059393
[03:55:26.055] iteration 9640 : loss : 0.459311, loss_ce: 0.082448
[03:55:33.076] iteration 9650 : loss : 0.469602, loss_ce: 0.097720
[03:55:40.094] iteration 9660 : loss : 0.462292, loss_ce: 0.068321
[03:55:47.122] iteration 9670 : loss : 0.464192, loss_ce: 0.079847
[03:55:54.139] iteration 9680 : loss : 0.470506, loss_ce: 0.081621
[03:56:01.158] iteration 9690 : loss : 0.452670, loss_ce: 0.066416
[03:56:08.175] iteration 9700 : loss : 0.468961, loss_ce: 0.081538
[03:56:15.188] iteration 9710 : loss : 0.454634, loss_ce: 0.059651
[03:56:22.195] iteration 9720 : loss : 0.459159, loss_ce: 0.064047
[03:56:29.221] iteration 9730 : loss : 0.454914, loss_ce: 0.056182
[03:56:36.239] iteration 9740 : loss : 0.457714, loss_ce: 0.059376
[03:56:43.268] iteration 9750 : loss : 0.463882, loss_ce: 0.067823
[03:56:50.276] iteration 9760 : loss : 0.461292, loss_ce: 0.059027
[03:56:57.212] iteration 9770 : loss : 0.454364, loss_ce: 0.055001
[03:57:04.229] iteration 9780 : loss : 0.461012, loss_ce: 0.052521
[03:57:11.254] iteration 9790 : loss : 0.454314, loss_ce: 0.056525
[03:57:18.280] iteration 9800 : loss : 0.457587, loss_ce: 0.069740
[03:57:25.295] iteration 9810 : loss : 0.457338, loss_ce: 0.054058
[03:57:32.297] iteration 9820 : loss : 0.461435, loss_ce: 0.079776
[03:57:39.319] iteration 9830 : loss : 0.458229, loss_ce: 0.070879
[03:57:56.366] iteration 9840 : loss : 0.461731, loss_ce: 0.080210
[03:58:03.379] iteration 9850 : loss : 0.457746, loss_ce: 0.059833
[03:58:10.387] iteration 9860 : loss : 0.456034, loss_ce: 0.062799
[03:58:17.408] iteration 9870 : loss : 0.457959, loss_ce: 0.072687
[03:58:24.409] iteration 9880 : loss : 0.453498, loss_ce: 0.056504
[03:58:31.427] iteration 9890 : loss : 0.470115, loss_ce: 0.090683
[03:58:38.437] iteration 9900 : loss : 0.468297, loss_ce: 0.081904
[03:58:45.457] iteration 9910 : loss : 0.459312, loss_ce: 0.075264
[03:58:52.487] iteration 9920 : loss : 0.461130, loss_ce: 0.065768
[03:58:59.503] iteration 9930 : loss : 0.460125, loss_ce: 0.072547
[03:59:06.515] iteration 9940 : loss : 0.453589, loss_ce: 0.072552
[03:59:13.529] iteration 9950 : loss : 0.453529, loss_ce: 0.060337
[03:59:20.540] iteration 9960 : loss : 0.464522, loss_ce: 0.074388
[03:59:27.550] iteration 9970 : loss : 0.457247, loss_ce: 0.063270
[03:59:34.563] iteration 9980 : loss : 0.466266, loss_ce: 0.075545
[03:59:41.583] iteration 9990 : loss : 0.458391, loss_ce: 0.075930
[03:59:48.595] iteration 10000 : loss : 0.454090, loss_ce: 0.058099
[03:59:55.613] iteration 10010 : loss : 0.464807, loss_ce: 0.073336
[04:00:02.623] iteration 10020 : loss : 0.454884, loss_ce: 0.063331
[04:00:09.682] iteration 10030 : loss : 0.462532, loss_ce: 0.077710
[04:00:16.704] iteration 10040 : loss : 0.466439, loss_ce: 0.086752
[04:00:23.715] iteration 10050 : loss : 0.449076, loss_ce: 0.055330
[04:00:30.731] iteration 10060 : loss : 0.459540, loss_ce: 0.067634
[04:00:37.681] iteration 10070 : loss : 0.460971, loss_ce: 0.075103
[04:00:44.690] iteration 10080 : loss : 0.470641, loss_ce: 0.089986
[04:00:51.711] iteration 10090 : loss : 0.456875, loss_ce: 0.053193
[04:00:58.745] iteration 10100 : loss : 0.466256, loss_ce: 0.084055
[04:01:05.769] iteration 10110 : loss : 0.453114, loss_ce: 0.074441
[04:01:12.784] iteration 10120 : loss : 0.466252, loss_ce: 0.062567
[04:01:19.796] iteration 10130 : loss : 0.461179, loss_ce: 0.071346
[04:01:36.843] iteration 10140 : loss : 0.444609, loss_ce: 0.054913
[04:01:43.853] iteration 10150 : loss : 0.454486, loss_ce: 0.060881
[04:01:50.853] iteration 10160 : loss : 0.454064, loss_ce: 0.075658
[04:01:57.887] iteration 10170 : loss : 0.454809, loss_ce: 0.077136
[04:02:04.899] iteration 10180 : loss : 0.458387, loss_ce: 0.067395
[04:02:11.939] iteration 10190 : loss : 0.464479, loss_ce: 0.074895
[04:02:18.943] iteration 10200 : loss : 0.460172, loss_ce: 0.063903
[04:02:25.963] iteration 10210 : loss : 0.467500, loss_ce: 0.093600
[04:02:32.969] iteration 10220 : loss : 0.463248, loss_ce: 0.068115
[04:02:39.989] iteration 10230 : loss : 0.462504, loss_ce: 0.077075
[04:02:46.996] iteration 10240 : loss : 0.445166, loss_ce: 0.067312
[04:02:54.034] iteration 10250 : loss : 0.465358, loss_ce: 0.082819
[04:03:01.045] iteration 10260 : loss : 0.464407, loss_ce: 0.068689
[04:03:08.076] iteration 10270 : loss : 0.458858, loss_ce: 0.073823
[04:03:15.078] iteration 10280 : loss : 0.464247, loss_ce: 0.073423
[04:03:22.107] iteration 10290 : loss : 0.460019, loss_ce: 0.081313
[04:03:29.114] iteration 10300 : loss : 0.464422, loss_ce: 0.070517
[04:03:36.130] iteration 10310 : loss : 0.452297, loss_ce: 0.050144
[04:03:43.134] iteration 10320 : loss : 0.476767, loss_ce: 0.115423
[04:03:50.158] iteration 10330 : loss : 0.459118, loss_ce: 0.074881
[04:03:57.171] iteration 10340 : loss : 0.461085, loss_ce: 0.068186
[04:04:04.186] iteration 10350 : loss : 0.451804, loss_ce: 0.064495
[04:04:11.096] iteration 10360 : loss : 0.484928, loss_ce: 0.114146
[04:04:18.130] iteration 10370 : loss : 0.467551, loss_ce: 0.081633
[04:04:25.145] iteration 10380 : loss : 0.468862, loss_ce: 0.093235
[04:04:32.163] iteration 10390 : loss : 0.460060, loss_ce: 0.067930
[04:04:39.169] iteration 10400 : loss : 0.463659, loss_ce: 0.070286
[04:04:46.185] iteration 10410 : loss : 0.472494, loss_ce: 0.085287
[04:04:53.220] iteration 10420 : loss : 0.456135, loss_ce: 0.059298
[04:05:00.067] iteration 10430 : loss : 0.455180, loss_ce: 0.062529
[04:05:17.351] iteration 10440 : loss : 0.451942, loss_ce: 0.069075
[04:05:24.366] iteration 10450 : loss : 0.454959, loss_ce: 0.071310
[04:05:31.373] iteration 10460 : loss : 0.453119, loss_ce: 0.058186
[04:05:38.391] iteration 10470 : loss : 0.453134, loss_ce: 0.052544
[04:05:45.394] iteration 10480 : loss : 0.458465, loss_ce: 0.062604
[04:05:52.420] iteration 10490 : loss : 0.461831, loss_ce: 0.078013
[04:05:59.427] iteration 10500 : loss : 0.459907, loss_ce: 0.075673
[04:06:06.447] iteration 10510 : loss : 0.449786, loss_ce: 0.048911
[04:06:13.457] iteration 10520 : loss : 0.460606, loss_ce: 0.074810
[04:06:20.472] iteration 10530 : loss : 0.459370, loss_ce: 0.071748
[04:06:27.475] iteration 10540 : loss : 0.457220, loss_ce: 0.063304
[04:06:34.496] iteration 10550 : loss : 0.454843, loss_ce: 0.068136
[04:06:41.511] iteration 10560 : loss : 0.472106, loss_ce: 0.079302
[04:06:48.526] iteration 10570 : loss : 0.458248, loss_ce: 0.076158
[04:06:55.535] iteration 10580 : loss : 0.458550, loss_ce: 0.065444
[04:07:02.557] iteration 10590 : loss : 0.462866, loss_ce: 0.065025
[04:07:09.567] iteration 10600 : loss : 0.461738, loss_ce: 0.061670
[04:07:16.588] iteration 10610 : loss : 0.451800, loss_ce: 0.059728
[04:07:23.595] iteration 10620 : loss : 0.461061, loss_ce: 0.074571
[04:07:30.617] iteration 10630 : loss : 0.460695, loss_ce: 0.073189
[04:07:37.631] iteration 10640 : loss : 0.459323, loss_ce: 0.061679
[04:07:44.655] iteration 10650 : loss : 0.457527, loss_ce: 0.068937
[04:07:51.603] iteration 10660 : loss : 0.459669, loss_ce: 0.068670
[04:07:58.621] iteration 10670 : loss : 0.462555, loss_ce: 0.079087
[04:08:05.637] iteration 10680 : loss : 0.451399, loss_ce: 0.069767
[04:08:12.658] iteration 10690 : loss : 0.459683, loss_ce: 0.067678
[04:08:19.688] iteration 10700 : loss : 0.455559, loss_ce: 0.061809
[04:08:26.701] iteration 10710 : loss : 0.460018, loss_ce: 0.054951
[04:08:33.707] iteration 10720 : loss : 0.464187, loss_ce: 0.070711
[04:08:53.070] iteration 10730 : loss : 0.456019, loss_ce: 0.061542
[04:09:00.068] iteration 10740 : loss : 0.463573, loss_ce: 0.079869
[04:09:07.081] iteration 10750 : loss : 0.462607, loss_ce: 0.074978
[04:09:14.083] iteration 10760 : loss : 0.456176, loss_ce: 0.064371
[04:09:21.097] iteration 10770 : loss : 0.460805, loss_ce: 0.051080
[04:09:28.101] iteration 10780 : loss : 0.476302, loss_ce: 0.099999
[04:09:35.125] iteration 10790 : loss : 0.456276, loss_ce: 0.065539
[04:09:42.132] iteration 10800 : loss : 0.462508, loss_ce: 0.073730
[04:09:49.154] iteration 10810 : loss : 0.463196, loss_ce: 0.076353
[04:09:56.162] iteration 10820 : loss : 0.457640, loss_ce: 0.064169
[04:10:03.198] iteration 10830 : loss : 0.452077, loss_ce: 0.070356
[04:10:10.198] iteration 10840 : loss : 0.463009, loss_ce: 0.069229
[04:10:17.217] iteration 10850 : loss : 0.464896, loss_ce: 0.072967
[04:10:24.229] iteration 10860 : loss : 0.464630, loss_ce: 0.077485
[04:10:31.250] iteration 10870 : loss : 0.455740, loss_ce: 0.069262
[04:10:38.264] iteration 10880 : loss : 0.451247, loss_ce: 0.065412
[04:10:45.280] iteration 10890 : loss : 0.457668, loss_ce: 0.065072
[04:10:52.284] iteration 10900 : loss : 0.459257, loss_ce: 0.077412
[04:10:59.307] iteration 10910 : loss : 0.464324, loss_ce: 0.075557
[04:11:06.322] iteration 10920 : loss : 0.458767, loss_ce: 0.066554
[04:11:13.346] iteration 10930 : loss : 0.464472, loss_ce: 0.081649
[04:11:20.349] iteration 10940 : loss : 0.456615, loss_ce: 0.070363
[04:11:27.283] iteration 10950 : loss : 0.449892, loss_ce: 0.059381
[04:11:34.285] iteration 10960 : loss : 0.461355, loss_ce: 0.069509
[04:11:41.303] iteration 10970 : loss : 0.456225, loss_ce: 0.068210
[04:11:48.310] iteration 10980 : loss : 0.460073, loss_ce: 0.076856
[04:11:55.359] iteration 10990 : loss : 0.461618, loss_ce: 0.078926
[04:12:02.376] iteration 11000 : loss : 0.462808, loss_ce: 0.083735
[04:12:09.439] iteration 11010 : loss : 0.458065, loss_ce: 0.062405
[04:12:16.447] iteration 11020 : loss : 0.461624, loss_ce: 0.066382
[04:12:33.556] iteration 11030 : loss : 0.460266, loss_ce: 0.078355
[04:12:40.555] iteration 11040 : loss : 0.462488, loss_ce: 0.073304
[04:12:47.580] iteration 11050 : loss : 0.452121, loss_ce: 0.066926
[04:12:54.581] iteration 11060 : loss : 0.462817, loss_ce: 0.068936
[04:13:01.595] iteration 11070 : loss : 0.459200, loss_ce: 0.078956
[04:13:08.606] iteration 11080 : loss : 0.459822, loss_ce: 0.065609
[04:13:15.618] iteration 11090 : loss : 0.456394, loss_ce: 0.070224
[04:13:22.636] iteration 11100 : loss : 0.449359, loss_ce: 0.060621
[04:13:29.661] iteration 11110 : loss : 0.462716, loss_ce: 0.070877
[04:13:36.712] iteration 11120 : loss : 0.462525, loss_ce: 0.071087
[04:13:43.729] iteration 11130 : loss : 0.468955, loss_ce: 0.084465
[04:13:50.747] iteration 11140 : loss : 0.452889, loss_ce: 0.069509
[04:13:57.773] iteration 11150 : loss : 0.463572, loss_ce: 0.065794
[04:14:04.792] iteration 11160 : loss : 0.453213, loss_ce: 0.060307
[04:14:11.809] iteration 11170 : loss : 0.465834, loss_ce: 0.075816
[04:14:18.826] iteration 11180 : loss : 0.474774, loss_ce: 0.108315
[04:14:25.839] iteration 11190 : loss : 0.468644, loss_ce: 0.094403
[04:14:32.862] iteration 11200 : loss : 0.469267, loss_ce: 0.081363
[04:14:39.890] iteration 11210 : loss : 0.458448, loss_ce: 0.064404
[04:14:46.894] iteration 11220 : loss : 0.459635, loss_ce: 0.073762
[04:14:53.935] iteration 11230 : loss : 0.465652, loss_ce: 0.085229
[04:15:00.941] iteration 11240 : loss : 0.466666, loss_ce: 0.084085
[04:15:07.899] iteration 11250 : loss : 0.462441, loss_ce: 0.078251
[04:15:14.917] iteration 11260 : loss : 0.466148, loss_ce: 0.082356
[04:15:21.927] iteration 11270 : loss : 0.462705, loss_ce: 0.075224
[04:15:28.934] iteration 11280 : loss : 0.458863, loss_ce: 0.055980
[04:15:35.947] iteration 11290 : loss : 0.452789, loss_ce: 0.058359
[04:15:42.950] iteration 11300 : loss : 0.462457, loss_ce: 0.055925
[04:15:49.968] iteration 11310 : loss : 0.468661, loss_ce: 0.089247
[04:15:56.975] iteration 11320 : loss : 0.462705, loss_ce: 0.067308
[04:16:14.046] iteration 11330 : loss : 0.471931, loss_ce: 0.093970
[04:16:21.049] iteration 11340 : loss : 0.460008, loss_ce: 0.072746
[04:16:28.067] iteration 11350 : loss : 0.461655, loss_ce: 0.073621
[04:16:35.077] iteration 11360 : loss : 0.458452, loss_ce: 0.072699
[04:16:42.094] iteration 11370 : loss : 0.463596, loss_ce: 0.079878
[04:16:49.109] iteration 11380 : loss : 0.461826, loss_ce: 0.050255
[04:16:56.124] iteration 11390 : loss : 0.462433, loss_ce: 0.065579
[04:17:03.129] iteration 11400 : loss : 0.460949, loss_ce: 0.057610
[04:17:10.142] iteration 11410 : loss : 0.457521, loss_ce: 0.058422
[04:17:17.148] iteration 11420 : loss : 0.446246, loss_ce: 0.060902
[04:17:24.179] iteration 11430 : loss : 0.458091, loss_ce: 0.072467
[04:17:31.194] iteration 11440 : loss : 0.450471, loss_ce: 0.060609
[04:17:38.209] iteration 11450 : loss : 0.457591, loss_ce: 0.061797
[04:17:45.218] iteration 11460 : loss : 0.464925, loss_ce: 0.058417
[04:17:52.256] iteration 11470 : loss : 0.451702, loss_ce: 0.075865
[04:17:59.270] iteration 11480 : loss : 0.452770, loss_ce: 0.072040
[04:18:06.301] iteration 11490 : loss : 0.466717, loss_ce: 0.078310
[04:18:13.313] iteration 11500 : loss : 0.467132, loss_ce: 0.081860
[04:18:20.330] iteration 11510 : loss : 0.463484, loss_ce: 0.068112
[04:18:27.336] iteration 11520 : loss : 0.461688, loss_ce: 0.073577
[04:18:34.355] iteration 11530 : loss : 0.457441, loss_ce: 0.082653
[04:18:41.360] iteration 11540 : loss : 0.450199, loss_ce: 0.063246
[04:18:48.319] iteration 11550 : loss : 0.462796, loss_ce: 0.070598
[04:18:55.327] iteration 11560 : loss : 0.482876, loss_ce: 0.120926
[04:19:02.361] iteration 11570 : loss : 0.463473, loss_ce: 0.063530
[04:19:09.373] iteration 11580 : loss : 0.460558, loss_ce: 0.074296
[04:19:16.412] iteration 11590 : loss : 0.454480, loss_ce: 0.054916
[04:19:23.421] iteration 11600 : loss : 0.464177, loss_ce: 0.063990
[04:19:30.432] iteration 11610 : loss : 0.458625, loss_ce: 0.063426
[04:19:37.438] iteration 11620 : loss : 0.457987, loss_ce: 0.059546
[04:19:54.508] iteration 11630 : loss : 0.454592, loss_ce: 0.061364
[04:20:01.507] iteration 11640 : loss : 0.463415, loss_ce: 0.074750
[04:20:08.529] iteration 11650 : loss : 0.458876, loss_ce: 0.058522
[04:20:15.519] iteration 11660 : loss : 0.457882, loss_ce: 0.065135
[04:20:22.531] iteration 11670 : loss : 0.452035, loss_ce: 0.055616
[04:20:29.544] iteration 11680 : loss : 0.461299, loss_ce: 0.054995
[04:20:36.555] iteration 11690 : loss : 0.464480, loss_ce: 0.084288
[04:20:43.560] iteration 11700 : loss : 0.459345, loss_ce: 0.067290
[04:20:50.573] iteration 11710 : loss : 0.469801, loss_ce: 0.089244
[04:20:57.573] iteration 11720 : loss : 0.463122, loss_ce: 0.074434
[04:21:04.638] iteration 11730 : loss : 0.459184, loss_ce: 0.069583
[04:21:11.647] iteration 11740 : loss : 0.466856, loss_ce: 0.088557
[04:21:18.681] iteration 11750 : loss : 0.479139, loss_ce: 0.125691
[04:21:25.688] iteration 11760 : loss : 0.457336, loss_ce: 0.055761
[04:21:32.707] iteration 11770 : loss : 0.463312, loss_ce: 0.067524
[04:21:39.719] iteration 11780 : loss : 0.465986, loss_ce: 0.087055
[04:21:46.746] iteration 11790 : loss : 0.468803, loss_ce: 0.079824
[04:21:53.754] iteration 11800 : loss : 0.466993, loss_ce: 0.078351
[04:22:00.777] iteration 11810 : loss : 0.454548, loss_ce: 0.068789
[04:22:07.782] iteration 11820 : loss : 0.459832, loss_ce: 0.076398
[04:22:14.812] iteration 11830 : loss : 0.458163, loss_ce: 0.068520
[04:22:21.723] iteration 11840 : loss : 0.463031, loss_ce: 0.077399
[04:22:28.745] iteration 11850 : loss : 0.450412, loss_ce: 0.067314
[04:22:35.776] iteration 11860 : loss : 0.454690, loss_ce: 0.062118
[04:22:42.789] iteration 11870 : loss : 0.459977, loss_ce: 0.093356
[04:22:49.801] iteration 11880 : loss : 0.436925, loss_ce: 0.065543
[04:22:56.815] iteration 11890 : loss : 0.458732, loss_ce: 0.074620
[04:23:03.823] iteration 11900 : loss : 0.466381, loss_ce: 0.077893
[04:23:10.837] iteration 11910 : loss : 0.457367, loss_ce: 0.070976
[04:23:17.663] iteration 11920 : loss : 0.471385, loss_ce: 0.097850
[04:23:18.330] save model to ./finetune_tpgm_kits23\finetuned_epoch_39.pth
[04:23:34.990] iteration 11930 : loss : 0.472893, loss_ce: 0.086585
[04:23:41.987] iteration 11940 : loss : 0.460331, loss_ce: 0.070330
[04:23:49.011] iteration 11950 : loss : 0.461484, loss_ce: 0.072574
[04:23:56.020] iteration 11960 : loss : 0.456696, loss_ce: 0.075550
[04:24:03.040] iteration 11970 : loss : 0.457945, loss_ce: 0.047131
[04:24:10.043] iteration 11980 : loss : 0.461509, loss_ce: 0.060922
[04:24:17.064] iteration 11990 : loss : 0.459840, loss_ce: 0.062324
[04:24:24.065] iteration 12000 : loss : 0.459596, loss_ce: 0.075263
[04:24:31.086] iteration 12010 : loss : 0.454918, loss_ce: 0.063982
[04:24:38.107] iteration 12020 : loss : 0.463710, loss_ce: 0.059004
[04:24:45.126] iteration 12030 : loss : 0.458362, loss_ce: 0.076554
[04:24:52.140] iteration 12040 : loss : 0.468637, loss_ce: 0.091384
[04:24:59.155] iteration 12050 : loss : 0.454769, loss_ce: 0.058610
[04:25:06.170] iteration 12060 : loss : 0.465607, loss_ce: 0.088567
[04:25:13.182] iteration 12070 : loss : 0.459486, loss_ce: 0.080604
[04:25:20.184] iteration 12080 : loss : 0.460750, loss_ce: 0.055770
[04:25:27.217] iteration 12090 : loss : 0.463123, loss_ce: 0.072517
[04:25:34.235] iteration 12100 : loss : 0.467773, loss_ce: 0.083406
[04:25:41.257] iteration 12110 : loss : 0.464628, loss_ce: 0.078795
[04:25:48.267] iteration 12120 : loss : 0.466776, loss_ce: 0.094966
[04:25:55.289] iteration 12130 : loss : 0.463154, loss_ce: 0.061658
[04:26:02.212] iteration 12140 : loss : 0.457858, loss_ce: 0.072889
[04:26:09.230] iteration 12150 : loss : 0.457949, loss_ce: 0.055781
[04:26:16.257] iteration 12160 : loss : 0.458190, loss_ce: 0.073020
[04:26:23.267] iteration 12170 : loss : 0.460635, loss_ce: 0.071250
[04:26:30.270] iteration 12180 : loss : 0.463208, loss_ce: 0.069762
[04:26:37.288] iteration 12190 : loss : 0.467904, loss_ce: 0.094035
[04:26:44.289] iteration 12200 : loss : 0.464369, loss_ce: 0.065270
[04:26:51.304] iteration 12210 : loss : 0.458021, loss_ce: 0.064507
[04:27:10.746] iteration 12220 : loss : 0.469890, loss_ce: 0.084743
[04:27:17.753] iteration 12230 : loss : 0.463306, loss_ce: 0.071009
[04:27:24.749] iteration 12240 : loss : 0.466560, loss_ce: 0.066959
[04:27:31.758] iteration 12250 : loss : 0.454976, loss_ce: 0.084344
[04:27:38.753] iteration 12260 : loss : 0.462469, loss_ce: 0.073542
[04:27:45.766] iteration 12270 : loss : 0.461015, loss_ce: 0.072459
[04:27:52.768] iteration 12280 : loss : 0.459899, loss_ce: 0.069413
[04:27:59.779] iteration 12290 : loss : 0.466433, loss_ce: 0.079656
[04:28:06.781] iteration 12300 : loss : 0.467549, loss_ce: 0.078658
[04:28:13.796] iteration 12310 : loss : 0.468098, loss_ce: 0.072053
[04:28:20.811] iteration 12320 : loss : 0.467637, loss_ce: 0.078353
[04:28:27.827] iteration 12330 : loss : 0.453743, loss_ce: 0.051583
[04:28:34.831] iteration 12340 : loss : 0.464811, loss_ce: 0.061776
[04:28:41.863] iteration 12350 : loss : 0.458531, loss_ce: 0.070896
[04:28:48.871] iteration 12360 : loss : 0.474845, loss_ce: 0.093451
[04:28:55.885] iteration 12370 : loss : 0.464399, loss_ce: 0.084645
[04:29:02.906] iteration 12380 : loss : 0.459840, loss_ce: 0.079483
[04:29:09.925] iteration 12390 : loss : 0.451899, loss_ce: 0.056634
[04:29:16.930] iteration 12400 : loss : 0.469468, loss_ce: 0.076610
[04:29:23.971] iteration 12410 : loss : 0.461273, loss_ce: 0.073724
[04:29:30.984] iteration 12420 : loss : 0.461825, loss_ce: 0.071102
[04:29:37.924] iteration 12430 : loss : 0.465294, loss_ce: 0.074494
[04:29:44.929] iteration 12440 : loss : 0.455787, loss_ce: 0.063541
[04:29:51.961] iteration 12450 : loss : 0.460578, loss_ce: 0.058539
[04:29:58.972] iteration 12460 : loss : 0.454917, loss_ce: 0.072149
[04:30:05.997] iteration 12470 : loss : 0.464416, loss_ce: 0.070597
[04:30:13.022] iteration 12480 : loss : 0.460114, loss_ce: 0.073543
[04:30:20.048] iteration 12490 : loss : 0.463211, loss_ce: 0.074365
[04:30:27.059] iteration 12500 : loss : 0.458881, loss_ce: 0.058453
[04:30:34.074] iteration 12510 : loss : 0.463744, loss_ce: 0.066074
[04:30:51.223] iteration 12520 : loss : 0.463651, loss_ce: 0.067505
[04:30:58.247] iteration 12530 : loss : 0.463185, loss_ce: 0.081365
[04:31:05.272] iteration 12540 : loss : 0.450286, loss_ce: 0.057639
[04:31:12.300] iteration 12550 : loss : 0.461032, loss_ce: 0.075310
[04:31:19.305] iteration 12560 : loss : 0.460704, loss_ce: 0.070734
[04:31:26.321] iteration 12570 : loss : 0.464628, loss_ce: 0.084837
[04:31:33.328] iteration 12580 : loss : 0.465708, loss_ce: 0.079846
[04:31:40.355] iteration 12590 : loss : 0.459996, loss_ce: 0.061695
[04:31:47.362] iteration 12600 : loss : 0.456134, loss_ce: 0.061926
[04:31:54.381] iteration 12610 : loss : 0.456625, loss_ce: 0.067985
[04:32:01.395] iteration 12620 : loss : 0.460160, loss_ce: 0.062192
[04:32:08.411] iteration 12630 : loss : 0.463123, loss_ce: 0.061893
[04:32:15.423] iteration 12640 : loss : 0.460669, loss_ce: 0.062411
[04:32:22.446] iteration 12650 : loss : 0.462604, loss_ce: 0.067347
[04:32:29.451] iteration 12660 : loss : 0.457339, loss_ce: 0.069248
[04:32:36.480] iteration 12670 : loss : 0.457087, loss_ce: 0.063739
[04:32:43.490] iteration 12680 : loss : 0.460389, loss_ce: 0.078235
[04:32:50.517] iteration 12690 : loss : 0.471242, loss_ce: 0.089908
[04:32:57.533] iteration 12700 : loss : 0.462774, loss_ce: 0.059571
[04:33:04.557] iteration 12710 : loss : 0.464282, loss_ce: 0.067284
[04:33:11.572] iteration 12720 : loss : 0.457313, loss_ce: 0.066281
[04:33:18.534] iteration 12730 : loss : 0.460299, loss_ce: 0.070250
[04:33:25.543] iteration 12740 : loss : 0.459010, loss_ce: 0.064548
[04:33:32.563] iteration 12750 : loss : 0.461726, loss_ce: 0.064669
[04:33:39.578] iteration 12760 : loss : 0.448957, loss_ce: 0.051408
[04:33:46.601] iteration 12770 : loss : 0.463638, loss_ce: 0.082214
[04:33:53.617] iteration 12780 : loss : 0.457743, loss_ce: 0.076565
[04:34:00.651] iteration 12790 : loss : 0.460721, loss_ce: 0.064164
[04:34:07.661] iteration 12800 : loss : 0.454253, loss_ce: 0.064226
[04:34:14.687] iteration 12810 : loss : 0.463382, loss_ce: 0.075008
[04:34:31.697] iteration 12820 : loss : 0.475895, loss_ce: 0.097888
[04:34:38.711] iteration 12830 : loss : 0.466792, loss_ce: 0.090133
[04:34:45.714] iteration 12840 : loss : 0.465181, loss_ce: 0.072931
[04:34:52.724] iteration 12850 : loss : 0.460219, loss_ce: 0.061113
[04:34:59.734] iteration 12860 : loss : 0.455491, loss_ce: 0.063575
[04:35:06.754] iteration 12870 : loss : 0.459077, loss_ce: 0.065067
[04:35:13.764] iteration 12880 : loss : 0.454890, loss_ce: 0.060036
[04:35:20.786] iteration 12890 : loss : 0.457622, loss_ce: 0.079764
[04:35:27.792] iteration 12900 : loss : 0.460422, loss_ce: 0.066393
[04:35:34.815] iteration 12910 : loss : 0.453652, loss_ce: 0.065211
[04:35:41.821] iteration 12920 : loss : 0.463917, loss_ce: 0.068101
[04:35:48.857] iteration 12930 : loss : 0.464404, loss_ce: 0.069777
[04:35:55.864] iteration 12940 : loss : 0.459039, loss_ce: 0.072094
[04:36:02.901] iteration 12950 : loss : 0.455989, loss_ce: 0.050260
[04:36:09.910] iteration 12960 : loss : 0.466356, loss_ce: 0.070105
[04:36:16.940] iteration 12970 : loss : 0.456884, loss_ce: 0.061132
[04:36:23.963] iteration 12980 : loss : 0.462248, loss_ce: 0.066413
[04:36:30.990] iteration 12990 : loss : 0.457879, loss_ce: 0.073480
[04:36:37.996] iteration 13000 : loss : 0.456735, loss_ce: 0.070003
[04:36:45.017] iteration 13010 : loss : 0.462569, loss_ce: 0.066857
[04:36:52.022] iteration 13020 : loss : 0.448985, loss_ce: 0.066918
[04:36:58.980] iteration 13030 : loss : 0.464210, loss_ce: 0.067105
[04:37:05.988] iteration 13040 : loss : 0.460433, loss_ce: 0.072443
[04:37:13.018] iteration 13050 : loss : 0.450819, loss_ce: 0.060176
[04:37:20.023] iteration 13060 : loss : 0.457268, loss_ce: 0.054439
[04:37:27.051] iteration 13070 : loss : 0.461723, loss_ce: 0.071731
[04:37:34.057] iteration 13080 : loss : 0.465032, loss_ce: 0.074252
[04:37:41.072] iteration 13090 : loss : 0.455857, loss_ce: 0.067828
[04:37:48.082] iteration 13100 : loss : 0.481131, loss_ce: 0.111280
[04:37:55.110] iteration 13110 : loss : 0.465680, loss_ce: 0.083425
[04:38:12.194] iteration 13120 : loss : 0.453529, loss_ce: 0.073013
[04:38:19.207] iteration 13130 : loss : 0.471470, loss_ce: 0.087292
[04:38:26.216] iteration 13140 : loss : 0.470156, loss_ce: 0.093157
[04:38:33.260] iteration 13150 : loss : 0.458034, loss_ce: 0.056641
[04:38:40.263] iteration 13160 : loss : 0.457190, loss_ce: 0.074712
[04:38:47.276] iteration 13170 : loss : 0.473087, loss_ce: 0.087735
[04:38:54.282] iteration 13180 : loss : 0.460158, loss_ce: 0.066503
[04:39:01.295] iteration 13190 : loss : 0.466497, loss_ce: 0.081244
[04:39:08.320] iteration 13200 : loss : 0.459701, loss_ce: 0.076193
[04:39:15.332] iteration 13210 : loss : 0.454289, loss_ce: 0.060206
[04:39:22.334] iteration 13220 : loss : 0.462317, loss_ce: 0.080643
[04:39:29.357] iteration 13230 : loss : 0.467006, loss_ce: 0.072519
[04:39:36.365] iteration 13240 : loss : 0.470857, loss_ce: 0.086799
[04:39:43.383] iteration 13250 : loss : 0.465040, loss_ce: 0.071619
[04:39:50.447] iteration 13260 : loss : 0.480966, loss_ce: 0.106453
[04:39:57.468] iteration 13270 : loss : 0.465705, loss_ce: 0.068899
[04:40:04.505] iteration 13280 : loss : 0.461525, loss_ce: 0.083786
[04:40:11.529] iteration 13290 : loss : 0.461257, loss_ce: 0.078176
[04:40:18.547] iteration 13300 : loss : 0.459001, loss_ce: 0.056835
[04:40:25.566] iteration 13310 : loss : 0.460659, loss_ce: 0.076080
[04:40:32.475] iteration 13320 : loss : 0.455565, loss_ce: 0.048473
[04:40:39.505] iteration 13330 : loss : 0.457466, loss_ce: 0.062480
[04:40:46.508] iteration 13340 : loss : 0.451719, loss_ce: 0.057740
[04:40:53.526] iteration 13350 : loss : 0.466207, loss_ce: 0.081813
[04:41:00.566] iteration 13360 : loss : 0.457391, loss_ce: 0.056245
[04:41:07.603] iteration 13370 : loss : 0.459613, loss_ce: 0.069204
[04:41:14.612] iteration 13380 : loss : 0.458251, loss_ce: 0.068811
[04:41:21.648] iteration 13390 : loss : 0.465845, loss_ce: 0.091166
[04:41:28.667] iteration 13400 : loss : 0.469307, loss_ce: 0.097253
[04:41:35.495] iteration 13410 : loss : 0.450912, loss_ce: 0.049444
[04:41:52.871] iteration 13420 : loss : 0.466376, loss_ce: 0.088433
[04:41:59.879] iteration 13430 : loss : 0.468336, loss_ce: 0.082933
[04:42:06.887] iteration 13440 : loss : 0.467390, loss_ce: 0.058646
[04:42:13.902] iteration 13450 : loss : 0.462861, loss_ce: 0.088718
[04:42:20.923] iteration 13460 : loss : 0.464920, loss_ce: 0.070344
[04:42:27.932] iteration 13470 : loss : 0.452793, loss_ce: 0.057411
[04:42:34.941] iteration 13480 : loss : 0.463466, loss_ce: 0.075301
[04:42:41.958] iteration 13490 : loss : 0.445270, loss_ce: 0.062984
[04:42:48.961] iteration 13500 : loss : 0.457756, loss_ce: 0.072318
[04:42:55.981] iteration 13510 : loss : 0.463765, loss_ce: 0.066884
[04:43:02.996] iteration 13520 : loss : 0.466149, loss_ce: 0.071333
[04:43:10.025] iteration 13530 : loss : 0.461346, loss_ce: 0.080727
[04:43:17.034] iteration 13540 : loss : 0.457555, loss_ce: 0.055990
[04:43:24.052] iteration 13550 : loss : 0.464956, loss_ce: 0.072872
[04:43:31.066] iteration 13560 : loss : 0.463951, loss_ce: 0.067307
[04:43:38.082] iteration 13570 : loss : 0.466724, loss_ce: 0.077487
[04:43:45.098] iteration 13580 : loss : 0.463533, loss_ce: 0.070971
[04:43:52.119] iteration 13590 : loss : 0.459701, loss_ce: 0.066104
[04:43:59.129] iteration 13600 : loss : 0.454229, loss_ce: 0.069573
[04:44:06.158] iteration 13610 : loss : 0.462358, loss_ce: 0.065534
[04:44:13.099] iteration 13620 : loss : 0.458916, loss_ce: 0.063358
[04:44:20.127] iteration 13630 : loss : 0.458394, loss_ce: 0.070289
[04:44:27.134] iteration 13640 : loss : 0.468967, loss_ce: 0.090168
[04:44:34.173] iteration 13650 : loss : 0.457586, loss_ce: 0.052806
[04:44:41.193] iteration 13660 : loss : 0.456628, loss_ce: 0.064395
[04:44:48.209] iteration 13670 : loss : 0.465181, loss_ce: 0.081913
[04:44:55.223] iteration 13680 : loss : 0.465843, loss_ce: 0.076431
[04:45:02.245] iteration 13690 : loss : 0.469081, loss_ce: 0.071934
[04:45:09.261] iteration 13700 : loss : 0.454719, loss_ce: 0.052900
[04:45:28.611] iteration 13710 : loss : 0.457581, loss_ce: 0.060239
[04:45:35.612] iteration 13720 : loss : 0.461206, loss_ce: 0.060199
[04:45:42.620] iteration 13730 : loss : 0.464040, loss_ce: 0.074692
[04:45:49.623] iteration 13740 : loss : 0.454599, loss_ce: 0.056794
[04:45:56.649] iteration 13750 : loss : 0.463037, loss_ce: 0.075349
[04:46:03.653] iteration 13760 : loss : 0.458392, loss_ce: 0.080948
[04:46:10.684] iteration 13770 : loss : 0.467041, loss_ce: 0.091348
[04:46:17.695] iteration 13780 : loss : 0.465135, loss_ce: 0.082002
[04:46:24.717] iteration 13790 : loss : 0.452628, loss_ce: 0.070574
[04:46:31.727] iteration 13800 : loss : 0.456991, loss_ce: 0.067862
[04:46:38.753] iteration 13810 : loss : 0.456340, loss_ce: 0.064482
[04:46:45.763] iteration 13820 : loss : 0.465412, loss_ce: 0.088790
[04:46:52.778] iteration 13830 : loss : 0.463210, loss_ce: 0.068479
[04:46:59.785] iteration 13840 : loss : 0.452366, loss_ce: 0.076400
[04:47:06.804] iteration 13850 : loss : 0.452792, loss_ce: 0.066916
[04:47:13.842] iteration 13860 : loss : 0.460649, loss_ce: 0.061669
[04:47:20.882] iteration 13870 : loss : 0.453894, loss_ce: 0.059959
[04:47:27.893] iteration 13880 : loss : 0.460483, loss_ce: 0.085298
[04:47:34.913] iteration 13890 : loss : 0.468885, loss_ce: 0.075200
[04:47:41.930] iteration 13900 : loss : 0.466609, loss_ce: 0.079896
[04:47:48.860] iteration 13910 : loss : 0.455322, loss_ce: 0.068917
[04:47:55.870] iteration 13920 : loss : 0.455862, loss_ce: 0.062244
[04:48:02.902] iteration 13930 : loss : 0.460390, loss_ce: 0.057198
[04:48:09.920] iteration 13940 : loss : 0.469235, loss_ce: 0.078389
[04:48:16.937] iteration 13950 : loss : 0.457466, loss_ce: 0.054945
[04:48:23.947] iteration 13960 : loss : 0.461657, loss_ce: 0.064255
[04:48:30.976] iteration 13970 : loss : 0.455222, loss_ce: 0.051031
[04:48:37.988] iteration 13980 : loss : 0.451526, loss_ce: 0.052205
[04:48:45.028] iteration 13990 : loss : 0.455095, loss_ce: 0.064471
[04:48:52.033] iteration 14000 : loss : 0.448504, loss_ce: 0.050378
[04:49:09.113] iteration 14010 : loss : 0.465288, loss_ce: 0.081818
[04:49:16.113] iteration 14020 : loss : 0.465194, loss_ce: 0.070059
[04:49:23.121] iteration 14030 : loss : 0.450891, loss_ce: 0.060744
[04:49:30.148] iteration 14040 : loss : 0.459451, loss_ce: 0.065854
[04:49:37.163] iteration 14050 : loss : 0.465133, loss_ce: 0.081525
[04:49:44.170] iteration 14060 : loss : 0.456108, loss_ce: 0.054870
[04:49:51.196] iteration 14070 : loss : 0.465987, loss_ce: 0.070979
[04:49:58.204] iteration 14080 : loss : 0.463242, loss_ce: 0.066722
[04:50:05.239] iteration 14090 : loss : 0.460787, loss_ce: 0.070820
[04:50:12.243] iteration 14100 : loss : 0.482052, loss_ce: 0.108832
[04:50:19.259] iteration 14110 : loss : 0.449204, loss_ce: 0.064890
[04:50:26.290] iteration 14120 : loss : 0.468223, loss_ce: 0.089102
[04:50:33.315] iteration 14130 : loss : 0.465200, loss_ce: 0.082795
[04:50:40.333] iteration 14140 : loss : 0.460145, loss_ce: 0.059170
[04:50:47.360] iteration 14150 : loss : 0.451619, loss_ce: 0.078999
[04:50:54.394] iteration 14160 : loss : 0.463031, loss_ce: 0.076557
[04:51:01.414] iteration 14170 : loss : 0.465983, loss_ce: 0.085496
[04:51:08.428] iteration 14180 : loss : 0.451062, loss_ce: 0.058456
[04:51:15.452] iteration 14190 : loss : 0.468025, loss_ce: 0.081479
[04:51:22.465] iteration 14200 : loss : 0.462339, loss_ce: 0.070671
[04:51:29.413] iteration 14210 : loss : 0.462023, loss_ce: 0.077462
[04:51:36.421] iteration 14220 : loss : 0.461163, loss_ce: 0.070148
[04:51:43.451] iteration 14230 : loss : 0.453982, loss_ce: 0.063323
[04:51:50.455] iteration 14240 : loss : 0.470779, loss_ce: 0.083392
[04:51:57.474] iteration 14250 : loss : 0.459360, loss_ce: 0.066517
[04:52:04.480] iteration 14260 : loss : 0.455586, loss_ce: 0.051526
[04:52:11.504] iteration 14270 : loss : 0.470103, loss_ce: 0.092484
[04:52:18.511] iteration 14280 : loss : 0.469581, loss_ce: 0.082002
[04:52:25.532] iteration 14290 : loss : 0.458179, loss_ce: 0.072061
[04:52:32.538] iteration 14300 : loss : 0.455619, loss_ce: 0.069888
[04:52:49.578] iteration 14310 : loss : 0.459743, loss_ce: 0.072548
[04:52:56.579] iteration 14320 : loss : 0.461074, loss_ce: 0.073589
[04:53:03.598] iteration 14330 : loss : 0.462574, loss_ce: 0.074059
[04:53:10.607] iteration 14340 : loss : 0.450520, loss_ce: 0.053848
[04:53:17.624] iteration 14350 : loss : 0.459210, loss_ce: 0.076465
[04:53:24.650] iteration 14360 : loss : 0.457382, loss_ce: 0.066529
[04:53:31.688] iteration 14370 : loss : 0.465122, loss_ce: 0.079466
[04:53:38.721] iteration 14380 : loss : 0.461936, loss_ce: 0.061976
[04:53:45.747] iteration 14390 : loss : 0.459371, loss_ce: 0.052297
[04:53:52.758] iteration 14400 : loss : 0.457664, loss_ce: 0.066074
[04:53:59.791] iteration 14410 : loss : 0.450754, loss_ce: 0.055484
[04:54:06.802] iteration 14420 : loss : 0.459526, loss_ce: 0.069213
[04:54:13.830] iteration 14430 : loss : 0.455854, loss_ce: 0.070227
[04:54:20.849] iteration 14440 : loss : 0.455277, loss_ce: 0.064745
[04:54:27.871] iteration 14450 : loss : 0.468352, loss_ce: 0.072486
[04:54:34.886] iteration 14460 : loss : 0.465551, loss_ce: 0.092907
[04:54:41.903] iteration 14470 : loss : 0.447399, loss_ce: 0.055557
[04:54:48.937] iteration 14480 : loss : 0.459191, loss_ce: 0.062225
[04:54:55.959] iteration 14490 : loss : 0.463132, loss_ce: 0.082837
[04:55:02.973] iteration 14500 : loss : 0.466647, loss_ce: 0.082714
[04:55:09.930] iteration 14510 : loss : 0.454331, loss_ce: 0.058734
[04:55:16.945] iteration 14520 : loss : 0.453853, loss_ce: 0.060706
[04:55:23.992] iteration 14530 : loss : 0.474459, loss_ce: 0.104118
[04:55:31.001] iteration 14540 : loss : 0.461573, loss_ce: 0.068009
[04:55:38.018] iteration 14550 : loss : 0.461159, loss_ce: 0.078172
[04:55:45.050] iteration 14560 : loss : 0.444241, loss_ce: 0.071708
[04:55:52.074] iteration 14570 : loss : 0.457923, loss_ce: 0.074685
[04:55:59.087] iteration 14580 : loss : 0.464033, loss_ce: 0.067557
[04:56:06.148] iteration 14590 : loss : 0.459880, loss_ce: 0.069277
[04:56:13.167] iteration 14600 : loss : 0.457628, loss_ce: 0.071032
[04:56:30.265] iteration 14610 : loss : 0.452115, loss_ce: 0.056175
[04:56:37.273] iteration 14620 : loss : 0.457908, loss_ce: 0.062985
[04:56:44.290] iteration 14630 : loss : 0.465464, loss_ce: 0.083938
[04:56:51.311] iteration 14640 : loss : 0.453036, loss_ce: 0.069071
[04:56:58.335] iteration 14650 : loss : 0.453925, loss_ce: 0.071841
[04:57:05.355] iteration 14660 : loss : 0.465174, loss_ce: 0.078646
[04:57:12.404] iteration 14670 : loss : 0.458961, loss_ce: 0.065658
[04:57:19.426] iteration 14680 : loss : 0.463399, loss_ce: 0.080965
[04:57:26.441] iteration 14690 : loss : 0.461809, loss_ce: 0.070255
[04:57:33.465] iteration 14700 : loss : 0.475072, loss_ce: 0.099482
[04:57:40.498] iteration 14710 : loss : 0.461386, loss_ce: 0.067988
[04:57:47.515] iteration 14720 : loss : 0.455320, loss_ce: 0.069524
[04:57:54.542] iteration 14730 : loss : 0.462130, loss_ce: 0.071540
[04:58:01.557] iteration 14740 : loss : 0.458084, loss_ce: 0.064626
[04:58:08.583] iteration 14750 : loss : 0.456545, loss_ce: 0.053509
[04:58:15.597] iteration 14760 : loss : 0.481028, loss_ce: 0.120117
[04:58:22.622] iteration 14770 : loss : 0.461462, loss_ce: 0.065361
[04:58:29.641] iteration 14780 : loss : 0.461753, loss_ce: 0.082296
[04:58:36.654] iteration 14790 : loss : 0.463760, loss_ce: 0.078988
[04:58:43.579] iteration 14800 : loss : 0.473752, loss_ce: 0.093336
[04:58:50.601] iteration 14810 : loss : 0.459649, loss_ce: 0.071856
[04:58:57.610] iteration 14820 : loss : 0.454781, loss_ce: 0.076262
[04:59:04.660] iteration 14830 : loss : 0.458271, loss_ce: 0.068437
[04:59:11.685] iteration 14840 : loss : 0.454047, loss_ce: 0.058057
[04:59:18.705] iteration 14850 : loss : 0.461817, loss_ce: 0.066854
[04:59:25.715] iteration 14860 : loss : 0.467314, loss_ce: 0.088623
[04:59:32.752] iteration 14870 : loss : 0.462014, loss_ce: 0.078316
[04:59:39.760] iteration 14880 : loss : 0.463063, loss_ce: 0.075585
[04:59:46.772] iteration 14890 : loss : 0.457944, loss_ce: 0.062804
[04:59:53.603] iteration 14900 : loss : 0.467405, loss_ce: 0.080666
[04:59:54.257] save model to ./finetune_tpgm_kits23\finetuned_epoch_49.pth
