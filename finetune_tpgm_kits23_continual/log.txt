[02:36:03.990] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=100, tpgm_exclude=[])
[02:36:04.018] Using 8569/95221 samples for finetuning
[02:36:04.018] Using 953/95221 samples for TPGM
[02:36:04.018] Model has 9 total classes, training on 4 classes
[02:36:14.093] 268 iterations per epoch. 13400 max iterations 
[02:36:28.992] iteration 10 : loss : 0.461870, loss_ce: 0.081058
[02:36:33.128] iteration 20 : loss : 0.447126, loss_ce: 0.067454
[02:36:37.296] iteration 30 : loss : 0.454064, loss_ce: 0.061675
[02:36:41.486] iteration 40 : loss : 0.438523, loss_ce: 0.037797
[02:36:45.758] iteration 50 : loss : 0.427871, loss_ce: 0.060777
[02:36:49.904] iteration 60 : loss : 0.289397, loss_ce: 0.043874
[02:36:53.981] iteration 70 : loss : 0.430093, loss_ce: 0.078371
[02:36:58.053] iteration 80 : loss : 0.432416, loss_ce: 0.045691
[02:37:02.324] iteration 90 : loss : 0.438128, loss_ce: 0.059353
[02:37:06.707] iteration 100 : loss : 0.424840, loss_ce: 0.065352
[02:37:11.160] iteration 110 : loss : 0.289580, loss_ce: 0.027083
[02:37:15.509] iteration 120 : loss : 0.420143, loss_ce: 0.054165
[02:37:19.859] iteration 130 : loss : 0.428500, loss_ce: 0.046873
[02:37:24.202] iteration 140 : loss : 0.401049, loss_ce: 0.039351
[02:37:28.637] iteration 150 : loss : 0.411577, loss_ce: 0.030802
[02:37:33.104] iteration 160 : loss : 0.382576, loss_ce: 0.048536
[02:37:37.544] iteration 170 : loss : 0.386560, loss_ce: 0.035044
[02:37:42.042] iteration 180 : loss : 0.363701, loss_ce: 0.024090
[02:37:46.517] iteration 190 : loss : 0.240404, loss_ce: 0.031053
[02:37:50.989] iteration 200 : loss : 0.373939, loss_ce: 0.052010
[02:37:55.505] iteration 210 : loss : 0.388509, loss_ce: 0.043876
[02:37:59.998] iteration 220 : loss : 0.379827, loss_ce: 0.024280
[02:38:04.494] iteration 230 : loss : 0.358815, loss_ce: 0.026818
[02:38:08.956] iteration 240 : loss : 0.366219, loss_ce: 0.032203
[02:38:13.452] iteration 250 : loss : 0.391504, loss_ce: 0.052399
[02:38:17.904] iteration 260 : loss : 0.350837, loss_ce: 0.040769
[02:41:23.967] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=100, tpgm_exclude=[])
[02:41:23.996] Using 8569/95221 samples for finetuning
[02:41:23.996] Using 953/95221 samples for TPGM
[02:41:23.996] Model has 9 total classes, training on 4 classes
[02:41:34.633] 268 iterations per epoch. 13400 max iterations 
[02:41:52.638] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=100, tpgm_exclude=[])
[02:41:52.664] Using 8569/95221 samples for finetuning
[02:41:52.664] Using 953/95221 samples for TPGM
[02:41:52.664] Model has 9 total classes, training on 4 classes
[02:42:03.260] 268 iterations per epoch. 13400 max iterations 
[02:42:18.175] iteration 10 : loss : 0.461870, loss_ce: 0.081058
[02:42:22.276] iteration 20 : loss : 0.447126, loss_ce: 0.067454
[02:42:26.592] iteration 30 : loss : 0.454064, loss_ce: 0.061675
[02:42:30.924] iteration 40 : loss : 0.438523, loss_ce: 0.037797
[02:42:35.289] iteration 50 : loss : 0.427871, loss_ce: 0.060777
[02:42:39.648] iteration 60 : loss : 0.289397, loss_ce: 0.043874
[02:42:44.042] iteration 70 : loss : 0.430093, loss_ce: 0.078371
[02:42:48.447] iteration 80 : loss : 0.432416, loss_ce: 0.045691
[02:42:52.842] iteration 90 : loss : 0.438128, loss_ce: 0.059353
[02:42:57.247] iteration 100 : loss : 0.424840, loss_ce: 0.065352
[02:43:01.648] iteration 110 : loss : 0.289580, loss_ce: 0.027083
[02:43:06.049] iteration 120 : loss : 0.420143, loss_ce: 0.054165
[02:43:10.439] iteration 130 : loss : 0.428500, loss_ce: 0.046873
[02:43:14.830] iteration 140 : loss : 0.401049, loss_ce: 0.039351
[02:43:19.232] iteration 150 : loss : 0.411577, loss_ce: 0.030802
[02:43:23.612] iteration 160 : loss : 0.382576, loss_ce: 0.048536
[02:43:28.001] iteration 170 : loss : 0.386560, loss_ce: 0.035044
[02:43:32.379] iteration 180 : loss : 0.363701, loss_ce: 0.024090
[02:43:36.777] iteration 190 : loss : 0.240404, loss_ce: 0.031053
[02:43:41.167] iteration 200 : loss : 0.373939, loss_ce: 0.052010
[02:43:45.610] iteration 210 : loss : 0.388509, loss_ce: 0.043876
[02:43:50.038] iteration 220 : loss : 0.379827, loss_ce: 0.024280
[02:43:54.486] iteration 230 : loss : 0.358815, loss_ce: 0.026818
[02:43:58.983] iteration 240 : loss : 0.366219, loss_ce: 0.032203
[02:44:03.437] iteration 250 : loss : 0.391504, loss_ce: 0.052399
[02:44:07.720] iteration 260 : loss : 0.350837, loss_ce: 0.040769
[02:45:50.792] iteration 270 : loss : 0.462899, loss_ce: 0.035541
[02:45:55.154] iteration 280 : loss : 0.463287, loss_ce: 0.090943
[02:45:59.563] iteration 290 : loss : 0.287443, loss_ce: 0.085089
[02:46:04.000] iteration 300 : loss : 0.289705, loss_ce: 0.048987
[02:46:08.404] iteration 310 : loss : 0.247889, loss_ce: 0.024324
[02:46:12.819] iteration 320 : loss : 0.370369, loss_ce: 0.020717
[02:46:17.281] iteration 330 : loss : 0.390467, loss_ce: 0.018715
[02:46:21.552] iteration 340 : loss : 0.170275, loss_ce: 0.025486
[02:46:25.820] iteration 350 : loss : 0.183353, loss_ce: 0.026845
[02:46:30.000] iteration 360 : loss : 0.334774, loss_ce: 0.037828
[02:46:34.258] iteration 370 : loss : 0.332359, loss_ce: 0.029372
[02:46:38.457] iteration 380 : loss : 0.268784, loss_ce: 0.021640
[02:46:42.731] iteration 390 : loss : 0.276486, loss_ce: 0.032036
[02:46:46.985] iteration 400 : loss : 0.333686, loss_ce: 0.038715
[02:46:51.273] iteration 410 : loss : 0.242994, loss_ce: 0.024220
[02:46:55.548] iteration 420 : loss : 0.253154, loss_ce: 0.029021
[02:46:59.799] iteration 430 : loss : 0.410831, loss_ce: 0.081053
[02:47:04.026] iteration 440 : loss : 0.352406, loss_ce: 0.016934
[02:47:08.285] iteration 450 : loss : 0.335707, loss_ce: 0.015532
[02:47:12.525] iteration 460 : loss : 0.364337, loss_ce: 0.050660
[02:47:16.709] iteration 470 : loss : 0.275999, loss_ce: 0.018428
[02:47:20.976] iteration 480 : loss : 0.149995, loss_ce: 0.010344
[02:47:25.202] iteration 490 : loss : 0.328502, loss_ce: 0.039331
[02:47:29.463] iteration 500 : loss : 0.296283, loss_ce: 0.033470
[02:47:33.772] iteration 510 : loss : 0.286253, loss_ce: 0.018286
[02:47:38.073] iteration 520 : loss : 0.212901, loss_ce: 0.014486
[02:47:42.361] iteration 530 : loss : 0.287296, loss_ce: 0.025054
[02:49:25.980] iteration 540 : loss : 0.432019, loss_ce: 0.046167
[02:49:30.455] iteration 550 : loss : 0.296224, loss_ce: 0.024300
[02:49:34.906] iteration 560 : loss : 0.341721, loss_ce: 0.009760
[02:49:39.378] iteration 570 : loss : 0.312383, loss_ce: 0.029959
[02:49:43.847] iteration 580 : loss : 0.343419, loss_ce: 0.044977
[02:49:48.319] iteration 590 : loss : 0.268594, loss_ce: 0.014690
[02:49:52.777] iteration 600 : loss : 0.168865, loss_ce: 0.021438
[02:49:57.252] iteration 610 : loss : 0.318730, loss_ce: 0.052682
[02:50:01.722] iteration 620 : loss : 0.176595, loss_ce: 0.012783
[02:50:06.124] iteration 630 : loss : 0.326273, loss_ce: 0.022085
[02:50:10.383] iteration 640 : loss : 0.304206, loss_ce: 0.010232
[02:50:14.732] iteration 650 : loss : 0.335971, loss_ce: 0.040274
[02:50:19.069] iteration 660 : loss : 0.253794, loss_ce: 0.012850
[02:50:23.320] iteration 670 : loss : 0.206421, loss_ce: 0.037474
[02:50:27.576] iteration 680 : loss : 0.260626, loss_ce: 0.019785
[02:50:31.798] iteration 690 : loss : 0.235492, loss_ce: 0.015387
[02:50:35.960] iteration 700 : loss : 0.162915, loss_ce: 0.046460
[02:50:40.225] iteration 710 : loss : 0.322236, loss_ce: 0.011277
[02:50:44.478] iteration 720 : loss : 0.271329, loss_ce: 0.059038
[02:50:48.692] iteration 730 : loss : 0.135397, loss_ce: 0.009301
[02:50:52.953] iteration 740 : loss : 0.333250, loss_ce: 0.011422
[02:50:57.245] iteration 750 : loss : 0.282560, loss_ce: 0.026030
[02:51:01.492] iteration 760 : loss : 0.248993, loss_ce: 0.024772
[02:51:05.738] iteration 770 : loss : 0.075416, loss_ce: 0.013712
[02:51:09.949] iteration 780 : loss : 0.281418, loss_ce: 0.021314
[02:51:14.262] iteration 790 : loss : 0.061598, loss_ce: 0.009606
[02:51:18.536] iteration 800 : loss : 0.251707, loss_ce: 0.015652
[02:53:02.163] iteration 810 : loss : 0.345800, loss_ce: 0.034433
[02:53:06.419] iteration 820 : loss : 0.341800, loss_ce: 0.031620
[02:53:10.701] iteration 830 : loss : 0.377652, loss_ce: 0.073517
[02:53:15.001] iteration 840 : loss : 0.331360, loss_ce: 0.031119
[02:53:19.521] iteration 850 : loss : 0.311014, loss_ce: 0.016217
[02:53:23.873] iteration 860 : loss : 0.345801, loss_ce: 0.025131
[02:53:28.420] iteration 870 : loss : 0.353577, loss_ce: 0.024165
[02:53:32.838] iteration 880 : loss : 0.238100, loss_ce: 0.005567
[02:53:37.329] iteration 890 : loss : 0.303904, loss_ce: 0.030061
[02:53:41.849] iteration 900 : loss : 0.369860, loss_ce: 0.021992
[02:53:46.191] iteration 910 : loss : 0.303899, loss_ce: 0.032308
[02:53:50.492] iteration 920 : loss : 0.308978, loss_ce: 0.013411
[02:53:54.810] iteration 930 : loss : 0.246135, loss_ce: 0.010069
[02:53:59.107] iteration 940 : loss : 0.230122, loss_ce: 0.012223
[02:54:03.378] iteration 950 : loss : 0.130169, loss_ce: 0.012709
[02:54:07.669] iteration 960 : loss : 0.235335, loss_ce: 0.009371
[02:54:12.036] iteration 970 : loss : 0.312622, loss_ce: 0.012966
[02:54:16.528] iteration 980 : loss : 0.247983, loss_ce: 0.016275
[02:54:20.831] iteration 990 : loss : 0.245154, loss_ce: 0.014087
[02:54:25.132] iteration 1000 : loss : 0.142028, loss_ce: 0.024753
[02:54:29.442] iteration 1010 : loss : 0.241345, loss_ce: 0.003689
[02:54:33.713] iteration 1020 : loss : 0.289091, loss_ce: 0.014857
[02:54:37.983] iteration 1030 : loss : 0.318084, loss_ce: 0.013813
[02:54:42.252] iteration 1040 : loss : 0.348892, loss_ce: 0.016850
[02:54:46.550] iteration 1050 : loss : 0.312511, loss_ce: 0.039105
[02:54:50.843] iteration 1060 : loss : 0.263775, loss_ce: 0.012132
[02:54:55.108] iteration 1070 : loss : 0.254121, loss_ce: 0.027855
[02:56:53.446] iteration 1080 : loss : 0.373847, loss_ce: 0.090172
[02:56:57.929] iteration 1090 : loss : 0.526882, loss_ce: 0.193761
[02:57:02.409] iteration 1100 : loss : 0.512177, loss_ce: 0.153479
[02:57:06.894] iteration 1110 : loss : 0.319693, loss_ce: 0.053037
[02:57:11.090] iteration 1120 : loss : 0.453726, loss_ce: 0.037365
[02:57:15.267] iteration 1130 : loss : 0.308442, loss_ce: 0.028906
[02:57:19.529] iteration 1140 : loss : 0.462756, loss_ce: 0.086455
[02:57:23.775] iteration 1150 : loss : 0.457729, loss_ce: 0.047067
[02:57:28.078] iteration 1160 : loss : 0.459140, loss_ce: 0.060503
[02:57:32.334] iteration 1170 : loss : 0.463198, loss_ce: 0.069604
[02:57:36.443] iteration 1180 : loss : 0.461919, loss_ce: 0.077122
[02:57:40.568] iteration 1190 : loss : 0.456405, loss_ce: 0.052743
[02:57:44.725] iteration 1200 : loss : 0.459601, loss_ce: 0.088184
[02:57:48.898] iteration 1210 : loss : 0.474669, loss_ce: 0.074735
[02:57:52.988] iteration 1220 : loss : 0.473323, loss_ce: 0.069248
[02:57:57.431] iteration 1230 : loss : 0.311346, loss_ce: 0.068942
[02:58:01.748] iteration 1240 : loss : 0.458452, loss_ce: 0.054371
[02:58:05.925] iteration 1250 : loss : 0.467145, loss_ce: 0.066744
[02:58:10.093] iteration 1260 : loss : 0.460677, loss_ce: 0.060950
[02:58:14.276] iteration 1270 : loss : 0.459241, loss_ce: 0.066665
[02:58:18.447] iteration 1280 : loss : 0.464053, loss_ce: 0.072474
[02:58:22.626] iteration 1290 : loss : 0.470914, loss_ce: 0.089088
[02:58:26.791] iteration 1300 : loss : 0.310187, loss_ce: 0.049648
[02:58:30.969] iteration 1310 : loss : 0.462854, loss_ce: 0.063769
[02:58:35.139] iteration 1320 : loss : 0.313103, loss_ce: 0.075132
[02:58:39.316] iteration 1330 : loss : 0.462118, loss_ce: 0.072604
[02:58:43.385] iteration 1340 : loss : 0.468779, loss_ce: 0.076960
[03:00:42.092] iteration 1350 : loss : 0.497346, loss_ce: 0.123794
[03:00:46.504] iteration 1360 : loss : 0.451520, loss_ce: 0.047772
[03:00:50.959] iteration 1370 : loss : 0.454486, loss_ce: 0.078147
[03:00:55.371] iteration 1380 : loss : 0.433844, loss_ce: 0.090404
[03:00:59.907] iteration 1390 : loss : 0.269722, loss_ce: 0.032209
[03:01:04.362] iteration 1400 : loss : 0.332279, loss_ce: 0.079980
[03:01:08.838] iteration 1410 : loss : 0.455214, loss_ce: 0.392039
[03:01:13.274] iteration 1420 : loss : 0.466846, loss_ce: 0.070011
[03:01:17.800] iteration 1430 : loss : 0.458041, loss_ce: 0.077913
[03:01:22.309] iteration 1440 : loss : 0.429870, loss_ce: 0.044564
[03:01:26.877] iteration 1450 : loss : 0.417396, loss_ce: 0.039120
[03:01:31.390] iteration 1460 : loss : 0.420540, loss_ce: 0.074914
[03:01:35.908] iteration 1470 : loss : 0.171749, loss_ce: 0.026988
[03:01:40.407] iteration 1480 : loss : 0.200396, loss_ce: 0.081793
[03:01:44.936] iteration 1490 : loss : 0.281724, loss_ce: 0.024597
[03:01:49.442] iteration 1500 : loss : 0.183973, loss_ce: 0.010961
[03:01:53.971] iteration 1510 : loss : 0.158548, loss_ce: 0.022542
[03:01:58.485] iteration 1520 : loss : 0.330309, loss_ce: 0.008846
[03:02:03.007] iteration 1530 : loss : 0.318752, loss_ce: 0.011912
[03:02:07.489] iteration 1540 : loss : 0.112996, loss_ce: 0.013438
[03:02:11.977] iteration 1550 : loss : 0.164956, loss_ce: 0.006446
[03:02:16.449] iteration 1560 : loss : 0.198572, loss_ce: 0.006977
[03:02:20.934] iteration 1570 : loss : 0.098285, loss_ce: 0.025142
[03:02:25.409] iteration 1580 : loss : 0.257671, loss_ce: 0.031113
[03:02:29.911] iteration 1590 : loss : 0.075437, loss_ce: 0.011250
[03:02:34.418] iteration 1600 : loss : 0.325676, loss_ce: 0.011413
[03:04:24.379] iteration 1610 : loss : 0.467207, loss_ce: 0.147142
[03:04:29.013] iteration 1620 : loss : 0.456488, loss_ce: 0.032437
[03:04:33.546] iteration 1630 : loss : 0.460214, loss_ce: 0.047158
[03:04:37.927] iteration 1640 : loss : 0.441048, loss_ce: 0.106898
[03:04:42.474] iteration 1650 : loss : 0.246708, loss_ce: 0.042774
[03:04:47.000] iteration 1660 : loss : 0.382325, loss_ce: 0.012873
[03:04:51.564] iteration 1670 : loss : 0.300806, loss_ce: 0.027454
[03:04:56.100] iteration 1680 : loss : 0.321878, loss_ce: 0.059917
[03:05:00.659] iteration 1690 : loss : 0.170161, loss_ce: 0.023182
[03:05:05.129] iteration 1700 : loss : 0.349157, loss_ce: 0.027645
[03:05:09.461] iteration 1710 : loss : 0.167321, loss_ce: 0.018527
[03:05:13.769] iteration 1720 : loss : 0.288635, loss_ce: 0.044836
[03:05:18.213] iteration 1730 : loss : 0.330061, loss_ce: 0.023400
[03:05:22.586] iteration 1740 : loss : 0.224537, loss_ce: 0.012667
[03:05:26.912] iteration 1750 : loss : 0.343713, loss_ce: 0.032188
[03:05:31.236] iteration 1760 : loss : 0.335190, loss_ce: 0.016241
[03:05:35.674] iteration 1770 : loss : 0.307953, loss_ce: 0.021370
[03:05:40.099] iteration 1780 : loss : 0.323317, loss_ce: 0.030488
[03:05:44.420] iteration 1790 : loss : 0.354977, loss_ce: 0.039638
[03:05:48.731] iteration 1800 : loss : 0.342655, loss_ce: 0.020570
[03:05:53.130] iteration 1810 : loss : 0.263123, loss_ce: 0.011376
[03:05:57.493] iteration 1820 : loss : 0.312625, loss_ce: 0.065463
[03:06:01.840] iteration 1830 : loss : 0.238035, loss_ce: 0.029055
[03:06:06.172] iteration 1840 : loss : 0.172443, loss_ce: 0.010500
[03:06:10.511] iteration 1850 : loss : 0.329998, loss_ce: 0.013166
[03:06:14.843] iteration 1860 : loss : 0.340365, loss_ce: 0.076583
[03:06:19.189] iteration 1870 : loss : 0.259429, loss_ce: 0.023563
[03:08:13.794] iteration 1880 : loss : 0.504211, loss_ce: 0.186949
[03:08:17.920] iteration 1890 : loss : 0.235301, loss_ce: 0.016287
[03:08:22.041] iteration 1900 : loss : 0.359759, loss_ce: 0.047875
[03:08:26.170] iteration 1910 : loss : 0.312185, loss_ce: 0.045542
[03:08:30.288] iteration 1920 : loss : 0.297752, loss_ce: 0.030772
[03:08:34.423] iteration 1930 : loss : 0.395718, loss_ce: 0.031669
[03:08:38.545] iteration 1940 : loss : 0.316589, loss_ce: 0.060018
[03:08:42.683] iteration 1950 : loss : 0.232974, loss_ce: 0.013123
[03:08:46.809] iteration 1960 : loss : 0.344633, loss_ce: 0.029571
[03:08:50.948] iteration 1970 : loss : 0.345024, loss_ce: 0.022493
[03:08:55.076] iteration 1980 : loss : 0.270465, loss_ce: 0.012211
[03:08:59.230] iteration 1990 : loss : 0.286585, loss_ce: 0.025935
[03:09:03.370] iteration 2000 : loss : 0.323206, loss_ce: 0.026596
[03:09:07.508] iteration 2010 : loss : 0.402453, loss_ce: 0.065693
[03:09:11.640] iteration 2020 : loss : 0.259127, loss_ce: 0.018839
[03:09:15.787] iteration 2030 : loss : 0.327495, loss_ce: 0.010263
[03:09:19.921] iteration 2040 : loss : 0.314576, loss_ce: 0.046358
[03:09:24.063] iteration 2050 : loss : 0.159732, loss_ce: 0.039578
[03:09:28.192] iteration 2060 : loss : 0.350412, loss_ce: 0.051881
[03:09:32.337] iteration 2070 : loss : 0.129191, loss_ce: 0.036055
[03:09:36.472] iteration 2080 : loss : 0.249838, loss_ce: 0.045474
[03:09:40.612] iteration 2090 : loss : 0.263376, loss_ce: 0.011531
[03:09:44.739] iteration 2100 : loss : 0.249591, loss_ce: 0.014607
[03:09:48.882] iteration 2110 : loss : 0.237190, loss_ce: 0.020580
[03:09:53.014] iteration 2120 : loss : 0.259269, loss_ce: 0.033953
[03:09:57.160] iteration 2130 : loss : 0.267710, loss_ce: 0.019035
[03:10:01.293] iteration 2140 : loss : 0.251413, loss_ce: 0.018104
[03:11:40.528] iteration 2150 : loss : 0.337086, loss_ce: 0.019994
[03:11:44.618] iteration 2160 : loss : 0.293777, loss_ce: 0.012244
[03:11:48.729] iteration 2170 : loss : 0.334902, loss_ce: 0.022496
[03:11:52.820] iteration 2180 : loss : 0.204672, loss_ce: 0.014387
[03:11:56.922] iteration 2190 : loss : 0.215315, loss_ce: 0.046743
[03:12:01.017] iteration 2200 : loss : 0.288039, loss_ce: 0.013310
[03:12:05.124] iteration 2210 : loss : 0.262878, loss_ce: 0.016363
[03:12:09.220] iteration 2220 : loss : 0.262349, loss_ce: 0.028659
[03:12:13.330] iteration 2230 : loss : 0.255797, loss_ce: 0.022805
[03:12:17.429] iteration 2240 : loss : 0.132198, loss_ce: 0.021471
[03:12:21.539] iteration 2250 : loss : 0.325644, loss_ce: 0.011179
[03:12:25.643] iteration 2260 : loss : 0.224104, loss_ce: 0.010482
[03:12:29.754] iteration 2270 : loss : 0.326060, loss_ce: 0.009996
[03:12:33.857] iteration 2280 : loss : 0.339858, loss_ce: 0.021813
[03:12:37.968] iteration 2290 : loss : 0.082539, loss_ce: 0.011804
[03:12:42.070] iteration 2300 : loss : 0.267422, loss_ce: 0.027573
[03:12:46.183] iteration 2310 : loss : 0.179622, loss_ce: 0.010440
[03:12:50.285] iteration 2320 : loss : 0.235747, loss_ce: 0.012117
[03:12:54.401] iteration 2330 : loss : 0.328455, loss_ce: 0.012995
[03:12:58.503] iteration 2340 : loss : 0.231488, loss_ce: 0.012607
[03:13:02.616] iteration 2350 : loss : 0.229187, loss_ce: 0.018150
[03:13:06.719] iteration 2360 : loss : 0.315834, loss_ce: 0.010168
[03:13:10.829] iteration 2370 : loss : 0.104012, loss_ce: 0.015731
[03:13:14.934] iteration 2380 : loss : 0.224676, loss_ce: 0.016866
[03:13:19.043] iteration 2390 : loss : 0.122729, loss_ce: 0.021991
[03:13:23.144] iteration 2400 : loss : 0.276804, loss_ce: 0.037440
[03:13:27.255] iteration 2410 : loss : 0.247079, loss_ce: 0.020725
[03:15:05.680] iteration 2420 : loss : 0.350515, loss_ce: 0.037045
[03:15:09.781] iteration 2430 : loss : 0.350154, loss_ce: 0.041208
[03:15:13.874] iteration 2440 : loss : 0.291672, loss_ce: 0.022629
[03:15:17.978] iteration 2450 : loss : 0.222272, loss_ce: 0.026668
[03:15:22.074] iteration 2460 : loss : 0.351501, loss_ce: 0.022899
[03:15:26.187] iteration 2470 : loss : 0.340852, loss_ce: 0.032780
[03:15:30.285] iteration 2480 : loss : 0.354130, loss_ce: 0.049896
[03:15:34.399] iteration 2490 : loss : 0.288298, loss_ce: 0.017000
[03:15:38.499] iteration 2500 : loss : 0.183803, loss_ce: 0.016525
[03:15:42.608] iteration 2510 : loss : 0.245217, loss_ce: 0.018591
[03:15:46.707] iteration 2520 : loss : 0.244054, loss_ce: 0.019307
[03:15:50.815] iteration 2530 : loss : 0.199632, loss_ce: 0.005934
[03:15:54.916] iteration 2540 : loss : 0.146879, loss_ce: 0.022091
[03:15:59.026] iteration 2550 : loss : 0.149157, loss_ce: 0.013710
[03:16:03.128] iteration 2560 : loss : 0.322443, loss_ce: 0.014967
[03:16:07.234] iteration 2570 : loss : 0.121115, loss_ce: 0.009546
[03:16:11.335] iteration 2580 : loss : 0.286778, loss_ce: 0.015925
[03:16:15.444] iteration 2590 : loss : 0.322507, loss_ce: 0.008729
[03:16:19.547] iteration 2600 : loss : 0.161084, loss_ce: 0.021036
[03:16:23.657] iteration 2610 : loss : 0.055167, loss_ce: 0.007242
[03:16:27.763] iteration 2620 : loss : 0.178676, loss_ce: 0.016274
[03:16:31.876] iteration 2630 : loss : 0.254869, loss_ce: 0.015239
[03:16:35.974] iteration 2640 : loss : 0.153697, loss_ce: 0.006411
[03:16:40.085] iteration 2650 : loss : 0.340646, loss_ce: 0.020092
[03:16:44.183] iteration 2660 : loss : 0.329408, loss_ce: 0.020418
[03:16:48.291] iteration 2670 : loss : 0.216590, loss_ce: 0.024111
[03:16:52.293] iteration 2680 : loss : 0.268552, loss_ce: 0.022903
[03:18:27.590] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_9.pth
[03:18:41.431] iteration 2690 : loss : 0.350681, loss_ce: 0.059489
[03:18:45.522] iteration 2700 : loss : 0.128868, loss_ce: 0.023965
[03:18:49.620] iteration 2710 : loss : 0.320596, loss_ce: 0.022377
[03:18:53.710] iteration 2720 : loss : 0.263452, loss_ce: 0.012074
[03:18:57.814] iteration 2730 : loss : 0.289467, loss_ce: 0.027984
[03:19:01.909] iteration 2740 : loss : 0.340355, loss_ce: 0.034357
[03:19:06.019] iteration 2750 : loss : 0.335840, loss_ce: 0.030123
[03:19:10.116] iteration 2760 : loss : 0.301217, loss_ce: 0.008810
[03:19:14.226] iteration 2770 : loss : 0.331163, loss_ce: 0.009262
[03:19:18.325] iteration 2780 : loss : 0.326128, loss_ce: 0.014013
[03:19:22.437] iteration 2790 : loss : 0.291489, loss_ce: 0.014680
[03:19:26.538] iteration 2800 : loss : 0.201396, loss_ce: 0.010149
[03:19:30.647] iteration 2810 : loss : 0.211236, loss_ce: 0.014637
[03:19:34.751] iteration 2820 : loss : 0.284278, loss_ce: 0.009721
[03:19:38.863] iteration 2830 : loss : 0.347026, loss_ce: 0.024867
[03:19:42.964] iteration 2840 : loss : 0.333177, loss_ce: 0.007075
[03:19:47.077] iteration 2850 : loss : 0.228725, loss_ce: 0.020600
[03:19:51.179] iteration 2860 : loss : 0.181101, loss_ce: 0.021088
[03:19:55.290] iteration 2870 : loss : 0.312810, loss_ce: 0.016097
[03:19:59.393] iteration 2880 : loss : 0.251759, loss_ce: 0.027910
[03:20:03.505] iteration 2890 : loss : 0.126236, loss_ce: 0.011511
[03:20:07.609] iteration 2900 : loss : 0.208815, loss_ce: 0.003403
[03:20:11.720] iteration 2910 : loss : 0.260595, loss_ce: 0.011125
[03:20:15.820] iteration 2920 : loss : 0.076244, loss_ce: 0.011611
[03:20:19.936] iteration 2930 : loss : 0.259052, loss_ce: 0.018542
[03:20:24.039] iteration 2940 : loss : 0.224725, loss_ce: 0.015182
[03:22:02.312] iteration 2950 : loss : 0.175665, loss_ce: 0.022818
[03:22:06.397] iteration 2960 : loss : 0.184913, loss_ce: 0.009268
[03:22:10.495] iteration 2970 : loss : 0.177219, loss_ce: 0.018962
[03:22:14.587] iteration 2980 : loss : 0.385894, loss_ce: 0.038086
[03:22:18.691] iteration 2990 : loss : 0.339283, loss_ce: 0.097172
[03:22:22.786] iteration 3000 : loss : 0.460520, loss_ce: 0.045315
[03:22:26.892] iteration 3010 : loss : 0.484765, loss_ce: 0.085502
[03:22:30.986] iteration 3020 : loss : 0.324915, loss_ce: 0.069616
[03:22:35.093] iteration 3030 : loss : 0.463299, loss_ce: 0.053725
[03:22:39.191] iteration 3040 : loss : 0.458890, loss_ce: 0.046919
[03:22:43.296] iteration 3050 : loss : 0.456686, loss_ce: 0.050357
[03:22:47.392] iteration 3060 : loss : 0.439533, loss_ce: 0.029961
[03:22:51.501] iteration 3070 : loss : 0.278008, loss_ce: 0.042563
[03:22:55.600] iteration 3080 : loss : 0.442167, loss_ce: 0.048512
[03:22:59.710] iteration 3090 : loss : 0.431875, loss_ce: 0.072377
[03:23:03.812] iteration 3100 : loss : 0.418482, loss_ce: 0.033084
[03:23:07.925] iteration 3110 : loss : 0.407419, loss_ce: 0.063375
[03:23:12.025] iteration 3120 : loss : 0.207902, loss_ce: 0.044170
[03:23:16.136] iteration 3130 : loss : 0.330487, loss_ce: 0.034683
[03:23:20.237] iteration 3140 : loss : 0.339562, loss_ce: 0.012690
[03:23:24.350] iteration 3150 : loss : 0.310725, loss_ce: 0.027174
[03:23:28.451] iteration 3160 : loss : 0.466997, loss_ce: 0.041858
[03:23:32.557] iteration 3170 : loss : 0.454901, loss_ce: 0.055996
[03:23:36.653] iteration 3180 : loss : 0.446199, loss_ce: 0.058664
[03:23:40.759] iteration 3190 : loss : 0.433473, loss_ce: 0.048470
[03:23:44.857] iteration 3200 : loss : 0.439172, loss_ce: 0.054136
[03:23:48.969] iteration 3210 : loss : 0.438227, loss_ce: 0.084337
[03:25:27.467] iteration 3220 : loss : 0.474599, loss_ce: 0.060474
[03:25:31.560] iteration 3230 : loss : 0.469169, loss_ce: 0.062224
[03:25:35.650] iteration 3240 : loss : 0.415397, loss_ce: 0.047917
[03:25:39.753] iteration 3250 : loss : 0.360637, loss_ce: 0.033886
[03:25:43.845] iteration 3260 : loss : 0.217901, loss_ce: 0.026665
[03:25:47.953] iteration 3270 : loss : 0.363518, loss_ce: 0.069070
[03:25:52.050] iteration 3280 : loss : 0.263313, loss_ce: 0.017674
[03:25:56.161] iteration 3290 : loss : 0.335395, loss_ce: 0.024494
[03:26:00.264] iteration 3300 : loss : 0.204590, loss_ce: 0.038036
[03:26:04.377] iteration 3310 : loss : 0.366722, loss_ce: 0.022984
[03:26:08.472] iteration 3320 : loss : 0.342442, loss_ce: 0.023121
[03:26:12.584] iteration 3330 : loss : 0.220750, loss_ce: 0.039485
[03:26:16.685] iteration 3340 : loss : 0.283905, loss_ce: 0.021018
[03:26:20.798] iteration 3350 : loss : 0.322098, loss_ce: 0.041449
[03:26:24.901] iteration 3360 : loss : 0.253590, loss_ce: 0.011306
[03:26:29.017] iteration 3370 : loss : 0.337008, loss_ce: 0.014853
[03:26:33.120] iteration 3380 : loss : 0.315525, loss_ce: 0.010882
[03:26:37.235] iteration 3390 : loss : 0.325966, loss_ce: 0.011819
[03:26:41.338] iteration 3400 : loss : 0.372905, loss_ce: 0.021368
[03:26:45.452] iteration 3410 : loss : 0.207277, loss_ce: 0.013773
[03:26:49.551] iteration 3420 : loss : 0.205398, loss_ce: 0.049990
[03:26:53.664] iteration 3430 : loss : 0.133608, loss_ce: 0.031071
[03:26:57.768] iteration 3440 : loss : 0.277067, loss_ce: 0.014136
[03:27:01.882] iteration 3450 : loss : 0.285109, loss_ce: 0.021621
[03:27:05.985] iteration 3460 : loss : 0.357024, loss_ce: 0.032368
[03:27:10.103] iteration 3470 : loss : 0.317593, loss_ce: 0.009902
[03:27:14.205] iteration 3480 : loss : 0.290002, loss_ce: 0.007539
[03:29:03.223] iteration 3490 : loss : 0.380191, loss_ce: 0.035792
[03:29:07.310] iteration 3500 : loss : 0.286732, loss_ce: 0.047812
[03:29:11.406] iteration 3510 : loss : 0.300448, loss_ce: 0.038302
[03:29:15.495] iteration 3520 : loss : 0.319923, loss_ce: 0.025748
[03:29:19.600] iteration 3530 : loss : 0.393179, loss_ce: 0.095760
[03:29:23.697] iteration 3540 : loss : 0.180782, loss_ce: 0.029903
[03:29:27.802] iteration 3550 : loss : 0.161714, loss_ce: 0.023016
[03:29:31.900] iteration 3560 : loss : 0.314476, loss_ce: 0.024029
[03:29:36.010] iteration 3570 : loss : 0.322871, loss_ce: 0.015002
[03:29:40.111] iteration 3580 : loss : 0.279325, loss_ce: 0.020124
[03:29:44.217] iteration 3590 : loss : 0.264016, loss_ce: 0.023044
[03:29:48.315] iteration 3600 : loss : 0.250462, loss_ce: 0.024834
[03:29:52.425] iteration 3610 : loss : 0.297516, loss_ce: 0.018588
[03:29:56.526] iteration 3620 : loss : 0.372348, loss_ce: 0.034233
[03:30:00.634] iteration 3630 : loss : 0.279871, loss_ce: 0.026286
[03:30:04.735] iteration 3640 : loss : 0.099087, loss_ce: 0.024068
[03:30:08.843] iteration 3650 : loss : 0.312179, loss_ce: 0.020207
[03:30:12.941] iteration 3660 : loss : 0.298885, loss_ce: 0.025489
[03:30:17.052] iteration 3670 : loss : 0.287305, loss_ce: 0.025104
[03:30:21.150] iteration 3680 : loss : 0.331062, loss_ce: 0.009737
[03:30:25.263] iteration 3690 : loss : 0.293676, loss_ce: 0.013639
[03:30:29.366] iteration 3700 : loss : 0.316866, loss_ce: 0.006266
[03:30:33.477] iteration 3710 : loss : 0.314910, loss_ce: 0.019296
[03:30:37.576] iteration 3720 : loss : 0.252609, loss_ce: 0.026448
[03:30:41.687] iteration 3730 : loss : 0.251521, loss_ce: 0.008859
[03:30:45.788] iteration 3740 : loss : 0.316479, loss_ce: 0.022712
[03:30:49.896] iteration 3750 : loss : 0.152960, loss_ce: 0.007953
[03:32:28.176] iteration 3760 : loss : 0.230420, loss_ce: 0.039627
[03:32:32.276] iteration 3770 : loss : 0.327428, loss_ce: 0.010923
[03:32:36.365] iteration 3780 : loss : 0.411889, loss_ce: 0.034061
[03:32:40.461] iteration 3790 : loss : 0.355704, loss_ce: 0.055385
[03:32:44.552] iteration 3800 : loss : 0.357286, loss_ce: 0.070134
[03:32:48.660] iteration 3810 : loss : 0.155040, loss_ce: 0.007119
[03:32:52.757] iteration 3820 : loss : 0.267773, loss_ce: 0.013040
[03:32:56.865] iteration 3830 : loss : 0.144698, loss_ce: 0.017211
[03:33:00.967] iteration 3840 : loss : 0.139471, loss_ce: 0.008903
[03:33:05.076] iteration 3850 : loss : 0.247706, loss_ce: 0.018866
[03:33:09.176] iteration 3860 : loss : 0.149153, loss_ce: 0.010562
[03:33:13.288] iteration 3870 : loss : 0.325656, loss_ce: 0.003023
[03:33:17.388] iteration 3880 : loss : 0.319984, loss_ce: 0.021052
[03:33:21.498] iteration 3890 : loss : 0.317719, loss_ce: 0.003561
[03:33:25.602] iteration 3900 : loss : 0.252665, loss_ce: 0.028651
[03:33:29.714] iteration 3910 : loss : 0.342196, loss_ce: 0.029277
[03:33:33.820] iteration 3920 : loss : 0.246644, loss_ce: 0.014287
[03:33:37.929] iteration 3930 : loss : 0.250081, loss_ce: 0.014249
[03:33:42.029] iteration 3940 : loss : 0.331683, loss_ce: 0.016296
[03:33:46.141] iteration 3950 : loss : 0.243672, loss_ce: 0.033656
[03:33:50.245] iteration 3960 : loss : 0.230033, loss_ce: 0.015650
[03:33:54.359] iteration 3970 : loss : 0.207434, loss_ce: 0.017322
[03:33:58.459] iteration 3980 : loss : 0.321221, loss_ce: 0.014080
[03:34:02.577] iteration 3990 : loss : 0.248315, loss_ce: 0.012628
[03:34:06.678] iteration 4000 : loss : 0.298943, loss_ce: 0.019124
[03:34:10.792] iteration 4010 : loss : 0.329182, loss_ce: 0.015975
[03:34:14.793] iteration 4020 : loss : 0.328329, loss_ce: 0.006487
[03:35:53.140] iteration 4030 : loss : 0.335620, loss_ce: 0.027044
[03:35:57.229] iteration 4040 : loss : 0.444690, loss_ce: 0.116946
[03:36:01.326] iteration 4050 : loss : 0.178657, loss_ce: 0.010428
[03:36:05.421] iteration 4060 : loss : 0.354846, loss_ce: 0.034359
[03:36:09.528] iteration 4070 : loss : 0.335909, loss_ce: 0.010001
[03:36:13.623] iteration 4080 : loss : 0.331528, loss_ce: 0.036570
[03:36:17.730] iteration 4090 : loss : 0.075914, loss_ce: 0.008670
[03:36:21.829] iteration 4100 : loss : 0.330266, loss_ce: 0.016590
[03:36:25.938] iteration 4110 : loss : 0.347021, loss_ce: 0.038949
[03:36:30.036] iteration 4120 : loss : 0.269340, loss_ce: 0.029472
[03:36:34.147] iteration 4130 : loss : 0.292498, loss_ce: 0.017055
[03:36:38.247] iteration 4140 : loss : 0.299194, loss_ce: 0.039909
[03:36:42.352] iteration 4150 : loss : 0.325231, loss_ce: 0.019348
[03:36:46.450] iteration 4160 : loss : 0.149598, loss_ce: 0.012190
[03:36:50.560] iteration 4170 : loss : 0.246397, loss_ce: 0.011471
[03:36:54.658] iteration 4180 : loss : 0.291479, loss_ce: 0.015205
[03:36:58.770] iteration 4190 : loss : 0.334740, loss_ce: 0.057767
[03:37:02.869] iteration 4200 : loss : 0.216235, loss_ce: 0.016461
[03:37:06.977] iteration 4210 : loss : 0.236285, loss_ce: 0.021706
[03:37:11.075] iteration 4220 : loss : 0.122959, loss_ce: 0.012903
[03:37:15.184] iteration 4230 : loss : 0.281711, loss_ce: 0.048316
[03:37:19.283] iteration 4240 : loss : 0.267754, loss_ce: 0.016399
[03:37:23.393] iteration 4250 : loss : 0.251378, loss_ce: 0.019197
[03:37:27.491] iteration 4260 : loss : 0.237977, loss_ce: 0.007242
[03:37:31.603] iteration 4270 : loss : 0.244971, loss_ce: 0.016046
[03:37:35.701] iteration 4280 : loss : 0.296466, loss_ce: 0.024389
[03:39:24.385] iteration 4290 : loss : 0.391200, loss_ce: 0.048961
[03:39:28.470] iteration 4300 : loss : 0.305001, loss_ce: 0.028161
[03:39:32.570] iteration 4310 : loss : 0.204124, loss_ce: 0.033577
[03:39:36.658] iteration 4320 : loss : 0.362651, loss_ce: 0.020248
[03:39:40.756] iteration 4330 : loss : 0.365663, loss_ce: 0.035890
[03:39:44.850] iteration 4340 : loss : 0.294247, loss_ce: 0.060279
[03:39:48.952] iteration 4350 : loss : 0.288379, loss_ce: 0.026562
[03:39:53.048] iteration 4360 : loss : 0.316856, loss_ce: 0.016171
[03:39:57.155] iteration 4370 : loss : 0.183522, loss_ce: 0.022136
[03:40:01.254] iteration 4380 : loss : 0.256992, loss_ce: 0.018890
[03:40:05.366] iteration 4390 : loss : 0.331051, loss_ce: 0.009385
[03:40:09.465] iteration 4400 : loss : 0.334442, loss_ce: 0.018802
[03:40:13.575] iteration 4410 : loss : 0.301089, loss_ce: 0.022621
[03:40:17.673] iteration 4420 : loss : 0.247487, loss_ce: 0.016611
[03:40:21.787] iteration 4430 : loss : 0.229322, loss_ce: 0.009769
[03:40:25.890] iteration 4440 : loss : 0.308984, loss_ce: 0.029914
[03:40:30.000] iteration 4450 : loss : 0.263656, loss_ce: 0.023040
[03:40:34.104] iteration 4460 : loss : 0.151291, loss_ce: 0.023207
[03:40:38.214] iteration 4470 : loss : 0.245561, loss_ce: 0.008877
[03:40:42.314] iteration 4480 : loss : 0.274597, loss_ce: 0.005587
[03:40:46.428] iteration 4490 : loss : 0.235578, loss_ce: 0.031482
[03:40:50.529] iteration 4500 : loss : 0.076670, loss_ce: 0.010209
[03:40:54.641] iteration 4510 : loss : 0.324440, loss_ce: 0.018149
[03:40:58.745] iteration 4520 : loss : 0.166699, loss_ce: 0.011670
[03:41:02.859] iteration 4530 : loss : 0.166486, loss_ce: 0.009361
[03:41:06.960] iteration 4540 : loss : 0.246974, loss_ce: 0.010092
[03:41:11.071] iteration 4550 : loss : 0.355045, loss_ce: 0.007546
[03:42:49.359] iteration 4560 : loss : 0.349575, loss_ce: 0.023402
[03:42:53.459] iteration 4570 : loss : 0.388942, loss_ce: 0.049504
[03:42:57.552] iteration 4580 : loss : 0.345276, loss_ce: 0.026160
[03:43:01.656] iteration 4590 : loss : 0.172226, loss_ce: 0.031490
[03:43:05.752] iteration 4600 : loss : 0.319920, loss_ce: 0.017922
[03:43:09.863] iteration 4610 : loss : 0.195071, loss_ce: 0.042553
[03:43:13.963] iteration 4620 : loss : 0.289825, loss_ce: 0.023318
[03:43:18.070] iteration 4630 : loss : 0.272802, loss_ce: 0.010804
[03:43:22.174] iteration 4640 : loss : 0.366246, loss_ce: 0.041286
[03:43:26.287] iteration 4650 : loss : 0.280324, loss_ce: 0.015542
[03:43:30.393] iteration 4660 : loss : 0.263652, loss_ce: 0.032281
[03:43:34.505] iteration 4670 : loss : 0.238417, loss_ce: 0.018742
[03:43:38.606] iteration 4680 : loss : 0.324076, loss_ce: 0.020795
[03:43:42.720] iteration 4690 : loss : 0.271288, loss_ce: 0.023788
[03:43:46.822] iteration 4700 : loss : 0.218772, loss_ce: 0.011264
[03:43:50.935] iteration 4710 : loss : 0.201310, loss_ce: 0.009223
[03:43:55.035] iteration 4720 : loss : 0.231419, loss_ce: 0.014609
[03:43:59.151] iteration 4730 : loss : 0.211832, loss_ce: 0.005814
[03:44:03.255] iteration 4740 : loss : 0.273334, loss_ce: 0.024391
[03:44:07.368] iteration 4750 : loss : 0.333789, loss_ce: 0.037221
[03:44:11.468] iteration 4760 : loss : 0.270530, loss_ce: 0.035368
[03:44:15.584] iteration 4770 : loss : 0.068639, loss_ce: 0.008525
[03:44:19.688] iteration 4780 : loss : 0.321165, loss_ce: 0.009570
[03:44:23.804] iteration 4790 : loss : 0.206941, loss_ce: 0.013453
[03:44:27.906] iteration 4800 : loss : 0.204208, loss_ce: 0.005039
[03:44:32.024] iteration 4810 : loss : 0.315761, loss_ce: 0.005133
[03:44:36.129] iteration 4820 : loss : 0.266326, loss_ce: 0.005047
[03:46:14.702] iteration 4830 : loss : 0.329470, loss_ce: 0.008113
[03:46:18.785] iteration 4840 : loss : 0.349757, loss_ce: 0.026627
[03:46:22.882] iteration 4850 : loss : 0.317231, loss_ce: 0.007635
[03:46:26.978] iteration 4860 : loss : 0.327088, loss_ce: 0.026440
[03:46:31.083] iteration 4870 : loss : 0.218788, loss_ce: 0.022347
[03:46:35.179] iteration 4880 : loss : 0.105698, loss_ce: 0.012948
[03:46:39.285] iteration 4890 : loss : 0.122653, loss_ce: 0.016367
[03:46:43.382] iteration 4900 : loss : 0.336129, loss_ce: 0.056705
[03:46:47.489] iteration 4910 : loss : 0.235411, loss_ce: 0.016365
[03:46:51.582] iteration 4920 : loss : 0.248883, loss_ce: 0.016611
[03:46:55.688] iteration 4930 : loss : 0.251352, loss_ce: 0.023787
[03:46:59.787] iteration 4940 : loss : 0.330846, loss_ce: 0.030845
[03:47:03.896] iteration 4950 : loss : 0.345595, loss_ce: 0.023244
[03:47:07.992] iteration 4960 : loss : 0.292602, loss_ce: 0.003101
[03:47:12.102] iteration 4970 : loss : 0.304603, loss_ce: 0.009637
[03:47:16.202] iteration 4980 : loss : 0.280094, loss_ce: 0.039073
[03:47:20.309] iteration 4990 : loss : 0.331576, loss_ce: 0.013455
[03:47:24.409] iteration 5000 : loss : 0.242944, loss_ce: 0.009350
[03:47:28.522] iteration 5010 : loss : 0.219530, loss_ce: 0.010851
[03:47:32.622] iteration 5020 : loss : 0.331922, loss_ce: 0.026833
[03:47:36.728] iteration 5030 : loss : 0.232380, loss_ce: 0.007174
[03:47:40.827] iteration 5040 : loss : 0.123016, loss_ce: 0.015089
[03:47:44.938] iteration 5050 : loss : 0.250101, loss_ce: 0.021311
[03:47:49.038] iteration 5060 : loss : 0.208331, loss_ce: 0.008156
[03:47:53.146] iteration 5070 : loss : 0.266832, loss_ce: 0.009843
[03:47:57.246] iteration 5080 : loss : 0.277859, loss_ce: 0.016860
[03:48:01.356] iteration 5090 : loss : 0.270488, loss_ce: 0.019404
[03:49:50.462] iteration 5100 : loss : 0.344698, loss_ce: 0.040526
[03:49:54.559] iteration 5110 : loss : 0.355091, loss_ce: 0.029354
[03:49:58.649] iteration 5120 : loss : 0.328564, loss_ce: 0.023135
[03:50:02.751] iteration 5130 : loss : 0.332436, loss_ce: 0.012697
[03:50:06.843] iteration 5140 : loss : 0.333764, loss_ce: 0.011120
[03:50:10.950] iteration 5150 : loss : 0.333399, loss_ce: 0.028462
[03:50:15.048] iteration 5160 : loss : 0.329397, loss_ce: 0.007840
[03:50:19.158] iteration 5170 : loss : 0.294813, loss_ce: 0.019832
[03:50:23.257] iteration 5180 : loss : 0.300543, loss_ce: 0.033358
[03:50:27.368] iteration 5190 : loss : 0.286292, loss_ce: 0.016785
[03:50:31.465] iteration 5200 : loss : 0.258716, loss_ce: 0.017201
[03:50:35.576] iteration 5210 : loss : 0.168899, loss_ce: 0.013037
[03:50:39.675] iteration 5220 : loss : 0.293528, loss_ce: 0.013783
[03:50:43.785] iteration 5230 : loss : 0.207771, loss_ce: 0.010585
[03:50:47.887] iteration 5240 : loss : 0.292308, loss_ce: 0.023649
[03:50:51.998] iteration 5250 : loss : 0.327821, loss_ce: 0.015284
[03:50:56.098] iteration 5260 : loss : 0.241044, loss_ce: 0.019037
[03:51:00.210] iteration 5270 : loss : 0.224018, loss_ce: 0.020529
[03:51:04.314] iteration 5280 : loss : 0.182896, loss_ce: 0.012708
[03:51:08.425] iteration 5290 : loss : 0.202147, loss_ce: 0.013294
[03:51:12.526] iteration 5300 : loss : 0.240526, loss_ce: 0.028048
[03:51:16.638] iteration 5310 : loss : 0.298744, loss_ce: 0.005884
[03:51:20.738] iteration 5320 : loss : 0.152572, loss_ce: 0.012071
[03:51:24.850] iteration 5330 : loss : 0.356790, loss_ce: 0.011097
[03:51:28.955] iteration 5340 : loss : 0.183936, loss_ce: 0.026853
[03:51:33.070] iteration 5350 : loss : 0.311607, loss_ce: 0.009723
[03:51:37.072] iteration 5360 : loss : 0.122217, loss_ce: 0.013859
[03:53:01.842] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_19.pth
[03:53:15.812] iteration 5370 : loss : 0.339852, loss_ce: 0.024994
[03:53:19.903] iteration 5380 : loss : 0.230430, loss_ce: 0.034271
[03:53:23.997] iteration 5390 : loss : 0.242674, loss_ce: 0.016491
[03:53:28.090] iteration 5400 : loss : 0.333282, loss_ce: 0.032955
[03:53:32.191] iteration 5410 : loss : 0.171196, loss_ce: 0.005237
[03:53:36.287] iteration 5420 : loss : 0.313918, loss_ce: 0.029040
[03:53:40.396] iteration 5430 : loss : 0.331439, loss_ce: 0.017228
[03:53:44.490] iteration 5440 : loss : 0.139443, loss_ce: 0.008003
[03:53:48.595] iteration 5450 : loss : 0.271471, loss_ce: 0.014825
[03:53:52.694] iteration 5460 : loss : 0.341946, loss_ce: 0.014239
[03:53:56.801] iteration 5470 : loss : 0.254248, loss_ce: 0.019761
[03:54:00.904] iteration 5480 : loss : 0.234462, loss_ce: 0.015603
[03:54:05.013] iteration 5490 : loss : 0.247446, loss_ce: 0.019346
[03:54:09.109] iteration 5500 : loss : 0.156747, loss_ce: 0.007204
[03:54:13.220] iteration 5510 : loss : 0.344049, loss_ce: 0.033674
[03:54:17.320] iteration 5520 : loss : 0.102633, loss_ce: 0.009747
[03:54:21.427] iteration 5530 : loss : 0.329445, loss_ce: 0.014715
[03:54:25.526] iteration 5540 : loss : 0.169108, loss_ce: 0.050180
[03:54:29.636] iteration 5550 : loss : 0.294436, loss_ce: 0.037923
[03:54:33.734] iteration 5560 : loss : 0.333319, loss_ce: 0.019007
[03:54:37.843] iteration 5570 : loss : 0.294748, loss_ce: 0.028408
[03:54:41.941] iteration 5580 : loss : 0.241415, loss_ce: 0.019231
[03:54:46.050] iteration 5590 : loss : 0.169345, loss_ce: 0.005799
[03:54:50.151] iteration 5600 : loss : 0.305985, loss_ce: 0.013156
[03:54:54.258] iteration 5610 : loss : 0.304172, loss_ce: 0.016948
[03:54:58.355] iteration 5620 : loss : 0.206265, loss_ce: 0.007123
[03:56:36.686] iteration 5630 : loss : 0.364298, loss_ce: 0.038593
[03:56:40.773] iteration 5640 : loss : 0.340109, loss_ce: 0.027756
[03:56:44.870] iteration 5650 : loss : 0.327766, loss_ce: 0.013655
[03:56:48.961] iteration 5660 : loss : 0.310445, loss_ce: 0.030395
[03:56:53.063] iteration 5670 : loss : 0.346104, loss_ce: 0.034392
[03:56:57.154] iteration 5680 : loss : 0.315963, loss_ce: 0.015110
[03:57:01.257] iteration 5690 : loss : 0.161281, loss_ce: 0.006784
[03:57:05.356] iteration 5700 : loss : 0.346817, loss_ce: 0.023537
[03:57:09.462] iteration 5710 : loss : 0.264152, loss_ce: 0.023452
[03:57:13.559] iteration 5720 : loss : 0.195488, loss_ce: 0.019272
[03:57:17.662] iteration 5730 : loss : 0.359900, loss_ce: 0.046599
[03:57:21.759] iteration 5740 : loss : 0.356738, loss_ce: 0.020700
[03:57:25.870] iteration 5750 : loss : 0.289592, loss_ce: 0.019924
[03:57:29.969] iteration 5760 : loss : 0.154728, loss_ce: 0.012421
[03:57:34.078] iteration 5770 : loss : 0.204463, loss_ce: 0.013598
[03:57:38.178] iteration 5780 : loss : 0.316879, loss_ce: 0.014568
[03:57:42.288] iteration 5790 : loss : 0.282882, loss_ce: 0.015442
[03:57:46.386] iteration 5800 : loss : 0.213336, loss_ce: 0.016156
[03:57:50.495] iteration 5810 : loss : 0.246733, loss_ce: 0.019107
[03:57:54.595] iteration 5820 : loss : 0.304726, loss_ce: 0.033360
[03:57:58.706] iteration 5830 : loss : 0.181848, loss_ce: 0.016568
[03:58:02.810] iteration 5840 : loss : 0.288602, loss_ce: 0.018293
[03:58:06.920] iteration 5850 : loss : 0.319431, loss_ce: 0.022819
[03:58:11.019] iteration 5860 : loss : 0.316925, loss_ce: 0.021391
[03:58:15.129] iteration 5870 : loss : 0.258376, loss_ce: 0.009796
[03:58:19.228] iteration 5880 : loss : 0.335573, loss_ce: 0.036949
[03:58:23.336] iteration 5890 : loss : 0.310903, loss_ce: 0.008529
[04:00:12.030] iteration 5900 : loss : 0.362242, loss_ce: 0.069578
[04:00:16.126] iteration 5910 : loss : 0.226644, loss_ce: 0.068177
[04:00:20.214] iteration 5920 : loss : 0.328550, loss_ce: 0.008777
[04:00:24.310] iteration 5930 : loss : 0.324164, loss_ce: 0.013729
[04:00:28.406] iteration 5940 : loss : 0.165718, loss_ce: 0.008325
[04:00:32.508] iteration 5950 : loss : 0.347706, loss_ce: 0.028966
[04:00:36.608] iteration 5960 : loss : 0.335549, loss_ce: 0.012324
[04:00:40.717] iteration 5970 : loss : 0.329902, loss_ce: 0.005052
[04:00:44.814] iteration 5980 : loss : 0.228959, loss_ce: 0.017437
[04:00:48.925] iteration 5990 : loss : 0.364059, loss_ce: 0.015614
[04:00:53.025] iteration 6000 : loss : 0.200533, loss_ce: 0.010448
[04:00:57.130] iteration 6010 : loss : 0.324790, loss_ce: 0.010914
[04:01:01.232] iteration 6020 : loss : 0.279737, loss_ce: 0.021959
[04:01:05.342] iteration 6030 : loss : 0.224287, loss_ce: 0.010836
[04:01:09.443] iteration 6040 : loss : 0.258412, loss_ce: 0.019676
[04:01:13.550] iteration 6050 : loss : 0.236531, loss_ce: 0.010729
[04:01:17.652] iteration 6060 : loss : 0.094701, loss_ce: 0.014442
[04:01:21.762] iteration 6070 : loss : 0.252374, loss_ce: 0.012109
[04:01:25.866] iteration 6080 : loss : 0.281301, loss_ce: 0.014622
[04:01:29.978] iteration 6090 : loss : 0.130243, loss_ce: 0.019650
[04:01:34.081] iteration 6100 : loss : 0.257847, loss_ce: 0.026957
[04:01:38.194] iteration 6110 : loss : 0.049716, loss_ce: 0.009980
[04:01:42.296] iteration 6120 : loss : 0.234736, loss_ce: 0.010549
[04:01:46.407] iteration 6130 : loss : 0.269252, loss_ce: 0.029318
[04:01:50.508] iteration 6140 : loss : 0.212913, loss_ce: 0.015141
[04:01:54.621] iteration 6150 : loss : 0.264191, loss_ce: 0.023842
[04:01:58.721] iteration 6160 : loss : 0.093298, loss_ce: 0.005648
[04:03:37.193] iteration 6170 : loss : 0.436389, loss_ce: 0.108279
[04:03:41.279] iteration 6180 : loss : 0.350959, loss_ce: 0.030897
[04:03:45.379] iteration 6190 : loss : 0.330704, loss_ce: 0.013030
[04:03:49.470] iteration 6200 : loss : 0.332784, loss_ce: 0.013035
[04:03:53.570] iteration 6210 : loss : 0.343451, loss_ce: 0.043946
[04:03:57.665] iteration 6220 : loss : 0.404363, loss_ce: 0.111931
[04:04:01.775] iteration 6230 : loss : 0.358702, loss_ce: 0.070031
[04:04:05.876] iteration 6240 : loss : 0.364274, loss_ce: 0.055504
[04:04:09.983] iteration 6250 : loss : 0.175535, loss_ce: 0.013426
[04:04:14.085] iteration 6260 : loss : 0.331435, loss_ce: 0.031059
[04:04:18.197] iteration 6270 : loss : 0.339196, loss_ce: 0.040328
[04:04:22.302] iteration 6280 : loss : 0.328819, loss_ce: 0.022083
[04:04:26.408] iteration 6290 : loss : 0.337425, loss_ce: 0.036307
[04:04:30.506] iteration 6300 : loss : 0.319948, loss_ce: 0.007407
[04:04:34.624] iteration 6310 : loss : 0.333528, loss_ce: 0.030288
[04:04:38.730] iteration 6320 : loss : 0.153108, loss_ce: 0.015373
[04:04:42.841] iteration 6330 : loss : 0.325582, loss_ce: 0.011317
[04:04:46.941] iteration 6340 : loss : 0.082741, loss_ce: 0.006292
[04:04:51.059] iteration 6350 : loss : 0.419406, loss_ce: 0.049858
[04:04:55.165] iteration 6360 : loss : 0.192486, loss_ce: 0.024763
[04:04:59.278] iteration 6370 : loss : 0.355163, loss_ce: 0.019694
[04:05:03.382] iteration 6380 : loss : 0.299760, loss_ce: 0.024436
[04:05:07.496] iteration 6390 : loss : 0.278100, loss_ce: 0.013739
[04:05:11.602] iteration 6400 : loss : 0.127382, loss_ce: 0.025627
[04:05:15.714] iteration 6410 : loss : 0.273923, loss_ce: 0.019434
[04:05:19.817] iteration 6420 : loss : 0.316535, loss_ce: 0.007768
[04:05:23.930] iteration 6430 : loss : 0.255008, loss_ce: 0.022830
[04:07:02.360] iteration 6440 : loss : 0.341991, loss_ce: 0.040283
[04:07:06.459] iteration 6450 : loss : 0.316394, loss_ce: 0.012298
[04:07:10.549] iteration 6460 : loss : 0.185010, loss_ce: 0.035809
[04:07:14.653] iteration 6470 : loss : 0.172972, loss_ce: 0.019905
[04:07:18.748] iteration 6480 : loss : 0.354997, loss_ce: 0.035346
[04:07:22.854] iteration 6490 : loss : 0.176659, loss_ce: 0.025709
[04:07:26.954] iteration 6500 : loss : 0.320085, loss_ce: 0.027493
[04:07:31.061] iteration 6510 : loss : 0.343282, loss_ce: 0.026340
[04:07:35.164] iteration 6520 : loss : 0.287351, loss_ce: 0.020044
[04:07:39.273] iteration 6530 : loss : 0.337515, loss_ce: 0.021718
[04:07:43.372] iteration 6540 : loss : 0.248786, loss_ce: 0.026538
[04:07:47.481] iteration 6550 : loss : 0.089926, loss_ce: 0.013211
[04:07:51.582] iteration 6560 : loss : 0.365008, loss_ce: 0.011030
[04:07:55.695] iteration 6570 : loss : 0.106939, loss_ce: 0.015508
[04:07:59.798] iteration 6580 : loss : 0.317027, loss_ce: 0.015500
[04:08:03.910] iteration 6590 : loss : 0.247690, loss_ce: 0.014864
[04:08:08.013] iteration 6600 : loss : 0.164013, loss_ce: 0.006659
[04:08:12.125] iteration 6610 : loss : 0.203890, loss_ce: 0.010526
[04:08:16.228] iteration 6620 : loss : 0.220201, loss_ce: 0.022595
[04:08:20.343] iteration 6630 : loss : 0.266548, loss_ce: 0.012644
[04:08:24.446] iteration 6640 : loss : 0.232826, loss_ce: 0.014533
[04:08:28.560] iteration 6650 : loss : 0.246321, loss_ce: 0.020626
[04:08:32.662] iteration 6660 : loss : 0.314330, loss_ce: 0.002841
[04:08:36.772] iteration 6670 : loss : 0.323644, loss_ce: 0.014839
[04:08:40.872] iteration 6680 : loss : 0.218529, loss_ce: 0.016894
[04:08:44.983] iteration 6690 : loss : 0.322312, loss_ce: 0.006660
[04:08:48.985] iteration 6700 : loss : 0.126739, loss_ce: 0.015589
[04:10:37.915] iteration 6710 : loss : 0.276948, loss_ce: 0.031395
[04:10:42.004] iteration 6720 : loss : 0.333492, loss_ce: 0.046205
[04:10:46.102] iteration 6730 : loss : 0.280390, loss_ce: 0.020372
[04:10:50.193] iteration 6740 : loss : 0.311287, loss_ce: 0.021039
[04:10:54.296] iteration 6750 : loss : 0.334654, loss_ce: 0.019833
[04:10:58.391] iteration 6760 : loss : 0.181351, loss_ce: 0.011174
[04:11:02.496] iteration 6770 : loss : 0.335034, loss_ce: 0.021558
[04:11:06.592] iteration 6780 : loss : 0.119678, loss_ce: 0.012354
[04:11:10.698] iteration 6790 : loss : 0.171477, loss_ce: 0.010908
[04:11:14.793] iteration 6800 : loss : 0.055250, loss_ce: 0.004106
[04:11:18.903] iteration 6810 : loss : 0.096499, loss_ce: 0.007269
[04:11:23.002] iteration 6820 : loss : 0.335270, loss_ce: 0.033474
[04:11:27.113] iteration 6830 : loss : 0.350657, loss_ce: 0.054459
[04:11:31.210] iteration 6840 : loss : 0.247216, loss_ce: 0.015249
[04:11:35.319] iteration 6850 : loss : 0.242384, loss_ce: 0.012081
[04:11:39.417] iteration 6860 : loss : 0.151881, loss_ce: 0.014856
[04:11:43.527] iteration 6870 : loss : 0.217051, loss_ce: 0.025946
[04:11:47.627] iteration 6880 : loss : 0.263517, loss_ce: 0.023233
[04:11:51.734] iteration 6890 : loss : 0.290580, loss_ce: 0.024312
[04:11:55.832] iteration 6900 : loss : 0.217166, loss_ce: 0.006875
[04:11:59.938] iteration 6910 : loss : 0.129094, loss_ce: 0.010086
[04:12:04.036] iteration 6920 : loss : 0.287111, loss_ce: 0.010558
[04:12:08.145] iteration 6930 : loss : 0.170019, loss_ce: 0.004490
[04:12:12.241] iteration 6940 : loss : 0.315466, loss_ce: 0.022752
[04:12:16.348] iteration 6950 : loss : 0.310116, loss_ce: 0.005426
[04:12:20.446] iteration 6960 : loss : 0.120917, loss_ce: 0.007070
[04:13:58.984] iteration 6970 : loss : 0.217315, loss_ce: 0.035460
[04:14:03.071] iteration 6980 : loss : 0.307064, loss_ce: 0.014255
[04:14:07.168] iteration 6990 : loss : 0.384099, loss_ce: 0.065165
[04:14:11.256] iteration 7000 : loss : 0.349403, loss_ce: 0.036292
[04:14:15.357] iteration 7010 : loss : 0.351730, loss_ce: 0.047515
[04:14:19.450] iteration 7020 : loss : 0.330987, loss_ce: 0.022838
[04:14:23.557] iteration 7030 : loss : 0.333758, loss_ce: 0.028685
[04:14:27.656] iteration 7040 : loss : 0.339835, loss_ce: 0.019558
[04:14:31.765] iteration 7050 : loss : 0.325055, loss_ce: 0.019418
[04:14:35.867] iteration 7060 : loss : 0.328172, loss_ce: 0.059125
[04:14:39.976] iteration 7070 : loss : 0.262385, loss_ce: 0.017728
[04:14:44.076] iteration 7080 : loss : 0.288469, loss_ce: 0.033825
[04:14:48.186] iteration 7090 : loss : 0.215737, loss_ce: 0.011437
[04:14:52.283] iteration 7100 : loss : 0.333588, loss_ce: 0.019652
[04:14:56.395] iteration 7110 : loss : 0.345240, loss_ce: 0.060205
[04:15:00.497] iteration 7120 : loss : 0.230744, loss_ce: 0.019345
[04:15:04.607] iteration 7130 : loss : 0.316599, loss_ce: 0.006726
[04:15:08.708] iteration 7140 : loss : 0.303344, loss_ce: 0.007683
[04:15:12.820] iteration 7150 : loss : 0.227018, loss_ce: 0.010612
[04:15:16.920] iteration 7160 : loss : 0.182526, loss_ce: 0.007070
[04:15:21.033] iteration 7170 : loss : 0.347279, loss_ce: 0.032094
[04:15:25.133] iteration 7180 : loss : 0.062962, loss_ce: 0.006365
[04:15:29.250] iteration 7190 : loss : 0.241784, loss_ce: 0.017414
[04:15:33.351] iteration 7200 : loss : 0.292541, loss_ce: 0.016141
[04:15:37.460] iteration 7210 : loss : 0.307153, loss_ce: 0.020318
[04:15:41.561] iteration 7220 : loss : 0.314612, loss_ce: 0.008562
[04:15:45.671] iteration 7230 : loss : 0.204866, loss_ce: 0.011211
[04:17:24.130] iteration 7240 : loss : 0.345938, loss_ce: 0.016528
[04:17:28.230] iteration 7250 : loss : 0.320554, loss_ce: 0.014800
[04:17:32.325] iteration 7260 : loss : 0.386477, loss_ce: 0.121312
[04:17:36.429] iteration 7270 : loss : 0.298721, loss_ce: 0.023736
[04:17:40.523] iteration 7280 : loss : 0.309633, loss_ce: 0.022676
[04:17:44.628] iteration 7290 : loss : 0.289070, loss_ce: 0.011642
[04:17:48.725] iteration 7300 : loss : 0.329017, loss_ce: 0.026450
[04:17:52.831] iteration 7310 : loss : 0.070725, loss_ce: 0.009397
[04:17:56.927] iteration 7320 : loss : 0.317431, loss_ce: 0.010445
[04:18:01.033] iteration 7330 : loss : 0.233110, loss_ce: 0.017510
[04:18:05.131] iteration 7340 : loss : 0.099838, loss_ce: 0.009954
[04:18:09.240] iteration 7350 : loss : 0.097133, loss_ce: 0.008817
[04:18:13.342] iteration 7360 : loss : 0.220781, loss_ce: 0.015032
[04:18:17.452] iteration 7370 : loss : 0.172038, loss_ce: 0.013068
[04:18:21.554] iteration 7380 : loss : 0.182450, loss_ce: 0.008489
[04:18:25.669] iteration 7390 : loss : 0.174660, loss_ce: 0.003999
[04:18:29.773] iteration 7400 : loss : 0.193208, loss_ce: 0.027092
[04:18:33.886] iteration 7410 : loss : 0.324465, loss_ce: 0.032170
[04:18:37.991] iteration 7420 : loss : 0.210944, loss_ce: 0.007561
[04:18:42.103] iteration 7430 : loss : 0.240385, loss_ce: 0.015791
[04:18:46.205] iteration 7440 : loss : 0.231591, loss_ce: 0.010720
[04:18:50.315] iteration 7450 : loss : 0.258197, loss_ce: 0.018406
[04:18:54.416] iteration 7460 : loss : 0.239309, loss_ce: 0.015838
[04:18:58.527] iteration 7470 : loss : 0.172238, loss_ce: 0.008197
[04:19:02.627] iteration 7480 : loss : 0.259719, loss_ce: 0.033169
[04:19:06.734] iteration 7490 : loss : 0.175517, loss_ce: 0.002709
[04:19:10.837] iteration 7500 : loss : 0.333059, loss_ce: 0.039578
[04:20:59.684] iteration 7510 : loss : 0.333030, loss_ce: 0.016204
[04:21:03.773] iteration 7520 : loss : 0.356966, loss_ce: 0.087217
[04:21:07.874] iteration 7530 : loss : 0.341143, loss_ce: 0.012278
[04:21:11.965] iteration 7540 : loss : 0.330510, loss_ce: 0.025460
[04:21:16.067] iteration 7550 : loss : 0.326953, loss_ce: 0.016124
[04:21:20.165] iteration 7560 : loss : 0.240471, loss_ce: 0.020127
[04:21:24.274] iteration 7570 : loss : 0.268738, loss_ce: 0.019203
[04:21:28.376] iteration 7580 : loss : 0.316113, loss_ce: 0.019343
[04:21:32.487] iteration 7590 : loss : 0.209250, loss_ce: 0.063713
[04:21:36.588] iteration 7600 : loss : 0.313649, loss_ce: 0.018587
[04:21:40.697] iteration 7610 : loss : 0.324526, loss_ce: 0.009836
[04:21:44.799] iteration 7620 : loss : 0.308081, loss_ce: 0.005898
[04:21:48.910] iteration 7630 : loss : 0.235752, loss_ce: 0.016115
[04:21:53.011] iteration 7640 : loss : 0.255296, loss_ce: 0.022253
[04:21:57.124] iteration 7650 : loss : 0.294281, loss_ce: 0.033816
[04:22:01.228] iteration 7660 : loss : 0.318774, loss_ce: 0.008352
[04:22:05.340] iteration 7670 : loss : 0.211871, loss_ce: 0.012233
[04:22:09.443] iteration 7680 : loss : 0.047818, loss_ce: 0.010120
[04:22:13.554] iteration 7690 : loss : 0.227566, loss_ce: 0.016449
[04:22:17.656] iteration 7700 : loss : 0.226015, loss_ce: 0.024040
[04:22:21.768] iteration 7710 : loss : 0.240818, loss_ce: 0.009227
[04:22:25.871] iteration 7720 : loss : 0.238707, loss_ce: 0.019181
[04:22:29.985] iteration 7730 : loss : 0.292553, loss_ce: 0.008380
[04:22:34.088] iteration 7740 : loss : 0.274798, loss_ce: 0.022393
[04:22:38.199] iteration 7750 : loss : 0.326736, loss_ce: 0.014685
[04:22:42.301] iteration 7760 : loss : 0.262822, loss_ce: 0.008857
[04:22:46.413] iteration 7770 : loss : 0.076727, loss_ce: 0.011997
[04:24:25.043] iteration 7780 : loss : 0.328266, loss_ce: 0.024142
[04:24:29.141] iteration 7790 : loss : 0.250438, loss_ce: 0.016021
[04:24:33.232] iteration 7800 : loss : 0.376089, loss_ce: 0.055856
[04:24:37.333] iteration 7810 : loss : 0.182250, loss_ce: 0.041724
[04:24:41.428] iteration 7820 : loss : 0.281924, loss_ce: 0.019330
[04:24:45.536] iteration 7830 : loss : 0.289968, loss_ce: 0.009822
[04:24:49.632] iteration 7840 : loss : 0.295074, loss_ce: 0.013823
[04:24:53.742] iteration 7850 : loss : 0.233497, loss_ce: 0.018587
[04:24:57.839] iteration 7860 : loss : 0.319938, loss_ce: 0.031876
[04:25:01.952] iteration 7870 : loss : 0.315042, loss_ce: 0.038787
[04:25:06.051] iteration 7880 : loss : 0.253709, loss_ce: 0.025964
[04:25:10.161] iteration 7890 : loss : 0.079757, loss_ce: 0.012171
[04:25:14.262] iteration 7900 : loss : 0.198774, loss_ce: 0.031134
[04:25:18.371] iteration 7910 : loss : 0.324044, loss_ce: 0.018318
[04:25:22.472] iteration 7920 : loss : 0.067516, loss_ce: 0.010466
[04:25:26.585] iteration 7930 : loss : 0.224088, loss_ce: 0.007140
[04:25:30.690] iteration 7940 : loss : 0.247477, loss_ce: 0.015256
[04:25:34.803] iteration 7950 : loss : 0.225872, loss_ce: 0.011277
[04:25:38.906] iteration 7960 : loss : 0.285641, loss_ce: 0.029428
[04:25:43.017] iteration 7970 : loss : 0.230522, loss_ce: 0.008712
[04:25:47.119] iteration 7980 : loss : 0.237571, loss_ce: 0.016925
[04:25:51.234] iteration 7990 : loss : 0.315030, loss_ce: 0.008179
[04:25:55.335] iteration 8000 : loss : 0.331748, loss_ce: 0.014447
[04:25:59.445] iteration 8010 : loss : 0.224377, loss_ce: 0.025423
[04:26:03.546] iteration 8020 : loss : 0.291477, loss_ce: 0.010087
[04:26:07.658] iteration 8030 : loss : 0.064111, loss_ce: 0.012686
[04:26:11.660] iteration 8040 : loss : 0.301865, loss_ce: 0.024981
[04:27:36.364] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_29.pth
[04:27:50.196] iteration 8050 : loss : 0.321985, loss_ce: 0.010329
[04:27:54.288] iteration 8060 : loss : 0.292690, loss_ce: 0.021556
[04:27:58.387] iteration 8070 : loss : 0.303588, loss_ce: 0.014648
[04:28:02.478] iteration 8080 : loss : 0.206725, loss_ce: 0.013514
[04:28:06.581] iteration 8090 : loss : 0.297060, loss_ce: 0.016693
[04:28:10.677] iteration 8100 : loss : 0.323349, loss_ce: 0.003620
[04:28:14.780] iteration 8110 : loss : 0.220993, loss_ce: 0.020038
[04:28:18.877] iteration 8120 : loss : 0.320457, loss_ce: 0.015384
[04:28:22.981] iteration 8130 : loss : 0.288602, loss_ce: 0.016283
[04:28:27.081] iteration 8140 : loss : 0.347929, loss_ce: 0.015626
[04:28:31.190] iteration 8150 : loss : 0.165168, loss_ce: 0.020018
[04:28:35.290] iteration 8160 : loss : 0.248694, loss_ce: 0.013989
[04:28:39.398] iteration 8170 : loss : 0.294456, loss_ce: 0.007470
[04:28:43.495] iteration 8180 : loss : 0.247126, loss_ce: 0.024417
[04:28:47.603] iteration 8190 : loss : 0.275268, loss_ce: 0.010294
[04:28:51.702] iteration 8200 : loss : 0.240079, loss_ce: 0.011869
[04:28:55.812] iteration 8210 : loss : 0.221040, loss_ce: 0.015761
[04:28:59.914] iteration 8220 : loss : 0.280507, loss_ce: 0.020895
[04:29:04.030] iteration 8230 : loss : 0.206973, loss_ce: 0.013079
[04:29:08.129] iteration 8240 : loss : 0.203208, loss_ce: 0.002592
[04:29:12.238] iteration 8250 : loss : 0.277857, loss_ce: 0.037219
[04:29:16.338] iteration 8260 : loss : 0.083665, loss_ce: 0.018865
[04:29:20.446] iteration 8270 : loss : 0.266569, loss_ce: 0.016214
[04:29:24.548] iteration 8280 : loss : 0.213619, loss_ce: 0.015690
[04:29:28.656] iteration 8290 : loss : 0.215722, loss_ce: 0.008503
[04:29:32.758] iteration 8300 : loss : 0.318841, loss_ce: 0.014902
[04:31:21.661] iteration 8310 : loss : 0.333421, loss_ce: 0.040349
[04:31:25.749] iteration 8320 : loss : 0.474919, loss_ce: 0.066376
[04:31:29.843] iteration 8330 : loss : 0.481272, loss_ce: 0.090249
[04:31:33.936] iteration 8340 : loss : 0.281465, loss_ce: 0.041144
[04:31:38.037] iteration 8350 : loss : 0.253456, loss_ce: 0.038627
[04:31:42.134] iteration 8360 : loss : 0.383354, loss_ce: 0.024343
[04:31:46.239] iteration 8370 : loss : 0.358029, loss_ce: 0.028566
[04:31:50.333] iteration 8380 : loss : 0.334964, loss_ce: 0.049173
[04:31:54.442] iteration 8390 : loss : 0.236464, loss_ce: 0.026157
[04:31:58.539] iteration 8400 : loss : 0.233441, loss_ce: 0.022708
[04:32:02.648] iteration 8410 : loss : 0.160937, loss_ce: 0.008255
[04:32:06.745] iteration 8420 : loss : 0.270809, loss_ce: 0.021779
[04:32:10.856] iteration 8430 : loss : 0.216302, loss_ce: 0.007889
[04:32:14.955] iteration 8440 : loss : 0.077783, loss_ce: 0.009188
[04:32:19.068] iteration 8450 : loss : 0.288866, loss_ce: 0.011774
[04:32:23.167] iteration 8460 : loss : 0.155500, loss_ce: 0.008601
[04:32:27.281] iteration 8470 : loss : 0.287550, loss_ce: 0.020326
[04:32:31.381] iteration 8480 : loss : 0.301572, loss_ce: 0.009344
[04:32:35.495] iteration 8490 : loss : 0.180051, loss_ce: 0.009897
[04:32:39.595] iteration 8500 : loss : 0.274258, loss_ce: 0.018147
[04:32:43.707] iteration 8510 : loss : 0.039904, loss_ce: 0.006864
[04:32:47.808] iteration 8520 : loss : 0.222355, loss_ce: 0.008481
[04:32:51.920] iteration 8530 : loss : 0.125075, loss_ce: 0.011708
[04:32:56.020] iteration 8540 : loss : 0.283069, loss_ce: 0.038354
[04:33:00.133] iteration 8550 : loss : 0.294662, loss_ce: 0.012432
[04:33:04.236] iteration 8560 : loss : 0.226294, loss_ce: 0.014323
[04:33:08.345] iteration 8570 : loss : 0.263502, loss_ce: 0.022833
[04:34:46.822] iteration 8580 : loss : 0.395216, loss_ce: 0.030138
[04:34:50.920] iteration 8590 : loss : 0.335162, loss_ce: 0.024878
[04:34:55.009] iteration 8600 : loss : 0.079568, loss_ce: 0.014732
[04:34:59.113] iteration 8610 : loss : 0.238707, loss_ce: 0.016808
[04:35:03.207] iteration 8620 : loss : 0.245744, loss_ce: 0.021642
[04:35:07.314] iteration 8630 : loss : 0.347047, loss_ce: 0.023184
[04:35:11.412] iteration 8640 : loss : 0.330548, loss_ce: 0.015802
[04:35:15.521] iteration 8650 : loss : 0.100255, loss_ce: 0.011926
[04:35:19.623] iteration 8660 : loss : 0.263772, loss_ce: 0.015520
[04:35:23.735] iteration 8670 : loss : 0.205065, loss_ce: 0.005373
[04:35:27.840] iteration 8680 : loss : 0.120276, loss_ce: 0.013537
[04:35:31.950] iteration 8690 : loss : 0.322238, loss_ce: 0.005964
[04:35:36.055] iteration 8700 : loss : 0.251406, loss_ce: 0.014558
[04:35:40.164] iteration 8710 : loss : 0.209625, loss_ce: 0.008850
[04:35:44.264] iteration 8720 : loss : 0.262086, loss_ce: 0.015955
[04:35:48.378] iteration 8730 : loss : 0.245539, loss_ce: 0.017006
[04:35:52.479] iteration 8740 : loss : 0.246392, loss_ce: 0.013946
[04:35:56.590] iteration 8750 : loss : 0.207619, loss_ce: 0.014683
[04:36:00.697] iteration 8760 : loss : 0.102162, loss_ce: 0.008146
[04:36:04.810] iteration 8770 : loss : 0.249118, loss_ce: 0.014530
[04:36:08.914] iteration 8780 : loss : 0.304099, loss_ce: 0.012390
[04:36:13.029] iteration 8790 : loss : 0.331049, loss_ce: 0.013144
[04:36:17.131] iteration 8800 : loss : 0.303615, loss_ce: 0.023480
[04:36:21.241] iteration 8810 : loss : 0.156190, loss_ce: 0.005607
[04:36:25.343] iteration 8820 : loss : 0.241200, loss_ce: 0.023065
[04:36:29.462] iteration 8830 : loss : 0.230611, loss_ce: 0.011991
[04:36:33.569] iteration 8840 : loss : 0.191406, loss_ce: 0.010132
[04:38:12.571] iteration 8850 : loss : 0.316759, loss_ce: 0.028234
[04:38:16.662] iteration 8860 : loss : 0.286818, loss_ce: 0.038202
[04:38:20.770] iteration 8870 : loss : 0.296175, loss_ce: 0.011066
[04:38:24.869] iteration 8880 : loss : 0.338865, loss_ce: 0.029328
[04:38:28.978] iteration 8890 : loss : 0.324990, loss_ce: 0.016070
[04:38:33.079] iteration 8900 : loss : 0.316514, loss_ce: 0.016225
[04:38:37.191] iteration 8910 : loss : 0.308216, loss_ce: 0.017383
[04:38:41.285] iteration 8920 : loss : 0.125006, loss_ce: 0.032835
[04:38:45.392] iteration 8930 : loss : 0.214123, loss_ce: 0.012928
[04:38:49.496] iteration 8940 : loss : 0.303225, loss_ce: 0.038837
[04:38:53.608] iteration 8950 : loss : 0.303454, loss_ce: 0.010148
[04:38:57.710] iteration 8960 : loss : 0.190777, loss_ce: 0.005908
[04:39:01.827] iteration 8970 : loss : 0.318780, loss_ce: 0.006910
[04:39:05.930] iteration 8980 : loss : 0.218932, loss_ce: 0.012842
[04:39:10.046] iteration 8990 : loss : 0.170521, loss_ce: 0.009196
[04:39:14.150] iteration 9000 : loss : 0.248199, loss_ce: 0.007227
[04:39:18.266] iteration 9010 : loss : 0.305903, loss_ce: 0.011743
[04:39:22.371] iteration 9020 : loss : 0.059752, loss_ce: 0.010438
[04:39:26.483] iteration 9030 : loss : 0.210378, loss_ce: 0.024192
[04:39:30.590] iteration 9040 : loss : 0.194689, loss_ce: 0.018932
[04:39:34.709] iteration 9050 : loss : 0.288369, loss_ce: 0.006073
[04:39:38.815] iteration 9060 : loss : 0.197886, loss_ce: 0.007578
[04:39:42.932] iteration 9070 : loss : 0.206185, loss_ce: 0.004923
[04:39:47.036] iteration 9080 : loss : 0.188605, loss_ce: 0.006696
[04:39:51.150] iteration 9090 : loss : 0.090462, loss_ce: 0.014642
[04:39:55.258] iteration 9100 : loss : 0.247752, loss_ce: 0.009750
[04:39:59.375] iteration 9110 : loss : 0.276335, loss_ce: 0.007548
[04:41:50.553] iteration 9120 : loss : 0.331228, loss_ce: 0.014982
[04:41:54.660] iteration 9130 : loss : 0.466856, loss_ce: 0.052045
[04:41:58.753] iteration 9140 : loss : 0.458770, loss_ce: 0.100388
[04:42:02.856] iteration 9150 : loss : 0.476737, loss_ce: 0.120151
[04:42:06.946] iteration 9160 : loss : 0.268992, loss_ce: 0.049281
[04:42:11.053] iteration 9170 : loss : 0.372948, loss_ce: 0.040328
[04:42:15.147] iteration 9180 : loss : 0.318860, loss_ce: 0.018181
[04:42:19.259] iteration 9190 : loss : 0.243201, loss_ce: 0.018143
[04:42:23.360] iteration 9200 : loss : 0.257337, loss_ce: 0.024637
[04:42:27.473] iteration 9210 : loss : 0.086978, loss_ce: 0.016489
[04:42:31.576] iteration 9220 : loss : 0.216590, loss_ce: 0.021615
[04:42:35.692] iteration 9230 : loss : 0.246482, loss_ce: 0.017647
[04:42:39.794] iteration 9240 : loss : 0.280720, loss_ce: 0.022525
[04:42:43.907] iteration 9250 : loss : 0.323653, loss_ce: 0.002877
[04:42:48.009] iteration 9260 : loss : 0.317136, loss_ce: 0.016688
[04:42:52.121] iteration 9270 : loss : 0.185828, loss_ce: 0.004425
[04:42:56.227] iteration 9280 : loss : 0.329380, loss_ce: 0.014068
[04:43:00.340] iteration 9290 : loss : 0.233394, loss_ce: 0.027752
[04:43:04.446] iteration 9300 : loss : 0.244967, loss_ce: 0.016376
[04:43:08.560] iteration 9310 : loss : 0.125692, loss_ce: 0.033305
[04:43:12.664] iteration 9320 : loss : 0.337973, loss_ce: 0.011987
[04:43:16.774] iteration 9330 : loss : 0.239018, loss_ce: 0.016892
[04:43:20.876] iteration 9340 : loss : 0.129328, loss_ce: 0.009340
[04:43:24.993] iteration 9350 : loss : 0.315108, loss_ce: 0.005286
[04:43:29.098] iteration 9360 : loss : 0.054571, loss_ce: 0.007934
[04:43:33.213] iteration 9370 : loss : 0.259055, loss_ce: 0.015554
[04:43:37.217] iteration 9380 : loss : 0.317309, loss_ce: 0.010927
[04:45:15.898] iteration 9390 : loss : 0.341189, loss_ce: 0.012363
[04:45:19.986] iteration 9400 : loss : 0.319378, loss_ce: 0.014065
[04:45:24.086] iteration 9410 : loss : 0.176124, loss_ce: 0.010004
[04:45:28.179] iteration 9420 : loss : 0.240837, loss_ce: 0.020085
[04:45:32.280] iteration 9430 : loss : 0.196000, loss_ce: 0.029855
[04:45:36.378] iteration 9440 : loss : 0.178829, loss_ce: 0.025593
[04:45:40.486] iteration 9450 : loss : 0.339352, loss_ce: 0.025874
[04:45:44.583] iteration 9460 : loss : 0.170671, loss_ce: 0.029461
[04:45:48.692] iteration 9470 : loss : 0.268037, loss_ce: 0.017946
[04:45:52.793] iteration 9480 : loss : 0.260981, loss_ce: 0.017140
[04:45:56.904] iteration 9490 : loss : 0.238055, loss_ce: 0.011560
[04:46:01.007] iteration 9500 : loss : 0.244599, loss_ce: 0.012965
[04:46:05.122] iteration 9510 : loss : 0.215110, loss_ce: 0.016263
[04:46:09.224] iteration 9520 : loss : 0.164854, loss_ce: 0.026762
[04:46:13.336] iteration 9530 : loss : 0.226081, loss_ce: 0.013185
[04:46:17.439] iteration 9540 : loss : 0.279559, loss_ce: 0.011940
[04:46:21.552] iteration 9550 : loss : 0.099541, loss_ce: 0.008884
[04:46:25.656] iteration 9560 : loss : 0.405269, loss_ce: 0.046814
[04:46:29.768] iteration 9570 : loss : 0.320413, loss_ce: 0.010641
[04:46:33.871] iteration 9580 : loss : 0.143709, loss_ce: 0.016121
[04:46:37.989] iteration 9590 : loss : 0.280353, loss_ce: 0.019709
[04:46:42.091] iteration 9600 : loss : 0.335125, loss_ce: 0.042071
[04:46:46.202] iteration 9610 : loss : 0.073180, loss_ce: 0.009203
[04:46:50.304] iteration 9620 : loss : 0.233479, loss_ce: 0.014738
[04:46:54.417] iteration 9630 : loss : 0.142819, loss_ce: 0.011082
[04:46:58.522] iteration 9640 : loss : 0.210425, loss_ce: 0.008685
[04:48:36.772] iteration 9650 : loss : 0.328990, loss_ce: 0.059433
[04:48:40.858] iteration 9660 : loss : 0.229564, loss_ce: 0.024757
[04:48:44.955] iteration 9670 : loss : 0.326887, loss_ce: 0.018837
[04:48:49.048] iteration 9680 : loss : 0.236663, loss_ce: 0.020159
[04:48:53.151] iteration 9690 : loss : 0.227049, loss_ce: 0.019456
[04:48:57.247] iteration 9700 : loss : 0.238865, loss_ce: 0.026321
[04:49:01.354] iteration 9710 : loss : 0.269630, loss_ce: 0.015958
[04:49:05.453] iteration 9720 : loss : 0.292957, loss_ce: 0.017347
[04:49:09.565] iteration 9730 : loss : 0.325243, loss_ce: 0.006546
[04:49:13.663] iteration 9740 : loss : 0.265809, loss_ce: 0.025790
[04:49:17.774] iteration 9750 : loss : 0.159336, loss_ce: 0.015387
[04:49:21.875] iteration 9760 : loss : 0.240071, loss_ce: 0.012842
[04:49:25.988] iteration 9770 : loss : 0.253106, loss_ce: 0.016674
[04:49:30.089] iteration 9780 : loss : 0.276851, loss_ce: 0.030020
[04:49:34.202] iteration 9790 : loss : 0.244887, loss_ce: 0.031833
[04:49:38.302] iteration 9800 : loss : 0.103724, loss_ce: 0.011674
[04:49:42.412] iteration 9810 : loss : 0.274052, loss_ce: 0.004751
[04:49:46.515] iteration 9820 : loss : 0.220305, loss_ce: 0.006984
[04:49:50.626] iteration 9830 : loss : 0.243263, loss_ce: 0.025049
[04:49:54.727] iteration 9840 : loss : 0.227523, loss_ce: 0.015975
[04:49:58.838] iteration 9850 : loss : 0.324615, loss_ce: 0.008397
[04:50:02.940] iteration 9860 : loss : 0.206154, loss_ce: 0.013687
[04:50:07.051] iteration 9870 : loss : 0.326193, loss_ce: 0.011135
[04:50:11.155] iteration 9880 : loss : 0.190117, loss_ce: 0.008770
[04:50:15.269] iteration 9890 : loss : 0.232470, loss_ce: 0.022282
[04:50:19.371] iteration 9900 : loss : 0.306238, loss_ce: 0.015565
[04:50:23.481] iteration 9910 : loss : 0.306348, loss_ce: 0.006156
[04:52:12.301] iteration 9920 : loss : 0.232133, loss_ce: 0.010469
[04:52:16.399] iteration 9930 : loss : 0.198329, loss_ce: 0.018466
[04:52:20.487] iteration 9940 : loss : 0.223451, loss_ce: 0.072433
[04:52:24.592] iteration 9950 : loss : 0.181350, loss_ce: 0.015298
[04:52:28.690] iteration 9960 : loss : 0.167497, loss_ce: 0.005347
[04:52:32.800] iteration 9970 : loss : 0.092149, loss_ce: 0.020712
[04:52:36.895] iteration 9980 : loss : 0.276101, loss_ce: 0.005162
[04:52:41.002] iteration 9990 : loss : 0.275456, loss_ce: 0.017311
[04:52:45.097] iteration 10000 : loss : 0.331518, loss_ce: 0.010490
[04:52:49.203] iteration 10010 : loss : 0.138668, loss_ce: 0.016395
[04:52:53.302] iteration 10020 : loss : 0.198551, loss_ce: 0.010903
[04:52:57.411] iteration 10030 : loss : 0.216381, loss_ce: 0.008743
[04:53:01.513] iteration 10040 : loss : 0.258677, loss_ce: 0.012436
[04:53:05.620] iteration 10050 : loss : 0.220451, loss_ce: 0.007602
[04:53:09.718] iteration 10060 : loss : 0.314651, loss_ce: 0.011846
[04:53:13.828] iteration 10070 : loss : 0.243034, loss_ce: 0.018867
[04:53:17.929] iteration 10080 : loss : 0.198482, loss_ce: 0.014107
[04:53:22.042] iteration 10090 : loss : 0.030111, loss_ce: 0.005733
[04:53:26.143] iteration 10100 : loss : 0.261959, loss_ce: 0.019893
[04:53:30.257] iteration 10110 : loss : 0.248129, loss_ce: 0.009592
[04:53:34.360] iteration 10120 : loss : 0.314065, loss_ce: 0.008142
[04:53:38.482] iteration 10130 : loss : 0.047884, loss_ce: 0.007215
[04:53:42.584] iteration 10140 : loss : 0.202043, loss_ce: 0.010191
[04:53:46.696] iteration 10150 : loss : 0.150077, loss_ce: 0.008504
[04:53:50.799] iteration 10160 : loss : 0.316373, loss_ce: 0.005347
[04:53:54.913] iteration 10170 : loss : 0.210627, loss_ce: 0.005915
[04:53:59.016] iteration 10180 : loss : 0.188769, loss_ce: 0.005479
[04:55:37.275] iteration 10190 : loss : 0.249031, loss_ce: 0.033602
[04:55:41.366] iteration 10200 : loss : 0.371851, loss_ce: 0.053088
[04:55:45.467] iteration 10210 : loss : 0.093542, loss_ce: 0.019879
[04:55:49.559] iteration 10220 : loss : 0.316300, loss_ce: 0.011669
[04:55:53.659] iteration 10230 : loss : 0.333123, loss_ce: 0.026823
[04:55:57.754] iteration 10240 : loss : 0.130316, loss_ce: 0.011183
[04:56:01.862] iteration 10250 : loss : 0.310752, loss_ce: 0.008208
[04:56:05.961] iteration 10260 : loss : 0.219445, loss_ce: 0.019836
[04:56:10.070] iteration 10270 : loss : 0.324753, loss_ce: 0.017207
[04:56:14.170] iteration 10280 : loss : 0.048717, loss_ce: 0.012208
[04:56:18.282] iteration 10290 : loss : 0.320199, loss_ce: 0.022391
[04:56:22.382] iteration 10300 : loss : 0.337794, loss_ce: 0.021850
[04:56:26.495] iteration 10310 : loss : 0.134396, loss_ce: 0.022908
[04:56:30.595] iteration 10320 : loss : 0.068602, loss_ce: 0.014917
[04:56:34.709] iteration 10330 : loss : 0.067765, loss_ce: 0.010215
[04:56:38.810] iteration 10340 : loss : 0.050104, loss_ce: 0.008246
[04:56:42.921] iteration 10350 : loss : 0.253895, loss_ce: 0.011511
[04:56:47.026] iteration 10360 : loss : 0.295786, loss_ce: 0.024402
[04:56:51.136] iteration 10370 : loss : 0.160591, loss_ce: 0.003273
[04:56:55.238] iteration 10380 : loss : 0.080087, loss_ce: 0.008609
[04:56:59.349] iteration 10390 : loss : 0.278115, loss_ce: 0.025813
[04:57:03.452] iteration 10400 : loss : 0.054178, loss_ce: 0.006307
[04:57:07.565] iteration 10410 : loss : 0.024880, loss_ce: 0.004663
[04:57:11.670] iteration 10420 : loss : 0.227415, loss_ce: 0.020283
[04:57:15.783] iteration 10430 : loss : 0.323507, loss_ce: 0.009597
[04:57:19.885] iteration 10440 : loss : 0.256590, loss_ce: 0.010447
[04:57:23.993] iteration 10450 : loss : 0.203669, loss_ce: 0.006727
[04:59:02.418] iteration 10460 : loss : 0.258538, loss_ce: 0.013149
[04:59:06.517] iteration 10470 : loss : 0.245210, loss_ce: 0.017894
[04:59:10.608] iteration 10480 : loss : 0.263796, loss_ce: 0.011365
[04:59:14.712] iteration 10490 : loss : 0.253968, loss_ce: 0.030124
[04:59:18.806] iteration 10500 : loss : 0.185163, loss_ce: 0.025243
[04:59:22.918] iteration 10510 : loss : 0.107821, loss_ce: 0.021080
[04:59:27.015] iteration 10520 : loss : 0.236598, loss_ce: 0.020578
[04:59:31.126] iteration 10530 : loss : 0.260595, loss_ce: 0.009199
[04:59:35.228] iteration 10540 : loss : 0.228364, loss_ce: 0.010840
[04:59:39.335] iteration 10550 : loss : 0.313725, loss_ce: 0.007975
[04:59:43.437] iteration 10560 : loss : 0.118697, loss_ce: 0.015433
[04:59:47.550] iteration 10570 : loss : 0.335222, loss_ce: 0.014525
[04:59:51.650] iteration 10580 : loss : 0.261970, loss_ce: 0.013325
[04:59:55.764] iteration 10590 : loss : 0.242585, loss_ce: 0.003884
[04:59:59.866] iteration 10600 : loss : 0.239664, loss_ce: 0.006183
[05:00:03.982] iteration 10610 : loss : 0.234022, loss_ce: 0.030314
[05:00:08.083] iteration 10620 : loss : 0.064612, loss_ce: 0.010240
[05:00:12.199] iteration 10630 : loss : 0.298796, loss_ce: 0.006204
[05:00:16.300] iteration 10640 : loss : 0.130602, loss_ce: 0.017039
[05:00:20.414] iteration 10650 : loss : 0.231250, loss_ce: 0.016173
[05:00:24.519] iteration 10660 : loss : 0.220274, loss_ce: 0.008833
[05:00:28.636] iteration 10670 : loss : 0.249797, loss_ce: 0.016214
[05:00:32.741] iteration 10680 : loss : 0.187591, loss_ce: 0.008152
[05:00:36.856] iteration 10690 : loss : 0.126492, loss_ce: 0.008095
[05:00:40.959] iteration 10700 : loss : 0.189749, loss_ce: 0.006908
[05:00:45.078] iteration 10710 : loss : 0.280389, loss_ce: 0.032964
[05:00:49.083] iteration 10720 : loss : 0.246796, loss_ce: 0.023349
[05:02:24.110] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_39.pth
[05:02:37.982] iteration 10730 : loss : 0.326378, loss_ce: 0.011354
[05:02:42.071] iteration 10740 : loss : 0.266033, loss_ce: 0.023501
[05:02:46.172] iteration 10750 : loss : 0.313206, loss_ce: 0.031752
[05:02:50.266] iteration 10760 : loss : 0.304133, loss_ce: 0.091511
[05:02:54.373] iteration 10770 : loss : 0.314980, loss_ce: 0.034814
[05:02:58.470] iteration 10780 : loss : 0.216777, loss_ce: 0.008267
[05:03:02.578] iteration 10790 : loss : 0.210712, loss_ce: 0.012642
[05:03:06.675] iteration 10800 : loss : 0.275170, loss_ce: 0.020089
[05:03:10.785] iteration 10810 : loss : 0.196839, loss_ce: 0.007412
[05:03:14.884] iteration 10820 : loss : 0.255417, loss_ce: 0.011441
[05:03:18.995] iteration 10830 : loss : 0.284718, loss_ce: 0.010823
[05:03:23.097] iteration 10840 : loss : 0.321372, loss_ce: 0.006647
[05:03:27.209] iteration 10850 : loss : 0.331073, loss_ce: 0.016645
[05:03:31.309] iteration 10860 : loss : 0.191686, loss_ce: 0.012250
[05:03:35.425] iteration 10870 : loss : 0.205493, loss_ce: 0.008821
[05:03:39.528] iteration 10880 : loss : 0.057414, loss_ce: 0.004490
[05:03:43.637] iteration 10890 : loss : 0.222680, loss_ce: 0.018105
[05:03:47.740] iteration 10900 : loss : 0.311562, loss_ce: 0.007643
[05:03:51.854] iteration 10910 : loss : 0.220244, loss_ce: 0.009082
[05:03:55.957] iteration 10920 : loss : 0.200954, loss_ce: 0.007759
[05:04:00.067] iteration 10930 : loss : 0.173150, loss_ce: 0.002888
[05:04:04.173] iteration 10940 : loss : 0.210675, loss_ce: 0.025315
[05:04:08.284] iteration 10950 : loss : 0.244721, loss_ce: 0.018580
[05:04:12.387] iteration 10960 : loss : 0.207590, loss_ce: 0.016376
[05:04:16.501] iteration 10970 : loss : 0.242534, loss_ce: 0.009041
[05:04:20.604] iteration 10980 : loss : 0.273963, loss_ce: 0.013678
[05:05:58.845] iteration 10990 : loss : 0.289188, loss_ce: 0.028526
[05:06:02.934] iteration 11000 : loss : 0.356910, loss_ce: 0.047474
[05:06:07.033] iteration 11010 : loss : 0.341132, loss_ce: 0.044802
[05:06:11.126] iteration 11020 : loss : 0.336619, loss_ce: 0.020709
[05:06:15.232] iteration 11030 : loss : 0.333408, loss_ce: 0.032344
[05:06:19.331] iteration 11040 : loss : 0.336233, loss_ce: 0.049211
[05:06:23.439] iteration 11050 : loss : 0.341380, loss_ce: 0.041974
[05:06:27.542] iteration 11060 : loss : 0.198029, loss_ce: 0.065935
[05:06:31.654] iteration 11070 : loss : 0.190593, loss_ce: 0.025845
[05:06:35.753] iteration 11080 : loss : 0.304976, loss_ce: 0.009633
[05:06:39.863] iteration 11090 : loss : 0.278827, loss_ce: 0.020938
[05:06:43.964] iteration 11100 : loss : 0.028622, loss_ce: 0.005634
[05:06:48.075] iteration 11110 : loss : 0.214491, loss_ce: 0.013568
[05:06:52.178] iteration 11120 : loss : 0.195436, loss_ce: 0.006035
[05:06:56.287] iteration 11130 : loss : 0.157465, loss_ce: 0.012152
[05:07:00.390] iteration 11140 : loss : 0.196216, loss_ce: 0.008698
[05:07:04.503] iteration 11150 : loss : 0.303734, loss_ce: 0.010974
[05:07:08.604] iteration 11160 : loss : 0.270013, loss_ce: 0.016713
[05:07:12.713] iteration 11170 : loss : 0.099200, loss_ce: 0.010700
[05:07:16.813] iteration 11180 : loss : 0.225225, loss_ce: 0.009738
[05:07:20.925] iteration 11190 : loss : 0.071733, loss_ce: 0.014393
[05:07:25.027] iteration 11200 : loss : 0.247866, loss_ce: 0.013446
[05:07:29.139] iteration 11210 : loss : 0.334430, loss_ce: 0.039610
[05:07:33.243] iteration 11220 : loss : 0.215199, loss_ce: 0.011074
[05:07:37.357] iteration 11230 : loss : 0.129824, loss_ce: 0.005981
[05:07:41.459] iteration 11240 : loss : 0.308870, loss_ce: 0.019274
[05:07:45.570] iteration 11250 : loss : 0.325710, loss_ce: 0.010436
[05:09:23.999] iteration 11260 : loss : 0.344592, loss_ce: 0.029782
[05:09:28.098] iteration 11270 : loss : 0.254357, loss_ce: 0.033768
[05:09:32.192] iteration 11280 : loss : 0.226989, loss_ce: 0.039864
[05:09:36.302] iteration 11290 : loss : 0.312567, loss_ce: 0.004530
[05:09:40.398] iteration 11300 : loss : 0.178429, loss_ce: 0.008173
[05:09:44.504] iteration 11310 : loss : 0.037431, loss_ce: 0.004862
[05:09:48.601] iteration 11320 : loss : 0.230071, loss_ce: 0.020033
[05:09:52.712] iteration 11330 : loss : 0.267888, loss_ce: 0.025594
[05:09:56.813] iteration 11340 : loss : 0.326580, loss_ce: 0.012769
[05:10:00.925] iteration 11350 : loss : 0.057995, loss_ce: 0.007213
[05:10:05.029] iteration 11360 : loss : 0.191830, loss_ce: 0.009040
[05:10:09.142] iteration 11370 : loss : 0.251271, loss_ce: 0.015764
[05:10:13.247] iteration 11380 : loss : 0.319222, loss_ce: 0.006424
[05:10:17.363] iteration 11390 : loss : 0.106215, loss_ce: 0.010221
[05:10:21.470] iteration 11400 : loss : 0.174472, loss_ce: 0.008194
[05:10:25.588] iteration 11410 : loss : 0.335375, loss_ce: 0.016362
[05:10:29.689] iteration 11420 : loss : 0.207299, loss_ce: 0.007173
[05:10:33.805] iteration 11430 : loss : 0.119998, loss_ce: 0.005695
[05:10:37.906] iteration 11440 : loss : 0.044090, loss_ce: 0.007418
[05:10:42.017] iteration 11450 : loss : 0.206001, loss_ce: 0.017375
[05:10:46.119] iteration 11460 : loss : 0.184921, loss_ce: 0.006367
[05:10:50.233] iteration 11470 : loss : 0.200761, loss_ce: 0.008774
[05:10:54.335] iteration 11480 : loss : 0.231087, loss_ce: 0.012672
[05:10:58.450] iteration 11490 : loss : 0.074755, loss_ce: 0.009755
[05:11:02.553] iteration 11500 : loss : 0.237359, loss_ce: 0.012507
[05:11:06.667] iteration 11510 : loss : 0.322275, loss_ce: 0.008848
[05:11:10.770] iteration 11520 : loss : 0.196662, loss_ce: 0.005633
[05:12:59.550] iteration 11530 : loss : 0.326165, loss_ce: 0.008864
[05:13:03.638] iteration 11540 : loss : 0.322369, loss_ce: 0.015270
[05:13:07.737] iteration 11550 : loss : 0.265260, loss_ce: 0.024376
[05:13:11.827] iteration 11560 : loss : 0.315897, loss_ce: 0.007713
[05:13:15.933] iteration 11570 : loss : 0.083759, loss_ce: 0.021979
[05:13:20.031] iteration 11580 : loss : 0.281208, loss_ce: 0.015881
[05:13:24.140] iteration 11590 : loss : 0.293389, loss_ce: 0.008155
[05:13:28.239] iteration 11600 : loss : 0.203648, loss_ce: 0.017651
[05:13:32.346] iteration 11610 : loss : 0.217236, loss_ce: 0.022297
[05:13:36.446] iteration 11620 : loss : 0.077516, loss_ce: 0.007664
[05:13:40.558] iteration 11630 : loss : 0.233401, loss_ce: 0.014222
[05:13:44.657] iteration 11640 : loss : 0.285602, loss_ce: 0.024863
[05:13:48.764] iteration 11650 : loss : 0.326092, loss_ce: 0.027328
[05:13:52.864] iteration 11660 : loss : 0.216600, loss_ce: 0.008151
[05:13:56.975] iteration 11670 : loss : 0.260168, loss_ce: 0.010319
[05:14:01.075] iteration 11680 : loss : 0.334155, loss_ce: 0.013446
[05:14:05.186] iteration 11690 : loss : 0.242492, loss_ce: 0.017817
[05:14:09.288] iteration 11700 : loss : 0.244153, loss_ce: 0.013883
[05:14:13.398] iteration 11710 : loss : 0.117523, loss_ce: 0.003095
[05:14:17.498] iteration 11720 : loss : 0.196498, loss_ce: 0.012316
[05:14:21.609] iteration 11730 : loss : 0.217764, loss_ce: 0.010114
[05:14:25.714] iteration 11740 : loss : 0.252266, loss_ce: 0.012396
[05:14:29.824] iteration 11750 : loss : 0.186379, loss_ce: 0.012628
[05:14:33.927] iteration 11760 : loss : 0.059965, loss_ce: 0.007180
[05:14:38.039] iteration 11770 : loss : 0.111692, loss_ce: 0.021342
[05:14:42.141] iteration 11780 : loss : 0.172353, loss_ce: 0.003853
[05:14:46.254] iteration 11790 : loss : 0.204233, loss_ce: 0.008823
[05:16:25.711] iteration 11800 : loss : 0.286104, loss_ce: 0.014756
[05:16:29.810] iteration 11810 : loss : 0.261639, loss_ce: 0.020138
[05:16:33.902] iteration 11820 : loss : 0.326293, loss_ce: 0.019580
[05:16:38.004] iteration 11830 : loss : 0.313355, loss_ce: 0.012595
[05:16:42.097] iteration 11840 : loss : 0.224032, loss_ce: 0.011195
[05:16:46.201] iteration 11850 : loss : 0.340361, loss_ce: 0.023496
[05:16:50.298] iteration 11860 : loss : 0.242571, loss_ce: 0.023844
[05:16:54.410] iteration 11870 : loss : 0.272465, loss_ce: 0.013473
[05:16:58.513] iteration 11880 : loss : 0.175422, loss_ce: 0.011764
[05:17:02.629] iteration 11890 : loss : 0.320579, loss_ce: 0.007231
[05:17:06.732] iteration 11900 : loss : 0.249791, loss_ce: 0.019132
[05:17:10.845] iteration 11910 : loss : 0.220735, loss_ce: 0.009336
[05:17:14.948] iteration 11920 : loss : 0.220521, loss_ce: 0.017828
[05:17:19.059] iteration 11930 : loss : 0.198677, loss_ce: 0.008651
[05:17:23.163] iteration 11940 : loss : 0.235326, loss_ce: 0.017264
[05:17:27.279] iteration 11950 : loss : 0.273423, loss_ce: 0.012068
[05:17:31.383] iteration 11960 : loss : 0.324839, loss_ce: 0.020076
[05:17:35.501] iteration 11970 : loss : 0.202210, loss_ce: 0.014896
[05:17:39.603] iteration 11980 : loss : 0.217460, loss_ce: 0.015523
[05:17:43.715] iteration 11990 : loss : 0.280680, loss_ce: 0.055374
[05:17:47.820] iteration 12000 : loss : 0.039535, loss_ce: 0.009499
[05:17:51.933] iteration 12010 : loss : 0.237900, loss_ce: 0.008720
[05:17:56.040] iteration 12020 : loss : 0.246628, loss_ce: 0.012828
[05:18:00.154] iteration 12030 : loss : 0.270373, loss_ce: 0.010975
[05:18:04.259] iteration 12040 : loss : 0.212546, loss_ce: 0.010924
[05:18:08.370] iteration 12050 : loss : 0.270226, loss_ce: 0.020676
[05:18:12.375] iteration 12060 : loss : 0.241374, loss_ce: 0.013292
[05:19:50.869] iteration 12070 : loss : 0.378875, loss_ce: 0.055711
[05:19:54.960] iteration 12080 : loss : 0.266624, loss_ce: 0.028036
[05:19:59.062] iteration 12090 : loss : 0.325828, loss_ce: 0.006796
[05:20:03.158] iteration 12100 : loss : 0.310456, loss_ce: 0.055200
[05:20:07.266] iteration 12110 : loss : 0.240672, loss_ce: 0.022220
[05:20:11.361] iteration 12120 : loss : 0.318674, loss_ce: 0.024983
[05:20:15.470] iteration 12130 : loss : 0.208162, loss_ce: 0.016884
[05:20:19.567] iteration 12140 : loss : 0.314096, loss_ce: 0.012597
[05:20:23.675] iteration 12150 : loss : 0.219772, loss_ce: 0.014010
[05:20:27.777] iteration 12160 : loss : 0.324221, loss_ce: 0.007600
[05:20:31.883] iteration 12170 : loss : 0.244262, loss_ce: 0.015712
[05:20:35.982] iteration 12180 : loss : 0.313484, loss_ce: 0.018328
[05:20:40.094] iteration 12190 : loss : 0.248535, loss_ce: 0.020689
[05:20:44.191] iteration 12200 : loss : 0.187136, loss_ce: 0.019559
[05:20:48.301] iteration 12210 : loss : 0.066294, loss_ce: 0.007375
[05:20:52.401] iteration 12220 : loss : 0.215307, loss_ce: 0.013568
[05:20:56.510] iteration 12230 : loss : 0.227855, loss_ce: 0.005918
[05:21:00.611] iteration 12240 : loss : 0.225892, loss_ce: 0.021666
[05:21:04.721] iteration 12250 : loss : 0.052893, loss_ce: 0.007716
[05:21:08.821] iteration 12260 : loss : 0.200531, loss_ce: 0.011152
[05:21:12.931] iteration 12270 : loss : 0.324749, loss_ce: 0.016161
[05:21:17.033] iteration 12280 : loss : 0.275788, loss_ce: 0.011564
[05:21:21.142] iteration 12290 : loss : 0.202266, loss_ce: 0.011852
[05:21:25.246] iteration 12300 : loss : 0.226602, loss_ce: 0.010419
[05:21:29.354] iteration 12310 : loss : 0.278743, loss_ce: 0.009424
[05:21:33.457] iteration 12320 : loss : 0.329115, loss_ce: 0.012566
[05:23:23.145] iteration 12330 : loss : 0.389370, loss_ce: 0.066548
[05:23:27.233] iteration 12340 : loss : 0.306907, loss_ce: 0.036804
[05:23:31.329] iteration 12350 : loss : 0.322978, loss_ce: 0.029966
[05:23:35.423] iteration 12360 : loss : 0.232137, loss_ce: 0.020069
[05:23:39.529] iteration 12370 : loss : 0.212313, loss_ce: 0.011955
[05:23:43.624] iteration 12380 : loss : 0.263228, loss_ce: 0.019297
[05:23:47.731] iteration 12390 : loss : 0.337722, loss_ce: 0.019585
[05:23:51.828] iteration 12400 : loss : 0.207499, loss_ce: 0.004783
[05:23:55.934] iteration 12410 : loss : 0.207120, loss_ce: 0.011324
[05:24:00.036] iteration 12420 : loss : 0.321722, loss_ce: 0.020583
[05:24:04.144] iteration 12430 : loss : 0.206851, loss_ce: 0.018454
[05:24:08.244] iteration 12440 : loss : 0.210385, loss_ce: 0.011070
[05:24:12.355] iteration 12450 : loss : 0.271139, loss_ce: 0.029512
[05:24:16.456] iteration 12460 : loss : 0.245701, loss_ce: 0.009051
[05:24:20.566] iteration 12470 : loss : 0.210803, loss_ce: 0.009255
[05:24:24.668] iteration 12480 : loss : 0.276525, loss_ce: 0.017651
[05:24:28.784] iteration 12490 : loss : 0.301409, loss_ce: 0.016830
[05:24:32.887] iteration 12500 : loss : 0.281686, loss_ce: 0.028211
[05:24:36.999] iteration 12510 : loss : 0.326423, loss_ce: 0.016150
[05:24:41.100] iteration 12520 : loss : 0.243583, loss_ce: 0.017251
[05:24:45.212] iteration 12530 : loss : 0.290929, loss_ce: 0.030529
[05:24:49.315] iteration 12540 : loss : 0.323247, loss_ce: 0.010354
[05:24:53.427] iteration 12550 : loss : 0.236165, loss_ce: 0.011815
[05:24:57.526] iteration 12560 : loss : 0.217322, loss_ce: 0.016158
[05:25:01.638] iteration 12570 : loss : 0.325988, loss_ce: 0.016797
[05:25:05.740] iteration 12580 : loss : 0.206528, loss_ce: 0.006873
[05:25:09.851] iteration 12590 : loss : 0.304094, loss_ce: 0.020124
[05:26:48.293] iteration 12600 : loss : 0.428445, loss_ce: 0.052787
[05:26:52.393] iteration 12610 : loss : 0.324342, loss_ce: 0.009681
[05:26:56.480] iteration 12620 : loss : 0.248919, loss_ce: 0.023978
[05:27:00.581] iteration 12630 : loss : 0.304962, loss_ce: 0.041718
[05:27:04.678] iteration 12640 : loss : 0.236725, loss_ce: 0.023846
[05:27:08.787] iteration 12650 : loss : 0.224766, loss_ce: 0.017894
[05:27:12.886] iteration 12660 : loss : 0.246857, loss_ce: 0.025464
[05:27:16.995] iteration 12670 : loss : 0.252482, loss_ce: 0.032590
[05:27:21.093] iteration 12680 : loss : 0.319761, loss_ce: 0.007667
[05:27:25.203] iteration 12690 : loss : 0.235826, loss_ce: 0.016432
[05:27:29.304] iteration 12700 : loss : 0.221355, loss_ce: 0.014862
[05:27:33.418] iteration 12710 : loss : 0.196067, loss_ce: 0.016775
[05:27:37.520] iteration 12720 : loss : 0.240953, loss_ce: 0.018366
[05:27:41.629] iteration 12730 : loss : 0.234753, loss_ce: 0.021837
[05:27:45.735] iteration 12740 : loss : 0.333266, loss_ce: 0.012803
[05:27:49.847] iteration 12750 : loss : 0.314941, loss_ce: 0.005109
[05:27:53.948] iteration 12760 : loss : 0.215939, loss_ce: 0.024773
[05:27:58.060] iteration 12770 : loss : 0.258191, loss_ce: 0.025597
[05:28:02.162] iteration 12780 : loss : 0.213277, loss_ce: 0.058317
[05:28:06.275] iteration 12790 : loss : 0.264622, loss_ce: 0.023681
[05:28:10.381] iteration 12800 : loss : 0.259223, loss_ce: 0.012292
[05:28:14.489] iteration 12810 : loss : 0.145751, loss_ce: 0.009528
[05:28:18.590] iteration 12820 : loss : 0.283503, loss_ce: 0.020637
[05:28:22.705] iteration 12830 : loss : 0.302672, loss_ce: 0.028204
[05:28:26.809] iteration 12840 : loss : 0.192339, loss_ce: 0.012230
[05:28:30.922] iteration 12850 : loss : 0.234192, loss_ce: 0.006971
[05:28:35.023] iteration 12860 : loss : 0.193698, loss_ce: 0.013179
[05:30:13.440] iteration 12870 : loss : 0.430150, loss_ce: 0.096439
[05:30:17.528] iteration 12880 : loss : 0.415981, loss_ce: 0.068853
[05:30:21.626] iteration 12890 : loss : 0.352185, loss_ce: 0.019735
[05:30:25.723] iteration 12900 : loss : 0.354384, loss_ce: 0.041203
[05:30:29.829] iteration 12910 : loss : 0.352294, loss_ce: 0.062540
[05:30:33.927] iteration 12920 : loss : 0.317447, loss_ce: 0.009896
[05:30:38.034] iteration 12930 : loss : 0.279067, loss_ce: 0.037595
[05:30:42.133] iteration 12940 : loss : 0.335178, loss_ce: 0.017259
[05:30:46.241] iteration 12950 : loss : 0.271933, loss_ce: 0.026085
[05:30:50.340] iteration 12960 : loss : 0.308646, loss_ce: 0.034994
[05:30:54.450] iteration 12970 : loss : 0.235889, loss_ce: 0.010186
[05:30:58.549] iteration 12980 : loss : 0.323318, loss_ce: 0.030624
[05:31:02.661] iteration 12990 : loss : 0.308444, loss_ce: 0.024987
[05:31:06.761] iteration 13000 : loss : 0.299122, loss_ce: 0.018209
[05:31:10.872] iteration 13010 : loss : 0.248393, loss_ce: 0.022797
[05:31:14.973] iteration 13020 : loss : 0.234546, loss_ce: 0.019408
[05:31:19.085] iteration 13030 : loss : 0.273565, loss_ce: 0.019688
[05:31:23.185] iteration 13040 : loss : 0.337684, loss_ce: 0.030172
[05:31:27.297] iteration 13050 : loss : 0.243651, loss_ce: 0.019376
[05:31:31.401] iteration 13060 : loss : 0.258916, loss_ce: 0.037076
[05:31:35.509] iteration 13070 : loss : 0.243646, loss_ce: 0.015330
[05:31:39.610] iteration 13080 : loss : 0.331808, loss_ce: 0.030783
[05:31:43.721] iteration 13090 : loss : 0.301334, loss_ce: 0.027603
[05:31:47.823] iteration 13100 : loss : 0.204213, loss_ce: 0.017072
[05:31:51.936] iteration 13110 : loss : 0.333552, loss_ce: 0.026296
[05:31:56.036] iteration 13120 : loss : 0.240741, loss_ce: 0.022346
[05:32:00.145] iteration 13130 : loss : 0.346840, loss_ce: 0.008923
[05:33:49.206] iteration 13140 : loss : 0.440036, loss_ce: 0.056319
[05:33:53.304] iteration 13150 : loss : 0.422733, loss_ce: 0.080947
[05:33:57.395] iteration 13160 : loss : 0.403705, loss_ce: 0.028635
[05:34:01.496] iteration 13170 : loss : 0.383532, loss_ce: 0.029829
[05:34:05.589] iteration 13180 : loss : 0.396745, loss_ce: 0.045654
[05:34:09.693] iteration 13190 : loss : 0.361420, loss_ce: 0.039654
[05:34:13.790] iteration 13200 : loss : 0.379936, loss_ce: 0.085610
[05:34:17.895] iteration 13210 : loss : 0.371456, loss_ce: 0.072624
[05:34:21.993] iteration 13220 : loss : 0.360026, loss_ce: 0.028242
[05:34:26.100] iteration 13230 : loss : 0.348591, loss_ce: 0.028938
[05:34:30.203] iteration 13240 : loss : 0.332721, loss_ce: 0.017100
[05:34:34.314] iteration 13250 : loss : 0.355774, loss_ce: 0.017994
[05:34:38.414] iteration 13260 : loss : 0.365722, loss_ce: 0.054939
[05:34:42.527] iteration 13270 : loss : 0.353599, loss_ce: 0.031545
[05:34:46.627] iteration 13280 : loss : 0.342831, loss_ce: 0.076661
[05:34:50.739] iteration 13290 : loss : 0.338216, loss_ce: 0.013151
[05:34:54.839] iteration 13300 : loss : 0.356186, loss_ce: 0.034282
[05:34:58.948] iteration 13310 : loss : 0.338799, loss_ce: 0.026388
[05:35:03.049] iteration 13320 : loss : 0.356039, loss_ce: 0.024156
[05:35:07.163] iteration 13330 : loss : 0.327731, loss_ce: 0.023371
[05:35:11.264] iteration 13340 : loss : 0.327265, loss_ce: 0.005643
[05:35:15.376] iteration 13350 : loss : 0.355196, loss_ce: 0.038914
[05:35:19.478] iteration 13360 : loss : 0.351617, loss_ce: 0.052373
[05:35:23.591] iteration 13370 : loss : 0.391224, loss_ce: 0.085043
[05:35:27.697] iteration 13380 : loss : 0.333383, loss_ce: 0.019551
[05:35:31.809] iteration 13390 : loss : 0.368077, loss_ce: 0.093373
[05:35:35.814] iteration 13400 : loss : 0.348302, loss_ce: 0.039287
[05:37:00.442] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_49.pth
[05:37:00.655] save final model to ./finetune_tpgm_kits23_continual\finetuned_final.pth
[13:11:27.371] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=100, tpgm_exclude=[])
[13:11:27.406] Using 8569/95221 samples for finetuning
[13:11:27.406] Using 953/95221 samples for TPGM
[13:11:27.406] Model has 9 total classes, training on 4 classes
[13:11:37.322] 268 iterations per epoch. 13400 max iterations 
[13:11:52.770] iteration 10 : loss : 0.461870, loss_ce: 0.081058
[13:11:57.099] iteration 20 : loss : 0.447126, loss_ce: 0.067454
[13:12:01.403] iteration 30 : loss : 0.454064, loss_ce: 0.061675
[13:12:06.612] iteration 40 : loss : 0.438523, loss_ce: 0.037797
[13:12:11.072] iteration 50 : loss : 0.427871, loss_ce: 0.060777
[13:12:15.560] iteration 60 : loss : 0.289397, loss_ce: 0.043874
[13:12:20.254] iteration 70 : loss : 0.430093, loss_ce: 0.078371
[13:12:24.731] iteration 80 : loss : 0.432416, loss_ce: 0.045691
[13:12:29.159] iteration 90 : loss : 0.438128, loss_ce: 0.059353
[13:12:33.572] iteration 100 : loss : 0.424840, loss_ce: 0.065352
[13:12:38.105] iteration 110 : loss : 0.289580, loss_ce: 0.027083
[13:12:42.638] iteration 120 : loss : 0.420143, loss_ce: 0.054165
[13:12:47.136] iteration 130 : loss : 0.428500, loss_ce: 0.046873
[13:12:51.586] iteration 140 : loss : 0.401049, loss_ce: 0.039351
[13:12:56.110] iteration 150 : loss : 0.411577, loss_ce: 0.030802
[13:13:00.428] iteration 160 : loss : 0.382576, loss_ce: 0.048536
[13:13:04.664] iteration 170 : loss : 0.386560, loss_ce: 0.035044
[13:13:09.136] iteration 180 : loss : 0.363701, loss_ce: 0.024090
[13:13:13.597] iteration 190 : loss : 0.240404, loss_ce: 0.031053
[13:13:18.089] iteration 200 : loss : 0.373939, loss_ce: 0.052010
[13:13:22.615] iteration 210 : loss : 0.388509, loss_ce: 0.043876
[13:13:27.003] iteration 220 : loss : 0.379827, loss_ce: 0.024280
[13:13:31.371] iteration 230 : loss : 0.358815, loss_ce: 0.026818
[13:13:35.754] iteration 240 : loss : 0.366219, loss_ce: 0.032203
[13:13:40.014] iteration 250 : loss : 0.391504, loss_ce: 0.052399
[13:13:44.212] iteration 260 : loss : 0.350837, loss_ce: 0.040769
[13:40:48.482] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=100, tpgm_exclude=[])
[13:40:48.510] Using 8569/95221 samples for finetuning
[13:40:48.510] Using 953/95221 samples for TPGM
[13:40:48.510] Model has 9 total classes, training on 4 classes
[13:41:15.141] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=100, tpgm_exclude=[])
[13:41:15.168] Using 8569/95221 samples for finetuning
[13:41:15.168] Using 953/95221 samples for TPGM
[13:41:15.168] Model has 9 total classes, training on 4 classes
[13:41:24.934] 268 iterations per epoch. 13400 max iterations 
[13:41:40.132] iteration 10 : loss : 0.461870, loss_ce: 0.081058
[13:41:44.211] iteration 20 : loss : 0.447126, loss_ce: 0.067454
[13:41:48.382] iteration 30 : loss : 0.454064, loss_ce: 0.061675
[13:41:52.990] iteration 40 : loss : 0.438523, loss_ce: 0.037797
[13:41:57.743] iteration 50 : loss : 0.427871, loss_ce: 0.060777
[13:42:01.826] iteration 60 : loss : 0.289397, loss_ce: 0.043874
[13:42:06.213] iteration 70 : loss : 0.430093, loss_ce: 0.078371
[13:42:10.459] iteration 80 : loss : 0.432416, loss_ce: 0.045691
[13:42:14.573] iteration 90 : loss : 0.438128, loss_ce: 0.059353
[13:42:18.672] iteration 100 : loss : 0.424840, loss_ce: 0.065352
[13:42:22.776] iteration 110 : loss : 0.289580, loss_ce: 0.027083
[13:42:26.906] iteration 120 : loss : 0.420143, loss_ce: 0.054165
[13:42:31.097] iteration 130 : loss : 0.428500, loss_ce: 0.046873
[13:42:35.227] iteration 140 : loss : 0.401049, loss_ce: 0.039351
[13:42:39.355] iteration 150 : loss : 0.411577, loss_ce: 0.030802
[13:42:43.560] iteration 160 : loss : 0.382576, loss_ce: 0.048536
[13:42:47.972] iteration 170 : loss : 0.386560, loss_ce: 0.035044
[13:42:52.273] iteration 180 : loss : 0.363701, loss_ce: 0.024090
[13:42:56.362] iteration 190 : loss : 0.240404, loss_ce: 0.031053
[13:43:00.454] iteration 200 : loss : 0.373939, loss_ce: 0.052010
[13:43:04.571] iteration 210 : loss : 0.388509, loss_ce: 0.043876
[13:43:08.710] iteration 220 : loss : 0.379827, loss_ce: 0.024280
[13:43:12.812] iteration 230 : loss : 0.358815, loss_ce: 0.026818
[13:43:16.907] iteration 240 : loss : 0.366219, loss_ce: 0.032203
[13:43:21.023] iteration 250 : loss : 0.391504, loss_ce: 0.052399
[13:43:25.144] iteration 260 : loss : 0.350837, loss_ce: 0.040769
[13:45:07.022] iteration 270 : loss : 0.431558, loss_ce: 0.044635
[13:45:11.120] iteration 280 : loss : 0.468822, loss_ce: 0.061862
[13:45:15.235] iteration 290 : loss : 0.274063, loss_ce: 0.057549
[13:45:19.469] iteration 300 : loss : 0.270081, loss_ce: 0.054276
[13:45:23.574] iteration 310 : loss : 0.256262, loss_ce: 0.036416
[13:45:27.651] iteration 320 : loss : 0.346272, loss_ce: 0.026979
[13:45:31.763] iteration 330 : loss : 0.311520, loss_ce: 0.014583
[13:45:35.901] iteration 340 : loss : 0.185030, loss_ce: 0.035326
[13:45:40.033] iteration 350 : loss : 0.174159, loss_ce: 0.030883
[13:45:44.123] iteration 360 : loss : 0.324581, loss_ce: 0.035363
[13:45:48.212] iteration 370 : loss : 0.414771, loss_ce: 0.027476
[13:45:52.288] iteration 380 : loss : 0.356712, loss_ce: 0.036411
[13:45:56.393] iteration 390 : loss : 0.263369, loss_ce: 0.026661
[13:46:00.491] iteration 400 : loss : 0.428507, loss_ce: 0.073718
[13:46:04.596] iteration 410 : loss : 0.319651, loss_ce: 0.031393
[13:46:08.739] iteration 420 : loss : 0.108334, loss_ce: 0.013181
[13:46:12.875] iteration 430 : loss : 0.265416, loss_ce: 0.031278
[13:46:16.980] iteration 440 : loss : 0.343777, loss_ce: 0.015692
[13:46:21.077] iteration 450 : loss : 0.283035, loss_ce: 0.008219
[13:46:25.165] iteration 460 : loss : 0.269485, loss_ce: 0.034662
[13:46:29.264] iteration 470 : loss : 0.244950, loss_ce: 0.014978
[13:46:33.358] iteration 480 : loss : 0.188623, loss_ce: 0.036209
[13:46:37.458] iteration 490 : loss : 0.298096, loss_ce: 0.026530
[13:46:42.431] iteration 500 : loss : 0.229659, loss_ce: 0.013865
[13:46:46.539] iteration 510 : loss : 0.282239, loss_ce: 0.017609
[13:46:50.632] iteration 520 : loss : 0.239397, loss_ce: 0.017117
[13:46:54.867] iteration 530 : loss : 0.278834, loss_ce: 0.026059
[13:48:39.973] iteration 540 : loss : 0.387081, loss_ce: 0.028197
[13:48:44.062] iteration 550 : loss : 0.212703, loss_ce: 0.014436
[13:48:48.135] iteration 560 : loss : 0.335133, loss_ce: 0.011951
[13:48:52.221] iteration 570 : loss : 0.277993, loss_ce: 0.029202
[13:48:56.304] iteration 580 : loss : 0.332210, loss_ce: 0.022860
[13:49:00.393] iteration 590 : loss : 0.276134, loss_ce: 0.020265
[13:49:04.491] iteration 600 : loss : 0.072526, loss_ce: 0.019333
[13:49:08.589] iteration 610 : loss : 0.199468, loss_ce: 0.007159
[13:49:12.676] iteration 620 : loss : 0.115351, loss_ce: 0.009687
[13:49:16.773] iteration 630 : loss : 0.302268, loss_ce: 0.014277
[13:49:20.865] iteration 640 : loss : 0.288711, loss_ce: 0.007664
[13:49:26.032] iteration 650 : loss : 0.267869, loss_ce: 0.018595
[13:49:30.148] iteration 660 : loss : 0.242770, loss_ce: 0.011826
[13:49:34.285] iteration 670 : loss : 0.197229, loss_ce: 0.028729
[13:49:38.397] iteration 680 : loss : 0.272281, loss_ce: 0.015689
[13:49:42.498] iteration 690 : loss : 0.235243, loss_ce: 0.015679
[13:49:46.586] iteration 700 : loss : 0.153753, loss_ce: 0.057413
[13:49:50.686] iteration 710 : loss : 0.321226, loss_ce: 0.013285
[13:49:54.772] iteration 720 : loss : 0.254498, loss_ce: 0.055200
[13:49:58.875] iteration 730 : loss : 0.200210, loss_ce: 0.018710
[13:50:02.965] iteration 740 : loss : 0.319142, loss_ce: 0.005506
[13:50:07.067] iteration 750 : loss : 0.303234, loss_ce: 0.027776
[13:50:11.158] iteration 760 : loss : 0.224114, loss_ce: 0.019287
[13:50:15.258] iteration 770 : loss : 0.084232, loss_ce: 0.019744
[13:50:19.349] iteration 780 : loss : 0.200993, loss_ce: 0.013042
[13:50:23.456] iteration 790 : loss : 0.251262, loss_ce: 0.043575
[13:50:27.549] iteration 800 : loss : 0.232497, loss_ce: 0.012762
[13:52:09.956] iteration 810 : loss : 0.237498, loss_ce: 0.017916
[13:52:14.670] iteration 820 : loss : 0.355005, loss_ce: 0.022840
[13:52:18.761] iteration 830 : loss : 0.335431, loss_ce: 0.034196
[13:52:22.848] iteration 840 : loss : 0.249290, loss_ce: 0.028962
[13:52:26.948] iteration 850 : loss : 0.243206, loss_ce: 0.008716
[13:52:31.034] iteration 860 : loss : 0.374595, loss_ce: 0.028424
[13:52:36.008] iteration 870 : loss : 0.347385, loss_ce: 0.026167
[13:52:40.098] iteration 880 : loss : 0.252869, loss_ce: 0.012913
[13:52:44.200] iteration 890 : loss : 0.340728, loss_ce: 0.029560
[13:52:48.286] iteration 900 : loss : 0.289546, loss_ce: 0.014240
[13:52:52.388] iteration 910 : loss : 0.261452, loss_ce: 0.027551
[13:52:56.480] iteration 920 : loss : 0.233687, loss_ce: 0.018056
[13:53:00.582] iteration 930 : loss : 0.230252, loss_ce: 0.008987
[13:53:04.671] iteration 940 : loss : 0.274276, loss_ce: 0.014371
[13:53:08.773] iteration 950 : loss : 0.159676, loss_ce: 0.016107
[13:53:12.864] iteration 960 : loss : 0.298395, loss_ce: 0.023167
[13:53:16.964] iteration 970 : loss : 0.267265, loss_ce: 0.008892
[13:53:21.056] iteration 980 : loss : 0.227217, loss_ce: 0.010904
[13:53:25.159] iteration 990 : loss : 0.234420, loss_ce: 0.016023
[13:53:29.252] iteration 1000 : loss : 0.151742, loss_ce: 0.034351
[13:53:33.355] iteration 1010 : loss : 0.251050, loss_ce: 0.003482
[13:53:37.445] iteration 1020 : loss : 0.293594, loss_ce: 0.014829
[13:53:41.554] iteration 1030 : loss : 0.293419, loss_ce: 0.010603
[13:53:46.517] iteration 1040 : loss : 0.349951, loss_ce: 0.018208
[13:53:51.533] iteration 1050 : loss : 0.314229, loss_ce: 0.036995
[13:53:55.630] iteration 1060 : loss : 0.268760, loss_ce: 0.013005
[13:53:59.728] iteration 1070 : loss : 0.256533, loss_ce: 0.021143
[13:55:51.443] iteration 1080 : loss : 0.206091, loss_ce: 0.042273
[13:55:55.529] iteration 1090 : loss : 0.356057, loss_ce: 0.063909
[13:55:59.614] iteration 1100 : loss : 0.276909, loss_ce: 0.039444
[13:56:03.708] iteration 1110 : loss : 0.110377, loss_ce: 0.012264
[13:56:08.861] iteration 1120 : loss : 0.380068, loss_ce: 0.029861
[13:56:12.958] iteration 1130 : loss : 0.174287, loss_ce: 0.007980
[13:56:17.039] iteration 1140 : loss : 0.242872, loss_ce: 0.012765
[13:56:21.135] iteration 1150 : loss : 0.250907, loss_ce: 0.009237
[13:56:25.222] iteration 1160 : loss : 0.252084, loss_ce: 0.012459
[13:56:29.325] iteration 1170 : loss : 0.330732, loss_ce: 0.050434
[13:56:33.420] iteration 1180 : loss : 0.282136, loss_ce: 0.029572
[13:56:37.516] iteration 1190 : loss : 0.312965, loss_ce: 0.014283
[13:56:41.603] iteration 1200 : loss : 0.276259, loss_ce: 0.020510
[13:56:45.701] iteration 1210 : loss : 0.334954, loss_ce: 0.025227
[13:56:49.786] iteration 1220 : loss : 0.324053, loss_ce: 0.028298
[13:56:53.882] iteration 1230 : loss : 0.164532, loss_ce: 0.014331
[13:56:57.970] iteration 1240 : loss : 0.323816, loss_ce: 0.015753
[13:57:02.076] iteration 1250 : loss : 0.282098, loss_ce: 0.030178
[13:57:06.162] iteration 1260 : loss : 0.260301, loss_ce: 0.015608
[13:57:10.259] iteration 1270 : loss : 0.214738, loss_ce: 0.007154
[13:57:15.485] iteration 1280 : loss : 0.208439, loss_ce: 0.011832
[13:57:20.788] iteration 1290 : loss : 0.322597, loss_ce: 0.047206
[13:57:24.880] iteration 1300 : loss : 0.152452, loss_ce: 0.013890
[13:57:29.692] iteration 1310 : loss : 0.232677, loss_ce: 0.011372
[13:57:33.797] iteration 1320 : loss : 0.123114, loss_ce: 0.020934
[13:57:37.903] iteration 1330 : loss : 0.300183, loss_ce: 0.022623
[13:57:41.890] iteration 1340 : loss : 0.284550, loss_ce: 0.020447
[13:59:24.244] iteration 1350 : loss : 0.336559, loss_ce: 0.024095
[13:59:28.332] iteration 1360 : loss : 0.265999, loss_ce: 0.012987
[13:59:32.425] iteration 1370 : loss : 0.318075, loss_ce: 0.075293
[13:59:36.514] iteration 1380 : loss : 0.276322, loss_ce: 0.024961
[13:59:40.613] iteration 1390 : loss : 0.147440, loss_ce: 0.027669
[13:59:44.702] iteration 1400 : loss : 0.077963, loss_ce: 0.026243
[13:59:48.805] iteration 1410 : loss : 0.151921, loss_ce: 0.031217
[13:59:52.888] iteration 1420 : loss : 0.248716, loss_ce: 0.016450
[13:59:56.990] iteration 1430 : loss : 0.343977, loss_ce: 0.008942
[14:00:01.086] iteration 1440 : loss : 0.271545, loss_ce: 0.012117
[14:00:05.189] iteration 1450 : loss : 0.227888, loss_ce: 0.011852
[14:00:09.280] iteration 1460 : loss : 0.249629, loss_ce: 0.024779
[14:00:13.387] iteration 1470 : loss : 0.124737, loss_ce: 0.030576
[14:00:18.766] iteration 1480 : loss : 0.120587, loss_ce: 0.034447
[14:00:22.883] iteration 1490 : loss : 0.265340, loss_ce: 0.022419
[14:00:26.985] iteration 1500 : loss : 0.167027, loss_ce: 0.009272
[14:00:31.087] iteration 1510 : loss : 0.142588, loss_ce: 0.014113
[14:00:35.178] iteration 1520 : loss : 0.318993, loss_ce: 0.010010
[14:00:39.283] iteration 1530 : loss : 0.323828, loss_ce: 0.017703
[14:00:43.381] iteration 1540 : loss : 0.070668, loss_ce: 0.008530
[14:00:48.628] iteration 1550 : loss : 0.134019, loss_ce: 0.005238
[14:00:52.732] iteration 1560 : loss : 0.209489, loss_ce: 0.008181
[14:00:56.833] iteration 1570 : loss : 0.056183, loss_ce: 0.011191
[14:01:00.930] iteration 1580 : loss : 0.223374, loss_ce: 0.015811
[14:01:05.034] iteration 1590 : loss : 0.064064, loss_ce: 0.011085
[14:01:09.124] iteration 1600 : loss : 0.203821, loss_ce: 0.004771
[14:02:52.090] iteration 1610 : loss : 0.148698, loss_ce: 0.015902
[14:02:56.173] iteration 1620 : loss : 0.244599, loss_ce: 0.011359
[14:03:00.283] iteration 1630 : loss : 0.229033, loss_ce: 0.016793
[14:03:06.195] iteration 1640 : loss : 0.202841, loss_ce: 0.015592
[14:03:10.442] iteration 1650 : loss : 0.097074, loss_ce: 0.006211
[14:03:14.527] iteration 1660 : loss : 0.339805, loss_ce: 0.009859
[14:03:18.621] iteration 1670 : loss : 0.263383, loss_ce: 0.025317
[14:03:22.718] iteration 1680 : loss : 0.301206, loss_ce: 0.023230
[14:03:26.815] iteration 1690 : loss : 0.111218, loss_ce: 0.015020
[14:03:30.902] iteration 1700 : loss : 0.215318, loss_ce: 0.011322
[14:03:35.002] iteration 1710 : loss : 0.130410, loss_ce: 0.013755
[14:03:40.198] iteration 1720 : loss : 0.246758, loss_ce: 0.024294
[14:03:44.296] iteration 1730 : loss : 0.189588, loss_ce: 0.011131
[14:03:48.380] iteration 1740 : loss : 0.034909, loss_ce: 0.002301
[14:03:52.473] iteration 1750 : loss : 0.248546, loss_ce: 0.023086
[14:03:56.564] iteration 1760 : loss : 0.315057, loss_ce: 0.017035
[14:04:00.671] iteration 1770 : loss : 0.256850, loss_ce: 0.012917
[14:04:05.389] iteration 1780 : loss : 0.271709, loss_ce: 0.022626
[14:04:09.499] iteration 1790 : loss : 0.341696, loss_ce: 0.008940
[14:04:13.593] iteration 1800 : loss : 0.317138, loss_ce: 0.013892
[14:04:17.893] iteration 1810 : loss : 0.267133, loss_ce: 0.009811
[14:04:22.511] iteration 1820 : loss : 0.271963, loss_ce: 0.039566
[14:04:26.618] iteration 1830 : loss : 0.200332, loss_ce: 0.018128
[14:04:30.711] iteration 1840 : loss : 0.141393, loss_ce: 0.004257
[14:04:35.507] iteration 1850 : loss : 0.315446, loss_ce: 0.006908
[14:04:39.599] iteration 1860 : loss : 0.299636, loss_ce: 0.023634
[14:04:43.703] iteration 1870 : loss : 0.189626, loss_ce: 0.011747
[14:06:37.440] iteration 1880 : loss : 0.131884, loss_ce: 0.009352
[14:06:41.530] iteration 1890 : loss : 0.244545, loss_ce: 0.031246
[14:06:45.613] iteration 1900 : loss : 0.473746, loss_ce: 0.063951
[14:06:50.563] iteration 1910 : loss : 0.469945, loss_ce: 0.079751
[14:06:54.681] iteration 1920 : loss : 0.438779, loss_ce: 0.084774
[14:06:59.381] iteration 1930 : loss : 0.460481, loss_ce: 0.049888
[14:07:03.467] iteration 1940 : loss : 0.460015, loss_ce: 0.085935
[14:07:07.564] iteration 1950 : loss : 0.487307, loss_ce: 0.091985
[14:07:12.428] iteration 1960 : loss : 0.479986, loss_ce: 0.074847
[14:07:16.518] iteration 1970 : loss : 0.442743, loss_ce: 0.058118
[14:07:20.605] iteration 1980 : loss : 0.450371, loss_ce: 0.047722
[14:07:24.707] iteration 1990 : loss : 0.442745, loss_ce: 0.068046
[14:07:28.795] iteration 2000 : loss : 0.442171, loss_ce: 0.066185
[14:07:32.884] iteration 2010 : loss : 0.434257, loss_ce: 0.090003
[14:07:36.965] iteration 2020 : loss : 0.434311, loss_ce: 0.059417
[14:07:41.817] iteration 2030 : loss : 0.479169, loss_ce: 0.113948
[14:07:45.903] iteration 2040 : loss : 0.449616, loss_ce: 0.059172
[14:07:50.003] iteration 2050 : loss : 0.290572, loss_ce: 0.058765
[14:07:54.089] iteration 2060 : loss : 0.426878, loss_ce: 0.048565
[14:07:58.187] iteration 2070 : loss : 0.289491, loss_ce: 0.046612
[14:08:02.276] iteration 2080 : loss : 0.434311, loss_ce: 0.067322
[14:08:07.067] iteration 2090 : loss : 0.433637, loss_ce: 0.057442
[14:08:11.164] iteration 2100 : loss : 0.435083, loss_ce: 0.063988
[14:08:15.262] iteration 2110 : loss : 0.427461, loss_ce: 0.067529
[14:08:20.957] iteration 2120 : loss : 0.434988, loss_ce: 0.069483
[14:08:25.060] iteration 2130 : loss : 0.411695, loss_ce: 0.059439
[14:08:29.153] iteration 2140 : loss : 0.402752, loss_ce: 0.043775
[14:10:10.993] iteration 2150 : loss : 0.457574, loss_ce: 0.081874
[14:10:15.074] iteration 2160 : loss : 0.363144, loss_ce: 0.033056
[14:10:19.165] iteration 2170 : loss : 0.414493, loss_ce: 0.033168
[14:10:23.241] iteration 2180 : loss : 0.168519, loss_ce: 0.008392
[14:10:27.339] iteration 2190 : loss : 0.136778, loss_ce: 0.030921
[14:10:31.420] iteration 2200 : loss : 0.395603, loss_ce: 0.023867
[14:10:35.510] iteration 2210 : loss : 0.226624, loss_ce: 0.012709
[14:10:39.595] iteration 2220 : loss : 0.283908, loss_ce: 0.037240
[14:10:43.692] iteration 2230 : loss : 0.261336, loss_ce: 0.024993
[14:10:47.778] iteration 2240 : loss : 0.094587, loss_ce: 0.015311
[14:10:51.876] iteration 2250 : loss : 0.331339, loss_ce: 0.013317
[14:10:55.962] iteration 2260 : loss : 0.253065, loss_ce: 0.020029
[14:11:00.307] iteration 2270 : loss : 0.321801, loss_ce: 0.014668
[14:11:04.707] iteration 2280 : loss : 0.324565, loss_ce: 0.019624
[14:11:09.068] iteration 2290 : loss : 0.056753, loss_ce: 0.006758
[14:11:13.423] iteration 2300 : loss : 0.229232, loss_ce: 0.021176
[14:11:17.641] iteration 2310 : loss : 0.181838, loss_ce: 0.013132
[14:11:21.730] iteration 2320 : loss : 0.233769, loss_ce: 0.013253
[14:11:25.826] iteration 2330 : loss : 0.327791, loss_ce: 0.012734
[14:11:29.915] iteration 2340 : loss : 0.234289, loss_ce: 0.011469
[14:11:34.016] iteration 2350 : loss : 0.221686, loss_ce: 0.016178
[14:11:38.107] iteration 2360 : loss : 0.322680, loss_ce: 0.008037
[14:11:42.215] iteration 2370 : loss : 0.071448, loss_ce: 0.015458
[14:11:46.308] iteration 2380 : loss : 0.252107, loss_ce: 0.024947
[14:11:50.413] iteration 2390 : loss : 0.145626, loss_ce: 0.032198
[14:11:56.026] iteration 2400 : loss : 0.285581, loss_ce: 0.035975
[14:12:00.134] iteration 2410 : loss : 0.269011, loss_ce: 0.030473
[14:13:43.737] iteration 2420 : loss : 0.234854, loss_ce: 0.019628
[14:13:48.299] iteration 2430 : loss : 0.258244, loss_ce: 0.017942
[14:13:52.928] iteration 2440 : loss : 0.302133, loss_ce: 0.018165
[14:13:57.022] iteration 2450 : loss : 0.177769, loss_ce: 0.012712
[14:14:02.103] iteration 2460 : loss : 0.315858, loss_ce: 0.018843
[14:14:06.203] iteration 2470 : loss : 0.355809, loss_ce: 0.020948
[14:14:10.796] iteration 2480 : loss : 0.347543, loss_ce: 0.021392
[14:14:14.925] iteration 2490 : loss : 0.207778, loss_ce: 0.007585
[14:14:19.013] iteration 2500 : loss : 0.175595, loss_ce: 0.010935
[14:14:23.111] iteration 2510 : loss : 0.218801, loss_ce: 0.012345
[14:14:27.203] iteration 2520 : loss : 0.237356, loss_ce: 0.020104
[14:14:31.303] iteration 2530 : loss : 0.239072, loss_ce: 0.007223
[14:14:35.392] iteration 2540 : loss : 0.145346, loss_ce: 0.038697
[14:14:39.495] iteration 2550 : loss : 0.156535, loss_ce: 0.016255
[14:14:43.587] iteration 2560 : loss : 0.304074, loss_ce: 0.025768
[14:14:47.689] iteration 2570 : loss : 0.102147, loss_ce: 0.010867
[14:14:51.781] iteration 2580 : loss : 0.301192, loss_ce: 0.021509
[14:14:55.879] iteration 2590 : loss : 0.315173, loss_ce: 0.007037
[14:14:59.970] iteration 2600 : loss : 0.136858, loss_ce: 0.016830
[14:15:04.068] iteration 2610 : loss : 0.069456, loss_ce: 0.008724
[14:15:08.157] iteration 2620 : loss : 0.160059, loss_ce: 0.026349
[14:15:12.256] iteration 2630 : loss : 0.273555, loss_ce: 0.016363
[14:15:16.340] iteration 2640 : loss : 0.175914, loss_ce: 0.010264
[14:15:20.440] iteration 2650 : loss : 0.316200, loss_ce: 0.006256
[14:15:24.533] iteration 2660 : loss : 0.322896, loss_ce: 0.014267
[14:15:28.632] iteration 2670 : loss : 0.204109, loss_ce: 0.017728
[14:15:32.626] iteration 2680 : loss : 0.262800, loss_ce: 0.024157
[14:17:13.770] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_9.pth
[14:17:27.813] iteration 2690 : loss : 0.376092, loss_ce: 0.086557
[14:17:32.612] iteration 2700 : loss : 0.089801, loss_ce: 0.011418
[14:17:37.015] iteration 2710 : loss : 0.312156, loss_ce: 0.016118
[14:17:41.090] iteration 2720 : loss : 0.312688, loss_ce: 0.022794
[14:17:45.201] iteration 2730 : loss : 0.276802, loss_ce: 0.025399
[14:17:49.308] iteration 2740 : loss : 0.233248, loss_ce: 0.021082
[14:17:54.639] iteration 2750 : loss : 0.337924, loss_ce: 0.018046
[14:17:58.723] iteration 2760 : loss : 0.319718, loss_ce: 0.007726
[14:18:02.820] iteration 2770 : loss : 0.357134, loss_ce: 0.017967
[14:18:06.908] iteration 2780 : loss : 0.382726, loss_ce: 0.026539
[14:18:11.005] iteration 2790 : loss : 0.218089, loss_ce: 0.011519
[14:18:15.926] iteration 2800 : loss : 0.215864, loss_ce: 0.018333
[14:18:20.023] iteration 2810 : loss : 0.228100, loss_ce: 0.021895
[14:18:24.112] iteration 2820 : loss : 0.327326, loss_ce: 0.011435
[14:18:28.212] iteration 2830 : loss : 0.245301, loss_ce: 0.016745
[14:18:32.302] iteration 2840 : loss : 0.282119, loss_ce: 0.002352
[14:18:36.397] iteration 2850 : loss : 0.212800, loss_ce: 0.017666
[14:18:40.486] iteration 2860 : loss : 0.158348, loss_ce: 0.013517
[14:18:45.972] iteration 2870 : loss : 0.269239, loss_ce: 0.012363
[14:18:50.067] iteration 2880 : loss : 0.257317, loss_ce: 0.026119
[14:18:55.304] iteration 2890 : loss : 0.172710, loss_ce: 0.018598
[14:18:59.389] iteration 2900 : loss : 0.180273, loss_ce: 0.001607
[14:19:03.491] iteration 2910 : loss : 0.240011, loss_ce: 0.009268
[14:19:07.584] iteration 2920 : loss : 0.029105, loss_ce: 0.004400
[14:19:11.684] iteration 2930 : loss : 0.284287, loss_ce: 0.026200
[14:19:15.772] iteration 2940 : loss : 0.196018, loss_ce: 0.010053
[14:20:57.978] iteration 2950 : loss : 0.089302, loss_ce: 0.018803
[14:21:02.062] iteration 2960 : loss : 0.182937, loss_ce: 0.009020
[14:21:06.155] iteration 2970 : loss : 0.138998, loss_ce: 0.009372
[14:21:10.234] iteration 2980 : loss : 0.306760, loss_ce: 0.011936
[14:21:14.328] iteration 2990 : loss : 0.132999, loss_ce: 0.029729
[14:21:18.411] iteration 3000 : loss : 0.247941, loss_ce: 0.012187
[14:21:22.503] iteration 3010 : loss : 0.340666, loss_ce: 0.035168
[14:21:26.588] iteration 3020 : loss : 0.073474, loss_ce: 0.018080
[14:21:30.686] iteration 3030 : loss : 0.220440, loss_ce: 0.015078
[14:21:34.776] iteration 3040 : loss : 0.234094, loss_ce: 0.010511
[14:21:38.876] iteration 3050 : loss : 0.282723, loss_ce: 0.023711
[14:21:42.965] iteration 3060 : loss : 0.325596, loss_ce: 0.012965
[14:21:47.057] iteration 3070 : loss : 0.142919, loss_ce: 0.013959
[14:21:51.148] iteration 3080 : loss : 0.266831, loss_ce: 0.012879
[14:21:55.245] iteration 3090 : loss : 0.257663, loss_ce: 0.029084
[14:21:59.334] iteration 3100 : loss : 0.283555, loss_ce: 0.016226
[14:22:03.428] iteration 3110 : loss : 0.219007, loss_ce: 0.013649
[14:22:07.519] iteration 3120 : loss : 0.166349, loss_ce: 0.023893
[14:22:11.621] iteration 3130 : loss : 0.304282, loss_ce: 0.026835
[14:22:15.711] iteration 3140 : loss : 0.328626, loss_ce: 0.007752
[14:22:21.566] iteration 3150 : loss : 0.161788, loss_ce: 0.001976
[14:22:25.658] iteration 3160 : loss : 0.266418, loss_ce: 0.010673
[14:22:29.762] iteration 3170 : loss : 0.285482, loss_ce: 0.018673
[14:22:33.853] iteration 3180 : loss : 0.188581, loss_ce: 0.008965
[14:22:37.954] iteration 3190 : loss : 0.302667, loss_ce: 0.013659
[14:22:42.042] iteration 3200 : loss : 0.216794, loss_ce: 0.005399
[14:22:46.148] iteration 3210 : loss : 0.258452, loss_ce: 0.021402
[14:24:28.132] iteration 3220 : loss : 0.265593, loss_ce: 0.011899
[14:24:32.222] iteration 3230 : loss : 0.313837, loss_ce: 0.071034
[14:24:36.300] iteration 3240 : loss : 0.344386, loss_ce: 0.034548
[14:24:40.396] iteration 3250 : loss : 0.223120, loss_ce: 0.018250
[14:24:44.488] iteration 3260 : loss : 0.095426, loss_ce: 0.014936
[14:24:48.590] iteration 3270 : loss : 0.262564, loss_ce: 0.014272
[14:24:52.676] iteration 3280 : loss : 0.325686, loss_ce: 0.018752
[14:24:56.771] iteration 3290 : loss : 0.274887, loss_ce: 0.020835
[14:25:01.194] iteration 3300 : loss : 0.115560, loss_ce: 0.012773
[14:25:05.285] iteration 3310 : loss : 0.314151, loss_ce: 0.018205
[14:25:09.370] iteration 3320 : loss : 0.303851, loss_ce: 0.024296
[14:25:13.466] iteration 3330 : loss : 0.122108, loss_ce: 0.025328
[14:25:17.555] iteration 3340 : loss : 0.249028, loss_ce: 0.019601
[14:25:21.651] iteration 3350 : loss : 0.282921, loss_ce: 0.030580
[14:25:25.747] iteration 3360 : loss : 0.238960, loss_ce: 0.008506
[14:25:29.853] iteration 3370 : loss : 0.326990, loss_ce: 0.008689
[14:25:33.939] iteration 3380 : loss : 0.256615, loss_ce: 0.007617
[14:25:38.042] iteration 3390 : loss : 0.309713, loss_ce: 0.004135
[14:25:42.131] iteration 3400 : loss : 0.337110, loss_ce: 0.016744
[14:25:47.808] iteration 3410 : loss : 0.213585, loss_ce: 0.016597
[14:25:51.910] iteration 3420 : loss : 0.078069, loss_ce: 0.021007
[14:25:56.008] iteration 3430 : loss : 0.095106, loss_ce: 0.017306
[14:26:00.099] iteration 3440 : loss : 0.225647, loss_ce: 0.009723
[14:26:04.198] iteration 3450 : loss : 0.261004, loss_ce: 0.022902
[14:26:09.331] iteration 3460 : loss : 0.204206, loss_ce: 0.018273
[14:26:13.966] iteration 3470 : loss : 0.321872, loss_ce: 0.010192
[14:26:18.711] iteration 3480 : loss : 0.319539, loss_ce: 0.010009
[14:28:12.658] iteration 3490 : loss : 0.336777, loss_ce: 0.029375
[14:28:16.744] iteration 3500 : loss : 0.182265, loss_ce: 0.015296
[14:28:20.838] iteration 3510 : loss : 0.301225, loss_ce: 0.024158
[14:28:24.923] iteration 3520 : loss : 0.273872, loss_ce: 0.030987
[14:28:29.022] iteration 3530 : loss : 0.315158, loss_ce: 0.062168
[14:28:33.104] iteration 3540 : loss : 0.041211, loss_ce: 0.009513
[14:28:37.199] iteration 3550 : loss : 0.058845, loss_ce: 0.009658
[14:28:41.291] iteration 3560 : loss : 0.241693, loss_ce: 0.021454
[14:28:45.387] iteration 3570 : loss : 0.334801, loss_ce: 0.025598
[14:28:49.472] iteration 3580 : loss : 0.254879, loss_ce: 0.016681
[14:28:53.568] iteration 3590 : loss : 0.268207, loss_ce: 0.024524
[14:28:57.658] iteration 3600 : loss : 0.222996, loss_ce: 0.017864
[14:29:02.539] iteration 3610 : loss : 0.273842, loss_ce: 0.014945
[14:29:06.629] iteration 3620 : loss : 0.340630, loss_ce: 0.015200
[14:29:10.718] iteration 3630 : loss : 0.266406, loss_ce: 0.024405
[14:29:14.801] iteration 3640 : loss : 0.077722, loss_ce: 0.020851
[14:29:18.897] iteration 3650 : loss : 0.311550, loss_ce: 0.018581
[14:29:22.984] iteration 3660 : loss : 0.272139, loss_ce: 0.022119
[14:29:27.080] iteration 3670 : loss : 0.263629, loss_ce: 0.023102
[14:29:31.171] iteration 3680 : loss : 0.330840, loss_ce: 0.013845
[14:29:35.271] iteration 3690 : loss : 0.228219, loss_ce: 0.006829
[14:29:39.360] iteration 3700 : loss : 0.318235, loss_ce: 0.010502
[14:29:43.463] iteration 3710 : loss : 0.315229, loss_ce: 0.019083
[14:29:49.041] iteration 3720 : loss : 0.241728, loss_ce: 0.020811
[14:29:55.115] iteration 3730 : loss : 0.242801, loss_ce: 0.008465
[14:29:59.213] iteration 3740 : loss : 0.308851, loss_ce: 0.018048
[14:30:05.600] iteration 3750 : loss : 0.204552, loss_ce: 0.012756
[14:31:48.412] iteration 3760 : loss : 0.055548, loss_ce: 0.010933
[14:31:52.502] iteration 3770 : loss : 0.240013, loss_ce: 0.008545
[14:31:56.582] iteration 3780 : loss : 0.352499, loss_ce: 0.052911
[14:32:00.675] iteration 3790 : loss : 0.240780, loss_ce: 0.029736
[14:32:04.753] iteration 3800 : loss : 0.289654, loss_ce: 0.024698
[14:32:08.845] iteration 3810 : loss : 0.166187, loss_ce: 0.005893
[14:32:12.927] iteration 3820 : loss : 0.335489, loss_ce: 0.013743
[14:32:17.022] iteration 3830 : loss : 0.117021, loss_ce: 0.015011
[14:32:21.109] iteration 3840 : loss : 0.184463, loss_ce: 0.012130
[14:32:26.886] iteration 3850 : loss : 0.230273, loss_ce: 0.017246
[14:32:30.978] iteration 3860 : loss : 0.096058, loss_ce: 0.009261
[14:32:35.074] iteration 3870 : loss : 0.347600, loss_ce: 0.008852
[14:32:39.154] iteration 3880 : loss : 0.236048, loss_ce: 0.016361
[14:32:43.249] iteration 3890 : loss : 0.310213, loss_ce: 0.002556
[14:32:47.338] iteration 3900 : loss : 0.187041, loss_ce: 0.008294
[14:32:51.443] iteration 3910 : loss : 0.342444, loss_ce: 0.045295
[14:32:55.539] iteration 3920 : loss : 0.245216, loss_ce: 0.013342
[14:32:59.640] iteration 3930 : loss : 0.255776, loss_ce: 0.016024
[14:33:03.733] iteration 3940 : loss : 0.325887, loss_ce: 0.016076
[14:33:07.834] iteration 3950 : loss : 0.237439, loss_ce: 0.028177
[14:33:11.922] iteration 3960 : loss : 0.209840, loss_ce: 0.009920
[14:33:16.022] iteration 3970 : loss : 0.201214, loss_ce: 0.015816
[14:33:20.880] iteration 3980 : loss : 0.316201, loss_ce: 0.010045
[14:33:24.991] iteration 3990 : loss : 0.262736, loss_ce: 0.015870
[14:33:29.084] iteration 4000 : loss : 0.283109, loss_ce: 0.016845
[14:33:33.184] iteration 4010 : loss : 0.318661, loss_ce: 0.011050
[14:33:38.467] iteration 4020 : loss : 0.342341, loss_ce: 0.013863
[14:35:22.326] iteration 4030 : loss : 0.366000, loss_ce: 0.120063
[14:35:26.414] iteration 4040 : loss : 0.325769, loss_ce: 0.049942
[14:35:30.505] iteration 4050 : loss : 0.183467, loss_ce: 0.012288
[14:35:34.587] iteration 4060 : loss : 0.304033, loss_ce: 0.031626
[14:35:38.680] iteration 4070 : loss : 0.345209, loss_ce: 0.019263
[14:35:42.759] iteration 4080 : loss : 0.290697, loss_ce: 0.022529
[14:35:46.850] iteration 4090 : loss : 0.048589, loss_ce: 0.005428
[14:35:50.932] iteration 4100 : loss : 0.311273, loss_ce: 0.023565
[14:35:55.027] iteration 4110 : loss : 0.273907, loss_ce: 0.032497
[14:35:59.108] iteration 4120 : loss : 0.348568, loss_ce: 0.097260
[14:36:03.199] iteration 4130 : loss : 0.313829, loss_ce: 0.023532
[14:36:08.784] iteration 4140 : loss : 0.257434, loss_ce: 0.027719
[14:36:12.862] iteration 4150 : loss : 0.326297, loss_ce: 0.011959
[14:36:16.944] iteration 4160 : loss : 0.120228, loss_ce: 0.008312
[14:36:21.042] iteration 4170 : loss : 0.192414, loss_ce: 0.007009
[14:36:25.131] iteration 4180 : loss : 0.326427, loss_ce: 0.014221
[14:36:29.225] iteration 4190 : loss : 0.345044, loss_ce: 0.044139
[14:36:33.314] iteration 4200 : loss : 0.204923, loss_ce: 0.011459
[14:36:37.409] iteration 4210 : loss : 0.220256, loss_ce: 0.018677
[14:36:41.493] iteration 4220 : loss : 0.086180, loss_ce: 0.007918
[14:36:45.586] iteration 4230 : loss : 0.224135, loss_ce: 0.016302
[14:36:49.673] iteration 4240 : loss : 0.226836, loss_ce: 0.008701
[14:36:53.769] iteration 4250 : loss : 0.218832, loss_ce: 0.011974
[14:36:58.063] iteration 4260 : loss : 0.249861, loss_ce: 0.005480
[14:37:02.164] iteration 4270 : loss : 0.216894, loss_ce: 0.014878
[14:37:06.251] iteration 4280 : loss : 0.276013, loss_ce: 0.013028
[14:38:57.948] iteration 4290 : loss : 0.326328, loss_ce: 0.014275
[14:39:02.030] iteration 4300 : loss : 0.234311, loss_ce: 0.015182
[14:39:06.128] iteration 4310 : loss : 0.130737, loss_ce: 0.025760
[14:39:10.204] iteration 4320 : loss : 0.290650, loss_ce: 0.006968
[14:39:14.297] iteration 4330 : loss : 0.311512, loss_ce: 0.012853
[14:39:18.381] iteration 4340 : loss : 0.276251, loss_ce: 0.031594
[14:39:22.472] iteration 4350 : loss : 0.269365, loss_ce: 0.022493
[14:39:26.563] iteration 4360 : loss : 0.303514, loss_ce: 0.007307
[14:39:30.663] iteration 4370 : loss : 0.271313, loss_ce: 0.035714
[14:39:34.753] iteration 4380 : loss : 0.220492, loss_ce: 0.012965
[14:39:39.888] iteration 4390 : loss : 0.334881, loss_ce: 0.014498
[14:39:44.094] iteration 4400 : loss : 0.320608, loss_ce: 0.019898
[14:39:48.188] iteration 4410 : loss : 0.339934, loss_ce: 0.031709
[14:39:52.274] iteration 4420 : loss : 0.286843, loss_ce: 0.023560
[14:39:56.373] iteration 4430 : loss : 0.238266, loss_ce: 0.011892
[14:40:00.463] iteration 4440 : loss : 0.306432, loss_ce: 0.025659
[14:40:04.563] iteration 4450 : loss : 0.239566, loss_ce: 0.016983
[14:40:08.870] iteration 4460 : loss : 0.193028, loss_ce: 0.021697
[14:40:14.133] iteration 4470 : loss : 0.217034, loss_ce: 0.007366
[14:40:18.217] iteration 4480 : loss : 0.317494, loss_ce: 0.014737
[14:40:22.314] iteration 4490 : loss : 0.293018, loss_ce: 0.056970
[14:40:26.403] iteration 4500 : loss : 0.063854, loss_ce: 0.006766
[14:40:30.497] iteration 4510 : loss : 0.283042, loss_ce: 0.009565
[14:40:34.579] iteration 4520 : loss : 0.141705, loss_ce: 0.009062
[14:40:38.673] iteration 4530 : loss : 0.162522, loss_ce: 0.002981
[14:40:42.759] iteration 4540 : loss : 0.291942, loss_ce: 0.017923
[14:40:46.855] iteration 4550 : loss : 0.363765, loss_ce: 0.019309
[14:42:30.190] iteration 4560 : loss : 0.271699, loss_ce: 0.013507
[14:42:34.272] iteration 4570 : loss : 0.272880, loss_ce: 0.038660
[14:42:38.348] iteration 4580 : loss : 0.284733, loss_ce: 0.010656
[14:42:42.433] iteration 4590 : loss : 0.149310, loss_ce: 0.023939
[14:42:46.509] iteration 4600 : loss : 0.279581, loss_ce: 0.014353
[14:42:50.612] iteration 4610 : loss : 0.173421, loss_ce: 0.018035
[14:42:54.693] iteration 4620 : loss : 0.203606, loss_ce: 0.010710
[14:42:58.779] iteration 4630 : loss : 0.262956, loss_ce: 0.007550
[14:43:02.859] iteration 4640 : loss : 0.276404, loss_ce: 0.018531
[14:43:06.960] iteration 4650 : loss : 0.317831, loss_ce: 0.012691
[14:43:11.050] iteration 4660 : loss : 0.203474, loss_ce: 0.015031
[14:43:15.148] iteration 4670 : loss : 0.321058, loss_ce: 0.027063
[14:43:19.232] iteration 4680 : loss : 0.236015, loss_ce: 0.011738
[14:43:23.332] iteration 4690 : loss : 0.314623, loss_ce: 0.028714
[14:43:27.424] iteration 4700 : loss : 0.213084, loss_ce: 0.011095
[14:43:31.522] iteration 4710 : loss : 0.200543, loss_ce: 0.010139
[14:43:35.608] iteration 4720 : loss : 0.241952, loss_ce: 0.029081
[14:43:39.709] iteration 4730 : loss : 0.227893, loss_ce: 0.007407
[14:43:43.802] iteration 4740 : loss : 0.262476, loss_ce: 0.023049
[14:43:47.898] iteration 4750 : loss : 0.339387, loss_ce: 0.040637
[14:43:51.991] iteration 4760 : loss : 0.242689, loss_ce: 0.025234
[14:43:56.928] iteration 4770 : loss : 0.106747, loss_ce: 0.019008
[14:44:01.017] iteration 4780 : loss : 0.318153, loss_ce: 0.008603
[14:44:06.238] iteration 4790 : loss : 0.216370, loss_ce: 0.011989
[14:44:11.762] iteration 4800 : loss : 0.189772, loss_ce: 0.005475
[14:44:17.118] iteration 4810 : loss : 0.311916, loss_ce: 0.006762
[14:44:21.917] iteration 4820 : loss : 0.293826, loss_ce: 0.007523
[14:46:11.169] iteration 4830 : loss : 0.316830, loss_ce: 0.006384
[14:46:15.478] iteration 4840 : loss : 0.301765, loss_ce: 0.011626
[14:46:20.017] iteration 4850 : loss : 0.325256, loss_ce: 0.019856
[14:46:24.345] iteration 4860 : loss : 0.312978, loss_ce: 0.024815
[14:46:28.658] iteration 4870 : loss : 0.180329, loss_ce: 0.012200
[14:46:33.039] iteration 4880 : loss : 0.108657, loss_ce: 0.016404
[14:46:37.454] iteration 4890 : loss : 0.140094, loss_ce: 0.015324
[14:46:41.820] iteration 4900 : loss : 0.226084, loss_ce: 0.018673
[14:46:46.114] iteration 4910 : loss : 0.184738, loss_ce: 0.004708
[14:46:50.238] iteration 4920 : loss : 0.222733, loss_ce: 0.012389
[14:46:54.326] iteration 4930 : loss : 0.247212, loss_ce: 0.011427
[14:46:58.516] iteration 4940 : loss : 0.222859, loss_ce: 0.014363
[14:47:02.840] iteration 4950 : loss : 0.328735, loss_ce: 0.021132
[14:47:07.200] iteration 4960 : loss : 0.320249, loss_ce: 0.010415
[14:47:11.597] iteration 4970 : loss : 0.303963, loss_ce: 0.017938
[14:47:15.933] iteration 4980 : loss : 0.201787, loss_ce: 0.013696
[14:47:20.288] iteration 4990 : loss : 0.305768, loss_ce: 0.012338
[14:47:24.661] iteration 5000 : loss : 0.259526, loss_ce: 0.014385
[14:47:28.998] iteration 5010 : loss : 0.275543, loss_ce: 0.018725
[14:47:33.169] iteration 5020 : loss : 0.239683, loss_ce: 0.006539
[14:47:37.266] iteration 5030 : loss : 0.263142, loss_ce: 0.012891
[14:47:41.668] iteration 5040 : loss : 0.077556, loss_ce: 0.010030
[14:47:45.764] iteration 5050 : loss : 0.223947, loss_ce: 0.019284
[14:47:49.857] iteration 5060 : loss : 0.210347, loss_ce: 0.010679
[14:47:53.950] iteration 5070 : loss : 0.227495, loss_ce: 0.007551
[14:47:58.039] iteration 5080 : loss : 0.311976, loss_ce: 0.011928
[14:48:02.134] iteration 5090 : loss : 0.264428, loss_ce: 0.017934
[14:49:54.111] iteration 5100 : loss : 0.225520, loss_ce: 0.015733
[14:49:58.194] iteration 5110 : loss : 0.265636, loss_ce: 0.015770
[14:50:02.277] iteration 5120 : loss : 0.232719, loss_ce: 0.013563
[14:50:06.367] iteration 5130 : loss : 0.340823, loss_ce: 0.019353
[14:50:10.445] iteration 5140 : loss : 0.311887, loss_ce: 0.012596
[14:50:14.540] iteration 5150 : loss : 0.228690, loss_ce: 0.021382
[14:50:18.624] iteration 5160 : loss : 0.312570, loss_ce: 0.004864
[14:50:22.720] iteration 5170 : loss : 0.333395, loss_ce: 0.026102
[14:50:26.810] iteration 5180 : loss : 0.284508, loss_ce: 0.031211
[14:50:30.908] iteration 5190 : loss : 0.227729, loss_ce: 0.010848
[14:50:34.997] iteration 5200 : loss : 0.240851, loss_ce: 0.013200
[14:50:39.098] iteration 5210 : loss : 0.143981, loss_ce: 0.021150
[14:50:43.190] iteration 5220 : loss : 0.422726, loss_ce: 0.100125
[14:50:47.288] iteration 5230 : loss : 0.232744, loss_ce: 0.019823
[14:50:51.378] iteration 5240 : loss : 0.284005, loss_ce: 0.018052
[14:50:55.479] iteration 5250 : loss : 0.319098, loss_ce: 0.015167
[14:50:59.570] iteration 5260 : loss : 0.268618, loss_ce: 0.028655
[14:51:03.671] iteration 5270 : loss : 0.135424, loss_ce: 0.019793
[14:51:07.764] iteration 5280 : loss : 0.277716, loss_ce: 0.089591
[14:51:11.867] iteration 5290 : loss : 0.239953, loss_ce: 0.021405
[14:51:16.430] iteration 5300 : loss : 0.358528, loss_ce: 0.044068
[14:51:22.007] iteration 5310 : loss : 0.249892, loss_ce: 0.018774
[14:51:26.101] iteration 5320 : loss : 0.353819, loss_ce: 0.041189
[14:51:30.197] iteration 5330 : loss : 0.365731, loss_ce: 0.053478
[14:51:34.287] iteration 5340 : loss : 0.354157, loss_ce: 0.144209
[14:51:38.391] iteration 5350 : loss : 0.466931, loss_ce: 0.047157
[14:51:42.380] iteration 5360 : loss : 0.331960, loss_ce: 0.045200
[14:53:12.158] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_19.pth
[14:53:26.233] iteration 5370 : loss : 0.439330, loss_ce: 0.043241
[14:53:30.316] iteration 5380 : loss : 0.182413, loss_ce: 0.007755
[14:53:34.411] iteration 5390 : loss : 0.279148, loss_ce: 0.014708
[14:53:39.005] iteration 5400 : loss : 0.220684, loss_ce: 0.009625
[14:53:43.612] iteration 5410 : loss : 0.176691, loss_ce: 0.005401
[14:53:47.696] iteration 5420 : loss : 0.306783, loss_ce: 0.036886
[14:53:52.171] iteration 5430 : loss : 0.304947, loss_ce: 0.021241
[14:53:56.266] iteration 5440 : loss : 0.156259, loss_ce: 0.012516
[14:54:00.353] iteration 5450 : loss : 0.251188, loss_ce: 0.013250
[14:54:04.431] iteration 5460 : loss : 0.219446, loss_ce: 0.008042
[14:54:08.523] iteration 5470 : loss : 0.236151, loss_ce: 0.011994
[14:54:12.610] iteration 5480 : loss : 0.237436, loss_ce: 0.012391
[14:54:16.707] iteration 5490 : loss : 0.252842, loss_ce: 0.025793
[14:54:20.794] iteration 5500 : loss : 0.119477, loss_ce: 0.006798
[14:54:24.892] iteration 5510 : loss : 0.225096, loss_ce: 0.010212
[14:54:28.984] iteration 5520 : loss : 0.113316, loss_ce: 0.006418
[14:54:33.081] iteration 5530 : loss : 0.315838, loss_ce: 0.005853
[14:54:38.508] iteration 5540 : loss : 0.135351, loss_ce: 0.016722
[14:54:43.200] iteration 5550 : loss : 0.285193, loss_ce: 0.033080
[14:54:47.295] iteration 5560 : loss : 0.336513, loss_ce: 0.018176
[14:54:51.385] iteration 5570 : loss : 0.294076, loss_ce: 0.027093
[14:54:55.469] iteration 5580 : loss : 0.264303, loss_ce: 0.022687
[14:54:59.563] iteration 5590 : loss : 0.156769, loss_ce: 0.003745
[14:55:03.651] iteration 5600 : loss : 0.303575, loss_ce: 0.010260
[14:55:08.551] iteration 5610 : loss : 0.311591, loss_ce: 0.021354
[14:55:12.642] iteration 5620 : loss : 0.231295, loss_ce: 0.009413
[14:56:54.987] iteration 5630 : loss : 0.236278, loss_ce: 0.011501
[14:57:00.352] iteration 5640 : loss : 0.296312, loss_ce: 0.018845
[14:57:04.446] iteration 5650 : loss : 0.252726, loss_ce: 0.008661
[14:57:08.515] iteration 5660 : loss : 0.241697, loss_ce: 0.014326
[14:57:12.608] iteration 5670 : loss : 0.286143, loss_ce: 0.026950
[14:57:16.694] iteration 5680 : loss : 0.343539, loss_ce: 0.020342
[14:57:20.786] iteration 5690 : loss : 0.124995, loss_ce: 0.006381
[14:57:24.871] iteration 5700 : loss : 0.340380, loss_ce: 0.024047
[14:57:28.967] iteration 5710 : loss : 0.252245, loss_ce: 0.021426
[14:57:33.059] iteration 5720 : loss : 0.094983, loss_ce: 0.010449
[14:57:37.156] iteration 5730 : loss : 0.294590, loss_ce: 0.009284
[14:57:42.006] iteration 5740 : loss : 0.356674, loss_ce: 0.017853
[14:57:46.104] iteration 5750 : loss : 0.277022, loss_ce: 0.014164
[14:57:50.185] iteration 5760 : loss : 0.065988, loss_ce: 0.014041
[14:57:54.278] iteration 5770 : loss : 0.213055, loss_ce: 0.012449
[14:57:59.968] iteration 5780 : loss : 0.323932, loss_ce: 0.023101
[14:58:04.165] iteration 5790 : loss : 0.218009, loss_ce: 0.006655
[14:58:08.249] iteration 5800 : loss : 0.220154, loss_ce: 0.015800
[14:58:12.348] iteration 5810 : loss : 0.310014, loss_ce: 0.024296
[14:58:16.434] iteration 5820 : loss : 0.218068, loss_ce: 0.006957
[14:58:20.535] iteration 5830 : loss : 0.219162, loss_ce: 0.060230
[14:58:24.624] iteration 5840 : loss : 0.277369, loss_ce: 0.016335
[14:58:28.725] iteration 5850 : loss : 0.264579, loss_ce: 0.015828
[14:58:32.811] iteration 5860 : loss : 0.274190, loss_ce: 0.015169
[14:58:36.909] iteration 5870 : loss : 0.243874, loss_ce: 0.014328
[14:58:42.395] iteration 5880 : loss : 0.270320, loss_ce: 0.021764
[14:58:48.014] iteration 5890 : loss : 0.313397, loss_ce: 0.006713
[15:00:43.078] iteration 5900 : loss : 0.269895, loss_ce: 0.023922
[15:00:47.173] iteration 5910 : loss : 0.061021, loss_ce: 0.013952
[15:00:51.250] iteration 5920 : loss : 0.375168, loss_ce: 0.044355
[15:00:55.340] iteration 5930 : loss : 0.332852, loss_ce: 0.033788
[15:00:59.794] iteration 5940 : loss : 0.165443, loss_ce: 0.002344
[15:01:04.951] iteration 5950 : loss : 0.348371, loss_ce: 0.020850
[15:01:11.321] iteration 5960 : loss : 0.324551, loss_ce: 0.010851
[15:01:16.496] iteration 5970 : loss : 0.320358, loss_ce: 0.003269
[15:01:22.595] iteration 5980 : loss : 0.217228, loss_ce: 0.017316
[15:01:26.682] iteration 5990 : loss : 0.281810, loss_ce: 0.010776
[15:01:30.758] iteration 6000 : loss : 0.196976, loss_ce: 0.007571
[15:01:34.844] iteration 6010 : loss : 0.288670, loss_ce: 0.005288
[15:01:39.094] iteration 6020 : loss : 0.249515, loss_ce: 0.015863
[15:01:43.630] iteration 6030 : loss : 0.235002, loss_ce: 0.012916
[15:01:48.051] iteration 6040 : loss : 0.256192, loss_ce: 0.020769
[15:01:52.438] iteration 6050 : loss : 0.181274, loss_ce: 0.006591
[15:01:56.796] iteration 6060 : loss : 0.074590, loss_ce: 0.016131
[15:02:01.162] iteration 6070 : loss : 0.269458, loss_ce: 0.008397
[15:02:05.542] iteration 6080 : loss : 0.252814, loss_ce: 0.013295
[15:02:09.785] iteration 6090 : loss : 0.100442, loss_ce: 0.013499
[15:02:13.871] iteration 6100 : loss : 0.189033, loss_ce: 0.007554
[15:02:17.971] iteration 6110 : loss : 0.045467, loss_ce: 0.009106
[15:02:22.055] iteration 6120 : loss : 0.269781, loss_ce: 0.018069
[15:02:26.152] iteration 6130 : loss : 0.240414, loss_ce: 0.020497
[15:02:30.239] iteration 6140 : loss : 0.205314, loss_ce: 0.014577
[15:02:34.337] iteration 6150 : loss : 0.220743, loss_ce: 0.012527
[15:02:38.423] iteration 6160 : loss : 0.154427, loss_ce: 0.004071
[15:04:31.310] iteration 6170 : loss : 0.339846, loss_ce: 0.037716
[15:04:37.127] iteration 6180 : loss : 0.325336, loss_ce: 0.014747
[15:04:42.136] iteration 6190 : loss : 0.304682, loss_ce: 0.012046
[15:04:46.213] iteration 6200 : loss : 0.322366, loss_ce: 0.006506
[15:04:50.303] iteration 6210 : loss : 0.231169, loss_ce: 0.013138
[15:04:55.563] iteration 6220 : loss : 0.303063, loss_ce: 0.035413
[15:04:59.664] iteration 6230 : loss : 0.211894, loss_ce: 0.013651
[15:05:03.757] iteration 6240 : loss : 0.328015, loss_ce: 0.037618
[15:05:07.851] iteration 6250 : loss : 0.107081, loss_ce: 0.005377
[15:05:11.932] iteration 6260 : loss : 0.227437, loss_ce: 0.011435
[15:05:16.034] iteration 6270 : loss : 0.236493, loss_ce: 0.032693
[15:05:20.120] iteration 6280 : loss : 0.234293, loss_ce: 0.012603
[15:05:24.226] iteration 6290 : loss : 0.233332, loss_ce: 0.025563
[15:05:30.094] iteration 6300 : loss : 0.251303, loss_ce: 0.007415
[15:05:34.506] iteration 6310 : loss : 0.240982, loss_ce: 0.013644
[15:05:38.594] iteration 6320 : loss : 0.042865, loss_ce: 0.006910
[15:05:42.692] iteration 6330 : loss : 0.276026, loss_ce: 0.009451
[15:05:46.778] iteration 6340 : loss : 0.139618, loss_ce: 0.007849
[15:05:50.881] iteration 6350 : loss : 0.305226, loss_ce: 0.034918
[15:05:54.970] iteration 6360 : loss : 0.132747, loss_ce: 0.012686
[15:06:00.478] iteration 6370 : loss : 0.208462, loss_ce: 0.009056
[15:06:06.023] iteration 6380 : loss : 0.267137, loss_ce: 0.018731
[15:06:10.128] iteration 6390 : loss : 0.290260, loss_ce: 0.014251
[15:06:14.217] iteration 6400 : loss : 0.056109, loss_ce: 0.006547
[15:06:18.319] iteration 6410 : loss : 0.212592, loss_ce: 0.006580
[15:06:22.417] iteration 6420 : loss : 0.262499, loss_ce: 0.005285
[15:06:26.519] iteration 6430 : loss : 0.242713, loss_ce: 0.013676
[15:08:41.056] iteration 6440 : loss : 0.262894, loss_ce: 0.020362
[15:08:45.143] iteration 6450 : loss : 0.343534, loss_ce: 0.015167
[15:08:49.208] iteration 6460 : loss : 0.168295, loss_ce: 0.011159
[15:08:53.290] iteration 6470 : loss : 0.135633, loss_ce: 0.015943
[15:08:57.366] iteration 6480 : loss : 0.299324, loss_ce: 0.024910
[15:09:01.453] iteration 6490 : loss : 0.087115, loss_ce: 0.009572
[15:09:05.534] iteration 6500 : loss : 0.312366, loss_ce: 0.009495
[15:09:09.631] iteration 6510 : loss : 0.243181, loss_ce: 0.007483
[15:09:13.717] iteration 6520 : loss : 0.304072, loss_ce: 0.018548
[15:09:17.815] iteration 6530 : loss : 0.222407, loss_ce: 0.013131
[15:09:21.901] iteration 6540 : loss : 0.278662, loss_ce: 0.043841
[15:09:25.997] iteration 6550 : loss : 0.118994, loss_ce: 0.014807
[15:09:30.093] iteration 6560 : loss : 0.323373, loss_ce: 0.006360
[15:09:34.197] iteration 6570 : loss : 0.164227, loss_ce: 0.017261
[15:09:38.285] iteration 6580 : loss : 0.198506, loss_ce: 0.009108
[15:09:42.391] iteration 6590 : loss : 0.236709, loss_ce: 0.016207
[15:09:46.478] iteration 6600 : loss : 0.168424, loss_ce: 0.012595
[15:09:50.583] iteration 6610 : loss : 0.194152, loss_ce: 0.009966
[15:09:54.672] iteration 6620 : loss : 0.265378, loss_ce: 0.029253
[15:09:58.776] iteration 6630 : loss : 0.241084, loss_ce: 0.008053
[15:10:02.862] iteration 6640 : loss : 0.247431, loss_ce: 0.018542
[15:10:06.967] iteration 6650 : loss : 0.216897, loss_ce: 0.014464
[15:10:11.053] iteration 6660 : loss : 0.302530, loss_ce: 0.001453
[15:10:15.151] iteration 6670 : loss : 0.270604, loss_ce: 0.013295
[15:10:19.241] iteration 6680 : loss : 0.224391, loss_ce: 0.014083
[15:10:23.341] iteration 6690 : loss : 0.318743, loss_ce: 0.006087
[15:10:27.890] iteration 6700 : loss : 0.082909, loss_ce: 0.010290
[15:12:48.743] iteration 6710 : loss : 0.282268, loss_ce: 0.036487
[15:12:52.828] iteration 6720 : loss : 0.310569, loss_ce: 0.038699
[15:12:56.908] iteration 6730 : loss : 0.340896, loss_ce: 0.015286
[15:13:00.985] iteration 6740 : loss : 0.302611, loss_ce: 0.017662
[15:13:05.074] iteration 6750 : loss : 0.272181, loss_ce: 0.014130
[15:13:09.157] iteration 6760 : loss : 0.075076, loss_ce: 0.004287
[15:13:13.246] iteration 6770 : loss : 0.338896, loss_ce: 0.018149
[15:13:17.328] iteration 6780 : loss : 0.117631, loss_ce: 0.014851
[15:13:21.603] iteration 6790 : loss : 0.086333, loss_ce: 0.006319
[15:13:27.847] iteration 6800 : loss : 0.030239, loss_ce: 0.002153
[15:13:31.952] iteration 6810 : loss : 0.047707, loss_ce: 0.003430
[15:13:36.038] iteration 6820 : loss : 0.290118, loss_ce: 0.019084
[15:13:40.128] iteration 6830 : loss : 0.335694, loss_ce: 0.039571
[15:13:44.206] iteration 6840 : loss : 0.230073, loss_ce: 0.013416
[15:13:48.302] iteration 6850 : loss : 0.230605, loss_ce: 0.013210
[15:13:52.388] iteration 6860 : loss : 0.191359, loss_ce: 0.034686
[15:13:56.490] iteration 6870 : loss : 0.166098, loss_ce: 0.003951
[15:14:02.508] iteration 6880 : loss : 0.262455, loss_ce: 0.027183
[15:14:06.961] iteration 6890 : loss : 0.221296, loss_ce: 0.013018
[15:14:11.040] iteration 6900 : loss : 0.232108, loss_ce: 0.009240
[15:14:15.131] iteration 6910 : loss : 0.072294, loss_ce: 0.008645
[15:14:19.214] iteration 6920 : loss : 0.241899, loss_ce: 0.008830
[15:14:23.313] iteration 6930 : loss : 0.164516, loss_ce: 0.004467
[15:14:27.402] iteration 6940 : loss : 0.207277, loss_ce: 0.014844
[15:14:31.505] iteration 6950 : loss : 0.310423, loss_ce: 0.004971
[15:14:35.594] iteration 6960 : loss : 0.063235, loss_ce: 0.005382
[15:16:55.253] iteration 6970 : loss : 0.125766, loss_ce: 0.028287
[15:16:59.653] iteration 6980 : loss : 0.309305, loss_ce: 0.008992
[15:17:03.797] iteration 6990 : loss : 0.279483, loss_ce: 0.026266
[15:17:07.934] iteration 7000 : loss : 0.229265, loss_ce: 0.023755
[15:17:12.091] iteration 7010 : loss : 0.242510, loss_ce: 0.021571
[15:17:16.231] iteration 7020 : loss : 0.329698, loss_ce: 0.020412
[15:17:20.386] iteration 7030 : loss : 0.221476, loss_ce: 0.012089
[15:17:24.528] iteration 7040 : loss : 0.235905, loss_ce: 0.010349
[15:17:28.686] iteration 7050 : loss : 0.322434, loss_ce: 0.024900
[15:17:32.879] iteration 7060 : loss : 0.239484, loss_ce: 0.046443
[15:17:37.094] iteration 7070 : loss : 0.215651, loss_ce: 0.017680
[15:17:41.244] iteration 7080 : loss : 0.203487, loss_ce: 0.006985
[15:17:45.405] iteration 7090 : loss : 0.226801, loss_ce: 0.014018
[15:17:49.614] iteration 7100 : loss : 0.293496, loss_ce: 0.008627
[15:17:53.878] iteration 7110 : loss : 0.228315, loss_ce: 0.013720
[15:17:58.036] iteration 7120 : loss : 0.239786, loss_ce: 0.032408
[15:18:02.288] iteration 7130 : loss : 0.316219, loss_ce: 0.008824
[15:18:07.261] iteration 7140 : loss : 0.299701, loss_ce: 0.008117
[15:18:11.480] iteration 7150 : loss : 0.197523, loss_ce: 0.006115
[15:18:15.600] iteration 7160 : loss : 0.192106, loss_ce: 0.008789
[15:18:19.729] iteration 7170 : loss : 0.338196, loss_ce: 0.008139
[15:18:23.848] iteration 7180 : loss : 0.065810, loss_ce: 0.006200
[15:18:27.972] iteration 7190 : loss : 0.226039, loss_ce: 0.013326
[15:18:32.090] iteration 7200 : loss : 0.221652, loss_ce: 0.014416
[15:18:36.226] iteration 7210 : loss : 0.219308, loss_ce: 0.010747
[15:18:40.342] iteration 7220 : loss : 0.303591, loss_ce: 0.010673
[15:18:44.475] iteration 7230 : loss : 0.208655, loss_ce: 0.009816
[15:20:29.584] iteration 7240 : loss : 0.271325, loss_ce: 0.007801
[15:20:33.698] iteration 7250 : loss : 0.320679, loss_ce: 0.013016
[15:20:37.808] iteration 7260 : loss : 0.236029, loss_ce: 0.010310
[15:20:41.922] iteration 7270 : loss : 0.276262, loss_ce: 0.017862
[15:20:46.025] iteration 7280 : loss : 0.274701, loss_ce: 0.025432
[15:20:50.138] iteration 7290 : loss : 0.328009, loss_ce: 0.040011
[15:20:54.243] iteration 7300 : loss : 0.304774, loss_ce: 0.024140
[15:20:58.360] iteration 7310 : loss : 0.058162, loss_ce: 0.005466
[15:21:02.469] iteration 7320 : loss : 0.277189, loss_ce: 0.007287
[15:21:06.586] iteration 7330 : loss : 0.232330, loss_ce: 0.019434
[15:21:10.696] iteration 7340 : loss : 0.059675, loss_ce: 0.005763
[15:21:14.817] iteration 7350 : loss : 0.103626, loss_ce: 0.009729
[15:21:18.925] iteration 7360 : loss : 0.235270, loss_ce: 0.018463
[15:21:23.046] iteration 7370 : loss : 0.166063, loss_ce: 0.011311
[15:21:27.158] iteration 7380 : loss : 0.092643, loss_ce: 0.005617
[15:21:31.349] iteration 7390 : loss : 0.198654, loss_ce: 0.006785
[15:21:35.533] iteration 7400 : loss : 0.168658, loss_ce: 0.008275
[15:21:39.931] iteration 7410 : loss : 0.300139, loss_ce: 0.031589
[15:21:44.195] iteration 7420 : loss : 0.203639, loss_ce: 0.006196
[15:21:48.297] iteration 7430 : loss : 0.263501, loss_ce: 0.017950
[15:21:52.387] iteration 7440 : loss : 0.224405, loss_ce: 0.009525
[15:21:56.488] iteration 7450 : loss : 0.222438, loss_ce: 0.013945
[15:22:00.580] iteration 7460 : loss : 0.209281, loss_ce: 0.007853
[15:22:04.680] iteration 7470 : loss : 0.170525, loss_ce: 0.004454
[15:22:08.823] iteration 7480 : loss : 0.218830, loss_ce: 0.014091
[15:22:12.985] iteration 7490 : loss : 0.184037, loss_ce: 0.003247
[15:22:17.372] iteration 7500 : loss : 0.212969, loss_ce: 0.014971
[15:24:35.438] iteration 7510 : loss : 0.303196, loss_ce: 0.008980
[15:24:39.583] iteration 7520 : loss : 0.302507, loss_ce: 0.018057
[15:24:43.828] iteration 7530 : loss : 0.325393, loss_ce: 0.008368
[15:24:48.015] iteration 7540 : loss : 0.276444, loss_ce: 0.021784
[15:24:52.140] iteration 7550 : loss : 0.337650, loss_ce: 0.020212
[15:24:56.250] iteration 7560 : loss : 0.224172, loss_ce: 0.019001
[15:25:00.424] iteration 7570 : loss : 0.215363, loss_ce: 0.014269
[15:25:04.540] iteration 7580 : loss : 0.311776, loss_ce: 0.016008
[15:25:08.669] iteration 7590 : loss : 0.116058, loss_ce: 0.008298
[15:25:12.784] iteration 7600 : loss : 0.270050, loss_ce: 0.014508
[15:25:16.920] iteration 7610 : loss : 0.311057, loss_ce: 0.012858
[15:25:21.034] iteration 7620 : loss : 0.289448, loss_ce: 0.003956
[15:25:25.169] iteration 7630 : loss : 0.214979, loss_ce: 0.015528
[15:25:29.291] iteration 7640 : loss : 0.291318, loss_ce: 0.027155
[15:25:33.423] iteration 7650 : loss : 0.275120, loss_ce: 0.029639
[15:25:37.548] iteration 7660 : loss : 0.328746, loss_ce: 0.009752
[15:25:41.682] iteration 7670 : loss : 0.330481, loss_ce: 0.026597
[15:25:45.803] iteration 7680 : loss : 0.090746, loss_ce: 0.026554
[15:25:49.941] iteration 7690 : loss : 0.280080, loss_ce: 0.019439
[15:25:54.066] iteration 7700 : loss : 0.226568, loss_ce: 0.023650
[15:25:58.205] iteration 7710 : loss : 0.321827, loss_ce: 0.037232
[15:26:02.330] iteration 7720 : loss : 0.242814, loss_ce: 0.020024
[15:26:06.464] iteration 7730 : loss : 0.316028, loss_ce: 0.012016
[15:26:10.587] iteration 7740 : loss : 0.228120, loss_ce: 0.009421
[15:26:14.724] iteration 7750 : loss : 0.327715, loss_ce: 0.011913
[15:26:18.851] iteration 7760 : loss : 0.292852, loss_ce: 0.011941
[15:26:22.987] iteration 7770 : loss : 0.070698, loss_ce: 0.017051
[15:28:26.550] iteration 7780 : loss : 0.294711, loss_ce: 0.019247
[15:28:30.663] iteration 7790 : loss : 0.260683, loss_ce: 0.025529
[15:28:34.772] iteration 7800 : loss : 0.195790, loss_ce: 0.011588
[15:28:38.897] iteration 7810 : loss : 0.116457, loss_ce: 0.032244
[15:28:43.009] iteration 7820 : loss : 0.259859, loss_ce: 0.021155
[15:28:47.137] iteration 7830 : loss : 0.299901, loss_ce: 0.016187
[15:28:51.253] iteration 7840 : loss : 0.250605, loss_ce: 0.012491
[15:28:55.380] iteration 7850 : loss : 0.225417, loss_ce: 0.019757
[15:28:59.498] iteration 7860 : loss : 0.290879, loss_ce: 0.035937
[15:29:03.633] iteration 7870 : loss : 0.288578, loss_ce: 0.016385
[15:29:07.751] iteration 7880 : loss : 0.227509, loss_ce: 0.018345
[15:29:11.885] iteration 7890 : loss : 0.079856, loss_ce: 0.009136
[15:29:16.006] iteration 7900 : loss : 0.185648, loss_ce: 0.006134
[15:29:20.138] iteration 7910 : loss : 0.330876, loss_ce: 0.018993
[15:29:24.262] iteration 7920 : loss : 0.043280, loss_ce: 0.006984
[15:29:28.394] iteration 7930 : loss : 0.208772, loss_ce: 0.006148
[15:29:32.511] iteration 7940 : loss : 0.256212, loss_ce: 0.016520
[15:29:36.646] iteration 7950 : loss : 0.187689, loss_ce: 0.005821
[15:29:40.769] iteration 7960 : loss : 0.282088, loss_ce: 0.021701
[15:29:44.907] iteration 7970 : loss : 0.212300, loss_ce: 0.005638
[15:29:49.031] iteration 7980 : loss : 0.230809, loss_ce: 0.016803
[15:29:53.168] iteration 7990 : loss : 0.301757, loss_ce: 0.005039
[15:29:57.291] iteration 8000 : loss : 0.350501, loss_ce: 0.013449
[15:30:01.427] iteration 8010 : loss : 0.211104, loss_ce: 0.020755
[15:30:05.550] iteration 8020 : loss : 0.267980, loss_ce: 0.008245
[15:30:09.686] iteration 8030 : loss : 0.029918, loss_ce: 0.007011
[15:30:13.715] iteration 8040 : loss : 0.283604, loss_ce: 0.029779
[15:32:02.491] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_29.pth
[15:32:17.515] iteration 8050 : loss : 0.225614, loss_ce: 0.007748
[15:32:21.623] iteration 8060 : loss : 0.259204, loss_ce: 0.013258
[15:32:25.742] iteration 8070 : loss : 0.300075, loss_ce: 0.011301
[15:32:29.850] iteration 8080 : loss : 0.240405, loss_ce: 0.012561
[15:32:33.969] iteration 8090 : loss : 0.220360, loss_ce: 0.010895
[15:32:38.084] iteration 8100 : loss : 0.315329, loss_ce: 0.002535
[15:32:42.207] iteration 8110 : loss : 0.286111, loss_ce: 0.036119
[15:32:46.322] iteration 8120 : loss : 0.272289, loss_ce: 0.012823
[15:32:50.448] iteration 8130 : loss : 0.253590, loss_ce: 0.016809
[15:32:54.565] iteration 8140 : loss : 0.330815, loss_ce: 0.011542
[15:32:58.693] iteration 8150 : loss : 0.142475, loss_ce: 0.016114
[15:33:02.812] iteration 8160 : loss : 0.275123, loss_ce: 0.017477
[15:33:06.943] iteration 8170 : loss : 0.294128, loss_ce: 0.004992
[15:33:11.062] iteration 8180 : loss : 0.217510, loss_ce: 0.017794
[15:33:15.197] iteration 8190 : loss : 0.316370, loss_ce: 0.017531
[15:33:19.316] iteration 8200 : loss : 0.252189, loss_ce: 0.009042
[15:33:23.451] iteration 8210 : loss : 0.239340, loss_ce: 0.020308
[15:33:27.574] iteration 8220 : loss : 0.258675, loss_ce: 0.013895
[15:33:31.708] iteration 8230 : loss : 0.197057, loss_ce: 0.010657
[15:33:35.830] iteration 8240 : loss : 0.213201, loss_ce: 0.002747
[15:33:39.960] iteration 8250 : loss : 0.201734, loss_ce: 0.015877
[15:33:44.082] iteration 8260 : loss : 0.098637, loss_ce: 0.024767
[15:33:48.219] iteration 8270 : loss : 0.259624, loss_ce: 0.008367
[15:33:52.344] iteration 8280 : loss : 0.219260, loss_ce: 0.016683
[15:33:56.478] iteration 8290 : loss : 0.197577, loss_ce: 0.006978
[15:34:00.603] iteration 8300 : loss : 0.313300, loss_ce: 0.013652
[15:36:16.309] iteration 8310 : loss : 0.322136, loss_ce: 0.024755
[15:36:20.678] iteration 8320 : loss : 0.220408, loss_ce: 0.018852
[15:36:24.797] iteration 8330 : loss : 0.222142, loss_ce: 0.024118
[15:36:28.909] iteration 8340 : loss : 0.194617, loss_ce: 0.031926
[15:36:33.036] iteration 8350 : loss : 0.062300, loss_ce: 0.010827
[15:36:37.150] iteration 8360 : loss : 0.329340, loss_ce: 0.014746
[15:36:41.278] iteration 8370 : loss : 0.321879, loss_ce: 0.007131
[15:36:45.397] iteration 8380 : loss : 0.199295, loss_ce: 0.010773
[15:36:49.531] iteration 8390 : loss : 0.243721, loss_ce: 0.018433
[15:36:53.647] iteration 8400 : loss : 0.198073, loss_ce: 0.012635
[15:36:57.777] iteration 8410 : loss : 0.147710, loss_ce: 0.007561
[15:37:01.896] iteration 8420 : loss : 0.305095, loss_ce: 0.021748
[15:37:06.033] iteration 8430 : loss : 0.187301, loss_ce: 0.005572
[15:37:10.155] iteration 8440 : loss : 0.029484, loss_ce: 0.005327
[15:37:14.292] iteration 8450 : loss : 0.222728, loss_ce: 0.005903
[15:37:18.418] iteration 8460 : loss : 0.105170, loss_ce: 0.010396
[15:37:22.557] iteration 8470 : loss : 0.286914, loss_ce: 0.019736
[15:37:26.683] iteration 8480 : loss : 0.321079, loss_ce: 0.009481
[15:37:30.818] iteration 8490 : loss : 0.177699, loss_ce: 0.010106
[15:37:34.946] iteration 8500 : loss : 0.250858, loss_ce: 0.015410
[15:37:39.082] iteration 8510 : loss : 0.029852, loss_ce: 0.003935
[15:37:43.207] iteration 8520 : loss : 0.221414, loss_ce: 0.007021
[15:37:47.346] iteration 8530 : loss : 0.076297, loss_ce: 0.005979
[15:37:51.470] iteration 8540 : loss : 0.179850, loss_ce: 0.026039
[15:37:55.606] iteration 8550 : loss : 0.289482, loss_ce: 0.010639
[15:37:59.732] iteration 8560 : loss : 0.215251, loss_ce: 0.012959
[15:38:03.869] iteration 8570 : loss : 0.259431, loss_ce: 0.023798
[15:40:08.820] iteration 8580 : loss : 0.308125, loss_ce: 0.015292
[15:40:13.070] iteration 8590 : loss : 0.311489, loss_ce: 0.024062
[15:40:17.175] iteration 8600 : loss : 0.069794, loss_ce: 0.012698
[15:40:21.300] iteration 8610 : loss : 0.277114, loss_ce: 0.023451
[15:40:25.413] iteration 8620 : loss : 0.303520, loss_ce: 0.032067
[15:40:29.536] iteration 8630 : loss : 0.348376, loss_ce: 0.036086
[15:40:33.648] iteration 8640 : loss : 0.327730, loss_ce: 0.020738
[15:40:37.773] iteration 8650 : loss : 0.039015, loss_ce: 0.006495
[15:40:41.889] iteration 8660 : loss : 0.247741, loss_ce: 0.017831
[15:40:46.020] iteration 8670 : loss : 0.255525, loss_ce: 0.012180
[15:40:50.140] iteration 8680 : loss : 0.046314, loss_ce: 0.007554
[15:40:54.277] iteration 8690 : loss : 0.298588, loss_ce: 0.005104
[15:40:58.399] iteration 8700 : loss : 0.214013, loss_ce: 0.010766
[15:41:02.531] iteration 8710 : loss : 0.197094, loss_ce: 0.006792
[15:41:06.658] iteration 8720 : loss : 0.209653, loss_ce: 0.008843
[15:41:10.795] iteration 8730 : loss : 0.189155, loss_ce: 0.008055
[15:41:14.916] iteration 8740 : loss : 0.195767, loss_ce: 0.008629
[15:41:19.058] iteration 8750 : loss : 0.195835, loss_ce: 0.011327
[15:41:23.185] iteration 8760 : loss : 0.076998, loss_ce: 0.006171
[15:41:27.318] iteration 8770 : loss : 0.240484, loss_ce: 0.014730
[15:41:31.442] iteration 8780 : loss : 0.225182, loss_ce: 0.010845
[15:41:35.577] iteration 8790 : loss : 0.324235, loss_ce: 0.010424
[15:41:39.703] iteration 8800 : loss : 0.318573, loss_ce: 0.023791
[15:41:43.843] iteration 8810 : loss : 0.161477, loss_ce: 0.004794
[15:41:47.970] iteration 8820 : loss : 0.195581, loss_ce: 0.011111
[15:41:52.113] iteration 8830 : loss : 0.191766, loss_ce: 0.007928
[15:41:56.241] iteration 8840 : loss : 0.191170, loss_ce: 0.011023
[15:44:02.877] iteration 8850 : loss : 0.234731, loss_ce: 0.013093
[15:44:07.067] iteration 8860 : loss : 0.212237, loss_ce: 0.017234
[15:44:11.256] iteration 8870 : loss : 0.314868, loss_ce: 0.012697
[15:44:15.435] iteration 8880 : loss : 0.238795, loss_ce: 0.014127
[15:44:19.626] iteration 8890 : loss : 0.352412, loss_ce: 0.017216
[15:44:23.807] iteration 8900 : loss : 0.305892, loss_ce: 0.022143
[15:44:28.005] iteration 8910 : loss : 0.267338, loss_ce: 0.017364
[15:44:32.192] iteration 8920 : loss : 0.128371, loss_ce: 0.022479
[15:44:36.394] iteration 8930 : loss : 0.228431, loss_ce: 0.012750
[15:44:40.584] iteration 8940 : loss : 0.314597, loss_ce: 0.048821
[15:44:44.781] iteration 8950 : loss : 0.280439, loss_ce: 0.011629
[15:44:48.973] iteration 8960 : loss : 0.154337, loss_ce: 0.013723
[15:44:53.177] iteration 8970 : loss : 0.308144, loss_ce: 0.004986
[15:44:57.372] iteration 8980 : loss : 0.222965, loss_ce: 0.014061
[15:45:01.579] iteration 8990 : loss : 0.168248, loss_ce: 0.008351
[15:45:05.771] iteration 9000 : loss : 0.216584, loss_ce: 0.003828
[15:45:09.977] iteration 9010 : loss : 0.353749, loss_ce: 0.041373
[15:45:14.170] iteration 9020 : loss : 0.087570, loss_ce: 0.014637
[15:45:18.376] iteration 9030 : loss : 0.221857, loss_ce: 0.022952
[15:45:22.569] iteration 9040 : loss : 0.193863, loss_ce: 0.020104
[15:45:26.773] iteration 9050 : loss : 0.289348, loss_ce: 0.003917
[15:45:30.977] iteration 9060 : loss : 0.211972, loss_ce: 0.008379
[15:45:35.187] iteration 9070 : loss : 0.217278, loss_ce: 0.004997
[15:45:39.382] iteration 9080 : loss : 0.187644, loss_ce: 0.005068
[15:45:43.591] iteration 9090 : loss : 0.054506, loss_ce: 0.009921
[15:45:47.793] iteration 9100 : loss : 0.228700, loss_ce: 0.007094
[15:45:52.002] iteration 9110 : loss : 0.304612, loss_ce: 0.006328
[15:47:30.555] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=100, tpgm_exclude=[])
[15:47:30.580] Using 8569/95221 samples for finetuning
[15:47:30.580] Using 953/95221 samples for TPGM
[15:47:30.580] Model has 9 total classes, training on 4 classes
[15:47:40.495] 268 iterations per epoch. 13400 max iterations 
[15:47:55.192] iteration 10 : loss : 0.461870, loss_ce: 0.081058
[15:47:59.429] iteration 20 : loss : 0.447126, loss_ce: 0.067454
[15:48:03.693] iteration 30 : loss : 0.454064, loss_ce: 0.061675
[15:48:07.816] iteration 40 : loss : 0.438523, loss_ce: 0.037797
[15:48:11.947] iteration 50 : loss : 0.427871, loss_ce: 0.060777
[15:48:16.087] iteration 60 : loss : 0.289397, loss_ce: 0.043874
[15:48:20.271] iteration 70 : loss : 0.430093, loss_ce: 0.078371
[15:48:24.390] iteration 80 : loss : 0.432416, loss_ce: 0.045691
[15:48:28.526] iteration 90 : loss : 0.438128, loss_ce: 0.059353
[15:48:32.654] iteration 100 : loss : 0.424840, loss_ce: 0.065352
[15:48:36.790] iteration 110 : loss : 0.289580, loss_ce: 0.027083
[15:48:40.919] iteration 120 : loss : 0.420143, loss_ce: 0.054165
[15:48:45.060] iteration 130 : loss : 0.428500, loss_ce: 0.046873
[15:48:49.190] iteration 140 : loss : 0.401049, loss_ce: 0.039351
[15:48:53.354] iteration 150 : loss : 0.411577, loss_ce: 0.030802
[15:48:57.488] iteration 160 : loss : 0.382576, loss_ce: 0.048536
[15:49:01.668] iteration 170 : loss : 0.386560, loss_ce: 0.035044
[15:49:05.851] iteration 180 : loss : 0.363701, loss_ce: 0.024090
[15:49:10.090] iteration 190 : loss : 0.240404, loss_ce: 0.031053
[15:49:14.294] iteration 200 : loss : 0.373939, loss_ce: 0.052010
[15:49:18.451] iteration 210 : loss : 0.388509, loss_ce: 0.043876
[15:49:22.601] iteration 220 : loss : 0.379827, loss_ce: 0.024280
[15:49:26.755] iteration 230 : loss : 0.358815, loss_ce: 0.026818
[15:49:30.886] iteration 240 : loss : 0.366219, loss_ce: 0.032203
[15:49:35.029] iteration 250 : loss : 0.391504, loss_ce: 0.052399
[15:49:39.165] iteration 260 : loss : 0.350837, loss_ce: 0.040769
[15:51:16.578] iteration 270 : loss : 0.413449, loss_ce: 0.035676
[15:51:20.694] iteration 280 : loss : 0.471316, loss_ce: 0.055697
[15:51:24.820] iteration 290 : loss : 0.314954, loss_ce: 0.066338
[15:51:28.953] iteration 300 : loss : 0.297737, loss_ce: 0.055584
[15:51:33.099] iteration 310 : loss : 0.292500, loss_ce: 0.035677
[15:51:37.252] iteration 320 : loss : 0.439465, loss_ce: 0.061151
[15:51:41.397] iteration 330 : loss : 0.443914, loss_ce: 0.036540
[15:51:45.526] iteration 340 : loss : 0.286842, loss_ce: 0.063016
[15:51:49.666] iteration 350 : loss : 0.282372, loss_ce: 0.060921
[15:51:53.796] iteration 360 : loss : 0.426073, loss_ce: 0.060079
[15:51:57.933] iteration 370 : loss : 0.433950, loss_ce: 0.036198
[15:52:02.062] iteration 380 : loss : 0.428730, loss_ce: 0.054513
[15:52:06.204] iteration 390 : loss : 0.432110, loss_ce: 0.073964
[15:52:10.338] iteration 400 : loss : 0.440570, loss_ce: 0.064844
[15:52:14.478] iteration 410 : loss : 0.420278, loss_ce: 0.069161
[15:52:18.611] iteration 420 : loss : 0.272933, loss_ce: 0.032221
[15:52:22.754] iteration 430 : loss : 0.430301, loss_ce: 0.082872
[15:52:26.889] iteration 440 : loss : 0.438703, loss_ce: 0.030832
[15:52:31.027] iteration 450 : loss : 0.424962, loss_ce: 0.044551
[15:52:35.161] iteration 460 : loss : 0.450137, loss_ce: 0.058071
[15:52:39.303] iteration 470 : loss : 0.413562, loss_ce: 0.042040
[15:52:43.435] iteration 480 : loss : 0.268727, loss_ce: 0.015669
[15:52:47.580] iteration 490 : loss : 0.385594, loss_ce: 0.050355
[15:52:51.714] iteration 500 : loss : 0.351497, loss_ce: 0.031897
[15:52:55.857] iteration 510 : loss : 0.398429, loss_ce: 0.045256
[15:52:59.992] iteration 520 : loss : 0.357848, loss_ce: 0.039546
[15:53:04.137] iteration 530 : loss : 0.360262, loss_ce: 0.045335
[15:54:41.332] iteration 540 : loss : 0.523442, loss_ce: 0.220085
[15:54:45.453] iteration 550 : loss : 0.465524, loss_ce: 0.066493
[15:54:49.568] iteration 560 : loss : 0.461184, loss_ce: 0.075637
[15:54:53.698] iteration 570 : loss : 0.455876, loss_ce: 0.102234
[15:54:57.823] iteration 580 : loss : 0.451177, loss_ce: 0.068559
[15:55:01.956] iteration 590 : loss : 0.441003, loss_ce: 0.062569
[15:55:06.082] iteration 600 : loss : 0.280859, loss_ce: 0.047170
[15:55:10.221] iteration 610 : loss : 0.435317, loss_ce: 0.060385
[15:55:14.350] iteration 620 : loss : 0.272516, loss_ce: 0.042941
[15:55:18.487] iteration 630 : loss : 0.444799, loss_ce: 0.065940
[15:55:22.614] iteration 640 : loss : 0.427582, loss_ce: 0.046773
[15:55:26.754] iteration 650 : loss : 0.434210, loss_ce: 0.074943
[15:55:30.915] iteration 660 : loss : 0.428651, loss_ce: 0.041384
[15:55:35.117] iteration 670 : loss : 0.269622, loss_ce: 0.047871
[15:55:39.258] iteration 680 : loss : 0.414775, loss_ce: 0.048115
[15:55:43.394] iteration 690 : loss : 0.403403, loss_ce: 0.040928
[15:55:47.481] iteration 700 : loss : 0.267154, loss_ce: 0.062641
[15:55:51.573] iteration 710 : loss : 0.433283, loss_ce: 0.042641
[15:55:55.678] iteration 720 : loss : 0.446331, loss_ce: 0.092551
[15:55:59.844] iteration 730 : loss : 0.245217, loss_ce: 0.022640
[15:56:03.979] iteration 740 : loss : 0.381428, loss_ce: 0.032246
[15:56:08.122] iteration 750 : loss : 0.373443, loss_ce: 0.060255
[15:56:12.254] iteration 760 : loss : 0.404855, loss_ce: 0.046759
[15:56:16.397] iteration 770 : loss : 0.246845, loss_ce: 0.042135
[15:56:20.531] iteration 780 : loss : 0.361725, loss_ce: 0.032357
[15:56:24.675] iteration 790 : loss : 0.189962, loss_ce: 0.038067
[15:56:28.808] iteration 800 : loss : 0.379945, loss_ce: 0.050406
[15:58:05.904] iteration 810 : loss : 0.470177, loss_ce: 0.067090
[15:58:10.025] iteration 820 : loss : 0.446246, loss_ce: 0.051768
[15:58:14.165] iteration 830 : loss : 0.459606, loss_ce: 0.048614
[15:58:18.292] iteration 840 : loss : 0.441686, loss_ce: 0.056358
[15:58:22.433] iteration 850 : loss : 0.452155, loss_ce: 0.087480
[15:58:26.560] iteration 860 : loss : 0.425805, loss_ce: 0.062812
[15:58:30.698] iteration 870 : loss : 0.417385, loss_ce: 0.039609
[15:58:34.827] iteration 880 : loss : 0.375633, loss_ce: 0.027353
[15:58:38.969] iteration 890 : loss : 0.342867, loss_ce: 0.023355
[15:58:43.100] iteration 900 : loss : 0.359105, loss_ce: 0.034174
[15:58:47.244] iteration 910 : loss : 0.409834, loss_ce: 0.045432
[15:58:51.376] iteration 920 : loss : 0.372696, loss_ce: 0.030794
[15:58:55.518] iteration 930 : loss : 0.290715, loss_ce: 0.016998
[15:58:59.653] iteration 940 : loss : 0.332447, loss_ce: 0.018736
[15:59:03.798] iteration 950 : loss : 0.161347, loss_ce: 0.024237
[15:59:07.932] iteration 960 : loss : 0.306404, loss_ce: 0.025165
[15:59:12.079] iteration 970 : loss : 0.356000, loss_ce: 0.034213
[15:59:16.217] iteration 980 : loss : 0.311104, loss_ce: 0.020812
[15:59:20.362] iteration 990 : loss : 0.303463, loss_ce: 0.020440
[15:59:24.500] iteration 1000 : loss : 0.186682, loss_ce: 0.036780
[15:59:28.648] iteration 1010 : loss : 0.316343, loss_ce: 0.016063
[15:59:32.784] iteration 1020 : loss : 0.355226, loss_ce: 0.026219
[15:59:36.929] iteration 1030 : loss : 0.246947, loss_ce: 0.013337
[15:59:41.067] iteration 1040 : loss : 0.280297, loss_ce: 0.024091
[15:59:45.215] iteration 1050 : loss : 0.302294, loss_ce: 0.040676
[15:59:49.352] iteration 1060 : loss : 0.298165, loss_ce: 0.019430
[15:59:53.496] iteration 1070 : loss : 0.283606, loss_ce: 0.022197
[16:01:41.813] iteration 1080 : loss : 0.358021, loss_ce: 0.136332
[16:01:45.954] iteration 1090 : loss : 0.524067, loss_ce: 0.181977
[16:01:50.073] iteration 1100 : loss : 0.505118, loss_ce: 0.136039
[16:01:54.207] iteration 1110 : loss : 0.318852, loss_ce: 0.064810
[16:01:58.328] iteration 1120 : loss : 0.457016, loss_ce: 0.023951
[16:02:02.463] iteration 1130 : loss : 0.315045, loss_ce: 0.042651
[16:02:06.592] iteration 1140 : loss : 0.461548, loss_ce: 0.076330
[16:02:10.729] iteration 1150 : loss : 0.457140, loss_ce: 0.039483
[16:02:14.857] iteration 1160 : loss : 0.450553, loss_ce: 0.070198
[16:02:18.996] iteration 1170 : loss : 0.445010, loss_ce: 0.080812
[16:02:23.126] iteration 1180 : loss : 0.433665, loss_ce: 0.062201
[16:02:27.266] iteration 1190 : loss : 0.439718, loss_ce: 0.064412
[16:02:31.394] iteration 1200 : loss : 0.445411, loss_ce: 0.058881
[16:02:35.620] iteration 1210 : loss : 0.454307, loss_ce: 0.063673
[16:02:39.793] iteration 1220 : loss : 0.439086, loss_ce: 0.063435
[16:02:43.979] iteration 1230 : loss : 0.300792, loss_ce: 0.074705
[16:02:48.173] iteration 1240 : loss : 0.445519, loss_ce: 0.055534
[16:02:52.362] iteration 1250 : loss : 0.445137, loss_ce: 0.078718
[16:02:56.513] iteration 1260 : loss : 0.439314, loss_ce: 0.049361
[16:03:00.659] iteration 1270 : loss : 0.434932, loss_ce: 0.044137
[16:03:04.795] iteration 1280 : loss : 0.427998, loss_ce: 0.058668
[16:03:08.947] iteration 1290 : loss : 0.453752, loss_ce: 0.071001
[16:03:13.078] iteration 1300 : loss : 0.294539, loss_ce: 0.038895
[16:03:17.223] iteration 1310 : loss : 0.439231, loss_ce: 0.066217
[16:03:21.356] iteration 1320 : loss : 0.291212, loss_ce: 0.061392
[16:03:25.499] iteration 1330 : loss : 0.437144, loss_ce: 0.067439
[16:03:29.532] iteration 1340 : loss : 0.434458, loss_ce: 0.069802
[16:05:06.833] iteration 1350 : loss : 0.482252, loss_ce: 0.085646
[16:05:10.957] iteration 1360 : loss : 0.490054, loss_ce: 0.128575
[16:05:15.091] iteration 1370 : loss : 0.489305, loss_ce: 0.129881
[16:05:19.216] iteration 1380 : loss : 0.451179, loss_ce: 0.066451
[16:05:23.352] iteration 1390 : loss : 0.293433, loss_ce: 0.039193
[16:05:27.478] iteration 1400 : loss : 0.298191, loss_ce: 0.051385
[16:05:31.617] iteration 1410 : loss : 0.293915, loss_ce: 0.076526
[16:05:35.747] iteration 1420 : loss : 0.430408, loss_ce: 0.058222
[16:05:39.888] iteration 1430 : loss : 0.444526, loss_ce: 0.051266
[16:05:44.022] iteration 1440 : loss : 0.429235, loss_ce: 0.044123
[16:05:48.167] iteration 1450 : loss : 0.433710, loss_ce: 0.033177
[16:05:52.301] iteration 1460 : loss : 0.423839, loss_ce: 0.060976
[16:05:56.446] iteration 1470 : loss : 0.256055, loss_ce: 0.044861
[16:06:00.580] iteration 1480 : loss : 0.288311, loss_ce: 0.073687
[16:06:04.724] iteration 1490 : loss : 0.437096, loss_ce: 0.057031
[16:06:08.858] iteration 1500 : loss : 0.287613, loss_ce: 0.052685
[16:06:13.003] iteration 1510 : loss : 0.298795, loss_ce: 0.076671
[16:06:17.135] iteration 1520 : loss : 0.441321, loss_ce: 0.060347
[16:06:21.343] iteration 1530 : loss : 0.442109, loss_ce: 0.074156
[16:06:25.576] iteration 1540 : loss : 0.288699, loss_ce: 0.066169
[16:06:29.734] iteration 1550 : loss : 0.282032, loss_ce: 0.055802
[16:06:34.035] iteration 1560 : loss : 0.414440, loss_ce: 0.040887
[16:06:38.198] iteration 1570 : loss : 0.277029, loss_ce: 0.049596
[16:06:42.333] iteration 1580 : loss : 0.402482, loss_ce: 0.063207
[16:06:46.545] iteration 1590 : loss : 0.281614, loss_ce: 0.075593
[16:06:50.666] iteration 1600 : loss : 0.405652, loss_ce: 0.029084
[16:08:26.659] iteration 1610 : loss : 0.532344, loss_ce: 0.231526
[16:08:30.716] iteration 1620 : loss : 0.477959, loss_ce: 0.069106
[16:08:34.789] iteration 1630 : loss : 0.494706, loss_ce: 0.110338
[16:08:38.856] iteration 1640 : loss : 0.510760, loss_ce: 0.149427
[16:08:42.933] iteration 1650 : loss : 0.323933, loss_ce: 0.060018
[16:08:47.000] iteration 1660 : loss : 0.460694, loss_ce: 0.029086
[16:08:51.077] iteration 1670 : loss : 0.466070, loss_ce: 0.082283
[16:08:55.145] iteration 1680 : loss : 0.465912, loss_ce: 0.064989
[16:08:59.227] iteration 1690 : loss : 0.310812, loss_ce: 0.062348
[16:09:03.341] iteration 1700 : loss : 0.460984, loss_ce: 0.058413
[16:09:07.452] iteration 1710 : loss : 0.312315, loss_ce: 0.065134
[16:09:11.551] iteration 1720 : loss : 0.465439, loss_ce: 0.079043
[16:09:15.661] iteration 1730 : loss : 0.308383, loss_ce: 0.053725
[16:09:19.913] iteration 1740 : loss : 0.306673, loss_ce: 0.056435
[16:09:24.055] iteration 1750 : loss : 0.460634, loss_ce: 0.069622
[16:09:28.189] iteration 1760 : loss : 0.454597, loss_ce: 0.051977
[16:09:32.325] iteration 1770 : loss : 0.443361, loss_ce: 0.062001
[16:09:36.477] iteration 1780 : loss : 0.447773, loss_ce: 0.054288
[16:09:40.641] iteration 1790 : loss : 0.443256, loss_ce: 0.040865
[16:09:45.030] iteration 1800 : loss : 0.446410, loss_ce: 0.050244
[16:09:49.181] iteration 1810 : loss : 0.445448, loss_ce: 0.064018
[16:09:53.469] iteration 1820 : loss : 0.454796, loss_ce: 0.096920
[16:09:57.846] iteration 1830 : loss : 0.438034, loss_ce: 0.060812
[16:10:02.044] iteration 1840 : loss : 0.293463, loss_ce: 0.044278
[16:10:06.317] iteration 1850 : loss : 0.442900, loss_ce: 0.043917
[16:10:10.697] iteration 1860 : loss : 0.439935, loss_ce: 0.067517
[16:10:14.910] iteration 1870 : loss : 0.434338, loss_ce: 0.054458
[16:12:06.749] iteration 1880 : loss : 0.473004, loss_ce: 0.060109
[16:12:10.957] iteration 1890 : loss : 0.323121, loss_ce: 0.067541
[16:12:15.320] iteration 1900 : loss : 0.454869, loss_ce: 0.061932
[16:12:19.676] iteration 1910 : loss : 0.450536, loss_ce: 0.073479
[16:12:24.028] iteration 1920 : loss : 0.446851, loss_ce: 0.066045
[16:12:28.234] iteration 1930 : loss : 0.439541, loss_ce: 0.055093
[16:12:32.546] iteration 1940 : loss : 0.422174, loss_ce: 0.068965
[16:12:36.943] iteration 1950 : loss : 0.429626, loss_ce: 0.043637
[16:12:41.135] iteration 1960 : loss : 0.426534, loss_ce: 0.049168
[16:12:45.318] iteration 1970 : loss : 0.429090, loss_ce: 0.058232
[16:12:49.461] iteration 1980 : loss : 0.451445, loss_ce: 0.078927
[16:12:53.779] iteration 1990 : loss : 0.438579, loss_ce: 0.053007
[16:12:58.212] iteration 2000 : loss : 0.439807, loss_ce: 0.041160
[16:13:02.639] iteration 2010 : loss : 0.433766, loss_ce: 0.094152
[16:13:07.054] iteration 2020 : loss : 0.391643, loss_ce: 0.044109
[16:13:11.349] iteration 2030 : loss : 0.385687, loss_ce: 0.022886
[16:13:15.449] iteration 2040 : loss : 0.458313, loss_ce: 0.084470
[16:13:19.563] iteration 2050 : loss : 0.278630, loss_ce: 0.042933
[16:13:23.672] iteration 2060 : loss : 0.439549, loss_ce: 0.048758
[16:13:27.821] iteration 2070 : loss : 0.261817, loss_ce: 0.069205
[16:13:31.920] iteration 2080 : loss : 0.361065, loss_ce: 0.046075
[16:13:36.162] iteration 2090 : loss : 0.357920, loss_ce: 0.035022
[16:13:40.261] iteration 2100 : loss : 0.371998, loss_ce: 0.038848
[16:13:45.009] iteration 2110 : loss : 0.273314, loss_ce: 0.025760
[16:13:49.286] iteration 2120 : loss : 0.346494, loss_ce: 0.041402
[16:13:53.849] iteration 2130 : loss : 0.308668, loss_ce: 0.023694
[16:13:58.369] iteration 2140 : loss : 0.371931, loss_ce: 0.073681
[16:15:46.273] iteration 2150 : loss : 0.454697, loss_ce: 0.041791
[16:15:51.043] iteration 2160 : loss : 0.459202, loss_ce: 0.041088
[16:15:55.732] iteration 2170 : loss : 0.452652, loss_ce: 0.083381
[16:16:00.420] iteration 2180 : loss : 0.296811, loss_ce: 0.040619
[16:16:05.167] iteration 2190 : loss : 0.294375, loss_ce: 0.040860
[16:16:09.982] iteration 2200 : loss : 0.444636, loss_ce: 0.033288
[16:16:14.747] iteration 2210 : loss : 0.288205, loss_ce: 0.031202
[16:16:19.435] iteration 2220 : loss : 0.430485, loss_ce: 0.046648
[16:16:24.121] iteration 2230 : loss : 0.428623, loss_ce: 0.058920
[16:16:28.838] iteration 2240 : loss : 0.264854, loss_ce: 0.060217
[16:16:33.631] iteration 2250 : loss : 0.430784, loss_ce: 0.034179
[16:16:38.351] iteration 2260 : loss : 0.417847, loss_ce: 0.038527
[16:16:43.084] iteration 2270 : loss : 0.431457, loss_ce: 0.049488
[16:16:47.790] iteration 2280 : loss : 0.433571, loss_ce: 0.059073
[16:16:52.521] iteration 2290 : loss : 0.279263, loss_ce: 0.033160
[16:16:57.297] iteration 2300 : loss : 0.398388, loss_ce: 0.072975
[16:17:02.020] iteration 2310 : loss : 0.241849, loss_ce: 0.027621
[16:17:06.752] iteration 2320 : loss : 0.361162, loss_ce: 0.041307
[16:17:11.474] iteration 2330 : loss : 0.388293, loss_ce: 0.054694
[16:17:16.398] iteration 2340 : loss : 0.344918, loss_ce: 0.031181
[16:17:21.396] iteration 2350 : loss : 0.321919, loss_ce: 0.031848
[16:17:26.130] iteration 2360 : loss : 0.402012, loss_ce: 0.024026
[16:17:30.900] iteration 2370 : loss : 0.306643, loss_ce: 0.021914
[16:17:35.691] iteration 2380 : loss : 0.297390, loss_ce: 0.027824
[16:17:40.693] iteration 2390 : loss : 0.270678, loss_ce: 0.061120
[16:17:45.614] iteration 2400 : loss : 0.299296, loss_ce: 0.039205
[16:17:50.490] iteration 2410 : loss : 0.299955, loss_ce: 0.038020
[16:19:37.154] iteration 2420 : loss : 0.459171, loss_ce: 0.078926
[16:19:41.882] iteration 2430 : loss : 0.477288, loss_ce: 0.070906
[16:19:46.716] iteration 2440 : loss : 0.466224, loss_ce: 0.043261
[16:19:51.669] iteration 2450 : loss : 0.298634, loss_ce: 0.067898
[16:19:56.819] iteration 2460 : loss : 0.445786, loss_ce: 0.037617
[16:20:01.613] iteration 2470 : loss : 0.451438, loss_ce: 0.055121
[16:20:06.617] iteration 2480 : loss : 0.441361, loss_ce: 0.052937
[16:20:12.062] iteration 2490 : loss : 0.429394, loss_ce: 0.048366
[16:20:17.240] iteration 2500 : loss : 0.285924, loss_ce: 0.058462
[16:20:22.227] iteration 2510 : loss : 0.423280, loss_ce: 0.053227
[16:20:26.965] iteration 2520 : loss : 0.425526, loss_ce: 0.044344
[16:20:31.786] iteration 2530 : loss : 0.437576, loss_ce: 0.076199
[16:20:36.448] iteration 2540 : loss : 0.267992, loss_ce: 0.049026
[16:20:41.160] iteration 2550 : loss : 0.279530, loss_ce: 0.071627
[16:20:45.887] iteration 2560 : loss : 0.394861, loss_ce: 0.045587
[16:20:50.611] iteration 2570 : loss : 0.217864, loss_ce: 0.043822
[16:20:55.326] iteration 2580 : loss : 0.429914, loss_ce: 0.053444
[16:21:00.379] iteration 2590 : loss : 0.349699, loss_ce: 0.024046
[16:21:05.449] iteration 2600 : loss : 0.196378, loss_ce: 0.014362
[16:21:10.526] iteration 2610 : loss : 0.227595, loss_ce: 0.022499
[16:21:15.375] iteration 2620 : loss : 0.170308, loss_ce: 0.019171
[16:21:20.387] iteration 2630 : loss : 0.315463, loss_ce: 0.024899
[16:21:25.659] iteration 2640 : loss : 0.185770, loss_ce: 0.024000
[16:21:30.589] iteration 2650 : loss : 0.328227, loss_ce: 0.008863
[16:21:35.270] iteration 2660 : loss : 0.320897, loss_ce: 0.011290
[16:21:40.000] iteration 2670 : loss : 0.334527, loss_ce: 0.090067
[16:21:44.698] iteration 2680 : loss : 0.289835, loss_ce: 0.027291
[16:23:28.482] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_9.pth
[16:23:43.566] iteration 2690 : loss : 0.549437, loss_ce: 0.245309
[16:23:48.271] iteration 2700 : loss : 0.449742, loss_ce: 0.045856
[16:23:52.980] iteration 2710 : loss : 0.448226, loss_ce: 0.076313
[16:23:57.689] iteration 2720 : loss : 0.450866, loss_ce: 0.053003
[16:24:02.423] iteration 2730 : loss : 0.435169, loss_ce: 0.068736
[16:24:07.128] iteration 2740 : loss : 0.433280, loss_ce: 0.064197
[16:24:11.854] iteration 2750 : loss : 0.436441, loss_ce: 0.082635
[16:24:16.548] iteration 2760 : loss : 0.443676, loss_ce: 0.042465
[16:24:21.330] iteration 2770 : loss : 0.438009, loss_ce: 0.027582
[16:24:26.442] iteration 2780 : loss : 0.447106, loss_ce: 0.062304
[16:24:31.250] iteration 2790 : loss : 0.426014, loss_ce: 0.050768
[16:24:36.183] iteration 2800 : loss : 0.429674, loss_ce: 0.061592
[16:24:40.899] iteration 2810 : loss : 0.420580, loss_ce: 0.072714
[16:24:45.608] iteration 2820 : loss : 0.446870, loss_ce: 0.068663
[16:24:50.332] iteration 2830 : loss : 0.442544, loss_ce: 0.062434
[16:24:55.075] iteration 2840 : loss : 0.457693, loss_ce: 0.053158
[16:24:59.847] iteration 2850 : loss : 0.446854, loss_ce: 0.051281
[16:25:04.529] iteration 2860 : loss : 0.458827, loss_ce: 0.042628
[16:25:09.205] iteration 2870 : loss : 0.459618, loss_ce: 0.064478
[16:25:13.880] iteration 2880 : loss : 0.460415, loss_ce: 0.070471
[16:25:18.846] iteration 2890 : loss : 0.456845, loss_ce: 0.053797
[16:25:23.600] iteration 2900 : loss : 0.458119, loss_ce: 0.041527
[16:25:28.327] iteration 2910 : loss : 0.453483, loss_ce: 0.057653
[16:25:33.002] iteration 2920 : loss : 0.442955, loss_ce: 0.061019
[16:25:37.755] iteration 2930 : loss : 0.444275, loss_ce: 0.048191
[16:25:42.472] iteration 2940 : loss : 0.439597, loss_ce: 0.058780
[16:27:30.212] iteration 2950 : loss : 0.429307, loss_ce: 0.096211
[16:27:34.964] iteration 2960 : loss : 0.307937, loss_ce: 0.030161
[16:27:39.774] iteration 2970 : loss : 0.309946, loss_ce: 0.034451
[16:27:44.604] iteration 2980 : loss : 0.453446, loss_ce: 0.051352
[16:27:49.443] iteration 2990 : loss : 0.303466, loss_ce: 0.061599
[16:27:54.292] iteration 3000 : loss : 0.454293, loss_ce: 0.048698
[16:27:59.041] iteration 3010 : loss : 0.455491, loss_ce: 0.069985
[16:28:04.024] iteration 3020 : loss : 0.285255, loss_ce: 0.069751
[16:28:09.238] iteration 3030 : loss : 0.437226, loss_ce: 0.063316
[16:28:14.203] iteration 3040 : loss : 0.436092, loss_ce: 0.047777
[16:28:18.896] iteration 3050 : loss : 0.433841, loss_ce: 0.050667
[16:28:23.624] iteration 3060 : loss : 0.440132, loss_ce: 0.034543
[16:28:28.496] iteration 3070 : loss : 0.287699, loss_ce: 0.047012
[16:28:33.406] iteration 3080 : loss : 0.436766, loss_ce: 0.044208
[16:28:38.427] iteration 3090 : loss : 0.429654, loss_ce: 0.066935
[16:28:43.524] iteration 3100 : loss : 0.431561, loss_ce: 0.051083
[16:28:48.755] iteration 3110 : loss : 0.437640, loss_ce: 0.053438
[16:28:53.545] iteration 3120 : loss : 0.293365, loss_ce: 0.062119
[16:28:58.363] iteration 3130 : loss : 0.441153, loss_ce: 0.064334
[16:29:03.214] iteration 3140 : loss : 0.441205, loss_ce: 0.052695
[16:29:07.992] iteration 3150 : loss : 0.293799, loss_ce: 0.045985
[16:29:12.972] iteration 3160 : loss : 0.435925, loss_ce: 0.052166
[16:29:17.992] iteration 3170 : loss : 0.427665, loss_ce: 0.044646
[16:29:22.828] iteration 3180 : loss : 0.422086, loss_ce: 0.063187
[16:29:28.034] iteration 3190 : loss : 0.419325, loss_ce: 0.038651
[16:29:33.378] iteration 3200 : loss : 0.408435, loss_ce: 0.039675
[16:29:38.692] iteration 3210 : loss : 0.430859, loss_ce: 0.096964
[16:31:30.303] iteration 3220 : loss : 0.471351, loss_ce: 0.052559
[16:31:34.956] iteration 3230 : loss : 0.483700, loss_ce: 0.085492
[16:31:39.682] iteration 3240 : loss : 0.470148, loss_ce: 0.062430
[16:31:44.501] iteration 3250 : loss : 0.459774, loss_ce: 0.056078
[16:31:49.360] iteration 3260 : loss : 0.298535, loss_ce: 0.054120
[16:31:54.466] iteration 3270 : loss : 0.450357, loss_ce: 0.053481
[16:31:59.670] iteration 3280 : loss : 0.444925, loss_ce: 0.066161
[16:32:04.825] iteration 3290 : loss : 0.435955, loss_ce: 0.070876
[16:32:09.925] iteration 3300 : loss : 0.296577, loss_ce: 0.050097
[16:32:14.730] iteration 3310 : loss : 0.450779, loss_ce: 0.057928
[16:32:19.475] iteration 3320 : loss : 0.440436, loss_ce: 0.053391
[16:32:24.203] iteration 3330 : loss : 0.283312, loss_ce: 0.072184
[16:32:28.902] iteration 3340 : loss : 0.430963, loss_ce: 0.062060
[16:32:33.635] iteration 3350 : loss : 0.436858, loss_ce: 0.060004
[16:32:38.377] iteration 3360 : loss : 0.431937, loss_ce: 0.057176
[16:32:43.344] iteration 3370 : loss : 0.439519, loss_ce: 0.049899
[16:32:48.389] iteration 3380 : loss : 0.435178, loss_ce: 0.045601
[16:32:53.172] iteration 3390 : loss : 0.441744, loss_ce: 0.042277
[16:32:57.947] iteration 3400 : loss : 0.437902, loss_ce: 0.053449
[16:33:02.858] iteration 3410 : loss : 0.419645, loss_ce: 0.071999
[16:33:07.716] iteration 3420 : loss : 0.292537, loss_ce: 0.052902
[16:33:12.769] iteration 3430 : loss : 0.265317, loss_ce: 0.053965
[16:33:18.016] iteration 3440 : loss : 0.405969, loss_ce: 0.044491
[16:33:24.169] iteration 3450 : loss : 0.454590, loss_ce: 0.109653
[16:33:29.328] iteration 3460 : loss : 0.451861, loss_ce: 0.048413
[16:33:34.182] iteration 3470 : loss : 0.451938, loss_ce: 0.037026
[16:33:39.129] iteration 3480 : loss : 0.440600, loss_ce: 0.033005
[16:35:42.850] iteration 3490 : loss : 0.466386, loss_ce: 0.079896
[16:35:48.682] iteration 3500 : loss : 0.370458, loss_ce: 0.035274
[16:35:54.357] iteration 3510 : loss : 0.455577, loss_ce: 0.058410
[16:35:59.256] iteration 3520 : loss : 0.449268, loss_ce: 0.065137
[16:36:04.024] iteration 3530 : loss : 0.468957, loss_ce: 0.075726
[16:36:08.802] iteration 3540 : loss : 0.299931, loss_ce: 0.060850
[16:36:13.533] iteration 3550 : loss : 0.293839, loss_ce: 0.086775
[16:36:18.230] iteration 3560 : loss : 0.429611, loss_ce: 0.078952
[16:36:22.991] iteration 3570 : loss : 0.441404, loss_ce: 0.058531
[16:36:27.943] iteration 3580 : loss : 0.432409, loss_ce: 0.066960
[16:36:33.428] iteration 3590 : loss : 0.424779, loss_ce: 0.058514
[16:36:38.765] iteration 3600 : loss : 0.417015, loss_ce: 0.062552
[16:36:44.239] iteration 3610 : loss : 0.419666, loss_ce: 0.049434
[16:36:49.400] iteration 3620 : loss : 0.433965, loss_ce: 0.054059
[16:36:54.528] iteration 3630 : loss : 0.425721, loss_ce: 0.057790
[16:36:59.549] iteration 3640 : loss : 0.254480, loss_ce: 0.064268
[16:37:04.393] iteration 3650 : loss : 0.402404, loss_ce: 0.064166
[16:37:09.157] iteration 3660 : loss : 0.403367, loss_ce: 0.066321
[16:37:13.892] iteration 3670 : loss : 0.414334, loss_ce: 0.090699
[16:37:18.687] iteration 3680 : loss : 0.450200, loss_ce: 0.029165
[16:37:23.443] iteration 3690 : loss : 0.402210, loss_ce: 0.036627
[16:37:28.152] iteration 3700 : loss : 0.367864, loss_ce: 0.031114
[16:37:32.909] iteration 3710 : loss : 0.353381, loss_ce: 0.031002
[16:37:37.635] iteration 3720 : loss : 0.360300, loss_ce: 0.045878
[16:37:42.704] iteration 3730 : loss : 0.324770, loss_ce: 0.025794
[16:37:48.294] iteration 3740 : loss : 0.396302, loss_ce: 0.074706
[16:37:53.287] iteration 3750 : loss : 0.251446, loss_ce: 0.064757
[16:39:44.518] iteration 3760 : loss : 0.321613, loss_ce: 0.060554
[16:39:49.782] iteration 3770 : loss : 0.455415, loss_ce: 0.044510
[16:39:54.657] iteration 3780 : loss : 0.463983, loss_ce: 0.067152
[16:39:59.548] iteration 3790 : loss : 0.469963, loss_ce: 0.074011
[16:40:04.296] iteration 3800 : loss : 0.448971, loss_ce: 0.056297
[16:40:09.090] iteration 3810 : loss : 0.296062, loss_ce: 0.031718
[16:40:13.900] iteration 3820 : loss : 0.468081, loss_ce: 0.101022
[16:40:18.866] iteration 3830 : loss : 0.281673, loss_ce: 0.054255
[16:40:23.778] iteration 3840 : loss : 0.289774, loss_ce: 0.027203
[16:40:28.535] iteration 3850 : loss : 0.435292, loss_ce: 0.086183
[16:40:33.982] iteration 3860 : loss : 0.305676, loss_ce: 0.069303
[16:40:39.072] iteration 3870 : loss : 0.447286, loss_ce: 0.039422
[16:40:43.895] iteration 3880 : loss : 0.432079, loss_ce: 0.052888
[16:40:48.644] iteration 3890 : loss : 0.434485, loss_ce: 0.032385
[16:40:53.397] iteration 3900 : loss : 0.431769, loss_ce: 0.065618
[16:40:58.140] iteration 3910 : loss : 0.417593, loss_ce: 0.051699
[16:41:02.885] iteration 3920 : loss : 0.435472, loss_ce: 0.063905
[16:41:07.652] iteration 3930 : loss : 0.447778, loss_ce: 0.110232
[16:41:12.511] iteration 3940 : loss : 0.443971, loss_ce: 0.039188
[16:41:17.282] iteration 3950 : loss : 0.427976, loss_ce: 0.071831
[16:41:22.729] iteration 3960 : loss : 0.461655, loss_ce: 0.059240
[16:41:28.092] iteration 3970 : loss : 0.470880, loss_ce: 0.100791
[16:41:33.515] iteration 3980 : loss : 0.453867, loss_ce: 0.047947
[16:41:39.162] iteration 3990 : loss : 0.467256, loss_ce: 0.051458
[16:41:44.154] iteration 4000 : loss : 0.462447, loss_ce: 0.048106
[16:41:48.930] iteration 4010 : loss : 0.471581, loss_ce: 0.054595
[16:41:53.634] iteration 4020 : loss : 0.463139, loss_ce: 0.034557
[16:43:45.266] iteration 4030 : loss : 0.450099, loss_ce: 0.059821
[16:43:50.947] iteration 4040 : loss : 0.459228, loss_ce: 0.053516
[16:43:56.480] iteration 4050 : loss : 0.295483, loss_ce: 0.058750
[16:44:01.979] iteration 4060 : loss : 0.431693, loss_ce: 0.045924
[16:44:07.546] iteration 4070 : loss : 0.422733, loss_ce: 0.032686
[16:44:12.656] iteration 4080 : loss : 0.421239, loss_ce: 0.048086
[16:44:17.423] iteration 4090 : loss : 0.262429, loss_ce: 0.049149
[16:44:22.386] iteration 4100 : loss : 0.411062, loss_ce: 0.051824
[16:44:27.338] iteration 4110 : loss : 0.396844, loss_ce: 0.057658
[16:44:32.700] iteration 4120 : loss : 0.386542, loss_ce: 0.086443
[16:44:37.716] iteration 4130 : loss : 0.416645, loss_ce: 0.057663
[16:44:42.776] iteration 4140 : loss : 0.342470, loss_ce: 0.031956
[16:44:47.547] iteration 4150 : loss : 0.349268, loss_ce: 0.015406
[16:44:52.463] iteration 4160 : loss : 0.239193, loss_ce: 0.020570
[16:44:57.266] iteration 4170 : loss : 0.339685, loss_ce: 0.036871
[16:45:02.143] iteration 4180 : loss : 0.367161, loss_ce: 0.019438
[16:45:07.061] iteration 4190 : loss : 0.311396, loss_ce: 0.033108
[16:45:11.864] iteration 4200 : loss : 0.310143, loss_ce: 0.031812
[16:45:16.630] iteration 4210 : loss : 0.275655, loss_ce: 0.041106
[16:45:21.402] iteration 4220 : loss : 0.178773, loss_ce: 0.017632
[16:45:26.262] iteration 4230 : loss : 0.337914, loss_ce: 0.037889
[16:45:30.995] iteration 4240 : loss : 0.323807, loss_ce: 0.017584
[16:45:35.803] iteration 4250 : loss : 0.278266, loss_ce: 0.021726
[16:45:40.617] iteration 4260 : loss : 0.359937, loss_ce: 0.019627
[16:45:45.437] iteration 4270 : loss : 0.296409, loss_ce: 0.024570
[16:45:50.353] iteration 4280 : loss : 0.327042, loss_ce: 0.040329
[16:47:50.897] iteration 4290 : loss : 0.490006, loss_ce: 0.127761
[16:47:55.807] iteration 4300 : loss : 0.502303, loss_ce: 0.144369
[16:48:00.575] iteration 4310 : loss : 0.396999, loss_ce: 0.119067
[16:48:05.472] iteration 4320 : loss : 0.462973, loss_ce: 0.032030
[16:48:10.771] iteration 4330 : loss : 0.445273, loss_ce: 0.039585
[16:48:15.639] iteration 4340 : loss : 0.452503, loss_ce: 0.081618
[16:48:20.639] iteration 4350 : loss : 0.439625, loss_ce: 0.040136
[16:48:25.696] iteration 4360 : loss : 0.438683, loss_ce: 0.046031
[16:48:30.836] iteration 4370 : loss : 0.290899, loss_ce: 0.078103
[16:48:35.328] iteration 4380 : loss : 0.433417, loss_ce: 0.046326
[16:48:40.161] iteration 4390 : loss : 0.450497, loss_ce: 0.072971
[16:48:45.002] iteration 4400 : loss : 0.426584, loss_ce: 0.041928
[16:48:50.131] iteration 4410 : loss : 0.443280, loss_ce: 0.065500
[16:48:55.027] iteration 4420 : loss : 0.420553, loss_ce: 0.066634
[16:48:59.789] iteration 4430 : loss : 0.427566, loss_ce: 0.050715
[16:49:04.461] iteration 4440 : loss : 0.411838, loss_ce: 0.051330
[16:49:09.282] iteration 4450 : loss : 0.428145, loss_ce: 0.059125
[16:49:14.049] iteration 4460 : loss : 0.280134, loss_ce: 0.044649
[16:49:18.921] iteration 4470 : loss : 0.410755, loss_ce: 0.042061
[16:49:23.687] iteration 4480 : loss : 0.405659, loss_ce: 0.038840
[16:49:28.462] iteration 4490 : loss : 0.367052, loss_ce: 0.062980
[16:49:33.138] iteration 4500 : loss : 0.226959, loss_ce: 0.037119
[16:49:37.896] iteration 4510 : loss : 0.372216, loss_ce: 0.046552
[16:49:42.625] iteration 4520 : loss : 0.234615, loss_ce: 0.047393
[16:49:47.440] iteration 4530 : loss : 0.217109, loss_ce: 0.028678
[16:49:52.165] iteration 4540 : loss : 0.351544, loss_ce: 0.029932
[16:49:57.023] iteration 4550 : loss : 0.389804, loss_ce: 0.012808
[16:51:44.705] iteration 4560 : loss : 0.480959, loss_ce: 0.076118
[16:51:49.608] iteration 4570 : loss : 0.460380, loss_ce: 0.087507
[16:51:54.497] iteration 4580 : loss : 0.457976, loss_ce: 0.027817
[16:51:59.060] iteration 4590 : loss : 0.278154, loss_ce: 0.054046
[16:52:03.516] iteration 4600 : loss : 0.425473, loss_ce: 0.059300
[16:52:08.003] iteration 4610 : loss : 0.274203, loss_ce: 0.041233
[16:52:12.470] iteration 4620 : loss : 0.386333, loss_ce: 0.034770
[16:52:16.910] iteration 4630 : loss : 0.353583, loss_ce: 0.021285
[16:52:21.359] iteration 4640 : loss : 0.389807, loss_ce: 0.035371
[16:52:25.815] iteration 4650 : loss : 0.351783, loss_ce: 0.029609
[16:52:30.271] iteration 4660 : loss : 0.312938, loss_ce: 0.045418
[16:52:34.733] iteration 4670 : loss : 0.292418, loss_ce: 0.037687
[16:52:39.183] iteration 4680 : loss : 0.300054, loss_ce: 0.022444
[16:52:43.638] iteration 4690 : loss : 0.299534, loss_ce: 0.045585
[16:52:48.019] iteration 4700 : loss : 0.260470, loss_ce: 0.020519
[16:52:52.312] iteration 4710 : loss : 0.250954, loss_ce: 0.024935
[16:52:56.531] iteration 4720 : loss : 0.261996, loss_ce: 0.023382
[16:53:01.506] iteration 4730 : loss : 0.246720, loss_ce: 0.013841
[16:53:06.492] iteration 4740 : loss : 0.280681, loss_ce: 0.033876
[16:53:11.528] iteration 4750 : loss : 0.335057, loss_ce: 0.039575
[16:53:16.670] iteration 4760 : loss : 0.300183, loss_ce: 0.048203
[16:53:21.468] iteration 4770 : loss : 0.123963, loss_ce: 0.025209
[16:53:26.131] iteration 4780 : loss : 0.330881, loss_ce: 0.010994
[16:53:30.797] iteration 4790 : loss : 0.231986, loss_ce: 0.016877
[16:53:35.516] iteration 4800 : loss : 0.275168, loss_ce: 0.026404
[16:53:40.265] iteration 4810 : loss : 0.321049, loss_ce: 0.010356
[16:53:44.978] iteration 4820 : loss : 0.314178, loss_ce: 0.009663
[16:55:31.972] iteration 4830 : loss : 0.455396, loss_ce: 0.026079
[16:55:36.813] iteration 4840 : loss : 0.502806, loss_ce: 0.145375
[16:55:41.719] iteration 4850 : loss : 0.474301, loss_ce: 0.065993
[16:55:46.567] iteration 4860 : loss : 0.469418, loss_ce: 0.051340
[16:55:51.281] iteration 4870 : loss : 0.302076, loss_ce: 0.036647
[16:55:55.983] iteration 4880 : loss : 0.294964, loss_ce: 0.043052
[16:56:00.984] iteration 4890 : loss : 0.291857, loss_ce: 0.058578
[16:56:05.925] iteration 4900 : loss : 0.436234, loss_ce: 0.057676
[16:56:10.702] iteration 4910 : loss : 0.428894, loss_ce: 0.048296
[16:56:15.571] iteration 4920 : loss : 0.423977, loss_ce: 0.051248
[16:56:20.299] iteration 4930 : loss : 0.426588, loss_ce: 0.050817
[16:56:25.020] iteration 4940 : loss : 0.425888, loss_ce: 0.065750
[16:56:29.683] iteration 4950 : loss : 0.439242, loss_ce: 0.070256
[16:56:34.392] iteration 4960 : loss : 0.446104, loss_ce: 0.054677
[16:56:39.237] iteration 4970 : loss : 0.436356, loss_ce: 0.050697
[16:56:43.956] iteration 4980 : loss : 0.381650, loss_ce: 0.041779
[16:56:48.760] iteration 4990 : loss : 0.397089, loss_ce: 0.028857
[16:56:53.556] iteration 5000 : loss : 0.399885, loss_ce: 0.044562
[16:56:58.295] iteration 5010 : loss : 0.357353, loss_ce: 0.020911
[16:57:03.103] iteration 5020 : loss : 0.391821, loss_ce: 0.052269
[16:57:07.964] iteration 5030 : loss : 0.370820, loss_ce: 0.038211
[16:57:12.720] iteration 5040 : loss : 0.213775, loss_ce: 0.022580
[16:57:17.429] iteration 5050 : loss : 0.348529, loss_ce: 0.046226
[16:57:22.124] iteration 5060 : loss : 0.381481, loss_ce: 0.048595
[16:57:26.889] iteration 5070 : loss : 0.337714, loss_ce: 0.017688
[16:57:31.714] iteration 5080 : loss : 0.321526, loss_ce: 0.024187
[16:57:36.476] iteration 5090 : loss : 0.301756, loss_ce: 0.026549
[16:59:35.116] iteration 5100 : loss : 0.479570, loss_ce: 0.081501
[16:59:39.891] iteration 5110 : loss : 0.468217, loss_ce: 0.049192
[16:59:44.592] iteration 5120 : loss : 0.459585, loss_ce: 0.056030
[16:59:49.266] iteration 5130 : loss : 0.447885, loss_ce: 0.049532
[16:59:54.291] iteration 5140 : loss : 0.431049, loss_ce: 0.039623
[16:59:59.641] iteration 5150 : loss : 0.446497, loss_ce: 0.047229
[17:00:04.832] iteration 5160 : loss : 0.445429, loss_ce: 0.033189
[17:00:10.222] iteration 5170 : loss : 0.436230, loss_ce: 0.052196
[17:00:15.221] iteration 5180 : loss : 0.438531, loss_ce: 0.071279
[17:00:20.291] iteration 5190 : loss : 0.432351, loss_ce: 0.051082
[17:00:25.158] iteration 5200 : loss : 0.418979, loss_ce: 0.053087
[17:00:30.142] iteration 5210 : loss : 0.273394, loss_ce: 0.036534
[17:00:35.309] iteration 5220 : loss : 0.440694, loss_ce: 0.053258
[17:00:40.225] iteration 5230 : loss : 0.394549, loss_ce: 0.049232
[17:00:45.041] iteration 5240 : loss : 0.432107, loss_ce: 0.055280
[17:00:49.993] iteration 5250 : loss : 0.443494, loss_ce: 0.042958
[17:00:54.709] iteration 5260 : loss : 0.439561, loss_ce: 0.051494
[17:00:59.392] iteration 5270 : loss : 0.259558, loss_ce: 0.044522
[17:01:04.273] iteration 5280 : loss : 0.258324, loss_ce: 0.052778
[17:01:09.030] iteration 5290 : loss : 0.305690, loss_ce: 0.030043
[17:01:13.734] iteration 5300 : loss : 0.398972, loss_ce: 0.070658
[17:01:18.437] iteration 5310 : loss : 0.345579, loss_ce: 0.013105
[17:01:23.300] iteration 5320 : loss : 0.195310, loss_ce: 0.025194
[17:01:28.372] iteration 5330 : loss : 0.330169, loss_ce: 0.012288
[17:01:33.615] iteration 5340 : loss : 0.144019, loss_ce: 0.022203
[17:01:38.569] iteration 5350 : loss : 0.319699, loss_ce: 0.007856
[17:01:43.473] iteration 5360 : loss : 0.201486, loss_ce: 0.018505
[17:03:16.736] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_19.pth
[17:03:31.693] iteration 5370 : loss : 0.465959, loss_ce: 0.080391
[17:03:36.304] iteration 5380 : loss : 0.449480, loss_ce: 0.038766
[17:03:41.045] iteration 5390 : loss : 0.460067, loss_ce: 0.077525
[17:03:45.968] iteration 5400 : loss : 0.462194, loss_ce: 0.054315
[17:03:50.699] iteration 5410 : loss : 0.297729, loss_ce: 0.048590
[17:03:55.455] iteration 5420 : loss : 0.455757, loss_ce: 0.050795
[17:04:00.147] iteration 5430 : loss : 0.459409, loss_ce: 0.033774
[17:04:04.954] iteration 5440 : loss : 0.296238, loss_ce: 0.037921
[17:04:09.767] iteration 5450 : loss : 0.445972, loss_ce: 0.055550
[17:04:14.438] iteration 5460 : loss : 0.450173, loss_ce: 0.028024
[17:04:19.123] iteration 5470 : loss : 0.445957, loss_ce: 0.051916
[17:04:23.815] iteration 5480 : loss : 0.436568, loss_ce: 0.070246
[17:04:28.538] iteration 5490 : loss : 0.440211, loss_ce: 0.072116
[17:04:33.255] iteration 5500 : loss : 0.285014, loss_ce: 0.040645
[17:04:37.958] iteration 5510 : loss : 0.429491, loss_ce: 0.034986
[17:04:42.779] iteration 5520 : loss : 0.294931, loss_ce: 0.078953
[17:04:47.686] iteration 5530 : loss : 0.437985, loss_ce: 0.050784
[17:04:52.596] iteration 5540 : loss : 0.270979, loss_ce: 0.058956
[17:04:57.289] iteration 5550 : loss : 0.425953, loss_ce: 0.077660
[17:05:01.997] iteration 5560 : loss : 0.427171, loss_ce: 0.042322
[17:05:06.895] iteration 5570 : loss : 0.409433, loss_ce: 0.042611
[17:05:11.750] iteration 5580 : loss : 0.433671, loss_ce: 0.040563
[17:05:16.480] iteration 5590 : loss : 0.249695, loss_ce: 0.016145
[17:05:21.317] iteration 5600 : loss : 0.395071, loss_ce: 0.046525
[17:05:26.231] iteration 5610 : loss : 0.364357, loss_ce: 0.039068
[17:05:30.952] iteration 5620 : loss : 0.327947, loss_ce: 0.029722
[17:07:19.041] iteration 5630 : loss : 0.458855, loss_ce: 0.098005
[17:07:23.821] iteration 5640 : loss : 0.500289, loss_ce: 0.124512
[17:07:28.497] iteration 5650 : loss : 0.473768, loss_ce: 0.058348
[17:07:33.331] iteration 5660 : loss : 0.467549, loss_ce: 0.048462
[17:07:38.197] iteration 5670 : loss : 0.462109, loss_ce: 0.070893
[17:07:43.320] iteration 5680 : loss : 0.452791, loss_ce: 0.063037
[17:07:48.365] iteration 5690 : loss : 0.295719, loss_ce: 0.040238
[17:07:53.514] iteration 5700 : loss : 0.450414, loss_ce: 0.060144
[17:07:58.535] iteration 5710 : loss : 0.444696, loss_ce: 0.081239
[17:08:03.328] iteration 5720 : loss : 0.283938, loss_ce: 0.047182
[17:08:08.138] iteration 5730 : loss : 0.448520, loss_ce: 0.057426
[17:08:12.892] iteration 5740 : loss : 0.436363, loss_ce: 0.040800
[17:08:17.851] iteration 5750 : loss : 0.437223, loss_ce: 0.063469
[17:08:22.580] iteration 5760 : loss : 0.270865, loss_ce: 0.058794
[17:08:27.308] iteration 5770 : loss : 0.416821, loss_ce: 0.065659
[17:08:32.039] iteration 5780 : loss : 0.444098, loss_ce: 0.077170
[17:08:36.892] iteration 5790 : loss : 0.430449, loss_ce: 0.039450
[17:08:41.688] iteration 5800 : loss : 0.422430, loss_ce: 0.066825
[17:08:47.133] iteration 5810 : loss : 0.413600, loss_ce: 0.061956
[17:08:52.527] iteration 5820 : loss : 0.410932, loss_ce: 0.030380
[17:08:57.961] iteration 5830 : loss : 0.259559, loss_ce: 0.034059
[17:09:03.159] iteration 5840 : loss : 0.387563, loss_ce: 0.023512
[17:09:08.210] iteration 5850 : loss : 0.339110, loss_ce: 0.026005
[17:09:13.002] iteration 5860 : loss : 0.345998, loss_ce: 0.026754
[17:09:17.809] iteration 5870 : loss : 0.350292, loss_ce: 0.016595
[17:09:22.743] iteration 5880 : loss : 0.352102, loss_ce: 0.039436
[17:09:27.525] iteration 5890 : loss : 0.328844, loss_ce: 0.012854
[17:11:27.186] iteration 5900 : loss : 0.511506, loss_ce: 0.152064
[17:11:31.907] iteration 5910 : loss : 0.316035, loss_ce: 0.061242
[17:11:36.670] iteration 5920 : loss : 0.451140, loss_ce: 0.065519
[17:11:41.383] iteration 5930 : loss : 0.452103, loss_ce: 0.069610
[17:11:46.168] iteration 5940 : loss : 0.297359, loss_ce: 0.038719
[17:11:51.037] iteration 5950 : loss : 0.442406, loss_ce: 0.051599
[17:11:55.825] iteration 5960 : loss : 0.416662, loss_ce: 0.049520
[17:12:00.884] iteration 5970 : loss : 0.429337, loss_ce: 0.017789
[17:12:05.713] iteration 5980 : loss : 0.414537, loss_ce: 0.043449
[17:12:10.548] iteration 5990 : loss : 0.402732, loss_ce: 0.038477
[17:12:15.452] iteration 6000 : loss : 0.390621, loss_ce: 0.037687
[17:12:20.366] iteration 6010 : loss : 0.432784, loss_ce: 0.087682
[17:12:25.260] iteration 6020 : loss : 0.352532, loss_ce: 0.036000
[17:12:30.144] iteration 6030 : loss : 0.386593, loss_ce: 0.054544
[17:12:35.026] iteration 6040 : loss : 0.359234, loss_ce: 0.028647
[17:12:39.792] iteration 6050 : loss : 0.319270, loss_ce: 0.030469
[17:12:44.567] iteration 6060 : loss : 0.202200, loss_ce: 0.040227
[17:12:49.352] iteration 6070 : loss : 0.385448, loss_ce: 0.046118
[17:12:54.097] iteration 6080 : loss : 0.349525, loss_ce: 0.024544
[17:12:58.938] iteration 6090 : loss : 0.147664, loss_ce: 0.021292
[17:13:03.638] iteration 6100 : loss : 0.223474, loss_ce: 0.011838
[17:13:08.401] iteration 6110 : loss : 0.069120, loss_ce: 0.017941
[17:13:13.310] iteration 6120 : loss : 0.307120, loss_ce: 0.033743
[17:13:18.155] iteration 6130 : loss : 0.270815, loss_ce: 0.033621
[17:13:23.018] iteration 6140 : loss : 0.233521, loss_ce: 0.017132
[17:13:27.875] iteration 6150 : loss : 0.325232, loss_ce: 0.029698
[17:13:32.773] iteration 6160 : loss : 0.175048, loss_ce: 0.008527
[17:15:23.903] iteration 6170 : loss : 0.482734, loss_ce: 0.086275
[17:15:29.505] iteration 6180 : loss : 0.482164, loss_ce: 0.086369
[17:15:35.030] iteration 6190 : loss : 0.454192, loss_ce: 0.054780
[17:15:40.607] iteration 6200 : loss : 0.446545, loss_ce: 0.049659
[17:15:45.797] iteration 6210 : loss : 0.438947, loss_ce: 0.048188
[17:15:51.001] iteration 6220 : loss : 0.400596, loss_ce: 0.050172
[17:15:56.426] iteration 6230 : loss : 0.387815, loss_ce: 0.044963
[17:16:01.504] iteration 6240 : loss : 0.370767, loss_ce: 0.048735
[17:16:06.333] iteration 6250 : loss : 0.203333, loss_ce: 0.012344
[17:16:11.247] iteration 6260 : loss : 0.298822, loss_ce: 0.020602
[17:16:16.301] iteration 6270 : loss : 0.265182, loss_ce: 0.041483
[17:16:21.024] iteration 6280 : loss : 0.342456, loss_ce: 0.022710
[17:16:25.808] iteration 6290 : loss : 0.283543, loss_ce: 0.032814
[17:16:30.604] iteration 6300 : loss : 0.366875, loss_ce: 0.045408
[17:16:35.426] iteration 6310 : loss : 0.289152, loss_ce: 0.023231
[17:16:40.253] iteration 6320 : loss : 0.201089, loss_ce: 0.030564
[17:16:45.242] iteration 6330 : loss : 0.319931, loss_ce: 0.013215
[17:16:50.029] iteration 6340 : loss : 0.147186, loss_ce: 0.009038
[17:16:54.784] iteration 6350 : loss : 0.356692, loss_ce: 0.045168
[17:16:59.804] iteration 6360 : loss : 0.191187, loss_ce: 0.016618
[17:17:05.126] iteration 6370 : loss : 0.320505, loss_ce: 0.013511
[17:17:10.223] iteration 6380 : loss : 0.241649, loss_ce: 0.018331
[17:17:15.008] iteration 6390 : loss : 0.307985, loss_ce: 0.036760
[17:17:19.761] iteration 6400 : loss : 0.073667, loss_ce: 0.012134
[17:17:24.910] iteration 6410 : loss : 0.289848, loss_ce: 0.018323
[17:17:30.301] iteration 6420 : loss : 0.316017, loss_ce: 0.007551
[17:17:35.298] iteration 6430 : loss : 0.294694, loss_ce: 0.021487
[17:19:28.125] iteration 6440 : loss : 0.477012, loss_ce: 0.080276
[17:19:33.746] iteration 6450 : loss : 0.465646, loss_ce: 0.047984
[17:19:39.502] iteration 6460 : loss : 0.304452, loss_ce: 0.052407
[17:19:44.792] iteration 6470 : loss : 0.297847, loss_ce: 0.054711
[17:19:49.640] iteration 6480 : loss : 0.446438, loss_ce: 0.060422
[17:19:54.851] iteration 6490 : loss : 0.282395, loss_ce: 0.064277
[17:20:00.084] iteration 6500 : loss : 0.440413, loss_ce: 0.052035
[17:20:05.272] iteration 6510 : loss : 0.444676, loss_ce: 0.034853
[17:20:10.521] iteration 6520 : loss : 0.438088, loss_ce: 0.058969
[17:20:15.515] iteration 6530 : loss : 0.429334, loss_ce: 0.057634
[17:20:20.315] iteration 6540 : loss : 0.440426, loss_ce: 0.059189
[17:20:25.153] iteration 6550 : loss : 0.268450, loss_ce: 0.054336
[17:20:29.920] iteration 6560 : loss : 0.425697, loss_ce: 0.031935
[17:20:35.105] iteration 6570 : loss : 0.246522, loss_ce: 0.038910
[17:20:40.095] iteration 6580 : loss : 0.415337, loss_ce: 0.050754
[17:20:44.800] iteration 6590 : loss : 0.426824, loss_ce: 0.037029
[17:20:49.928] iteration 6600 : loss : 0.276236, loss_ce: 0.029083
[17:20:54.863] iteration 6610 : loss : 0.361274, loss_ce: 0.035777
[17:20:59.684] iteration 6620 : loss : 0.382495, loss_ce: 0.056076
[17:21:04.465] iteration 6630 : loss : 0.352634, loss_ce: 0.042027
[17:21:09.366] iteration 6640 : loss : 0.359662, loss_ce: 0.033188
[17:21:14.155] iteration 6650 : loss : 0.320742, loss_ce: 0.029946
[17:21:19.062] iteration 6660 : loss : 0.340620, loss_ce: 0.017722
[17:21:24.077] iteration 6670 : loss : 0.358814, loss_ce: 0.023479
[17:21:28.956] iteration 6680 : loss : 0.305581, loss_ce: 0.036905
[17:21:33.825] iteration 6690 : loss : 0.334958, loss_ce: 0.009937
[17:21:38.715] iteration 6700 : loss : 0.168576, loss_ce: 0.022777
[17:23:38.972] iteration 6710 : loss : 0.504831, loss_ce: 0.135372
[17:23:43.585] iteration 6720 : loss : 0.495651, loss_ce: 0.112752
[17:23:48.393] iteration 6730 : loss : 0.475307, loss_ce: 0.088206
[17:23:53.249] iteration 6740 : loss : 0.463428, loss_ce: 0.064435
[17:23:58.325] iteration 6750 : loss : 0.445226, loss_ce: 0.061856
[17:24:03.205] iteration 6760 : loss : 0.285752, loss_ce: 0.037688
[17:24:08.120] iteration 6770 : loss : 0.438018, loss_ce: 0.068822
[17:24:13.185] iteration 6780 : loss : 0.314469, loss_ce: 0.109466
[17:24:18.194] iteration 6790 : loss : 0.280647, loss_ce: 0.044912
[17:24:23.069] iteration 6800 : loss : 0.275838, loss_ce: 0.039142
[17:24:27.961] iteration 6810 : loss : 0.274308, loss_ce: 0.052242
[17:24:32.908] iteration 6820 : loss : 0.457792, loss_ce: 0.120195
[17:24:37.747] iteration 6830 : loss : 0.469324, loss_ce: 0.060052
[17:24:42.717] iteration 6840 : loss : 0.456906, loss_ce: 0.049849
[17:24:47.545] iteration 6850 : loss : 0.449970, loss_ce: 0.046654
[17:24:52.797] iteration 6860 : loss : 0.294210, loss_ce: 0.049085
[17:24:57.889] iteration 6870 : loss : 0.305539, loss_ce: 0.057665
[17:25:02.958] iteration 6880 : loss : 0.444424, loss_ce: 0.066890
[17:25:07.733] iteration 6890 : loss : 0.436553, loss_ce: 0.055032
[17:25:12.620] iteration 6900 : loss : 0.432452, loss_ce: 0.046102
[17:25:17.713] iteration 6910 : loss : 0.282632, loss_ce: 0.054903
[17:25:22.818] iteration 6920 : loss : 0.436210, loss_ce: 0.054833
[17:25:27.740] iteration 6930 : loss : 0.300519, loss_ce: 0.038321
[17:25:32.683] iteration 6940 : loss : 0.440398, loss_ce: 0.086881
[17:25:37.572] iteration 6950 : loss : 0.450263, loss_ce: 0.067314
[17:25:42.300] iteration 6960 : loss : 0.295862, loss_ce: 0.043422
[17:27:32.590] iteration 6970 : loss : 0.468263, loss_ce: 0.115219
[17:27:37.557] iteration 6980 : loss : 0.475146, loss_ce: 0.062112
[17:27:42.432] iteration 6990 : loss : 0.462761, loss_ce: 0.063003
[17:27:47.248] iteration 7000 : loss : 0.451238, loss_ce: 0.064612
[17:27:52.314] iteration 7010 : loss : 0.451902, loss_ce: 0.052544
[17:27:57.230] iteration 7020 : loss : 0.448834, loss_ce: 0.074255
[17:28:02.063] iteration 7030 : loss : 0.448538, loss_ce: 0.052546
[17:28:06.930] iteration 7040 : loss : 0.442167, loss_ce: 0.040163
[17:28:11.771] iteration 7050 : loss : 0.442476, loss_ce: 0.049104
[17:28:16.761] iteration 7060 : loss : 0.440998, loss_ce: 0.094478
[17:28:21.510] iteration 7070 : loss : 0.431977, loss_ce: 0.065728
[17:28:26.317] iteration 7080 : loss : 0.438352, loss_ce: 0.046280
[17:28:31.210] iteration 7090 : loss : 0.426013, loss_ce: 0.044376
[17:28:35.980] iteration 7100 : loss : 0.430620, loss_ce: 0.044172
[17:28:40.959] iteration 7110 : loss : 0.423827, loss_ce: 0.042136
[17:28:45.980] iteration 7120 : loss : 0.424754, loss_ce: 0.062745
[17:28:50.865] iteration 7130 : loss : 0.421309, loss_ce: 0.029521
[17:28:55.721] iteration 7140 : loss : 0.407449, loss_ce: 0.045299
[17:29:00.498] iteration 7150 : loss : 0.391387, loss_ce: 0.033384
[17:29:05.353] iteration 7160 : loss : 0.240877, loss_ce: 0.015409
[17:29:10.219] iteration 7170 : loss : 0.370177, loss_ce: 0.022564
[17:29:15.043] iteration 7180 : loss : 0.187669, loss_ce: 0.015812
[17:29:19.913] iteration 7190 : loss : 0.452985, loss_ce: 0.061611
[17:29:24.860] iteration 7200 : loss : 0.346152, loss_ce: 0.031089
[17:29:29.931] iteration 7210 : loss : 0.334664, loss_ce: 0.043815
[17:29:34.827] iteration 7220 : loss : 0.341648, loss_ce: 0.020078
[17:29:39.766] iteration 7230 : loss : 0.242019, loss_ce: 0.017527
[17:31:28.746] iteration 7240 : loss : 0.474972, loss_ce: 0.061437
[17:31:33.525] iteration 7250 : loss : 0.453807, loss_ce: 0.063453
[17:31:38.175] iteration 7260 : loss : 0.453755, loss_ce: 0.044804
[17:31:42.841] iteration 7270 : loss : 0.449018, loss_ce: 0.051302
[17:31:47.887] iteration 7280 : loss : 0.447019, loss_ce: 0.069409
[17:31:52.811] iteration 7290 : loss : 0.434440, loss_ce: 0.056653
[17:31:58.054] iteration 7300 : loss : 0.447848, loss_ce: 0.074260
[17:32:03.442] iteration 7310 : loss : 0.279361, loss_ce: 0.040033
[17:32:08.324] iteration 7320 : loss : 0.443034, loss_ce: 0.073254
[17:32:13.485] iteration 7330 : loss : 0.420096, loss_ce: 0.067255
[17:32:18.309] iteration 7340 : loss : 0.278715, loss_ce: 0.037372
[17:32:23.180] iteration 7350 : loss : 0.295854, loss_ce: 0.036698
[17:32:28.122] iteration 7360 : loss : 0.419714, loss_ce: 0.056384
[17:32:33.034] iteration 7370 : loss : 0.281690, loss_ce: 0.048166
[17:32:37.810] iteration 7380 : loss : 0.246672, loss_ce: 0.018409
[17:32:42.820] iteration 7390 : loss : 0.393708, loss_ce: 0.040414
[17:32:47.646] iteration 7400 : loss : 0.227048, loss_ce: 0.017855
[17:32:52.478] iteration 7410 : loss : 0.386686, loss_ce: 0.048658
[17:32:57.174] iteration 7420 : loss : 0.333394, loss_ce: 0.035088
[17:33:02.002] iteration 7430 : loss : 0.334495, loss_ce: 0.032988
[17:33:06.766] iteration 7440 : loss : 0.319880, loss_ce: 0.018435
[17:33:11.588] iteration 7450 : loss : 0.325952, loss_ce: 0.039817
[17:33:16.586] iteration 7460 : loss : 0.354859, loss_ce: 0.025688
[17:33:21.570] iteration 7470 : loss : 0.171499, loss_ce: 0.006661
[17:33:26.519] iteration 7480 : loss : 0.310552, loss_ce: 0.022306
[17:33:31.311] iteration 7490 : loss : 0.192170, loss_ce: 0.008867
[17:33:36.064] iteration 7500 : loss : 0.279605, loss_ce: 0.026938
[17:35:38.052] iteration 7510 : loss : 0.480632, loss_ce: 0.075260
[17:35:42.869] iteration 7520 : loss : 0.452831, loss_ce: 0.058459
[17:35:47.651] iteration 7530 : loss : 0.451654, loss_ce: 0.048200
[17:35:52.502] iteration 7540 : loss : 0.444840, loss_ce: 0.098870
[17:35:57.726] iteration 7550 : loss : 0.431550, loss_ce: 0.044156
[17:36:02.685] iteration 7560 : loss : 0.452905, loss_ce: 0.052197
[17:36:07.877] iteration 7570 : loss : 0.435603, loss_ce: 0.091658
[17:36:12.617] iteration 7580 : loss : 0.436549, loss_ce: 0.070281
[17:36:17.423] iteration 7590 : loss : 0.272742, loss_ce: 0.051369
[17:36:22.200] iteration 7600 : loss : 0.414537, loss_ce: 0.044463
[17:36:27.053] iteration 7610 : loss : 0.418271, loss_ce: 0.052757
[17:36:32.005] iteration 7620 : loss : 0.394697, loss_ce: 0.034917
[17:36:37.719] iteration 7630 : loss : 0.340523, loss_ce: 0.047423
[17:36:43.251] iteration 7640 : loss : 0.359379, loss_ce: 0.052314
[17:36:47.982] iteration 7650 : loss : 0.357451, loss_ce: 0.040856
[17:36:52.670] iteration 7660 : loss : 0.350671, loss_ce: 0.016462
[17:36:57.742] iteration 7670 : loss : 0.275809, loss_ce: 0.035526
[17:37:02.668] iteration 7680 : loss : 0.158859, loss_ce: 0.026771
[17:37:07.468] iteration 7690 : loss : 0.263481, loss_ce: 0.022036
[17:37:12.252] iteration 7700 : loss : 0.324548, loss_ce: 0.064354
[17:37:17.273] iteration 7710 : loss : 0.333574, loss_ce: 0.015493
[17:37:22.089] iteration 7720 : loss : 0.323131, loss_ce: 0.027779
[17:37:26.976] iteration 7730 : loss : 0.328836, loss_ce: 0.017891
[17:37:31.785] iteration 7740 : loss : 0.313586, loss_ce: 0.018047
[17:37:36.587] iteration 7750 : loss : 0.322724, loss_ce: 0.008936
[17:37:41.719] iteration 7760 : loss : 0.309906, loss_ce: 0.015152
[17:37:46.773] iteration 7770 : loss : 0.064223, loss_ce: 0.012681
[17:39:34.922] iteration 7780 : loss : 0.458527, loss_ce: 0.059016
[17:39:39.863] iteration 7790 : loss : 0.461017, loss_ce: 0.054636
[17:39:45.163] iteration 7800 : loss : 0.451749, loss_ce: 0.093213
[17:39:49.938] iteration 7810 : loss : 0.284599, loss_ce: 0.049567
[17:39:54.699] iteration 7820 : loss : 0.426716, loss_ce: 0.044050
[17:39:59.760] iteration 7830 : loss : 0.441104, loss_ce: 0.039970
[17:40:04.543] iteration 7840 : loss : 0.446638, loss_ce: 0.057607
[17:40:09.339] iteration 7850 : loss : 0.434770, loss_ce: 0.054489
[17:40:14.108] iteration 7860 : loss : 0.406020, loss_ce: 0.074003
[17:40:18.881] iteration 7870 : loss : 0.395113, loss_ce: 0.034513
[17:40:23.615] iteration 7880 : loss : 0.386043, loss_ce: 0.041511
[17:40:28.441] iteration 7890 : loss : 0.200535, loss_ce: 0.020526
[17:40:33.611] iteration 7900 : loss : 0.282334, loss_ce: 0.074129
[17:40:38.898] iteration 7910 : loss : 0.334967, loss_ce: 0.022563
[17:40:43.609] iteration 7920 : loss : 0.149301, loss_ce: 0.014889
[17:40:48.322] iteration 7930 : loss : 0.279522, loss_ce: 0.020025
[17:40:53.049] iteration 7940 : loss : 0.343742, loss_ce: 0.027832
[17:40:58.212] iteration 7950 : loss : 0.314306, loss_ce: 0.010951
[17:41:04.105] iteration 7960 : loss : 0.315141, loss_ce: 0.033664
[17:41:08.907] iteration 7970 : loss : 0.253103, loss_ce: 0.010188
[17:41:13.636] iteration 7980 : loss : 0.332376, loss_ce: 0.078393
[17:41:18.356] iteration 7990 : loss : 0.346567, loss_ce: 0.019339
[17:41:23.067] iteration 8000 : loss : 0.355860, loss_ce: 0.021931
[17:41:27.881] iteration 8010 : loss : 0.270078, loss_ce: 0.030392
[17:41:32.609] iteration 8020 : loss : 0.324523, loss_ce: 0.020423
[17:41:37.460] iteration 8030 : loss : 0.105450, loss_ce: 0.022647
[17:41:42.908] iteration 8040 : loss : 0.232514, loss_ce: 0.022143
[17:43:19.435] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_29.pth
[17:43:34.665] iteration 8050 : loss : 0.475914, loss_ce: 0.104526
[17:43:39.474] iteration 8060 : loss : 0.458244, loss_ce: 0.063919
[17:43:44.337] iteration 8070 : loss : 0.435242, loss_ce: 0.065994
[17:43:49.138] iteration 8080 : loss : 0.434030, loss_ce: 0.044057
[17:43:53.868] iteration 8090 : loss : 0.438929, loss_ce: 0.062126
[17:43:59.063] iteration 8100 : loss : 0.424984, loss_ce: 0.030783
[17:44:03.988] iteration 8110 : loss : 0.391444, loss_ce: 0.046371
[17:44:08.766] iteration 8120 : loss : 0.362151, loss_ce: 0.028513
[17:44:13.608] iteration 8130 : loss : 0.323279, loss_ce: 0.031687
[17:44:18.636] iteration 8140 : loss : 0.355528, loss_ce: 0.051969
[17:44:23.413] iteration 8150 : loss : 0.203951, loss_ce: 0.034317
[17:44:28.133] iteration 8160 : loss : 0.306949, loss_ce: 0.024020
[17:44:32.889] iteration 8170 : loss : 0.333436, loss_ce: 0.006343
[17:44:37.610] iteration 8180 : loss : 0.316010, loss_ce: 0.025254
[17:44:42.345] iteration 8190 : loss : 0.369652, loss_ce: 0.025696
[17:44:47.076] iteration 8200 : loss : 0.260778, loss_ce: 0.017057
[17:44:51.851] iteration 8210 : loss : 0.304847, loss_ce: 0.022682
[17:44:56.648] iteration 8220 : loss : 0.241980, loss_ce: 0.013915
[17:45:01.416] iteration 8230 : loss : 0.234744, loss_ce: 0.020104
[17:45:06.097] iteration 8240 : loss : 0.319292, loss_ce: 0.035114
[17:45:10.890] iteration 8250 : loss : 0.214305, loss_ce: 0.017513
[17:45:15.649] iteration 8260 : loss : 0.103420, loss_ce: 0.021068
[17:45:20.432] iteration 8270 : loss : 0.297293, loss_ce: 0.013344
[17:45:25.224] iteration 8280 : loss : 0.237447, loss_ce: 0.022652
[17:45:30.909] iteration 8290 : loss : 0.287395, loss_ce: 0.021108
[17:45:36.546] iteration 8300 : loss : 0.234788, loss_ce: 0.010685
[17:47:38.951] iteration 8310 : loss : 0.463560, loss_ce: 0.092051
[17:47:43.694] iteration 8320 : loss : 0.463960, loss_ce: 0.073730
[17:47:48.335] iteration 8330 : loss : 0.454221, loss_ce: 0.074346
[17:47:52.957] iteration 8340 : loss : 0.286562, loss_ce: 0.056910
[17:47:57.582] iteration 8350 : loss : 0.286357, loss_ce: 0.052175
[17:48:03.046] iteration 8360 : loss : 0.430940, loss_ce: 0.040118
[17:48:08.218] iteration 8370 : loss : 0.436994, loss_ce: 0.029809
[17:48:12.885] iteration 8380 : loss : 0.414223, loss_ce: 0.056300
[17:48:17.574] iteration 8390 : loss : 0.396257, loss_ce: 0.042315
[17:48:22.354] iteration 8400 : loss : 0.366905, loss_ce: 0.065724
[17:48:27.080] iteration 8410 : loss : 0.146317, loss_ce: 0.013903
[17:48:31.963] iteration 8420 : loss : 0.334022, loss_ce: 0.028894
[17:48:36.662] iteration 8430 : loss : 0.315630, loss_ce: 0.016705
[17:48:41.390] iteration 8440 : loss : 0.164561, loss_ce: 0.018710
[17:48:46.127] iteration 8450 : loss : 0.294285, loss_ce: 0.013508
[17:48:50.824] iteration 8460 : loss : 0.247796, loss_ce: 0.019943
[17:48:55.683] iteration 8470 : loss : 0.386054, loss_ce: 0.035530
[17:49:00.566] iteration 8480 : loss : 0.324073, loss_ce: 0.011608
[17:49:05.254] iteration 8490 : loss : 0.198310, loss_ce: 0.011408
[17:49:09.975] iteration 8500 : loss : 0.291971, loss_ce: 0.027208
[17:49:14.708] iteration 8510 : loss : 0.091946, loss_ce: 0.012874
[17:49:19.487] iteration 8520 : loss : 0.240061, loss_ce: 0.009991
[17:49:24.420] iteration 8530 : loss : 0.134278, loss_ce: 0.010707
[17:49:29.568] iteration 8540 : loss : 0.288747, loss_ce: 0.043930
[17:49:34.395] iteration 8550 : loss : 0.269665, loss_ce: 0.013028
[17:49:39.151] iteration 8560 : loss : 0.242550, loss_ce: 0.020554
[17:49:44.048] iteration 8570 : loss : 0.286966, loss_ce: 0.023702
[17:51:32.390] iteration 8580 : loss : 0.393826, loss_ce: 0.022776
[17:51:37.304] iteration 8590 : loss : 0.413430, loss_ce: 0.039176
[17:51:42.118] iteration 8600 : loss : 0.236308, loss_ce: 0.074810
[17:51:47.716] iteration 8610 : loss : 0.332572, loss_ce: 0.023739
[17:51:52.915] iteration 8620 : loss : 0.370720, loss_ce: 0.033033
[17:51:57.593] iteration 8630 : loss : 0.348750, loss_ce: 0.027575
[17:52:02.323] iteration 8640 : loss : 0.350032, loss_ce: 0.030533
[17:52:07.035] iteration 8650 : loss : 0.143493, loss_ce: 0.020890
[17:52:11.747] iteration 8660 : loss : 0.281683, loss_ce: 0.018095
[17:52:16.481] iteration 8670 : loss : 0.236885, loss_ce: 0.010265
[17:52:21.196] iteration 8680 : loss : 0.151168, loss_ce: 0.027263
[17:52:25.946] iteration 8690 : loss : 0.330375, loss_ce: 0.012276
[17:52:30.689] iteration 8700 : loss : 0.247295, loss_ce: 0.011454
[17:52:35.471] iteration 8710 : loss : 0.243464, loss_ce: 0.017353
[17:52:40.195] iteration 8720 : loss : 0.237802, loss_ce: 0.012084
[17:52:45.140] iteration 8730 : loss : 0.325271, loss_ce: 0.024469
[17:52:49.870] iteration 8740 : loss : 0.232378, loss_ce: 0.013283
[17:52:54.621] iteration 8750 : loss : 0.288193, loss_ce: 0.025792
[17:52:59.437] iteration 8760 : loss : 0.180895, loss_ce: 0.008925
[17:53:04.295] iteration 8770 : loss : 0.275477, loss_ce: 0.016326
[17:53:09.060] iteration 8780 : loss : 0.272509, loss_ce: 0.013818
[17:53:13.993] iteration 8790 : loss : 0.359563, loss_ce: 0.020185
[17:53:18.800] iteration 8800 : loss : 0.335746, loss_ce: 0.027885
[17:53:23.666] iteration 8810 : loss : 0.172939, loss_ce: 0.006860
[17:53:28.686] iteration 8820 : loss : 0.259383, loss_ce: 0.021661
[17:53:33.663] iteration 8830 : loss : 0.205051, loss_ce: 0.009220
[17:53:38.608] iteration 8840 : loss : 0.252518, loss_ce: 0.017364
[17:55:27.554] iteration 8850 : loss : 0.400404, loss_ce: 0.050340
[17:55:32.261] iteration 8860 : loss : 0.380923, loss_ce: 0.076635
[17:55:37.060] iteration 8870 : loss : 0.331496, loss_ce: 0.013646
[17:55:42.089] iteration 8880 : loss : 0.343278, loss_ce: 0.029831
[17:55:46.817] iteration 8890 : loss : 0.335377, loss_ce: 0.019453
[17:55:52.711] iteration 8900 : loss : 0.296591, loss_ce: 0.023543
[17:55:57.493] iteration 8910 : loss : 0.320025, loss_ce: 0.016680
[17:56:02.256] iteration 8920 : loss : 0.234625, loss_ce: 0.056175
[17:56:06.945] iteration 8930 : loss : 0.364265, loss_ce: 0.037906
[17:56:11.828] iteration 8940 : loss : 0.337572, loss_ce: 0.043164
[17:56:16.578] iteration 8950 : loss : 0.345655, loss_ce: 0.010464
[17:56:21.333] iteration 8960 : loss : 0.160385, loss_ce: 0.005428
[17:56:26.094] iteration 8970 : loss : 0.357854, loss_ce: 0.025060
[17:56:30.761] iteration 8980 : loss : 0.239663, loss_ce: 0.020736
[17:56:35.645] iteration 8990 : loss : 0.189019, loss_ce: 0.019631
[17:56:40.486] iteration 9000 : loss : 0.317777, loss_ce: 0.007499
[17:56:45.528] iteration 9010 : loss : 0.296035, loss_ce: 0.011768
[17:56:50.615] iteration 9020 : loss : 0.056714, loss_ce: 0.008960
[17:56:55.734] iteration 9030 : loss : 0.248249, loss_ce: 0.029379
[17:57:00.573] iteration 9040 : loss : 0.216561, loss_ce: 0.022879
[17:57:05.560] iteration 9050 : loss : 0.312323, loss_ce: 0.005738
[17:57:10.521] iteration 9060 : loss : 0.229929, loss_ce: 0.008115
[17:57:15.391] iteration 9070 : loss : 0.292008, loss_ce: 0.008693
[17:57:20.200] iteration 9080 : loss : 0.192972, loss_ce: 0.006180
[17:57:24.980] iteration 9090 : loss : 0.078356, loss_ce: 0.014758
[17:57:29.756] iteration 9100 : loss : 0.249677, loss_ce: 0.012007
[17:57:34.566] iteration 9110 : loss : 0.295930, loss_ce: 0.008117
[17:59:38.143] iteration 9120 : loss : 0.331224, loss_ce: 0.073569
[17:59:43.972] iteration 9130 : loss : 0.461823, loss_ce: 0.050401
[17:59:49.299] iteration 9140 : loss : 0.448396, loss_ce: 0.061669
[17:59:55.665] iteration 9150 : loss : 0.442658, loss_ce: 0.057373
[18:00:02.192] iteration 9160 : loss : 0.259439, loss_ce: 0.029297
[18:00:09.674] iteration 9170 : loss : 0.399994, loss_ce: 0.033970
[18:00:17.392] iteration 9180 : loss : 0.326992, loss_ce: 0.023738
[18:00:25.679] iteration 9190 : loss : 0.337511, loss_ce: 0.026937
[18:00:33.132] iteration 9200 : loss : 0.327742, loss_ce: 0.029830
[18:00:40.580] iteration 9210 : loss : 0.167956, loss_ce: 0.017245
[18:00:48.116] iteration 9220 : loss : 0.292378, loss_ce: 0.059492
[18:00:54.121] iteration 9230 : loss : 0.293777, loss_ce: 0.021726
[18:00:59.854] iteration 9240 : loss : 0.302939, loss_ce: 0.027565
[18:01:06.738] iteration 9250 : loss : 0.335309, loss_ce: 0.003890
[18:01:12.188] iteration 9260 : loss : 0.319036, loss_ce: 0.020362
[18:01:17.060] iteration 9270 : loss : 0.193019, loss_ce: 0.006636
[18:01:22.500] iteration 9280 : loss : 0.300884, loss_ce: 0.009037
[18:01:27.701] iteration 9290 : loss : 0.271394, loss_ce: 0.037792
[18:01:32.553] iteration 9300 : loss : 0.298703, loss_ce: 0.032575
[18:01:37.527] iteration 9310 : loss : 0.168548, loss_ce: 0.052305
[18:01:42.321] iteration 9320 : loss : 0.314707, loss_ce: 0.009810
[18:01:47.092] iteration 9330 : loss : 0.303519, loss_ce: 0.043943
[18:01:52.076] iteration 9340 : loss : 0.126132, loss_ce: 0.007186
[18:01:57.375] iteration 9350 : loss : 0.315837, loss_ce: 0.004820
[18:02:02.235] iteration 9360 : loss : 0.064194, loss_ce: 0.010443
[18:02:08.108] iteration 9370 : loss : 0.327598, loss_ce: 0.039197
[18:02:13.407] iteration 9380 : loss : 0.341631, loss_ce: 0.024266
[18:04:07.737] iteration 9390 : loss : 0.367219, loss_ce: 0.019049
[18:04:12.841] iteration 9400 : loss : 0.351118, loss_ce: 0.024769
[18:04:17.862] iteration 9410 : loss : 0.217504, loss_ce: 0.014960
[18:04:22.558] iteration 9420 : loss : 0.195521, loss_ce: 0.031595
[18:04:27.303] iteration 9430 : loss : 0.185928, loss_ce: 0.021725
[18:04:32.228] iteration 9440 : loss : 0.176764, loss_ce: 0.032596
[18:04:37.301] iteration 9450 : loss : 0.338360, loss_ce: 0.030223
[18:04:42.087] iteration 9460 : loss : 0.107420, loss_ce: 0.020353
[18:04:46.812] iteration 9470 : loss : 0.336022, loss_ce: 0.016706
[18:04:52.020] iteration 9480 : loss : 0.263483, loss_ce: 0.015300
[18:04:57.053] iteration 9490 : loss : 0.314890, loss_ce: 0.044399
[18:05:02.082] iteration 9500 : loss : 0.311644, loss_ce: 0.035865
[18:05:07.025] iteration 9510 : loss : 0.247551, loss_ce: 0.024145
[18:05:12.002] iteration 9520 : loss : 0.151065, loss_ce: 0.020276
[18:05:17.134] iteration 9530 : loss : 0.271045, loss_ce: 0.016044
[18:05:22.311] iteration 9540 : loss : 0.243267, loss_ce: 0.013223
[18:05:27.436] iteration 9550 : loss : 0.048571, loss_ce: 0.005521
[18:05:32.578] iteration 9560 : loss : 0.317989, loss_ce: 0.015856
[18:05:37.497] iteration 9570 : loss : 0.337330, loss_ce: 0.029734
[18:05:42.581] iteration 9580 : loss : 0.116826, loss_ce: 0.013393
[18:05:47.630] iteration 9590 : loss : 0.288044, loss_ce: 0.018108
[18:05:52.424] iteration 9600 : loss : 0.331545, loss_ce: 0.043245
[18:05:58.228] iteration 9610 : loss : 0.052706, loss_ce: 0.005960
[18:06:04.761] iteration 9620 : loss : 0.309142, loss_ce: 0.027621
[18:06:10.298] iteration 9630 : loss : 0.167239, loss_ce: 0.005637
[18:06:16.722] iteration 9640 : loss : 0.244257, loss_ce: 0.017886
[18:08:15.857] iteration 9650 : loss : 0.450701, loss_ce: 0.099734
[18:08:20.577] iteration 9660 : loss : 0.225935, loss_ce: 0.022638
[18:08:25.322] iteration 9670 : loss : 0.338700, loss_ce: 0.024778
[18:08:30.037] iteration 9680 : loss : 0.351030, loss_ce: 0.030165
[18:08:34.790] iteration 9690 : loss : 0.289581, loss_ce: 0.028167
[18:08:39.530] iteration 9700 : loss : 0.364808, loss_ce: 0.052255
[18:08:44.271] iteration 9710 : loss : 0.357110, loss_ce: 0.042673
[18:08:48.999] iteration 9720 : loss : 0.331624, loss_ce: 0.017427
[18:08:53.750] iteration 9730 : loss : 0.313310, loss_ce: 0.005342
[18:08:58.483] iteration 9740 : loss : 0.336562, loss_ce: 0.033323
[18:09:03.245] iteration 9750 : loss : 0.161403, loss_ce: 0.013125
[18:09:07.971] iteration 9760 : loss : 0.321816, loss_ce: 0.022226
[18:09:12.707] iteration 9770 : loss : 0.339497, loss_ce: 0.019460
[18:09:17.442] iteration 9780 : loss : 0.325517, loss_ce: 0.033304
[18:09:22.182] iteration 9790 : loss : 0.372401, loss_ce: 0.055183
[18:09:26.916] iteration 9800 : loss : 0.047054, loss_ce: 0.004501
[18:09:31.667] iteration 9810 : loss : 0.315408, loss_ce: 0.005783
[18:09:36.410] iteration 9820 : loss : 0.269392, loss_ce: 0.009581
[18:09:41.150] iteration 9830 : loss : 0.260878, loss_ce: 0.026503
[18:09:45.874] iteration 9840 : loss : 0.258685, loss_ce: 0.019442
[18:09:50.611] iteration 9850 : loss : 0.319800, loss_ce: 0.006869
[18:09:55.332] iteration 9860 : loss : 0.235919, loss_ce: 0.017323
[18:10:00.065] iteration 9870 : loss : 0.333737, loss_ce: 0.014207
[18:10:04.822] iteration 9880 : loss : 0.219044, loss_ce: 0.011992
[18:10:09.563] iteration 9890 : loss : 0.236403, loss_ce: 0.028885
[18:10:14.292] iteration 9900 : loss : 0.308163, loss_ce: 0.015838
[18:10:19.100] iteration 9910 : loss : 0.313631, loss_ce: 0.008621
[18:12:16.508] iteration 9920 : loss : 0.379891, loss_ce: 0.035908
[18:12:21.254] iteration 9930 : loss : 0.302856, loss_ce: 0.018546
[18:12:25.987] iteration 9940 : loss : 0.348563, loss_ce: 0.124889
[18:12:30.728] iteration 9950 : loss : 0.251888, loss_ce: 0.027343
[18:12:35.451] iteration 9960 : loss : 0.196639, loss_ce: 0.011571
[18:12:40.182] iteration 9970 : loss : 0.200420, loss_ce: 0.056499
[18:12:44.915] iteration 9980 : loss : 0.329615, loss_ce: 0.009946
[18:12:49.665] iteration 9990 : loss : 0.348391, loss_ce: 0.020681
[18:12:54.381] iteration 10000 : loss : 0.345259, loss_ce: 0.036834
[18:12:59.124] iteration 10010 : loss : 0.225672, loss_ce: 0.111381
[18:13:03.885] iteration 10020 : loss : 0.338496, loss_ce: 0.023560
[18:13:08.629] iteration 10030 : loss : 0.328245, loss_ce: 0.018385
[18:13:13.368] iteration 10040 : loss : 0.324406, loss_ce: 0.011973
[18:13:18.099] iteration 10050 : loss : 0.325518, loss_ce: 0.013562
[18:13:22.814] iteration 10060 : loss : 0.319399, loss_ce: 0.005511
[18:13:27.549] iteration 10070 : loss : 0.320548, loss_ce: 0.026264
[18:13:32.283] iteration 10080 : loss : 0.263635, loss_ce: 0.025287
[18:13:37.032] iteration 10090 : loss : 0.077289, loss_ce: 0.016781
[18:13:41.759] iteration 10100 : loss : 0.278956, loss_ce: 0.028579
[18:13:46.513] iteration 10110 : loss : 0.269775, loss_ce: 0.011255
[18:13:51.255] iteration 10120 : loss : 0.322858, loss_ce: 0.010642
[18:13:56.015] iteration 10130 : loss : 0.134281, loss_ce: 0.013452
[18:14:00.760] iteration 10140 : loss : 0.227961, loss_ce: 0.015201
[18:14:05.504] iteration 10150 : loss : 0.125862, loss_ce: 0.004961
[18:14:10.235] iteration 10160 : loss : 0.303358, loss_ce: 0.009761
[18:14:15.005] iteration 10170 : loss : 0.201576, loss_ce: 0.008282
[18:14:19.734] iteration 10180 : loss : 0.220338, loss_ce: 0.008177
[18:16:06.204] iteration 10190 : loss : 0.446078, loss_ce: 0.103099
[18:16:10.955] iteration 10200 : loss : 0.363288, loss_ce: 0.062618
[18:16:15.700] iteration 10210 : loss : 0.196479, loss_ce: 0.057259
[18:16:20.441] iteration 10220 : loss : 0.339678, loss_ce: 0.016891
[18:16:25.176] iteration 10230 : loss : 0.342212, loss_ce: 0.040664
[18:16:29.932] iteration 10240 : loss : 0.176058, loss_ce: 0.014656
[18:16:34.684] iteration 10250 : loss : 0.327362, loss_ce: 0.017479
[18:16:39.414] iteration 10260 : loss : 0.367749, loss_ce: 0.114212
[18:16:44.148] iteration 10270 : loss : 0.330961, loss_ce: 0.017784
[18:16:48.872] iteration 10280 : loss : 0.218330, loss_ce: 0.127654
[18:16:53.603] iteration 10290 : loss : 0.326840, loss_ce: 0.030303
[18:16:58.355] iteration 10300 : loss : 0.317707, loss_ce: 0.018249
[18:17:03.100] iteration 10310 : loss : 0.183891, loss_ce: 0.024338
[18:17:07.827] iteration 10320 : loss : 0.191891, loss_ce: 0.068900
[18:17:12.558] iteration 10330 : loss : 0.195692, loss_ce: 0.049820
[18:17:17.299] iteration 10340 : loss : 0.184051, loss_ce: 0.027805
[18:17:22.027] iteration 10350 : loss : 0.321674, loss_ce: 0.020083
[18:17:26.781] iteration 10360 : loss : 0.342809, loss_ce: 0.057190
[18:17:31.522] iteration 10370 : loss : 0.187330, loss_ce: 0.007634
[18:17:36.271] iteration 10380 : loss : 0.179231, loss_ce: 0.032113
[18:17:41.069] iteration 10390 : loss : 0.364856, loss_ce: 0.074862
[18:17:47.990] iteration 10400 : loss : 0.174709, loss_ce: 0.024141
[18:17:59.916] iteration 10410 : loss : 0.176688, loss_ce: 0.032681
[18:18:04.951] iteration 10420 : loss : 0.356555, loss_ce: 0.075803
[18:18:09.894] iteration 10430 : loss : 0.325113, loss_ce: 0.009019
[18:18:14.845] iteration 10440 : loss : 0.316029, loss_ce: 0.010795
[18:18:19.644] iteration 10450 : loss : 0.316654, loss_ce: 0.013999
[18:20:06.068] iteration 10460 : loss : 0.332181, loss_ce: 0.032888
[18:20:10.824] iteration 10470 : loss : 0.357285, loss_ce: 0.038966
[18:20:15.555] iteration 10480 : loss : 0.336222, loss_ce: 0.020788
[18:20:20.288] iteration 10490 : loss : 0.388917, loss_ce: 0.054623
[18:20:25.026] iteration 10500 : loss : 0.184316, loss_ce: 0.034470
[18:20:29.760] iteration 10510 : loss : 0.225252, loss_ce: 0.051559
[18:20:34.509] iteration 10520 : loss : 0.334142, loss_ce: 0.037798
[18:20:39.244] iteration 10530 : loss : 0.317787, loss_ce: 0.013420
[18:20:43.999] iteration 10540 : loss : 0.322910, loss_ce: 0.017173
[18:20:48.752] iteration 10550 : loss : 0.319215, loss_ce: 0.007713
[18:20:53.472] iteration 10560 : loss : 0.233641, loss_ce: 0.049289
[18:20:58.234] iteration 10570 : loss : 0.324545, loss_ce: 0.012201
[18:21:02.959] iteration 10580 : loss : 0.272553, loss_ce: 0.012076
[18:21:07.723] iteration 10590 : loss : 0.329642, loss_ce: 0.021374
[18:21:12.463] iteration 10600 : loss : 0.316991, loss_ce: 0.009528
[18:21:17.220] iteration 10610 : loss : 0.360924, loss_ce: 0.077006
[18:21:21.935] iteration 10620 : loss : 0.207883, loss_ce: 0.050773
[18:21:26.690] iteration 10630 : loss : 0.324454, loss_ce: 0.014064
[18:21:31.433] iteration 10640 : loss : 0.182743, loss_ce: 0.033810
[18:21:36.185] iteration 10650 : loss : 0.360913, loss_ce: 0.073248
[18:21:40.922] iteration 10660 : loss : 0.329406, loss_ce: 0.041413
[18:21:45.650] iteration 10670 : loss : 0.335636, loss_ce: 0.037287
[18:21:50.418] iteration 10680 : loss : 0.347387, loss_ce: 0.051342
[18:21:55.153] iteration 10690 : loss : 0.173841, loss_ce: 0.017024
[18:21:59.909] iteration 10700 : loss : 0.320548, loss_ce: 0.028597
[18:22:04.669] iteration 10710 : loss : 0.358700, loss_ce: 0.052336
[18:22:09.280] iteration 10720 : loss : 0.352215, loss_ce: 0.073613
[18:23:52.111] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_39.pth
[18:24:07.122] iteration 10730 : loss : 0.347988, loss_ce: 0.014849
[18:24:11.833] iteration 10740 : loss : 0.342274, loss_ce: 0.033083
[18:24:16.582] iteration 10750 : loss : 0.340237, loss_ce: 0.035104
[18:24:21.323] iteration 10760 : loss : 0.363161, loss_ce: 0.098345
[18:24:26.072] iteration 10770 : loss : 0.328128, loss_ce: 0.018164
[18:24:30.798] iteration 10780 : loss : 0.331595, loss_ce: 0.015812
[18:24:35.551] iteration 10790 : loss : 0.333585, loss_ce: 0.038019
[18:24:40.299] iteration 10800 : loss : 0.329495, loss_ce: 0.018820
[18:24:45.060] iteration 10810 : loss : 0.323715, loss_ce: 0.012927
[18:24:49.778] iteration 10820 : loss : 0.331526, loss_ce: 0.021479
[18:24:54.515] iteration 10830 : loss : 0.330222, loss_ce: 0.018361
[18:24:59.262] iteration 10840 : loss : 0.351799, loss_ce: 0.015609
[18:25:04.020] iteration 10850 : loss : 0.336214, loss_ce: 0.038964
[18:25:08.748] iteration 10860 : loss : 0.280241, loss_ce: 0.022980
[18:25:13.477] iteration 10870 : loss : 0.219968, loss_ce: 0.010172
[18:25:18.245] iteration 10880 : loss : 0.112945, loss_ce: 0.004878
[18:25:22.977] iteration 10890 : loss : 0.241466, loss_ce: 0.019506
[18:25:27.705] iteration 10900 : loss : 0.327800, loss_ce: 0.010863
[18:25:32.472] iteration 10910 : loss : 0.320409, loss_ce: 0.030300
[18:25:37.223] iteration 10920 : loss : 0.258049, loss_ce: 0.016647
[18:25:41.964] iteration 10930 : loss : 0.318093, loss_ce: 0.017199
[18:25:46.699] iteration 10940 : loss : 0.233084, loss_ce: 0.028458
[18:25:51.445] iteration 10950 : loss : 0.282226, loss_ce: 0.026736
[18:25:56.186] iteration 10960 : loss : 0.246825, loss_ce: 0.025778
[18:26:00.911] iteration 10970 : loss : 0.209975, loss_ce: 0.007979
[18:26:05.695] iteration 10980 : loss : 0.259562, loss_ce: 0.012831
[18:27:52.288] iteration 10990 : loss : 0.481259, loss_ce: 0.117170
[18:27:57.028] iteration 11000 : loss : 0.379378, loss_ce: 0.064295
[18:28:01.769] iteration 11010 : loss : 0.366204, loss_ce: 0.063055
[18:28:06.488] iteration 11020 : loss : 0.338842, loss_ce: 0.023289
[18:28:11.226] iteration 11030 : loss : 0.340419, loss_ce: 0.037885
[18:28:15.951] iteration 11040 : loss : 0.347341, loss_ce: 0.061801
[18:28:20.713] iteration 11050 : loss : 0.345553, loss_ce: 0.050522
[18:28:25.429] iteration 11060 : loss : 0.211106, loss_ce: 0.093906
[18:28:30.157] iteration 11070 : loss : 0.207041, loss_ce: 0.043890
[18:28:34.927] iteration 11080 : loss : 0.325893, loss_ce: 0.017219
[18:28:39.694] iteration 11090 : loss : 0.338320, loss_ce: 0.044802
[18:28:44.421] iteration 11100 : loss : 0.179701, loss_ce: 0.048052
[18:28:49.166] iteration 11110 : loss : 0.335957, loss_ce: 0.043338
[18:28:53.890] iteration 11120 : loss : 0.320593, loss_ce: 0.014931
[18:28:58.639] iteration 11130 : loss : 0.275505, loss_ce: 0.013365
[18:29:03.390] iteration 11140 : loss : 0.289581, loss_ce: 0.020785
[18:29:08.140] iteration 11150 : loss : 0.323811, loss_ce: 0.014979
[18:29:12.897] iteration 11160 : loss : 0.300843, loss_ce: 0.023330
[18:29:17.663] iteration 11170 : loss : 0.122428, loss_ce: 0.012942
[18:29:22.420] iteration 11180 : loss : 0.239327, loss_ce: 0.015809
[18:29:27.170] iteration 11190 : loss : 0.101188, loss_ce: 0.020231
[18:29:31.894] iteration 11200 : loss : 0.284679, loss_ce: 0.023282
[18:29:36.679] iteration 11210 : loss : 0.351975, loss_ce: 0.040749
[18:29:41.423] iteration 11220 : loss : 0.244316, loss_ce: 0.020501
[18:29:46.199] iteration 11230 : loss : 0.160192, loss_ce: 0.008434
[18:29:50.936] iteration 11240 : loss : 0.291639, loss_ce: 0.015428
[18:29:55.666] iteration 11250 : loss : 0.317437, loss_ce: 0.009840
[18:31:42.374] iteration 11260 : loss : 0.412151, loss_ce: 0.055075
[18:31:47.118] iteration 11270 : loss : 0.388224, loss_ce: 0.092965
[18:31:51.880] iteration 11280 : loss : 0.385532, loss_ce: 0.124401
[18:31:56.697] iteration 11290 : loss : 0.183854, loss_ce: 0.006607
[18:32:01.434] iteration 11300 : loss : 0.159717, loss_ce: 0.003827
[18:32:06.177] iteration 11310 : loss : 0.203369, loss_ce: 0.040337
[18:32:10.937] iteration 11320 : loss : 0.376708, loss_ce: 0.078783
[18:32:15.702] iteration 11330 : loss : 0.333319, loss_ce: 0.040788
[18:32:20.456] iteration 11340 : loss : 0.330052, loss_ce: 0.017106
[18:32:25.199] iteration 11350 : loss : 0.183819, loss_ce: 0.028594
[18:32:29.973] iteration 11360 : loss : 0.325960, loss_ce: 0.033354
[18:32:34.732] iteration 11370 : loss : 0.329188, loss_ce: 0.030043
[18:32:39.484] iteration 11380 : loss : 0.314931, loss_ce: 0.006910
[18:32:44.237] iteration 11390 : loss : 0.199949, loss_ce: 0.029899
[18:32:48.979] iteration 11400 : loss : 0.020417, loss_ce: 0.003321
[18:32:53.747] iteration 11410 : loss : 0.318297, loss_ce: 0.010546
[18:32:58.514] iteration 11420 : loss : 0.331460, loss_ce: 0.017159
[18:33:03.280] iteration 11430 : loss : 0.167315, loss_ce: 0.013201
[18:33:08.010] iteration 11440 : loss : 0.177575, loss_ce: 0.024780
[18:33:12.747] iteration 11450 : loss : 0.331877, loss_ce: 0.047652
[18:33:17.489] iteration 11460 : loss : 0.321447, loss_ce: 0.030232
[18:33:22.218] iteration 11470 : loss : 0.338530, loss_ce: 0.027756
[18:33:26.951] iteration 11480 : loss : 0.323528, loss_ce: 0.022487
[18:33:31.692] iteration 11490 : loss : 0.176605, loss_ce: 0.028223
[18:33:36.457] iteration 11500 : loss : 0.328151, loss_ce: 0.032096
[18:33:41.230] iteration 11510 : loss : 0.321562, loss_ce: 0.012708
[18:33:45.986] iteration 11520 : loss : 0.315459, loss_ce: 0.015719
[18:35:43.445] iteration 11530 : loss : 0.391138, loss_ce: 0.027702
[18:35:48.185] iteration 11540 : loss : 0.365661, loss_ce: 0.030197
[18:35:52.921] iteration 11550 : loss : 0.351369, loss_ce: 0.041432
[18:35:57.647] iteration 11560 : loss : 0.337724, loss_ce: 0.016502
[18:36:02.392] iteration 11570 : loss : 0.215340, loss_ce: 0.074784
[18:36:07.112] iteration 11580 : loss : 0.323965, loss_ce: 0.015207
[18:36:11.862] iteration 11590 : loss : 0.323564, loss_ce: 0.013362
[18:36:16.604] iteration 11600 : loss : 0.355255, loss_ce: 0.078664
[18:36:21.357] iteration 11610 : loss : 0.360983, loss_ce: 0.066344
[18:36:26.090] iteration 11620 : loss : 0.176180, loss_ce: 0.021660
[18:36:30.865] iteration 11630 : loss : 0.349393, loss_ce: 0.046217
[18:36:35.588] iteration 11640 : loss : 0.357711, loss_ce: 0.064829
[18:36:40.324] iteration 11650 : loss : 0.336848, loss_ce: 0.042490
[18:36:45.078] iteration 11660 : loss : 0.318985, loss_ce: 0.017148
[18:36:49.855] iteration 11670 : loss : 0.318047, loss_ce: 0.014960
[18:36:54.583] iteration 11680 : loss : 0.319782, loss_ce: 0.008111
[18:36:59.322] iteration 11690 : loss : 0.340088, loss_ce: 0.036862
[18:37:04.066] iteration 11700 : loss : 0.329093, loss_ce: 0.023896
[18:37:08.809] iteration 11710 : loss : 0.169194, loss_ce: 0.005576
[18:37:13.586] iteration 11720 : loss : 0.347125, loss_ce: 0.070727
[18:37:18.354] iteration 11730 : loss : 0.327718, loss_ce: 0.015706
[18:37:23.081] iteration 11740 : loss : 0.324194, loss_ce: 0.019221
[18:37:27.842] iteration 11750 : loss : 0.337014, loss_ce: 0.053612
[18:37:32.594] iteration 11760 : loss : 0.181230, loss_ce: 0.027075
[18:37:37.341] iteration 11770 : loss : 0.182438, loss_ce: 0.032817
[18:37:42.107] iteration 11780 : loss : 0.168304, loss_ce: 0.004627
[18:37:46.861] iteration 11790 : loss : 0.326950, loss_ce: 0.024478
[18:39:33.326] iteration 11800 : loss : 0.344800, loss_ce: 0.022945
[18:39:38.066] iteration 11810 : loss : 0.344341, loss_ce: 0.025224
[18:39:42.816] iteration 11820 : loss : 0.329502, loss_ce: 0.020495
[18:39:47.588] iteration 11830 : loss : 0.324520, loss_ce: 0.010331
[18:39:52.332] iteration 11840 : loss : 0.326969, loss_ce: 0.015518
[18:39:57.074] iteration 11850 : loss : 0.347750, loss_ce: 0.029468
[18:40:01.820] iteration 11860 : loss : 0.344578, loss_ce: 0.048108
[18:40:06.564] iteration 11870 : loss : 0.328973, loss_ce: 0.015371
[18:40:11.271] iteration 11880 : loss : 0.166934, loss_ce: 0.004264
[18:40:16.014] iteration 11890 : loss : 0.318136, loss_ce: 0.007491
[18:40:20.781] iteration 11900 : loss : 0.331599, loss_ce: 0.037662
[18:40:25.549] iteration 11910 : loss : 0.318791, loss_ce: 0.014799
[18:40:30.328] iteration 11920 : loss : 0.335403, loss_ce: 0.034795
[18:40:35.108] iteration 11930 : loss : 0.316925, loss_ce: 0.017300
[18:40:39.836] iteration 11940 : loss : 0.338143, loss_ce: 0.036878
[18:40:44.580] iteration 11950 : loss : 0.330024, loss_ce: 0.015966
[18:40:49.316] iteration 11960 : loss : 0.337893, loss_ce: 0.026438
[18:40:54.091] iteration 11970 : loss : 0.331564, loss_ce: 0.032244
[18:40:58.847] iteration 11980 : loss : 0.339367, loss_ce: 0.042421
[18:41:03.592] iteration 11990 : loss : 0.348790, loss_ce: 0.075182
[18:41:08.367] iteration 12000 : loss : 0.171899, loss_ce: 0.021970
[18:41:13.125] iteration 12010 : loss : 0.321195, loss_ce: 0.011977
[18:41:17.837] iteration 12020 : loss : 0.334042, loss_ce: 0.020569
[18:41:22.599] iteration 12030 : loss : 0.314021, loss_ce: 0.017796
[18:41:27.343] iteration 12040 : loss : 0.306390, loss_ce: 0.014733
[18:41:32.074] iteration 12050 : loss : 0.276218, loss_ce: 0.019646
[18:41:36.693] iteration 12060 : loss : 0.283497, loss_ce: 0.029875
[18:43:23.232] iteration 12070 : loss : 0.405175, loss_ce: 0.079293
[18:43:28.007] iteration 12080 : loss : 0.373525, loss_ce: 0.067913
[18:43:32.744] iteration 12090 : loss : 0.322988, loss_ce: 0.005909
[18:43:37.449] iteration 12100 : loss : 0.339944, loss_ce: 0.043513
[18:43:42.198] iteration 12110 : loss : 0.342620, loss_ce: 0.044573
[18:43:46.947] iteration 12120 : loss : 0.343243, loss_ce: 0.023029
[18:43:51.682] iteration 12130 : loss : 0.350361, loss_ce: 0.088616
[18:43:56.412] iteration 12140 : loss : 0.325578, loss_ce: 0.013237
[18:44:01.144] iteration 12150 : loss : 0.341266, loss_ce: 0.039639
[18:44:05.891] iteration 12160 : loss : 0.319937, loss_ce: 0.006750
[18:44:10.643] iteration 12170 : loss : 0.329563, loss_ce: 0.026962
[18:44:15.402] iteration 12180 : loss : 0.340107, loss_ce: 0.023661
[18:44:20.135] iteration 12190 : loss : 0.330474, loss_ce: 0.028252
[18:44:24.861] iteration 12200 : loss : 0.177107, loss_ce: 0.023536
[18:44:29.632] iteration 12210 : loss : 0.178734, loss_ce: 0.012553
[18:44:34.348] iteration 12220 : loss : 0.321775, loss_ce: 0.024416
[18:44:39.095] iteration 12230 : loss : 0.317520, loss_ce: 0.008980
[18:44:43.845] iteration 12240 : loss : 0.332102, loss_ce: 0.035060
[18:44:48.600] iteration 12250 : loss : 0.197757, loss_ce: 0.034069
[18:44:53.351] iteration 12260 : loss : 0.327206, loss_ce: 0.028488
[18:44:58.083] iteration 12270 : loss : 0.324928, loss_ce: 0.016699
[18:45:02.814] iteration 12280 : loss : 0.324676, loss_ce: 0.013118
[18:45:07.546] iteration 12290 : loss : 0.328443, loss_ce: 0.031648
[18:45:12.284] iteration 12300 : loss : 0.319786, loss_ce: 0.019782
[18:45:17.022] iteration 12310 : loss : 0.325220, loss_ce: 0.011819
[18:45:21.767] iteration 12320 : loss : 0.359689, loss_ce: 0.023121
[18:47:19.354] iteration 12330 : loss : 0.483174, loss_ce: 0.118126
[18:47:24.082] iteration 12340 : loss : 0.407899, loss_ce: 0.092895
[18:47:28.800] iteration 12350 : loss : 0.347895, loss_ce: 0.032888
[18:47:33.521] iteration 12360 : loss : 0.349278, loss_ce: 0.060709
[18:47:38.243] iteration 12370 : loss : 0.327147, loss_ce: 0.027381
[18:47:42.961] iteration 12380 : loss : 0.340366, loss_ce: 0.022546
[18:47:47.705] iteration 12390 : loss : 0.193699, loss_ce: 0.014176
[18:47:52.420] iteration 12400 : loss : 0.168856, loss_ce: 0.011709
[18:47:57.222] iteration 12410 : loss : 0.329397, loss_ce: 0.036129
[18:48:01.971] iteration 12420 : loss : 0.333247, loss_ce: 0.026117
[18:48:06.729] iteration 12430 : loss : 0.352575, loss_ce: 0.072147
[18:48:11.469] iteration 12440 : loss : 0.333498, loss_ce: 0.028714
[18:48:16.214] iteration 12450 : loss : 0.333753, loss_ce: 0.035153
[18:48:20.948] iteration 12460 : loss : 0.341481, loss_ce: 0.035540
[18:48:25.710] iteration 12470 : loss : 0.182063, loss_ce: 0.041103
[18:48:30.432] iteration 12480 : loss : 0.350119, loss_ce: 0.023232
[18:48:35.200] iteration 12490 : loss : 0.331287, loss_ce: 0.017218
[18:48:39.934] iteration 12500 : loss : 0.346790, loss_ce: 0.044710
[18:48:44.666] iteration 12510 : loss : 0.319439, loss_ce: 0.012782
[18:48:49.412] iteration 12520 : loss : 0.332164, loss_ce: 0.025445
[18:48:54.179] iteration 12530 : loss : 0.179202, loss_ce: 0.026336
[18:48:58.932] iteration 12540 : loss : 0.320363, loss_ce: 0.010001
[18:49:03.689] iteration 12550 : loss : 0.340756, loss_ce: 0.043445
[18:49:08.468] iteration 12560 : loss : 0.302278, loss_ce: 0.051517
[18:49:13.218] iteration 12570 : loss : 0.327535, loss_ce: 0.016355
[18:49:17.958] iteration 12580 : loss : 0.251701, loss_ce: 0.014350
[18:49:22.689] iteration 12590 : loss : 0.331399, loss_ce: 0.032682
[18:51:09.395] iteration 12600 : loss : 0.465760, loss_ce: 0.068000
[18:51:14.132] iteration 12610 : loss : 0.339147, loss_ce: 0.014314
[18:51:18.870] iteration 12620 : loss : 0.360466, loss_ce: 0.065351
[18:51:23.611] iteration 12630 : loss : 0.370135, loss_ce: 0.082201
[18:51:28.349] iteration 12640 : loss : 0.362737, loss_ce: 0.061102
[18:51:33.103] iteration 12650 : loss : 0.339217, loss_ce: 0.040025
[18:51:37.888] iteration 12660 : loss : 0.336436, loss_ce: 0.042675
[18:51:42.650] iteration 12670 : loss : 0.375008, loss_ce: 0.111157
[18:51:47.415] iteration 12680 : loss : 0.306823, loss_ce: 0.001237
[18:51:52.205] iteration 12690 : loss : 0.331645, loss_ce: 0.024400
[18:51:56.932] iteration 12700 : loss : 0.362443, loss_ce: 0.069556
[18:52:01.672] iteration 12710 : loss : 0.352690, loss_ce: 0.077295
[18:52:06.426] iteration 12720 : loss : 0.343722, loss_ce: 0.037214
[18:52:11.154] iteration 12730 : loss : 0.340534, loss_ce: 0.050296
[18:52:15.897] iteration 12740 : loss : 0.322473, loss_ce: 0.005193
[18:52:20.669] iteration 12750 : loss : 0.321653, loss_ce: 0.005759
[18:52:25.426] iteration 12760 : loss : 0.352558, loss_ce: 0.045558
[18:52:30.179] iteration 12770 : loss : 0.338023, loss_ce: 0.038275
[18:52:34.945] iteration 12780 : loss : 0.350230, loss_ce: 0.085244
[18:52:39.734] iteration 12790 : loss : 0.322348, loss_ce: 0.038267
[18:52:44.502] iteration 12800 : loss : 0.329342, loss_ce: 0.013919
[18:52:49.237] iteration 12810 : loss : 0.280453, loss_ce: 0.019814
[18:52:54.009] iteration 12820 : loss : 0.327433, loss_ce: 0.021780
[18:52:58.771] iteration 12830 : loss : 0.350760, loss_ce: 0.032937
[18:53:03.562] iteration 12840 : loss : 0.215533, loss_ce: 0.016341
[18:53:08.364] iteration 12850 : loss : 0.241548, loss_ce: 0.007563
[18:53:13.149] iteration 12860 : loss : 0.223390, loss_ce: 0.016390
[18:55:01.125] iteration 12870 : loss : 0.482457, loss_ce: 0.133414
[18:55:05.857] iteration 12880 : loss : 0.454915, loss_ce: 0.104714
[18:55:10.583] iteration 12890 : loss : 0.380452, loss_ce: 0.030575
[18:55:15.308] iteration 12900 : loss : 0.369917, loss_ce: 0.048751
[18:55:20.039] iteration 12910 : loss : 0.378657, loss_ce: 0.120474
[18:55:24.773] iteration 12920 : loss : 0.321108, loss_ce: 0.011367
[18:55:29.529] iteration 12930 : loss : 0.361906, loss_ce: 0.055709
[18:55:34.246] iteration 12940 : loss : 0.328160, loss_ce: 0.016021
[18:55:38.990] iteration 12950 : loss : 0.353274, loss_ce: 0.059960
[18:55:43.737] iteration 12960 : loss : 0.383940, loss_ce: 0.078771
[18:55:48.491] iteration 12970 : loss : 0.348057, loss_ce: 0.025049
[18:55:53.222] iteration 12980 : loss : 0.330597, loss_ce: 0.024089
[18:55:57.964] iteration 12990 : loss : 0.337299, loss_ce: 0.025999
[18:56:02.727] iteration 13000 : loss : 0.330742, loss_ce: 0.022521
[18:56:07.481] iteration 13010 : loss : 0.344917, loss_ce: 0.047699
[18:56:12.240] iteration 13020 : loss : 0.337373, loss_ce: 0.040965
[18:56:16.978] iteration 13030 : loss : 0.328584, loss_ce: 0.022558
[18:56:21.748] iteration 13040 : loss : 0.334045, loss_ce: 0.025866
[18:56:26.494] iteration 13050 : loss : 0.349023, loss_ce: 0.046634
[18:56:31.228] iteration 13060 : loss : 0.367312, loss_ce: 0.080912
[18:56:35.997] iteration 13070 : loss : 0.331507, loss_ce: 0.025372
[18:56:40.752] iteration 13080 : loss : 0.329982, loss_ce: 0.033099
[18:56:45.559] iteration 13090 : loss : 0.323382, loss_ce: 0.023522
[18:56:50.295] iteration 13100 : loss : 0.326610, loss_ce: 0.028988
[18:56:55.053] iteration 13110 : loss : 0.329415, loss_ce: 0.020995
[18:56:59.783] iteration 13120 : loss : 0.345499, loss_ce: 0.052179
[18:57:04.557] iteration 13130 : loss : 0.350093, loss_ce: 0.006811
[18:59:02.175] iteration 13140 : loss : 0.464424, loss_ce: 0.071732
[18:59:06.919] iteration 13150 : loss : 0.459330, loss_ce: 0.109973
[18:59:11.664] iteration 13160 : loss : 0.425947, loss_ce: 0.034595
[18:59:16.402] iteration 13170 : loss : 0.400413, loss_ce: 0.033615
[18:59:21.166] iteration 13180 : loss : 0.407063, loss_ce: 0.047799
[18:59:25.932] iteration 13190 : loss : 0.365707, loss_ce: 0.038038
[18:59:30.668] iteration 13200 : loss : 0.389447, loss_ce: 0.102179
[18:59:35.411] iteration 13210 : loss : 0.374981, loss_ce: 0.079362
[18:59:40.150] iteration 13220 : loss : 0.362940, loss_ce: 0.029276
[18:59:44.912] iteration 13230 : loss : 0.346442, loss_ce: 0.028619
[18:59:49.635] iteration 13240 : loss : 0.331354, loss_ce: 0.016949
[18:59:54.401] iteration 13250 : loss : 0.352508, loss_ce: 0.017239
[18:59:59.133] iteration 13260 : loss : 0.366435, loss_ce: 0.059129
[19:00:03.894] iteration 13270 : loss : 0.355753, loss_ce: 0.032230
[19:00:08.659] iteration 13280 : loss : 0.354830, loss_ce: 0.097093
[19:00:13.395] iteration 13290 : loss : 0.265351, loss_ce: 0.013462
[19:00:18.126] iteration 13300 : loss : 0.359006, loss_ce: 0.035302
[19:00:22.865] iteration 13310 : loss : 0.336987, loss_ce: 0.025101
[19:00:27.632] iteration 13320 : loss : 0.358983, loss_ce: 0.024484
[19:00:32.362] iteration 13330 : loss : 0.327010, loss_ce: 0.022668
[19:00:37.082] iteration 13340 : loss : 0.327958, loss_ce: 0.005619
[19:00:41.871] iteration 13350 : loss : 0.356915, loss_ce: 0.039484
[19:00:46.636] iteration 13360 : loss : 0.353677, loss_ce: 0.059316
[19:00:51.379] iteration 13370 : loss : 0.388284, loss_ce: 0.083046
[19:00:56.137] iteration 13380 : loss : 0.331946, loss_ce: 0.020176
[19:01:00.911] iteration 13390 : loss : 0.368810, loss_ce: 0.097573
[19:01:05.541] iteration 13400 : loss : 0.350766, loss_ce: 0.052571
[19:02:37.473] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_49.pth
[19:02:37.662] save final model to ./finetune_tpgm_kits23_continual\finetuned_final.pth
