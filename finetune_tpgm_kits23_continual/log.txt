[02:36:03.990] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=100, tpgm_exclude=[])
[02:36:04.018] Using 8569/95221 samples for finetuning
[02:36:04.018] Using 953/95221 samples for TPGM
[02:36:04.018] Model has 9 total classes, training on 4 classes
[02:36:14.093] 268 iterations per epoch. 13400 max iterations 
[02:36:28.992] iteration 10 : loss : 0.461870, loss_ce: 0.081058
[02:36:33.128] iteration 20 : loss : 0.447126, loss_ce: 0.067454
[02:36:37.296] iteration 30 : loss : 0.454064, loss_ce: 0.061675
[02:36:41.486] iteration 40 : loss : 0.438523, loss_ce: 0.037797
[02:36:45.758] iteration 50 : loss : 0.427871, loss_ce: 0.060777
[02:36:49.904] iteration 60 : loss : 0.289397, loss_ce: 0.043874
[02:36:53.981] iteration 70 : loss : 0.430093, loss_ce: 0.078371
[02:36:58.053] iteration 80 : loss : 0.432416, loss_ce: 0.045691
[02:37:02.324] iteration 90 : loss : 0.438128, loss_ce: 0.059353
[02:37:06.707] iteration 100 : loss : 0.424840, loss_ce: 0.065352
[02:37:11.160] iteration 110 : loss : 0.289580, loss_ce: 0.027083
[02:37:15.509] iteration 120 : loss : 0.420143, loss_ce: 0.054165
[02:37:19.859] iteration 130 : loss : 0.428500, loss_ce: 0.046873
[02:37:24.202] iteration 140 : loss : 0.401049, loss_ce: 0.039351
[02:37:28.637] iteration 150 : loss : 0.411577, loss_ce: 0.030802
[02:37:33.104] iteration 160 : loss : 0.382576, loss_ce: 0.048536
[02:37:37.544] iteration 170 : loss : 0.386560, loss_ce: 0.035044
[02:37:42.042] iteration 180 : loss : 0.363701, loss_ce: 0.024090
[02:37:46.517] iteration 190 : loss : 0.240404, loss_ce: 0.031053
[02:37:50.989] iteration 200 : loss : 0.373939, loss_ce: 0.052010
[02:37:55.505] iteration 210 : loss : 0.388509, loss_ce: 0.043876
[02:37:59.998] iteration 220 : loss : 0.379827, loss_ce: 0.024280
[02:38:04.494] iteration 230 : loss : 0.358815, loss_ce: 0.026818
[02:38:08.956] iteration 240 : loss : 0.366219, loss_ce: 0.032203
[02:38:13.452] iteration 250 : loss : 0.391504, loss_ce: 0.052399
[02:38:17.904] iteration 260 : loss : 0.350837, loss_ce: 0.040769
[02:41:23.967] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=100, tpgm_exclude=[])
[02:41:23.996] Using 8569/95221 samples for finetuning
[02:41:23.996] Using 953/95221 samples for TPGM
[02:41:23.996] Model has 9 total classes, training on 4 classes
[02:41:34.633] 268 iterations per epoch. 13400 max iterations 
[02:41:52.638] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=100, tpgm_exclude=[])
[02:41:52.664] Using 8569/95221 samples for finetuning
[02:41:52.664] Using 953/95221 samples for TPGM
[02:41:52.664] Model has 9 total classes, training on 4 classes
[02:42:03.260] 268 iterations per epoch. 13400 max iterations 
[02:42:18.175] iteration 10 : loss : 0.461870, loss_ce: 0.081058
[02:42:22.276] iteration 20 : loss : 0.447126, loss_ce: 0.067454
[02:42:26.592] iteration 30 : loss : 0.454064, loss_ce: 0.061675
[02:42:30.924] iteration 40 : loss : 0.438523, loss_ce: 0.037797
[02:42:35.289] iteration 50 : loss : 0.427871, loss_ce: 0.060777
[02:42:39.648] iteration 60 : loss : 0.289397, loss_ce: 0.043874
[02:42:44.042] iteration 70 : loss : 0.430093, loss_ce: 0.078371
[02:42:48.447] iteration 80 : loss : 0.432416, loss_ce: 0.045691
[02:42:52.842] iteration 90 : loss : 0.438128, loss_ce: 0.059353
[02:42:57.247] iteration 100 : loss : 0.424840, loss_ce: 0.065352
[02:43:01.648] iteration 110 : loss : 0.289580, loss_ce: 0.027083
[02:43:06.049] iteration 120 : loss : 0.420143, loss_ce: 0.054165
[02:43:10.439] iteration 130 : loss : 0.428500, loss_ce: 0.046873
[02:43:14.830] iteration 140 : loss : 0.401049, loss_ce: 0.039351
[02:43:19.232] iteration 150 : loss : 0.411577, loss_ce: 0.030802
[02:43:23.612] iteration 160 : loss : 0.382576, loss_ce: 0.048536
[02:43:28.001] iteration 170 : loss : 0.386560, loss_ce: 0.035044
[02:43:32.379] iteration 180 : loss : 0.363701, loss_ce: 0.024090
[02:43:36.777] iteration 190 : loss : 0.240404, loss_ce: 0.031053
[02:43:41.167] iteration 200 : loss : 0.373939, loss_ce: 0.052010
[02:43:45.610] iteration 210 : loss : 0.388509, loss_ce: 0.043876
[02:43:50.038] iteration 220 : loss : 0.379827, loss_ce: 0.024280
[02:43:54.486] iteration 230 : loss : 0.358815, loss_ce: 0.026818
[02:43:58.983] iteration 240 : loss : 0.366219, loss_ce: 0.032203
[02:44:03.437] iteration 250 : loss : 0.391504, loss_ce: 0.052399
[02:44:07.720] iteration 260 : loss : 0.350837, loss_ce: 0.040769
[02:45:50.792] iteration 270 : loss : 0.462899, loss_ce: 0.035541
[02:45:55.154] iteration 280 : loss : 0.463287, loss_ce: 0.090943
[02:45:59.563] iteration 290 : loss : 0.287443, loss_ce: 0.085089
[02:46:04.000] iteration 300 : loss : 0.289705, loss_ce: 0.048987
[02:46:08.404] iteration 310 : loss : 0.247889, loss_ce: 0.024324
[02:46:12.819] iteration 320 : loss : 0.370369, loss_ce: 0.020717
[02:46:17.281] iteration 330 : loss : 0.390467, loss_ce: 0.018715
[02:46:21.552] iteration 340 : loss : 0.170275, loss_ce: 0.025486
[02:46:25.820] iteration 350 : loss : 0.183353, loss_ce: 0.026845
[02:46:30.000] iteration 360 : loss : 0.334774, loss_ce: 0.037828
[02:46:34.258] iteration 370 : loss : 0.332359, loss_ce: 0.029372
[02:46:38.457] iteration 380 : loss : 0.268784, loss_ce: 0.021640
[02:46:42.731] iteration 390 : loss : 0.276486, loss_ce: 0.032036
[02:46:46.985] iteration 400 : loss : 0.333686, loss_ce: 0.038715
[02:46:51.273] iteration 410 : loss : 0.242994, loss_ce: 0.024220
[02:46:55.548] iteration 420 : loss : 0.253154, loss_ce: 0.029021
[02:46:59.799] iteration 430 : loss : 0.410831, loss_ce: 0.081053
[02:47:04.026] iteration 440 : loss : 0.352406, loss_ce: 0.016934
[02:47:08.285] iteration 450 : loss : 0.335707, loss_ce: 0.015532
[02:47:12.525] iteration 460 : loss : 0.364337, loss_ce: 0.050660
[02:47:16.709] iteration 470 : loss : 0.275999, loss_ce: 0.018428
[02:47:20.976] iteration 480 : loss : 0.149995, loss_ce: 0.010344
[02:47:25.202] iteration 490 : loss : 0.328502, loss_ce: 0.039331
[02:47:29.463] iteration 500 : loss : 0.296283, loss_ce: 0.033470
[02:47:33.772] iteration 510 : loss : 0.286253, loss_ce: 0.018286
[02:47:38.073] iteration 520 : loss : 0.212901, loss_ce: 0.014486
[02:47:42.361] iteration 530 : loss : 0.287296, loss_ce: 0.025054
[02:49:25.980] iteration 540 : loss : 0.432019, loss_ce: 0.046167
[02:49:30.455] iteration 550 : loss : 0.296224, loss_ce: 0.024300
[02:49:34.906] iteration 560 : loss : 0.341721, loss_ce: 0.009760
[02:49:39.378] iteration 570 : loss : 0.312383, loss_ce: 0.029959
[02:49:43.847] iteration 580 : loss : 0.343419, loss_ce: 0.044977
[02:49:48.319] iteration 590 : loss : 0.268594, loss_ce: 0.014690
[02:49:52.777] iteration 600 : loss : 0.168865, loss_ce: 0.021438
[02:49:57.252] iteration 610 : loss : 0.318730, loss_ce: 0.052682
[02:50:01.722] iteration 620 : loss : 0.176595, loss_ce: 0.012783
[02:50:06.124] iteration 630 : loss : 0.326273, loss_ce: 0.022085
[02:50:10.383] iteration 640 : loss : 0.304206, loss_ce: 0.010232
[02:50:14.732] iteration 650 : loss : 0.335971, loss_ce: 0.040274
[02:50:19.069] iteration 660 : loss : 0.253794, loss_ce: 0.012850
[02:50:23.320] iteration 670 : loss : 0.206421, loss_ce: 0.037474
[02:50:27.576] iteration 680 : loss : 0.260626, loss_ce: 0.019785
[02:50:31.798] iteration 690 : loss : 0.235492, loss_ce: 0.015387
[02:50:35.960] iteration 700 : loss : 0.162915, loss_ce: 0.046460
[02:50:40.225] iteration 710 : loss : 0.322236, loss_ce: 0.011277
[02:50:44.478] iteration 720 : loss : 0.271329, loss_ce: 0.059038
[02:50:48.692] iteration 730 : loss : 0.135397, loss_ce: 0.009301
[02:50:52.953] iteration 740 : loss : 0.333250, loss_ce: 0.011422
[02:50:57.245] iteration 750 : loss : 0.282560, loss_ce: 0.026030
[02:51:01.492] iteration 760 : loss : 0.248993, loss_ce: 0.024772
[02:51:05.738] iteration 770 : loss : 0.075416, loss_ce: 0.013712
[02:51:09.949] iteration 780 : loss : 0.281418, loss_ce: 0.021314
[02:51:14.262] iteration 790 : loss : 0.061598, loss_ce: 0.009606
[02:51:18.536] iteration 800 : loss : 0.251707, loss_ce: 0.015652
[02:53:02.163] iteration 810 : loss : 0.345800, loss_ce: 0.034433
[02:53:06.419] iteration 820 : loss : 0.341800, loss_ce: 0.031620
[02:53:10.701] iteration 830 : loss : 0.377652, loss_ce: 0.073517
[02:53:15.001] iteration 840 : loss : 0.331360, loss_ce: 0.031119
[02:53:19.521] iteration 850 : loss : 0.311014, loss_ce: 0.016217
[02:53:23.873] iteration 860 : loss : 0.345801, loss_ce: 0.025131
[02:53:28.420] iteration 870 : loss : 0.353577, loss_ce: 0.024165
[02:53:32.838] iteration 880 : loss : 0.238100, loss_ce: 0.005567
[02:53:37.329] iteration 890 : loss : 0.303904, loss_ce: 0.030061
[02:53:41.849] iteration 900 : loss : 0.369860, loss_ce: 0.021992
[02:53:46.191] iteration 910 : loss : 0.303899, loss_ce: 0.032308
[02:53:50.492] iteration 920 : loss : 0.308978, loss_ce: 0.013411
[02:53:54.810] iteration 930 : loss : 0.246135, loss_ce: 0.010069
[02:53:59.107] iteration 940 : loss : 0.230122, loss_ce: 0.012223
[02:54:03.378] iteration 950 : loss : 0.130169, loss_ce: 0.012709
[02:54:07.669] iteration 960 : loss : 0.235335, loss_ce: 0.009371
[02:54:12.036] iteration 970 : loss : 0.312622, loss_ce: 0.012966
[02:54:16.528] iteration 980 : loss : 0.247983, loss_ce: 0.016275
[02:54:20.831] iteration 990 : loss : 0.245154, loss_ce: 0.014087
[02:54:25.132] iteration 1000 : loss : 0.142028, loss_ce: 0.024753
[02:54:29.442] iteration 1010 : loss : 0.241345, loss_ce: 0.003689
[02:54:33.713] iteration 1020 : loss : 0.289091, loss_ce: 0.014857
[02:54:37.983] iteration 1030 : loss : 0.318084, loss_ce: 0.013813
[02:54:42.252] iteration 1040 : loss : 0.348892, loss_ce: 0.016850
[02:54:46.550] iteration 1050 : loss : 0.312511, loss_ce: 0.039105
[02:54:50.843] iteration 1060 : loss : 0.263775, loss_ce: 0.012132
[02:54:55.108] iteration 1070 : loss : 0.254121, loss_ce: 0.027855
[02:56:53.446] iteration 1080 : loss : 0.373847, loss_ce: 0.090172
[02:56:57.929] iteration 1090 : loss : 0.526882, loss_ce: 0.193761
[02:57:02.409] iteration 1100 : loss : 0.512177, loss_ce: 0.153479
[02:57:06.894] iteration 1110 : loss : 0.319693, loss_ce: 0.053037
[02:57:11.090] iteration 1120 : loss : 0.453726, loss_ce: 0.037365
[02:57:15.267] iteration 1130 : loss : 0.308442, loss_ce: 0.028906
[02:57:19.529] iteration 1140 : loss : 0.462756, loss_ce: 0.086455
[02:57:23.775] iteration 1150 : loss : 0.457729, loss_ce: 0.047067
[02:57:28.078] iteration 1160 : loss : 0.459140, loss_ce: 0.060503
[02:57:32.334] iteration 1170 : loss : 0.463198, loss_ce: 0.069604
[02:57:36.443] iteration 1180 : loss : 0.461919, loss_ce: 0.077122
[02:57:40.568] iteration 1190 : loss : 0.456405, loss_ce: 0.052743
[02:57:44.725] iteration 1200 : loss : 0.459601, loss_ce: 0.088184
[02:57:48.898] iteration 1210 : loss : 0.474669, loss_ce: 0.074735
[02:57:52.988] iteration 1220 : loss : 0.473323, loss_ce: 0.069248
[02:57:57.431] iteration 1230 : loss : 0.311346, loss_ce: 0.068942
[02:58:01.748] iteration 1240 : loss : 0.458452, loss_ce: 0.054371
[02:58:05.925] iteration 1250 : loss : 0.467145, loss_ce: 0.066744
[02:58:10.093] iteration 1260 : loss : 0.460677, loss_ce: 0.060950
[02:58:14.276] iteration 1270 : loss : 0.459241, loss_ce: 0.066665
[02:58:18.447] iteration 1280 : loss : 0.464053, loss_ce: 0.072474
[02:58:22.626] iteration 1290 : loss : 0.470914, loss_ce: 0.089088
[02:58:26.791] iteration 1300 : loss : 0.310187, loss_ce: 0.049648
[02:58:30.969] iteration 1310 : loss : 0.462854, loss_ce: 0.063769
[02:58:35.139] iteration 1320 : loss : 0.313103, loss_ce: 0.075132
[02:58:39.316] iteration 1330 : loss : 0.462118, loss_ce: 0.072604
[02:58:43.385] iteration 1340 : loss : 0.468779, loss_ce: 0.076960
[03:00:42.092] iteration 1350 : loss : 0.497346, loss_ce: 0.123794
[03:00:46.504] iteration 1360 : loss : 0.451520, loss_ce: 0.047772
[03:00:50.959] iteration 1370 : loss : 0.454486, loss_ce: 0.078147
[03:00:55.371] iteration 1380 : loss : 0.433844, loss_ce: 0.090404
[03:00:59.907] iteration 1390 : loss : 0.269722, loss_ce: 0.032209
[03:01:04.362] iteration 1400 : loss : 0.332279, loss_ce: 0.079980
[03:01:08.838] iteration 1410 : loss : 0.455214, loss_ce: 0.392039
[03:01:13.274] iteration 1420 : loss : 0.466846, loss_ce: 0.070011
[03:01:17.800] iteration 1430 : loss : 0.458041, loss_ce: 0.077913
[03:01:22.309] iteration 1440 : loss : 0.429870, loss_ce: 0.044564
[03:01:26.877] iteration 1450 : loss : 0.417396, loss_ce: 0.039120
[03:01:31.390] iteration 1460 : loss : 0.420540, loss_ce: 0.074914
[03:01:35.908] iteration 1470 : loss : 0.171749, loss_ce: 0.026988
[03:01:40.407] iteration 1480 : loss : 0.200396, loss_ce: 0.081793
[03:01:44.936] iteration 1490 : loss : 0.281724, loss_ce: 0.024597
[03:01:49.442] iteration 1500 : loss : 0.183973, loss_ce: 0.010961
[03:01:53.971] iteration 1510 : loss : 0.158548, loss_ce: 0.022542
[03:01:58.485] iteration 1520 : loss : 0.330309, loss_ce: 0.008846
[03:02:03.007] iteration 1530 : loss : 0.318752, loss_ce: 0.011912
[03:02:07.489] iteration 1540 : loss : 0.112996, loss_ce: 0.013438
[03:02:11.977] iteration 1550 : loss : 0.164956, loss_ce: 0.006446
[03:02:16.449] iteration 1560 : loss : 0.198572, loss_ce: 0.006977
[03:02:20.934] iteration 1570 : loss : 0.098285, loss_ce: 0.025142
[03:02:25.409] iteration 1580 : loss : 0.257671, loss_ce: 0.031113
[03:02:29.911] iteration 1590 : loss : 0.075437, loss_ce: 0.011250
[03:02:34.418] iteration 1600 : loss : 0.325676, loss_ce: 0.011413
[03:04:24.379] iteration 1610 : loss : 0.467207, loss_ce: 0.147142
[03:04:29.013] iteration 1620 : loss : 0.456488, loss_ce: 0.032437
[03:04:33.546] iteration 1630 : loss : 0.460214, loss_ce: 0.047158
[03:04:37.927] iteration 1640 : loss : 0.441048, loss_ce: 0.106898
[03:04:42.474] iteration 1650 : loss : 0.246708, loss_ce: 0.042774
[03:04:47.000] iteration 1660 : loss : 0.382325, loss_ce: 0.012873
[03:04:51.564] iteration 1670 : loss : 0.300806, loss_ce: 0.027454
[03:04:56.100] iteration 1680 : loss : 0.321878, loss_ce: 0.059917
[03:05:00.659] iteration 1690 : loss : 0.170161, loss_ce: 0.023182
[03:05:05.129] iteration 1700 : loss : 0.349157, loss_ce: 0.027645
[03:05:09.461] iteration 1710 : loss : 0.167321, loss_ce: 0.018527
[03:05:13.769] iteration 1720 : loss : 0.288635, loss_ce: 0.044836
[03:05:18.213] iteration 1730 : loss : 0.330061, loss_ce: 0.023400
[03:05:22.586] iteration 1740 : loss : 0.224537, loss_ce: 0.012667
[03:05:26.912] iteration 1750 : loss : 0.343713, loss_ce: 0.032188
[03:05:31.236] iteration 1760 : loss : 0.335190, loss_ce: 0.016241
[03:05:35.674] iteration 1770 : loss : 0.307953, loss_ce: 0.021370
[03:05:40.099] iteration 1780 : loss : 0.323317, loss_ce: 0.030488
[03:05:44.420] iteration 1790 : loss : 0.354977, loss_ce: 0.039638
[03:05:48.731] iteration 1800 : loss : 0.342655, loss_ce: 0.020570
[03:05:53.130] iteration 1810 : loss : 0.263123, loss_ce: 0.011376
[03:05:57.493] iteration 1820 : loss : 0.312625, loss_ce: 0.065463
[03:06:01.840] iteration 1830 : loss : 0.238035, loss_ce: 0.029055
[03:06:06.172] iteration 1840 : loss : 0.172443, loss_ce: 0.010500
[03:06:10.511] iteration 1850 : loss : 0.329998, loss_ce: 0.013166
[03:06:14.843] iteration 1860 : loss : 0.340365, loss_ce: 0.076583
[03:06:19.189] iteration 1870 : loss : 0.259429, loss_ce: 0.023563
[03:08:13.794] iteration 1880 : loss : 0.504211, loss_ce: 0.186949
[03:08:17.920] iteration 1890 : loss : 0.235301, loss_ce: 0.016287
[03:08:22.041] iteration 1900 : loss : 0.359759, loss_ce: 0.047875
[03:08:26.170] iteration 1910 : loss : 0.312185, loss_ce: 0.045542
[03:08:30.288] iteration 1920 : loss : 0.297752, loss_ce: 0.030772
[03:08:34.423] iteration 1930 : loss : 0.395718, loss_ce: 0.031669
[03:08:38.545] iteration 1940 : loss : 0.316589, loss_ce: 0.060018
[03:08:42.683] iteration 1950 : loss : 0.232974, loss_ce: 0.013123
[03:08:46.809] iteration 1960 : loss : 0.344633, loss_ce: 0.029571
[03:08:50.948] iteration 1970 : loss : 0.345024, loss_ce: 0.022493
[03:08:55.076] iteration 1980 : loss : 0.270465, loss_ce: 0.012211
[03:08:59.230] iteration 1990 : loss : 0.286585, loss_ce: 0.025935
[03:09:03.370] iteration 2000 : loss : 0.323206, loss_ce: 0.026596
[03:09:07.508] iteration 2010 : loss : 0.402453, loss_ce: 0.065693
[03:09:11.640] iteration 2020 : loss : 0.259127, loss_ce: 0.018839
[03:09:15.787] iteration 2030 : loss : 0.327495, loss_ce: 0.010263
[03:09:19.921] iteration 2040 : loss : 0.314576, loss_ce: 0.046358
[03:09:24.063] iteration 2050 : loss : 0.159732, loss_ce: 0.039578
[03:09:28.192] iteration 2060 : loss : 0.350412, loss_ce: 0.051881
[03:09:32.337] iteration 2070 : loss : 0.129191, loss_ce: 0.036055
[03:09:36.472] iteration 2080 : loss : 0.249838, loss_ce: 0.045474
[03:09:40.612] iteration 2090 : loss : 0.263376, loss_ce: 0.011531
[03:09:44.739] iteration 2100 : loss : 0.249591, loss_ce: 0.014607
[03:09:48.882] iteration 2110 : loss : 0.237190, loss_ce: 0.020580
[03:09:53.014] iteration 2120 : loss : 0.259269, loss_ce: 0.033953
[03:09:57.160] iteration 2130 : loss : 0.267710, loss_ce: 0.019035
[03:10:01.293] iteration 2140 : loss : 0.251413, loss_ce: 0.018104
[03:11:40.528] iteration 2150 : loss : 0.337086, loss_ce: 0.019994
[03:11:44.618] iteration 2160 : loss : 0.293777, loss_ce: 0.012244
[03:11:48.729] iteration 2170 : loss : 0.334902, loss_ce: 0.022496
[03:11:52.820] iteration 2180 : loss : 0.204672, loss_ce: 0.014387
[03:11:56.922] iteration 2190 : loss : 0.215315, loss_ce: 0.046743
[03:12:01.017] iteration 2200 : loss : 0.288039, loss_ce: 0.013310
[03:12:05.124] iteration 2210 : loss : 0.262878, loss_ce: 0.016363
[03:12:09.220] iteration 2220 : loss : 0.262349, loss_ce: 0.028659
[03:12:13.330] iteration 2230 : loss : 0.255797, loss_ce: 0.022805
[03:12:17.429] iteration 2240 : loss : 0.132198, loss_ce: 0.021471
[03:12:21.539] iteration 2250 : loss : 0.325644, loss_ce: 0.011179
[03:12:25.643] iteration 2260 : loss : 0.224104, loss_ce: 0.010482
[03:12:29.754] iteration 2270 : loss : 0.326060, loss_ce: 0.009996
[03:12:33.857] iteration 2280 : loss : 0.339858, loss_ce: 0.021813
[03:12:37.968] iteration 2290 : loss : 0.082539, loss_ce: 0.011804
[03:12:42.070] iteration 2300 : loss : 0.267422, loss_ce: 0.027573
[03:12:46.183] iteration 2310 : loss : 0.179622, loss_ce: 0.010440
[03:12:50.285] iteration 2320 : loss : 0.235747, loss_ce: 0.012117
[03:12:54.401] iteration 2330 : loss : 0.328455, loss_ce: 0.012995
[03:12:58.503] iteration 2340 : loss : 0.231488, loss_ce: 0.012607
[03:13:02.616] iteration 2350 : loss : 0.229187, loss_ce: 0.018150
[03:13:06.719] iteration 2360 : loss : 0.315834, loss_ce: 0.010168
[03:13:10.829] iteration 2370 : loss : 0.104012, loss_ce: 0.015731
[03:13:14.934] iteration 2380 : loss : 0.224676, loss_ce: 0.016866
[03:13:19.043] iteration 2390 : loss : 0.122729, loss_ce: 0.021991
[03:13:23.144] iteration 2400 : loss : 0.276804, loss_ce: 0.037440
[03:13:27.255] iteration 2410 : loss : 0.247079, loss_ce: 0.020725
[03:15:05.680] iteration 2420 : loss : 0.350515, loss_ce: 0.037045
[03:15:09.781] iteration 2430 : loss : 0.350154, loss_ce: 0.041208
[03:15:13.874] iteration 2440 : loss : 0.291672, loss_ce: 0.022629
[03:15:17.978] iteration 2450 : loss : 0.222272, loss_ce: 0.026668
[03:15:22.074] iteration 2460 : loss : 0.351501, loss_ce: 0.022899
[03:15:26.187] iteration 2470 : loss : 0.340852, loss_ce: 0.032780
[03:15:30.285] iteration 2480 : loss : 0.354130, loss_ce: 0.049896
[03:15:34.399] iteration 2490 : loss : 0.288298, loss_ce: 0.017000
[03:15:38.499] iteration 2500 : loss : 0.183803, loss_ce: 0.016525
[03:15:42.608] iteration 2510 : loss : 0.245217, loss_ce: 0.018591
[03:15:46.707] iteration 2520 : loss : 0.244054, loss_ce: 0.019307
[03:15:50.815] iteration 2530 : loss : 0.199632, loss_ce: 0.005934
[03:15:54.916] iteration 2540 : loss : 0.146879, loss_ce: 0.022091
[03:15:59.026] iteration 2550 : loss : 0.149157, loss_ce: 0.013710
[03:16:03.128] iteration 2560 : loss : 0.322443, loss_ce: 0.014967
[03:16:07.234] iteration 2570 : loss : 0.121115, loss_ce: 0.009546
[03:16:11.335] iteration 2580 : loss : 0.286778, loss_ce: 0.015925
[03:16:15.444] iteration 2590 : loss : 0.322507, loss_ce: 0.008729
[03:16:19.547] iteration 2600 : loss : 0.161084, loss_ce: 0.021036
[03:16:23.657] iteration 2610 : loss : 0.055167, loss_ce: 0.007242
[03:16:27.763] iteration 2620 : loss : 0.178676, loss_ce: 0.016274
[03:16:31.876] iteration 2630 : loss : 0.254869, loss_ce: 0.015239
[03:16:35.974] iteration 2640 : loss : 0.153697, loss_ce: 0.006411
[03:16:40.085] iteration 2650 : loss : 0.340646, loss_ce: 0.020092
[03:16:44.183] iteration 2660 : loss : 0.329408, loss_ce: 0.020418
[03:16:48.291] iteration 2670 : loss : 0.216590, loss_ce: 0.024111
[03:16:52.293] iteration 2680 : loss : 0.268552, loss_ce: 0.022903
[03:18:27.590] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_9.pth
[03:18:41.431] iteration 2690 : loss : 0.350681, loss_ce: 0.059489
[03:18:45.522] iteration 2700 : loss : 0.128868, loss_ce: 0.023965
[03:18:49.620] iteration 2710 : loss : 0.320596, loss_ce: 0.022377
[03:18:53.710] iteration 2720 : loss : 0.263452, loss_ce: 0.012074
[03:18:57.814] iteration 2730 : loss : 0.289467, loss_ce: 0.027984
[03:19:01.909] iteration 2740 : loss : 0.340355, loss_ce: 0.034357
[03:19:06.019] iteration 2750 : loss : 0.335840, loss_ce: 0.030123
[03:19:10.116] iteration 2760 : loss : 0.301217, loss_ce: 0.008810
[03:19:14.226] iteration 2770 : loss : 0.331163, loss_ce: 0.009262
[03:19:18.325] iteration 2780 : loss : 0.326128, loss_ce: 0.014013
[03:19:22.437] iteration 2790 : loss : 0.291489, loss_ce: 0.014680
[03:19:26.538] iteration 2800 : loss : 0.201396, loss_ce: 0.010149
[03:19:30.647] iteration 2810 : loss : 0.211236, loss_ce: 0.014637
[03:19:34.751] iteration 2820 : loss : 0.284278, loss_ce: 0.009721
[03:19:38.863] iteration 2830 : loss : 0.347026, loss_ce: 0.024867
[03:19:42.964] iteration 2840 : loss : 0.333177, loss_ce: 0.007075
[03:19:47.077] iteration 2850 : loss : 0.228725, loss_ce: 0.020600
[03:19:51.179] iteration 2860 : loss : 0.181101, loss_ce: 0.021088
[03:19:55.290] iteration 2870 : loss : 0.312810, loss_ce: 0.016097
[03:19:59.393] iteration 2880 : loss : 0.251759, loss_ce: 0.027910
[03:20:03.505] iteration 2890 : loss : 0.126236, loss_ce: 0.011511
[03:20:07.609] iteration 2900 : loss : 0.208815, loss_ce: 0.003403
[03:20:11.720] iteration 2910 : loss : 0.260595, loss_ce: 0.011125
[03:20:15.820] iteration 2920 : loss : 0.076244, loss_ce: 0.011611
[03:20:19.936] iteration 2930 : loss : 0.259052, loss_ce: 0.018542
[03:20:24.039] iteration 2940 : loss : 0.224725, loss_ce: 0.015182
[03:22:02.312] iteration 2950 : loss : 0.175665, loss_ce: 0.022818
[03:22:06.397] iteration 2960 : loss : 0.184913, loss_ce: 0.009268
[03:22:10.495] iteration 2970 : loss : 0.177219, loss_ce: 0.018962
[03:22:14.587] iteration 2980 : loss : 0.385894, loss_ce: 0.038086
[03:22:18.691] iteration 2990 : loss : 0.339283, loss_ce: 0.097172
[03:22:22.786] iteration 3000 : loss : 0.460520, loss_ce: 0.045315
[03:22:26.892] iteration 3010 : loss : 0.484765, loss_ce: 0.085502
[03:22:30.986] iteration 3020 : loss : 0.324915, loss_ce: 0.069616
[03:22:35.093] iteration 3030 : loss : 0.463299, loss_ce: 0.053725
[03:22:39.191] iteration 3040 : loss : 0.458890, loss_ce: 0.046919
[03:22:43.296] iteration 3050 : loss : 0.456686, loss_ce: 0.050357
[03:22:47.392] iteration 3060 : loss : 0.439533, loss_ce: 0.029961
[03:22:51.501] iteration 3070 : loss : 0.278008, loss_ce: 0.042563
[03:22:55.600] iteration 3080 : loss : 0.442167, loss_ce: 0.048512
[03:22:59.710] iteration 3090 : loss : 0.431875, loss_ce: 0.072377
[03:23:03.812] iteration 3100 : loss : 0.418482, loss_ce: 0.033084
[03:23:07.925] iteration 3110 : loss : 0.407419, loss_ce: 0.063375
[03:23:12.025] iteration 3120 : loss : 0.207902, loss_ce: 0.044170
[03:23:16.136] iteration 3130 : loss : 0.330487, loss_ce: 0.034683
[03:23:20.237] iteration 3140 : loss : 0.339562, loss_ce: 0.012690
[03:23:24.350] iteration 3150 : loss : 0.310725, loss_ce: 0.027174
[03:23:28.451] iteration 3160 : loss : 0.466997, loss_ce: 0.041858
[03:23:32.557] iteration 3170 : loss : 0.454901, loss_ce: 0.055996
[03:23:36.653] iteration 3180 : loss : 0.446199, loss_ce: 0.058664
[03:23:40.759] iteration 3190 : loss : 0.433473, loss_ce: 0.048470
[03:23:44.857] iteration 3200 : loss : 0.439172, loss_ce: 0.054136
[03:23:48.969] iteration 3210 : loss : 0.438227, loss_ce: 0.084337
[03:25:27.467] iteration 3220 : loss : 0.474599, loss_ce: 0.060474
[03:25:31.560] iteration 3230 : loss : 0.469169, loss_ce: 0.062224
[03:25:35.650] iteration 3240 : loss : 0.415397, loss_ce: 0.047917
[03:25:39.753] iteration 3250 : loss : 0.360637, loss_ce: 0.033886
[03:25:43.845] iteration 3260 : loss : 0.217901, loss_ce: 0.026665
[03:25:47.953] iteration 3270 : loss : 0.363518, loss_ce: 0.069070
[03:25:52.050] iteration 3280 : loss : 0.263313, loss_ce: 0.017674
[03:25:56.161] iteration 3290 : loss : 0.335395, loss_ce: 0.024494
[03:26:00.264] iteration 3300 : loss : 0.204590, loss_ce: 0.038036
[03:26:04.377] iteration 3310 : loss : 0.366722, loss_ce: 0.022984
[03:26:08.472] iteration 3320 : loss : 0.342442, loss_ce: 0.023121
[03:26:12.584] iteration 3330 : loss : 0.220750, loss_ce: 0.039485
[03:26:16.685] iteration 3340 : loss : 0.283905, loss_ce: 0.021018
[03:26:20.798] iteration 3350 : loss : 0.322098, loss_ce: 0.041449
[03:26:24.901] iteration 3360 : loss : 0.253590, loss_ce: 0.011306
[03:26:29.017] iteration 3370 : loss : 0.337008, loss_ce: 0.014853
[03:26:33.120] iteration 3380 : loss : 0.315525, loss_ce: 0.010882
[03:26:37.235] iteration 3390 : loss : 0.325966, loss_ce: 0.011819
[03:26:41.338] iteration 3400 : loss : 0.372905, loss_ce: 0.021368
[03:26:45.452] iteration 3410 : loss : 0.207277, loss_ce: 0.013773
[03:26:49.551] iteration 3420 : loss : 0.205398, loss_ce: 0.049990
[03:26:53.664] iteration 3430 : loss : 0.133608, loss_ce: 0.031071
[03:26:57.768] iteration 3440 : loss : 0.277067, loss_ce: 0.014136
[03:27:01.882] iteration 3450 : loss : 0.285109, loss_ce: 0.021621
[03:27:05.985] iteration 3460 : loss : 0.357024, loss_ce: 0.032368
[03:27:10.103] iteration 3470 : loss : 0.317593, loss_ce: 0.009902
[03:27:14.205] iteration 3480 : loss : 0.290002, loss_ce: 0.007539
[03:29:03.223] iteration 3490 : loss : 0.380191, loss_ce: 0.035792
[03:29:07.310] iteration 3500 : loss : 0.286732, loss_ce: 0.047812
[03:29:11.406] iteration 3510 : loss : 0.300448, loss_ce: 0.038302
[03:29:15.495] iteration 3520 : loss : 0.319923, loss_ce: 0.025748
[03:29:19.600] iteration 3530 : loss : 0.393179, loss_ce: 0.095760
[03:29:23.697] iteration 3540 : loss : 0.180782, loss_ce: 0.029903
[03:29:27.802] iteration 3550 : loss : 0.161714, loss_ce: 0.023016
[03:29:31.900] iteration 3560 : loss : 0.314476, loss_ce: 0.024029
[03:29:36.010] iteration 3570 : loss : 0.322871, loss_ce: 0.015002
[03:29:40.111] iteration 3580 : loss : 0.279325, loss_ce: 0.020124
[03:29:44.217] iteration 3590 : loss : 0.264016, loss_ce: 0.023044
[03:29:48.315] iteration 3600 : loss : 0.250462, loss_ce: 0.024834
[03:29:52.425] iteration 3610 : loss : 0.297516, loss_ce: 0.018588
[03:29:56.526] iteration 3620 : loss : 0.372348, loss_ce: 0.034233
[03:30:00.634] iteration 3630 : loss : 0.279871, loss_ce: 0.026286
[03:30:04.735] iteration 3640 : loss : 0.099087, loss_ce: 0.024068
[03:30:08.843] iteration 3650 : loss : 0.312179, loss_ce: 0.020207
[03:30:12.941] iteration 3660 : loss : 0.298885, loss_ce: 0.025489
[03:30:17.052] iteration 3670 : loss : 0.287305, loss_ce: 0.025104
[03:30:21.150] iteration 3680 : loss : 0.331062, loss_ce: 0.009737
[03:30:25.263] iteration 3690 : loss : 0.293676, loss_ce: 0.013639
[03:30:29.366] iteration 3700 : loss : 0.316866, loss_ce: 0.006266
[03:30:33.477] iteration 3710 : loss : 0.314910, loss_ce: 0.019296
[03:30:37.576] iteration 3720 : loss : 0.252609, loss_ce: 0.026448
[03:30:41.687] iteration 3730 : loss : 0.251521, loss_ce: 0.008859
[03:30:45.788] iteration 3740 : loss : 0.316479, loss_ce: 0.022712
[03:30:49.896] iteration 3750 : loss : 0.152960, loss_ce: 0.007953
[03:32:28.176] iteration 3760 : loss : 0.230420, loss_ce: 0.039627
[03:32:32.276] iteration 3770 : loss : 0.327428, loss_ce: 0.010923
[03:32:36.365] iteration 3780 : loss : 0.411889, loss_ce: 0.034061
[03:32:40.461] iteration 3790 : loss : 0.355704, loss_ce: 0.055385
[03:32:44.552] iteration 3800 : loss : 0.357286, loss_ce: 0.070134
[03:32:48.660] iteration 3810 : loss : 0.155040, loss_ce: 0.007119
[03:32:52.757] iteration 3820 : loss : 0.267773, loss_ce: 0.013040
[03:32:56.865] iteration 3830 : loss : 0.144698, loss_ce: 0.017211
[03:33:00.967] iteration 3840 : loss : 0.139471, loss_ce: 0.008903
[03:33:05.076] iteration 3850 : loss : 0.247706, loss_ce: 0.018866
[03:33:09.176] iteration 3860 : loss : 0.149153, loss_ce: 0.010562
[03:33:13.288] iteration 3870 : loss : 0.325656, loss_ce: 0.003023
[03:33:17.388] iteration 3880 : loss : 0.319984, loss_ce: 0.021052
[03:33:21.498] iteration 3890 : loss : 0.317719, loss_ce: 0.003561
[03:33:25.602] iteration 3900 : loss : 0.252665, loss_ce: 0.028651
[03:33:29.714] iteration 3910 : loss : 0.342196, loss_ce: 0.029277
[03:33:33.820] iteration 3920 : loss : 0.246644, loss_ce: 0.014287
[03:33:37.929] iteration 3930 : loss : 0.250081, loss_ce: 0.014249
[03:33:42.029] iteration 3940 : loss : 0.331683, loss_ce: 0.016296
[03:33:46.141] iteration 3950 : loss : 0.243672, loss_ce: 0.033656
[03:33:50.245] iteration 3960 : loss : 0.230033, loss_ce: 0.015650
[03:33:54.359] iteration 3970 : loss : 0.207434, loss_ce: 0.017322
[03:33:58.459] iteration 3980 : loss : 0.321221, loss_ce: 0.014080
[03:34:02.577] iteration 3990 : loss : 0.248315, loss_ce: 0.012628
[03:34:06.678] iteration 4000 : loss : 0.298943, loss_ce: 0.019124
[03:34:10.792] iteration 4010 : loss : 0.329182, loss_ce: 0.015975
[03:34:14.793] iteration 4020 : loss : 0.328329, loss_ce: 0.006487
[03:35:53.140] iteration 4030 : loss : 0.335620, loss_ce: 0.027044
[03:35:57.229] iteration 4040 : loss : 0.444690, loss_ce: 0.116946
[03:36:01.326] iteration 4050 : loss : 0.178657, loss_ce: 0.010428
[03:36:05.421] iteration 4060 : loss : 0.354846, loss_ce: 0.034359
[03:36:09.528] iteration 4070 : loss : 0.335909, loss_ce: 0.010001
[03:36:13.623] iteration 4080 : loss : 0.331528, loss_ce: 0.036570
[03:36:17.730] iteration 4090 : loss : 0.075914, loss_ce: 0.008670
[03:36:21.829] iteration 4100 : loss : 0.330266, loss_ce: 0.016590
[03:36:25.938] iteration 4110 : loss : 0.347021, loss_ce: 0.038949
[03:36:30.036] iteration 4120 : loss : 0.269340, loss_ce: 0.029472
[03:36:34.147] iteration 4130 : loss : 0.292498, loss_ce: 0.017055
[03:36:38.247] iteration 4140 : loss : 0.299194, loss_ce: 0.039909
[03:36:42.352] iteration 4150 : loss : 0.325231, loss_ce: 0.019348
[03:36:46.450] iteration 4160 : loss : 0.149598, loss_ce: 0.012190
[03:36:50.560] iteration 4170 : loss : 0.246397, loss_ce: 0.011471
[03:36:54.658] iteration 4180 : loss : 0.291479, loss_ce: 0.015205
[03:36:58.770] iteration 4190 : loss : 0.334740, loss_ce: 0.057767
[03:37:02.869] iteration 4200 : loss : 0.216235, loss_ce: 0.016461
[03:37:06.977] iteration 4210 : loss : 0.236285, loss_ce: 0.021706
[03:37:11.075] iteration 4220 : loss : 0.122959, loss_ce: 0.012903
[03:37:15.184] iteration 4230 : loss : 0.281711, loss_ce: 0.048316
[03:37:19.283] iteration 4240 : loss : 0.267754, loss_ce: 0.016399
[03:37:23.393] iteration 4250 : loss : 0.251378, loss_ce: 0.019197
[03:37:27.491] iteration 4260 : loss : 0.237977, loss_ce: 0.007242
[03:37:31.603] iteration 4270 : loss : 0.244971, loss_ce: 0.016046
[03:37:35.701] iteration 4280 : loss : 0.296466, loss_ce: 0.024389
[03:39:24.385] iteration 4290 : loss : 0.391200, loss_ce: 0.048961
[03:39:28.470] iteration 4300 : loss : 0.305001, loss_ce: 0.028161
[03:39:32.570] iteration 4310 : loss : 0.204124, loss_ce: 0.033577
[03:39:36.658] iteration 4320 : loss : 0.362651, loss_ce: 0.020248
[03:39:40.756] iteration 4330 : loss : 0.365663, loss_ce: 0.035890
[03:39:44.850] iteration 4340 : loss : 0.294247, loss_ce: 0.060279
[03:39:48.952] iteration 4350 : loss : 0.288379, loss_ce: 0.026562
[03:39:53.048] iteration 4360 : loss : 0.316856, loss_ce: 0.016171
[03:39:57.155] iteration 4370 : loss : 0.183522, loss_ce: 0.022136
[03:40:01.254] iteration 4380 : loss : 0.256992, loss_ce: 0.018890
[03:40:05.366] iteration 4390 : loss : 0.331051, loss_ce: 0.009385
[03:40:09.465] iteration 4400 : loss : 0.334442, loss_ce: 0.018802
[03:40:13.575] iteration 4410 : loss : 0.301089, loss_ce: 0.022621
[03:40:17.673] iteration 4420 : loss : 0.247487, loss_ce: 0.016611
[03:40:21.787] iteration 4430 : loss : 0.229322, loss_ce: 0.009769
[03:40:25.890] iteration 4440 : loss : 0.308984, loss_ce: 0.029914
[03:40:30.000] iteration 4450 : loss : 0.263656, loss_ce: 0.023040
[03:40:34.104] iteration 4460 : loss : 0.151291, loss_ce: 0.023207
[03:40:38.214] iteration 4470 : loss : 0.245561, loss_ce: 0.008877
[03:40:42.314] iteration 4480 : loss : 0.274597, loss_ce: 0.005587
[03:40:46.428] iteration 4490 : loss : 0.235578, loss_ce: 0.031482
[03:40:50.529] iteration 4500 : loss : 0.076670, loss_ce: 0.010209
[03:40:54.641] iteration 4510 : loss : 0.324440, loss_ce: 0.018149
[03:40:58.745] iteration 4520 : loss : 0.166699, loss_ce: 0.011670
[03:41:02.859] iteration 4530 : loss : 0.166486, loss_ce: 0.009361
[03:41:06.960] iteration 4540 : loss : 0.246974, loss_ce: 0.010092
[03:41:11.071] iteration 4550 : loss : 0.355045, loss_ce: 0.007546
[03:42:49.359] iteration 4560 : loss : 0.349575, loss_ce: 0.023402
[03:42:53.459] iteration 4570 : loss : 0.388942, loss_ce: 0.049504
[03:42:57.552] iteration 4580 : loss : 0.345276, loss_ce: 0.026160
[03:43:01.656] iteration 4590 : loss : 0.172226, loss_ce: 0.031490
[03:43:05.752] iteration 4600 : loss : 0.319920, loss_ce: 0.017922
[03:43:09.863] iteration 4610 : loss : 0.195071, loss_ce: 0.042553
[03:43:13.963] iteration 4620 : loss : 0.289825, loss_ce: 0.023318
[03:43:18.070] iteration 4630 : loss : 0.272802, loss_ce: 0.010804
[03:43:22.174] iteration 4640 : loss : 0.366246, loss_ce: 0.041286
[03:43:26.287] iteration 4650 : loss : 0.280324, loss_ce: 0.015542
[03:43:30.393] iteration 4660 : loss : 0.263652, loss_ce: 0.032281
[03:43:34.505] iteration 4670 : loss : 0.238417, loss_ce: 0.018742
[03:43:38.606] iteration 4680 : loss : 0.324076, loss_ce: 0.020795
[03:43:42.720] iteration 4690 : loss : 0.271288, loss_ce: 0.023788
[03:43:46.822] iteration 4700 : loss : 0.218772, loss_ce: 0.011264
[03:43:50.935] iteration 4710 : loss : 0.201310, loss_ce: 0.009223
[03:43:55.035] iteration 4720 : loss : 0.231419, loss_ce: 0.014609
[03:43:59.151] iteration 4730 : loss : 0.211832, loss_ce: 0.005814
[03:44:03.255] iteration 4740 : loss : 0.273334, loss_ce: 0.024391
[03:44:07.368] iteration 4750 : loss : 0.333789, loss_ce: 0.037221
[03:44:11.468] iteration 4760 : loss : 0.270530, loss_ce: 0.035368
[03:44:15.584] iteration 4770 : loss : 0.068639, loss_ce: 0.008525
[03:44:19.688] iteration 4780 : loss : 0.321165, loss_ce: 0.009570
[03:44:23.804] iteration 4790 : loss : 0.206941, loss_ce: 0.013453
[03:44:27.906] iteration 4800 : loss : 0.204208, loss_ce: 0.005039
[03:44:32.024] iteration 4810 : loss : 0.315761, loss_ce: 0.005133
[03:44:36.129] iteration 4820 : loss : 0.266326, loss_ce: 0.005047
[03:46:14.702] iteration 4830 : loss : 0.329470, loss_ce: 0.008113
[03:46:18.785] iteration 4840 : loss : 0.349757, loss_ce: 0.026627
[03:46:22.882] iteration 4850 : loss : 0.317231, loss_ce: 0.007635
[03:46:26.978] iteration 4860 : loss : 0.327088, loss_ce: 0.026440
[03:46:31.083] iteration 4870 : loss : 0.218788, loss_ce: 0.022347
[03:46:35.179] iteration 4880 : loss : 0.105698, loss_ce: 0.012948
[03:46:39.285] iteration 4890 : loss : 0.122653, loss_ce: 0.016367
[03:46:43.382] iteration 4900 : loss : 0.336129, loss_ce: 0.056705
[03:46:47.489] iteration 4910 : loss : 0.235411, loss_ce: 0.016365
[03:46:51.582] iteration 4920 : loss : 0.248883, loss_ce: 0.016611
[03:46:55.688] iteration 4930 : loss : 0.251352, loss_ce: 0.023787
[03:46:59.787] iteration 4940 : loss : 0.330846, loss_ce: 0.030845
[03:47:03.896] iteration 4950 : loss : 0.345595, loss_ce: 0.023244
[03:47:07.992] iteration 4960 : loss : 0.292602, loss_ce: 0.003101
[03:47:12.102] iteration 4970 : loss : 0.304603, loss_ce: 0.009637
[03:47:16.202] iteration 4980 : loss : 0.280094, loss_ce: 0.039073
[03:47:20.309] iteration 4990 : loss : 0.331576, loss_ce: 0.013455
[03:47:24.409] iteration 5000 : loss : 0.242944, loss_ce: 0.009350
[03:47:28.522] iteration 5010 : loss : 0.219530, loss_ce: 0.010851
[03:47:32.622] iteration 5020 : loss : 0.331922, loss_ce: 0.026833
[03:47:36.728] iteration 5030 : loss : 0.232380, loss_ce: 0.007174
[03:47:40.827] iteration 5040 : loss : 0.123016, loss_ce: 0.015089
[03:47:44.938] iteration 5050 : loss : 0.250101, loss_ce: 0.021311
[03:47:49.038] iteration 5060 : loss : 0.208331, loss_ce: 0.008156
[03:47:53.146] iteration 5070 : loss : 0.266832, loss_ce: 0.009843
[03:47:57.246] iteration 5080 : loss : 0.277859, loss_ce: 0.016860
[03:48:01.356] iteration 5090 : loss : 0.270488, loss_ce: 0.019404
[03:49:50.462] iteration 5100 : loss : 0.344698, loss_ce: 0.040526
[03:49:54.559] iteration 5110 : loss : 0.355091, loss_ce: 0.029354
[03:49:58.649] iteration 5120 : loss : 0.328564, loss_ce: 0.023135
[03:50:02.751] iteration 5130 : loss : 0.332436, loss_ce: 0.012697
[03:50:06.843] iteration 5140 : loss : 0.333764, loss_ce: 0.011120
[03:50:10.950] iteration 5150 : loss : 0.333399, loss_ce: 0.028462
[03:50:15.048] iteration 5160 : loss : 0.329397, loss_ce: 0.007840
[03:50:19.158] iteration 5170 : loss : 0.294813, loss_ce: 0.019832
[03:50:23.257] iteration 5180 : loss : 0.300543, loss_ce: 0.033358
[03:50:27.368] iteration 5190 : loss : 0.286292, loss_ce: 0.016785
[03:50:31.465] iteration 5200 : loss : 0.258716, loss_ce: 0.017201
[03:50:35.576] iteration 5210 : loss : 0.168899, loss_ce: 0.013037
[03:50:39.675] iteration 5220 : loss : 0.293528, loss_ce: 0.013783
[03:50:43.785] iteration 5230 : loss : 0.207771, loss_ce: 0.010585
[03:50:47.887] iteration 5240 : loss : 0.292308, loss_ce: 0.023649
[03:50:51.998] iteration 5250 : loss : 0.327821, loss_ce: 0.015284
[03:50:56.098] iteration 5260 : loss : 0.241044, loss_ce: 0.019037
[03:51:00.210] iteration 5270 : loss : 0.224018, loss_ce: 0.020529
[03:51:04.314] iteration 5280 : loss : 0.182896, loss_ce: 0.012708
[03:51:08.425] iteration 5290 : loss : 0.202147, loss_ce: 0.013294
[03:51:12.526] iteration 5300 : loss : 0.240526, loss_ce: 0.028048
[03:51:16.638] iteration 5310 : loss : 0.298744, loss_ce: 0.005884
[03:51:20.738] iteration 5320 : loss : 0.152572, loss_ce: 0.012071
[03:51:24.850] iteration 5330 : loss : 0.356790, loss_ce: 0.011097
[03:51:28.955] iteration 5340 : loss : 0.183936, loss_ce: 0.026853
[03:51:33.070] iteration 5350 : loss : 0.311607, loss_ce: 0.009723
[03:51:37.072] iteration 5360 : loss : 0.122217, loss_ce: 0.013859
[03:53:01.842] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_19.pth
[03:53:15.812] iteration 5370 : loss : 0.339852, loss_ce: 0.024994
[03:53:19.903] iteration 5380 : loss : 0.230430, loss_ce: 0.034271
[03:53:23.997] iteration 5390 : loss : 0.242674, loss_ce: 0.016491
[03:53:28.090] iteration 5400 : loss : 0.333282, loss_ce: 0.032955
[03:53:32.191] iteration 5410 : loss : 0.171196, loss_ce: 0.005237
[03:53:36.287] iteration 5420 : loss : 0.313918, loss_ce: 0.029040
[03:53:40.396] iteration 5430 : loss : 0.331439, loss_ce: 0.017228
[03:53:44.490] iteration 5440 : loss : 0.139443, loss_ce: 0.008003
[03:53:48.595] iteration 5450 : loss : 0.271471, loss_ce: 0.014825
[03:53:52.694] iteration 5460 : loss : 0.341946, loss_ce: 0.014239
[03:53:56.801] iteration 5470 : loss : 0.254248, loss_ce: 0.019761
[03:54:00.904] iteration 5480 : loss : 0.234462, loss_ce: 0.015603
[03:54:05.013] iteration 5490 : loss : 0.247446, loss_ce: 0.019346
[03:54:09.109] iteration 5500 : loss : 0.156747, loss_ce: 0.007204
[03:54:13.220] iteration 5510 : loss : 0.344049, loss_ce: 0.033674
[03:54:17.320] iteration 5520 : loss : 0.102633, loss_ce: 0.009747
[03:54:21.427] iteration 5530 : loss : 0.329445, loss_ce: 0.014715
[03:54:25.526] iteration 5540 : loss : 0.169108, loss_ce: 0.050180
[03:54:29.636] iteration 5550 : loss : 0.294436, loss_ce: 0.037923
[03:54:33.734] iteration 5560 : loss : 0.333319, loss_ce: 0.019007
[03:54:37.843] iteration 5570 : loss : 0.294748, loss_ce: 0.028408
[03:54:41.941] iteration 5580 : loss : 0.241415, loss_ce: 0.019231
[03:54:46.050] iteration 5590 : loss : 0.169345, loss_ce: 0.005799
[03:54:50.151] iteration 5600 : loss : 0.305985, loss_ce: 0.013156
[03:54:54.258] iteration 5610 : loss : 0.304172, loss_ce: 0.016948
[03:54:58.355] iteration 5620 : loss : 0.206265, loss_ce: 0.007123
[03:56:36.686] iteration 5630 : loss : 0.364298, loss_ce: 0.038593
[03:56:40.773] iteration 5640 : loss : 0.340109, loss_ce: 0.027756
[03:56:44.870] iteration 5650 : loss : 0.327766, loss_ce: 0.013655
[03:56:48.961] iteration 5660 : loss : 0.310445, loss_ce: 0.030395
[03:56:53.063] iteration 5670 : loss : 0.346104, loss_ce: 0.034392
[03:56:57.154] iteration 5680 : loss : 0.315963, loss_ce: 0.015110
[03:57:01.257] iteration 5690 : loss : 0.161281, loss_ce: 0.006784
[03:57:05.356] iteration 5700 : loss : 0.346817, loss_ce: 0.023537
[03:57:09.462] iteration 5710 : loss : 0.264152, loss_ce: 0.023452
[03:57:13.559] iteration 5720 : loss : 0.195488, loss_ce: 0.019272
[03:57:17.662] iteration 5730 : loss : 0.359900, loss_ce: 0.046599
[03:57:21.759] iteration 5740 : loss : 0.356738, loss_ce: 0.020700
[03:57:25.870] iteration 5750 : loss : 0.289592, loss_ce: 0.019924
[03:57:29.969] iteration 5760 : loss : 0.154728, loss_ce: 0.012421
[03:57:34.078] iteration 5770 : loss : 0.204463, loss_ce: 0.013598
[03:57:38.178] iteration 5780 : loss : 0.316879, loss_ce: 0.014568
[03:57:42.288] iteration 5790 : loss : 0.282882, loss_ce: 0.015442
[03:57:46.386] iteration 5800 : loss : 0.213336, loss_ce: 0.016156
[03:57:50.495] iteration 5810 : loss : 0.246733, loss_ce: 0.019107
[03:57:54.595] iteration 5820 : loss : 0.304726, loss_ce: 0.033360
[03:57:58.706] iteration 5830 : loss : 0.181848, loss_ce: 0.016568
[03:58:02.810] iteration 5840 : loss : 0.288602, loss_ce: 0.018293
[03:58:06.920] iteration 5850 : loss : 0.319431, loss_ce: 0.022819
[03:58:11.019] iteration 5860 : loss : 0.316925, loss_ce: 0.021391
[03:58:15.129] iteration 5870 : loss : 0.258376, loss_ce: 0.009796
[03:58:19.228] iteration 5880 : loss : 0.335573, loss_ce: 0.036949
[03:58:23.336] iteration 5890 : loss : 0.310903, loss_ce: 0.008529
[04:00:12.030] iteration 5900 : loss : 0.362242, loss_ce: 0.069578
[04:00:16.126] iteration 5910 : loss : 0.226644, loss_ce: 0.068177
[04:00:20.214] iteration 5920 : loss : 0.328550, loss_ce: 0.008777
[04:00:24.310] iteration 5930 : loss : 0.324164, loss_ce: 0.013729
[04:00:28.406] iteration 5940 : loss : 0.165718, loss_ce: 0.008325
[04:00:32.508] iteration 5950 : loss : 0.347706, loss_ce: 0.028966
[04:00:36.608] iteration 5960 : loss : 0.335549, loss_ce: 0.012324
[04:00:40.717] iteration 5970 : loss : 0.329902, loss_ce: 0.005052
[04:00:44.814] iteration 5980 : loss : 0.228959, loss_ce: 0.017437
[04:00:48.925] iteration 5990 : loss : 0.364059, loss_ce: 0.015614
[04:00:53.025] iteration 6000 : loss : 0.200533, loss_ce: 0.010448
[04:00:57.130] iteration 6010 : loss : 0.324790, loss_ce: 0.010914
[04:01:01.232] iteration 6020 : loss : 0.279737, loss_ce: 0.021959
[04:01:05.342] iteration 6030 : loss : 0.224287, loss_ce: 0.010836
[04:01:09.443] iteration 6040 : loss : 0.258412, loss_ce: 0.019676
[04:01:13.550] iteration 6050 : loss : 0.236531, loss_ce: 0.010729
[04:01:17.652] iteration 6060 : loss : 0.094701, loss_ce: 0.014442
[04:01:21.762] iteration 6070 : loss : 0.252374, loss_ce: 0.012109
[04:01:25.866] iteration 6080 : loss : 0.281301, loss_ce: 0.014622
[04:01:29.978] iteration 6090 : loss : 0.130243, loss_ce: 0.019650
[04:01:34.081] iteration 6100 : loss : 0.257847, loss_ce: 0.026957
[04:01:38.194] iteration 6110 : loss : 0.049716, loss_ce: 0.009980
[04:01:42.296] iteration 6120 : loss : 0.234736, loss_ce: 0.010549
[04:01:46.407] iteration 6130 : loss : 0.269252, loss_ce: 0.029318
[04:01:50.508] iteration 6140 : loss : 0.212913, loss_ce: 0.015141
[04:01:54.621] iteration 6150 : loss : 0.264191, loss_ce: 0.023842
[04:01:58.721] iteration 6160 : loss : 0.093298, loss_ce: 0.005648
[04:03:37.193] iteration 6170 : loss : 0.436389, loss_ce: 0.108279
[04:03:41.279] iteration 6180 : loss : 0.350959, loss_ce: 0.030897
[04:03:45.379] iteration 6190 : loss : 0.330704, loss_ce: 0.013030
[04:03:49.470] iteration 6200 : loss : 0.332784, loss_ce: 0.013035
[04:03:53.570] iteration 6210 : loss : 0.343451, loss_ce: 0.043946
[04:03:57.665] iteration 6220 : loss : 0.404363, loss_ce: 0.111931
[04:04:01.775] iteration 6230 : loss : 0.358702, loss_ce: 0.070031
[04:04:05.876] iteration 6240 : loss : 0.364274, loss_ce: 0.055504
[04:04:09.983] iteration 6250 : loss : 0.175535, loss_ce: 0.013426
[04:04:14.085] iteration 6260 : loss : 0.331435, loss_ce: 0.031059
[04:04:18.197] iteration 6270 : loss : 0.339196, loss_ce: 0.040328
[04:04:22.302] iteration 6280 : loss : 0.328819, loss_ce: 0.022083
[04:04:26.408] iteration 6290 : loss : 0.337425, loss_ce: 0.036307
[04:04:30.506] iteration 6300 : loss : 0.319948, loss_ce: 0.007407
[04:04:34.624] iteration 6310 : loss : 0.333528, loss_ce: 0.030288
[04:04:38.730] iteration 6320 : loss : 0.153108, loss_ce: 0.015373
[04:04:42.841] iteration 6330 : loss : 0.325582, loss_ce: 0.011317
[04:04:46.941] iteration 6340 : loss : 0.082741, loss_ce: 0.006292
[04:04:51.059] iteration 6350 : loss : 0.419406, loss_ce: 0.049858
[04:04:55.165] iteration 6360 : loss : 0.192486, loss_ce: 0.024763
[04:04:59.278] iteration 6370 : loss : 0.355163, loss_ce: 0.019694
[04:05:03.382] iteration 6380 : loss : 0.299760, loss_ce: 0.024436
[04:05:07.496] iteration 6390 : loss : 0.278100, loss_ce: 0.013739
[04:05:11.602] iteration 6400 : loss : 0.127382, loss_ce: 0.025627
[04:05:15.714] iteration 6410 : loss : 0.273923, loss_ce: 0.019434
[04:05:19.817] iteration 6420 : loss : 0.316535, loss_ce: 0.007768
[04:05:23.930] iteration 6430 : loss : 0.255008, loss_ce: 0.022830
[04:07:02.360] iteration 6440 : loss : 0.341991, loss_ce: 0.040283
[04:07:06.459] iteration 6450 : loss : 0.316394, loss_ce: 0.012298
[04:07:10.549] iteration 6460 : loss : 0.185010, loss_ce: 0.035809
[04:07:14.653] iteration 6470 : loss : 0.172972, loss_ce: 0.019905
[04:07:18.748] iteration 6480 : loss : 0.354997, loss_ce: 0.035346
[04:07:22.854] iteration 6490 : loss : 0.176659, loss_ce: 0.025709
[04:07:26.954] iteration 6500 : loss : 0.320085, loss_ce: 0.027493
[04:07:31.061] iteration 6510 : loss : 0.343282, loss_ce: 0.026340
[04:07:35.164] iteration 6520 : loss : 0.287351, loss_ce: 0.020044
[04:07:39.273] iteration 6530 : loss : 0.337515, loss_ce: 0.021718
[04:07:43.372] iteration 6540 : loss : 0.248786, loss_ce: 0.026538
[04:07:47.481] iteration 6550 : loss : 0.089926, loss_ce: 0.013211
[04:07:51.582] iteration 6560 : loss : 0.365008, loss_ce: 0.011030
[04:07:55.695] iteration 6570 : loss : 0.106939, loss_ce: 0.015508
[04:07:59.798] iteration 6580 : loss : 0.317027, loss_ce: 0.015500
[04:08:03.910] iteration 6590 : loss : 0.247690, loss_ce: 0.014864
[04:08:08.013] iteration 6600 : loss : 0.164013, loss_ce: 0.006659
[04:08:12.125] iteration 6610 : loss : 0.203890, loss_ce: 0.010526
[04:08:16.228] iteration 6620 : loss : 0.220201, loss_ce: 0.022595
[04:08:20.343] iteration 6630 : loss : 0.266548, loss_ce: 0.012644
[04:08:24.446] iteration 6640 : loss : 0.232826, loss_ce: 0.014533
[04:08:28.560] iteration 6650 : loss : 0.246321, loss_ce: 0.020626
[04:08:32.662] iteration 6660 : loss : 0.314330, loss_ce: 0.002841
[04:08:36.772] iteration 6670 : loss : 0.323644, loss_ce: 0.014839
[04:08:40.872] iteration 6680 : loss : 0.218529, loss_ce: 0.016894
[04:08:44.983] iteration 6690 : loss : 0.322312, loss_ce: 0.006660
[04:08:48.985] iteration 6700 : loss : 0.126739, loss_ce: 0.015589
[04:10:37.915] iteration 6710 : loss : 0.276948, loss_ce: 0.031395
[04:10:42.004] iteration 6720 : loss : 0.333492, loss_ce: 0.046205
[04:10:46.102] iteration 6730 : loss : 0.280390, loss_ce: 0.020372
[04:10:50.193] iteration 6740 : loss : 0.311287, loss_ce: 0.021039
[04:10:54.296] iteration 6750 : loss : 0.334654, loss_ce: 0.019833
[04:10:58.391] iteration 6760 : loss : 0.181351, loss_ce: 0.011174
[04:11:02.496] iteration 6770 : loss : 0.335034, loss_ce: 0.021558
[04:11:06.592] iteration 6780 : loss : 0.119678, loss_ce: 0.012354
[04:11:10.698] iteration 6790 : loss : 0.171477, loss_ce: 0.010908
[04:11:14.793] iteration 6800 : loss : 0.055250, loss_ce: 0.004106
[04:11:18.903] iteration 6810 : loss : 0.096499, loss_ce: 0.007269
[04:11:23.002] iteration 6820 : loss : 0.335270, loss_ce: 0.033474
[04:11:27.113] iteration 6830 : loss : 0.350657, loss_ce: 0.054459
[04:11:31.210] iteration 6840 : loss : 0.247216, loss_ce: 0.015249
[04:11:35.319] iteration 6850 : loss : 0.242384, loss_ce: 0.012081
[04:11:39.417] iteration 6860 : loss : 0.151881, loss_ce: 0.014856
[04:11:43.527] iteration 6870 : loss : 0.217051, loss_ce: 0.025946
[04:11:47.627] iteration 6880 : loss : 0.263517, loss_ce: 0.023233
[04:11:51.734] iteration 6890 : loss : 0.290580, loss_ce: 0.024312
[04:11:55.832] iteration 6900 : loss : 0.217166, loss_ce: 0.006875
[04:11:59.938] iteration 6910 : loss : 0.129094, loss_ce: 0.010086
[04:12:04.036] iteration 6920 : loss : 0.287111, loss_ce: 0.010558
[04:12:08.145] iteration 6930 : loss : 0.170019, loss_ce: 0.004490
[04:12:12.241] iteration 6940 : loss : 0.315466, loss_ce: 0.022752
[04:12:16.348] iteration 6950 : loss : 0.310116, loss_ce: 0.005426
[04:12:20.446] iteration 6960 : loss : 0.120917, loss_ce: 0.007070
[04:13:58.984] iteration 6970 : loss : 0.217315, loss_ce: 0.035460
[04:14:03.071] iteration 6980 : loss : 0.307064, loss_ce: 0.014255
[04:14:07.168] iteration 6990 : loss : 0.384099, loss_ce: 0.065165
[04:14:11.256] iteration 7000 : loss : 0.349403, loss_ce: 0.036292
[04:14:15.357] iteration 7010 : loss : 0.351730, loss_ce: 0.047515
[04:14:19.450] iteration 7020 : loss : 0.330987, loss_ce: 0.022838
[04:14:23.557] iteration 7030 : loss : 0.333758, loss_ce: 0.028685
[04:14:27.656] iteration 7040 : loss : 0.339835, loss_ce: 0.019558
[04:14:31.765] iteration 7050 : loss : 0.325055, loss_ce: 0.019418
[04:14:35.867] iteration 7060 : loss : 0.328172, loss_ce: 0.059125
[04:14:39.976] iteration 7070 : loss : 0.262385, loss_ce: 0.017728
[04:14:44.076] iteration 7080 : loss : 0.288469, loss_ce: 0.033825
[04:14:48.186] iteration 7090 : loss : 0.215737, loss_ce: 0.011437
[04:14:52.283] iteration 7100 : loss : 0.333588, loss_ce: 0.019652
[04:14:56.395] iteration 7110 : loss : 0.345240, loss_ce: 0.060205
[04:15:00.497] iteration 7120 : loss : 0.230744, loss_ce: 0.019345
[04:15:04.607] iteration 7130 : loss : 0.316599, loss_ce: 0.006726
[04:15:08.708] iteration 7140 : loss : 0.303344, loss_ce: 0.007683
[04:15:12.820] iteration 7150 : loss : 0.227018, loss_ce: 0.010612
[04:15:16.920] iteration 7160 : loss : 0.182526, loss_ce: 0.007070
[04:15:21.033] iteration 7170 : loss : 0.347279, loss_ce: 0.032094
[04:15:25.133] iteration 7180 : loss : 0.062962, loss_ce: 0.006365
[04:15:29.250] iteration 7190 : loss : 0.241784, loss_ce: 0.017414
[04:15:33.351] iteration 7200 : loss : 0.292541, loss_ce: 0.016141
[04:15:37.460] iteration 7210 : loss : 0.307153, loss_ce: 0.020318
[04:15:41.561] iteration 7220 : loss : 0.314612, loss_ce: 0.008562
[04:15:45.671] iteration 7230 : loss : 0.204866, loss_ce: 0.011211
[04:17:24.130] iteration 7240 : loss : 0.345938, loss_ce: 0.016528
[04:17:28.230] iteration 7250 : loss : 0.320554, loss_ce: 0.014800
[04:17:32.325] iteration 7260 : loss : 0.386477, loss_ce: 0.121312
[04:17:36.429] iteration 7270 : loss : 0.298721, loss_ce: 0.023736
[04:17:40.523] iteration 7280 : loss : 0.309633, loss_ce: 0.022676
[04:17:44.628] iteration 7290 : loss : 0.289070, loss_ce: 0.011642
[04:17:48.725] iteration 7300 : loss : 0.329017, loss_ce: 0.026450
[04:17:52.831] iteration 7310 : loss : 0.070725, loss_ce: 0.009397
[04:17:56.927] iteration 7320 : loss : 0.317431, loss_ce: 0.010445
[04:18:01.033] iteration 7330 : loss : 0.233110, loss_ce: 0.017510
[04:18:05.131] iteration 7340 : loss : 0.099838, loss_ce: 0.009954
[04:18:09.240] iteration 7350 : loss : 0.097133, loss_ce: 0.008817
[04:18:13.342] iteration 7360 : loss : 0.220781, loss_ce: 0.015032
[04:18:17.452] iteration 7370 : loss : 0.172038, loss_ce: 0.013068
[04:18:21.554] iteration 7380 : loss : 0.182450, loss_ce: 0.008489
[04:18:25.669] iteration 7390 : loss : 0.174660, loss_ce: 0.003999
[04:18:29.773] iteration 7400 : loss : 0.193208, loss_ce: 0.027092
[04:18:33.886] iteration 7410 : loss : 0.324465, loss_ce: 0.032170
[04:18:37.991] iteration 7420 : loss : 0.210944, loss_ce: 0.007561
[04:18:42.103] iteration 7430 : loss : 0.240385, loss_ce: 0.015791
[04:18:46.205] iteration 7440 : loss : 0.231591, loss_ce: 0.010720
[04:18:50.315] iteration 7450 : loss : 0.258197, loss_ce: 0.018406
[04:18:54.416] iteration 7460 : loss : 0.239309, loss_ce: 0.015838
[04:18:58.527] iteration 7470 : loss : 0.172238, loss_ce: 0.008197
[04:19:02.627] iteration 7480 : loss : 0.259719, loss_ce: 0.033169
[04:19:06.734] iteration 7490 : loss : 0.175517, loss_ce: 0.002709
[04:19:10.837] iteration 7500 : loss : 0.333059, loss_ce: 0.039578
[04:20:59.684] iteration 7510 : loss : 0.333030, loss_ce: 0.016204
[04:21:03.773] iteration 7520 : loss : 0.356966, loss_ce: 0.087217
[04:21:07.874] iteration 7530 : loss : 0.341143, loss_ce: 0.012278
[04:21:11.965] iteration 7540 : loss : 0.330510, loss_ce: 0.025460
[04:21:16.067] iteration 7550 : loss : 0.326953, loss_ce: 0.016124
[04:21:20.165] iteration 7560 : loss : 0.240471, loss_ce: 0.020127
[04:21:24.274] iteration 7570 : loss : 0.268738, loss_ce: 0.019203
[04:21:28.376] iteration 7580 : loss : 0.316113, loss_ce: 0.019343
[04:21:32.487] iteration 7590 : loss : 0.209250, loss_ce: 0.063713
[04:21:36.588] iteration 7600 : loss : 0.313649, loss_ce: 0.018587
[04:21:40.697] iteration 7610 : loss : 0.324526, loss_ce: 0.009836
[04:21:44.799] iteration 7620 : loss : 0.308081, loss_ce: 0.005898
[04:21:48.910] iteration 7630 : loss : 0.235752, loss_ce: 0.016115
[04:21:53.011] iteration 7640 : loss : 0.255296, loss_ce: 0.022253
[04:21:57.124] iteration 7650 : loss : 0.294281, loss_ce: 0.033816
[04:22:01.228] iteration 7660 : loss : 0.318774, loss_ce: 0.008352
[04:22:05.340] iteration 7670 : loss : 0.211871, loss_ce: 0.012233
[04:22:09.443] iteration 7680 : loss : 0.047818, loss_ce: 0.010120
[04:22:13.554] iteration 7690 : loss : 0.227566, loss_ce: 0.016449
[04:22:17.656] iteration 7700 : loss : 0.226015, loss_ce: 0.024040
[04:22:21.768] iteration 7710 : loss : 0.240818, loss_ce: 0.009227
[04:22:25.871] iteration 7720 : loss : 0.238707, loss_ce: 0.019181
[04:22:29.985] iteration 7730 : loss : 0.292553, loss_ce: 0.008380
[04:22:34.088] iteration 7740 : loss : 0.274798, loss_ce: 0.022393
[04:22:38.199] iteration 7750 : loss : 0.326736, loss_ce: 0.014685
[04:22:42.301] iteration 7760 : loss : 0.262822, loss_ce: 0.008857
[04:22:46.413] iteration 7770 : loss : 0.076727, loss_ce: 0.011997
[04:24:25.043] iteration 7780 : loss : 0.328266, loss_ce: 0.024142
[04:24:29.141] iteration 7790 : loss : 0.250438, loss_ce: 0.016021
[04:24:33.232] iteration 7800 : loss : 0.376089, loss_ce: 0.055856
[04:24:37.333] iteration 7810 : loss : 0.182250, loss_ce: 0.041724
[04:24:41.428] iteration 7820 : loss : 0.281924, loss_ce: 0.019330
[04:24:45.536] iteration 7830 : loss : 0.289968, loss_ce: 0.009822
[04:24:49.632] iteration 7840 : loss : 0.295074, loss_ce: 0.013823
[04:24:53.742] iteration 7850 : loss : 0.233497, loss_ce: 0.018587
[04:24:57.839] iteration 7860 : loss : 0.319938, loss_ce: 0.031876
[04:25:01.952] iteration 7870 : loss : 0.315042, loss_ce: 0.038787
[04:25:06.051] iteration 7880 : loss : 0.253709, loss_ce: 0.025964
[04:25:10.161] iteration 7890 : loss : 0.079757, loss_ce: 0.012171
[04:25:14.262] iteration 7900 : loss : 0.198774, loss_ce: 0.031134
[04:25:18.371] iteration 7910 : loss : 0.324044, loss_ce: 0.018318
[04:25:22.472] iteration 7920 : loss : 0.067516, loss_ce: 0.010466
[04:25:26.585] iteration 7930 : loss : 0.224088, loss_ce: 0.007140
[04:25:30.690] iteration 7940 : loss : 0.247477, loss_ce: 0.015256
[04:25:34.803] iteration 7950 : loss : 0.225872, loss_ce: 0.011277
[04:25:38.906] iteration 7960 : loss : 0.285641, loss_ce: 0.029428
[04:25:43.017] iteration 7970 : loss : 0.230522, loss_ce: 0.008712
[04:25:47.119] iteration 7980 : loss : 0.237571, loss_ce: 0.016925
[04:25:51.234] iteration 7990 : loss : 0.315030, loss_ce: 0.008179
[04:25:55.335] iteration 8000 : loss : 0.331748, loss_ce: 0.014447
[04:25:59.445] iteration 8010 : loss : 0.224377, loss_ce: 0.025423
[04:26:03.546] iteration 8020 : loss : 0.291477, loss_ce: 0.010087
[04:26:07.658] iteration 8030 : loss : 0.064111, loss_ce: 0.012686
[04:26:11.660] iteration 8040 : loss : 0.301865, loss_ce: 0.024981
[04:27:36.364] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_29.pth
[04:27:50.196] iteration 8050 : loss : 0.321985, loss_ce: 0.010329
[04:27:54.288] iteration 8060 : loss : 0.292690, loss_ce: 0.021556
[04:27:58.387] iteration 8070 : loss : 0.303588, loss_ce: 0.014648
[04:28:02.478] iteration 8080 : loss : 0.206725, loss_ce: 0.013514
[04:28:06.581] iteration 8090 : loss : 0.297060, loss_ce: 0.016693
[04:28:10.677] iteration 8100 : loss : 0.323349, loss_ce: 0.003620
[04:28:14.780] iteration 8110 : loss : 0.220993, loss_ce: 0.020038
[04:28:18.877] iteration 8120 : loss : 0.320457, loss_ce: 0.015384
[04:28:22.981] iteration 8130 : loss : 0.288602, loss_ce: 0.016283
[04:28:27.081] iteration 8140 : loss : 0.347929, loss_ce: 0.015626
[04:28:31.190] iteration 8150 : loss : 0.165168, loss_ce: 0.020018
[04:28:35.290] iteration 8160 : loss : 0.248694, loss_ce: 0.013989
[04:28:39.398] iteration 8170 : loss : 0.294456, loss_ce: 0.007470
[04:28:43.495] iteration 8180 : loss : 0.247126, loss_ce: 0.024417
[04:28:47.603] iteration 8190 : loss : 0.275268, loss_ce: 0.010294
[04:28:51.702] iteration 8200 : loss : 0.240079, loss_ce: 0.011869
[04:28:55.812] iteration 8210 : loss : 0.221040, loss_ce: 0.015761
[04:28:59.914] iteration 8220 : loss : 0.280507, loss_ce: 0.020895
[04:29:04.030] iteration 8230 : loss : 0.206973, loss_ce: 0.013079
[04:29:08.129] iteration 8240 : loss : 0.203208, loss_ce: 0.002592
[04:29:12.238] iteration 8250 : loss : 0.277857, loss_ce: 0.037219
[04:29:16.338] iteration 8260 : loss : 0.083665, loss_ce: 0.018865
[04:29:20.446] iteration 8270 : loss : 0.266569, loss_ce: 0.016214
[04:29:24.548] iteration 8280 : loss : 0.213619, loss_ce: 0.015690
[04:29:28.656] iteration 8290 : loss : 0.215722, loss_ce: 0.008503
[04:29:32.758] iteration 8300 : loss : 0.318841, loss_ce: 0.014902
[04:31:21.661] iteration 8310 : loss : 0.333421, loss_ce: 0.040349
[04:31:25.749] iteration 8320 : loss : 0.474919, loss_ce: 0.066376
[04:31:29.843] iteration 8330 : loss : 0.481272, loss_ce: 0.090249
[04:31:33.936] iteration 8340 : loss : 0.281465, loss_ce: 0.041144
[04:31:38.037] iteration 8350 : loss : 0.253456, loss_ce: 0.038627
[04:31:42.134] iteration 8360 : loss : 0.383354, loss_ce: 0.024343
[04:31:46.239] iteration 8370 : loss : 0.358029, loss_ce: 0.028566
[04:31:50.333] iteration 8380 : loss : 0.334964, loss_ce: 0.049173
[04:31:54.442] iteration 8390 : loss : 0.236464, loss_ce: 0.026157
[04:31:58.539] iteration 8400 : loss : 0.233441, loss_ce: 0.022708
[04:32:02.648] iteration 8410 : loss : 0.160937, loss_ce: 0.008255
[04:32:06.745] iteration 8420 : loss : 0.270809, loss_ce: 0.021779
[04:32:10.856] iteration 8430 : loss : 0.216302, loss_ce: 0.007889
[04:32:14.955] iteration 8440 : loss : 0.077783, loss_ce: 0.009188
[04:32:19.068] iteration 8450 : loss : 0.288866, loss_ce: 0.011774
[04:32:23.167] iteration 8460 : loss : 0.155500, loss_ce: 0.008601
[04:32:27.281] iteration 8470 : loss : 0.287550, loss_ce: 0.020326
[04:32:31.381] iteration 8480 : loss : 0.301572, loss_ce: 0.009344
[04:32:35.495] iteration 8490 : loss : 0.180051, loss_ce: 0.009897
[04:32:39.595] iteration 8500 : loss : 0.274258, loss_ce: 0.018147
[04:32:43.707] iteration 8510 : loss : 0.039904, loss_ce: 0.006864
[04:32:47.808] iteration 8520 : loss : 0.222355, loss_ce: 0.008481
[04:32:51.920] iteration 8530 : loss : 0.125075, loss_ce: 0.011708
[04:32:56.020] iteration 8540 : loss : 0.283069, loss_ce: 0.038354
[04:33:00.133] iteration 8550 : loss : 0.294662, loss_ce: 0.012432
[04:33:04.236] iteration 8560 : loss : 0.226294, loss_ce: 0.014323
[04:33:08.345] iteration 8570 : loss : 0.263502, loss_ce: 0.022833
[04:34:46.822] iteration 8580 : loss : 0.395216, loss_ce: 0.030138
[04:34:50.920] iteration 8590 : loss : 0.335162, loss_ce: 0.024878
[04:34:55.009] iteration 8600 : loss : 0.079568, loss_ce: 0.014732
[04:34:59.113] iteration 8610 : loss : 0.238707, loss_ce: 0.016808
[04:35:03.207] iteration 8620 : loss : 0.245744, loss_ce: 0.021642
[04:35:07.314] iteration 8630 : loss : 0.347047, loss_ce: 0.023184
[04:35:11.412] iteration 8640 : loss : 0.330548, loss_ce: 0.015802
[04:35:15.521] iteration 8650 : loss : 0.100255, loss_ce: 0.011926
[04:35:19.623] iteration 8660 : loss : 0.263772, loss_ce: 0.015520
[04:35:23.735] iteration 8670 : loss : 0.205065, loss_ce: 0.005373
[04:35:27.840] iteration 8680 : loss : 0.120276, loss_ce: 0.013537
[04:35:31.950] iteration 8690 : loss : 0.322238, loss_ce: 0.005964
[04:35:36.055] iteration 8700 : loss : 0.251406, loss_ce: 0.014558
[04:35:40.164] iteration 8710 : loss : 0.209625, loss_ce: 0.008850
[04:35:44.264] iteration 8720 : loss : 0.262086, loss_ce: 0.015955
[04:35:48.378] iteration 8730 : loss : 0.245539, loss_ce: 0.017006
[04:35:52.479] iteration 8740 : loss : 0.246392, loss_ce: 0.013946
[04:35:56.590] iteration 8750 : loss : 0.207619, loss_ce: 0.014683
[04:36:00.697] iteration 8760 : loss : 0.102162, loss_ce: 0.008146
[04:36:04.810] iteration 8770 : loss : 0.249118, loss_ce: 0.014530
[04:36:08.914] iteration 8780 : loss : 0.304099, loss_ce: 0.012390
[04:36:13.029] iteration 8790 : loss : 0.331049, loss_ce: 0.013144
[04:36:17.131] iteration 8800 : loss : 0.303615, loss_ce: 0.023480
[04:36:21.241] iteration 8810 : loss : 0.156190, loss_ce: 0.005607
[04:36:25.343] iteration 8820 : loss : 0.241200, loss_ce: 0.023065
[04:36:29.462] iteration 8830 : loss : 0.230611, loss_ce: 0.011991
[04:36:33.569] iteration 8840 : loss : 0.191406, loss_ce: 0.010132
[04:38:12.571] iteration 8850 : loss : 0.316759, loss_ce: 0.028234
[04:38:16.662] iteration 8860 : loss : 0.286818, loss_ce: 0.038202
[04:38:20.770] iteration 8870 : loss : 0.296175, loss_ce: 0.011066
[04:38:24.869] iteration 8880 : loss : 0.338865, loss_ce: 0.029328
[04:38:28.978] iteration 8890 : loss : 0.324990, loss_ce: 0.016070
[04:38:33.079] iteration 8900 : loss : 0.316514, loss_ce: 0.016225
[04:38:37.191] iteration 8910 : loss : 0.308216, loss_ce: 0.017383
[04:38:41.285] iteration 8920 : loss : 0.125006, loss_ce: 0.032835
[04:38:45.392] iteration 8930 : loss : 0.214123, loss_ce: 0.012928
[04:38:49.496] iteration 8940 : loss : 0.303225, loss_ce: 0.038837
[04:38:53.608] iteration 8950 : loss : 0.303454, loss_ce: 0.010148
[04:38:57.710] iteration 8960 : loss : 0.190777, loss_ce: 0.005908
[04:39:01.827] iteration 8970 : loss : 0.318780, loss_ce: 0.006910
[04:39:05.930] iteration 8980 : loss : 0.218932, loss_ce: 0.012842
[04:39:10.046] iteration 8990 : loss : 0.170521, loss_ce: 0.009196
[04:39:14.150] iteration 9000 : loss : 0.248199, loss_ce: 0.007227
[04:39:18.266] iteration 9010 : loss : 0.305903, loss_ce: 0.011743
[04:39:22.371] iteration 9020 : loss : 0.059752, loss_ce: 0.010438
[04:39:26.483] iteration 9030 : loss : 0.210378, loss_ce: 0.024192
[04:39:30.590] iteration 9040 : loss : 0.194689, loss_ce: 0.018932
[04:39:34.709] iteration 9050 : loss : 0.288369, loss_ce: 0.006073
[04:39:38.815] iteration 9060 : loss : 0.197886, loss_ce: 0.007578
[04:39:42.932] iteration 9070 : loss : 0.206185, loss_ce: 0.004923
[04:39:47.036] iteration 9080 : loss : 0.188605, loss_ce: 0.006696
[04:39:51.150] iteration 9090 : loss : 0.090462, loss_ce: 0.014642
[04:39:55.258] iteration 9100 : loss : 0.247752, loss_ce: 0.009750
[04:39:59.375] iteration 9110 : loss : 0.276335, loss_ce: 0.007548
[04:41:50.553] iteration 9120 : loss : 0.331228, loss_ce: 0.014982
[04:41:54.660] iteration 9130 : loss : 0.466856, loss_ce: 0.052045
[04:41:58.753] iteration 9140 : loss : 0.458770, loss_ce: 0.100388
[04:42:02.856] iteration 9150 : loss : 0.476737, loss_ce: 0.120151
[04:42:06.946] iteration 9160 : loss : 0.268992, loss_ce: 0.049281
[04:42:11.053] iteration 9170 : loss : 0.372948, loss_ce: 0.040328
[04:42:15.147] iteration 9180 : loss : 0.318860, loss_ce: 0.018181
[04:42:19.259] iteration 9190 : loss : 0.243201, loss_ce: 0.018143
[04:42:23.360] iteration 9200 : loss : 0.257337, loss_ce: 0.024637
[04:42:27.473] iteration 9210 : loss : 0.086978, loss_ce: 0.016489
[04:42:31.576] iteration 9220 : loss : 0.216590, loss_ce: 0.021615
[04:42:35.692] iteration 9230 : loss : 0.246482, loss_ce: 0.017647
[04:42:39.794] iteration 9240 : loss : 0.280720, loss_ce: 0.022525
[04:42:43.907] iteration 9250 : loss : 0.323653, loss_ce: 0.002877
[04:42:48.009] iteration 9260 : loss : 0.317136, loss_ce: 0.016688
[04:42:52.121] iteration 9270 : loss : 0.185828, loss_ce: 0.004425
[04:42:56.227] iteration 9280 : loss : 0.329380, loss_ce: 0.014068
[04:43:00.340] iteration 9290 : loss : 0.233394, loss_ce: 0.027752
[04:43:04.446] iteration 9300 : loss : 0.244967, loss_ce: 0.016376
[04:43:08.560] iteration 9310 : loss : 0.125692, loss_ce: 0.033305
[04:43:12.664] iteration 9320 : loss : 0.337973, loss_ce: 0.011987
[04:43:16.774] iteration 9330 : loss : 0.239018, loss_ce: 0.016892
[04:43:20.876] iteration 9340 : loss : 0.129328, loss_ce: 0.009340
[04:43:24.993] iteration 9350 : loss : 0.315108, loss_ce: 0.005286
[04:43:29.098] iteration 9360 : loss : 0.054571, loss_ce: 0.007934
[04:43:33.213] iteration 9370 : loss : 0.259055, loss_ce: 0.015554
[04:43:37.217] iteration 9380 : loss : 0.317309, loss_ce: 0.010927
[04:45:15.898] iteration 9390 : loss : 0.341189, loss_ce: 0.012363
[04:45:19.986] iteration 9400 : loss : 0.319378, loss_ce: 0.014065
[04:45:24.086] iteration 9410 : loss : 0.176124, loss_ce: 0.010004
[04:45:28.179] iteration 9420 : loss : 0.240837, loss_ce: 0.020085
[04:45:32.280] iteration 9430 : loss : 0.196000, loss_ce: 0.029855
[04:45:36.378] iteration 9440 : loss : 0.178829, loss_ce: 0.025593
[04:45:40.486] iteration 9450 : loss : 0.339352, loss_ce: 0.025874
[04:45:44.583] iteration 9460 : loss : 0.170671, loss_ce: 0.029461
[04:45:48.692] iteration 9470 : loss : 0.268037, loss_ce: 0.017946
[04:45:52.793] iteration 9480 : loss : 0.260981, loss_ce: 0.017140
[04:45:56.904] iteration 9490 : loss : 0.238055, loss_ce: 0.011560
[04:46:01.007] iteration 9500 : loss : 0.244599, loss_ce: 0.012965
[04:46:05.122] iteration 9510 : loss : 0.215110, loss_ce: 0.016263
[04:46:09.224] iteration 9520 : loss : 0.164854, loss_ce: 0.026762
[04:46:13.336] iteration 9530 : loss : 0.226081, loss_ce: 0.013185
[04:46:17.439] iteration 9540 : loss : 0.279559, loss_ce: 0.011940
[04:46:21.552] iteration 9550 : loss : 0.099541, loss_ce: 0.008884
[04:46:25.656] iteration 9560 : loss : 0.405269, loss_ce: 0.046814
[04:46:29.768] iteration 9570 : loss : 0.320413, loss_ce: 0.010641
[04:46:33.871] iteration 9580 : loss : 0.143709, loss_ce: 0.016121
[04:46:37.989] iteration 9590 : loss : 0.280353, loss_ce: 0.019709
[04:46:42.091] iteration 9600 : loss : 0.335125, loss_ce: 0.042071
[04:46:46.202] iteration 9610 : loss : 0.073180, loss_ce: 0.009203
[04:46:50.304] iteration 9620 : loss : 0.233479, loss_ce: 0.014738
[04:46:54.417] iteration 9630 : loss : 0.142819, loss_ce: 0.011082
[04:46:58.522] iteration 9640 : loss : 0.210425, loss_ce: 0.008685
[04:48:36.772] iteration 9650 : loss : 0.328990, loss_ce: 0.059433
[04:48:40.858] iteration 9660 : loss : 0.229564, loss_ce: 0.024757
[04:48:44.955] iteration 9670 : loss : 0.326887, loss_ce: 0.018837
[04:48:49.048] iteration 9680 : loss : 0.236663, loss_ce: 0.020159
[04:48:53.151] iteration 9690 : loss : 0.227049, loss_ce: 0.019456
[04:48:57.247] iteration 9700 : loss : 0.238865, loss_ce: 0.026321
[04:49:01.354] iteration 9710 : loss : 0.269630, loss_ce: 0.015958
[04:49:05.453] iteration 9720 : loss : 0.292957, loss_ce: 0.017347
[04:49:09.565] iteration 9730 : loss : 0.325243, loss_ce: 0.006546
[04:49:13.663] iteration 9740 : loss : 0.265809, loss_ce: 0.025790
[04:49:17.774] iteration 9750 : loss : 0.159336, loss_ce: 0.015387
[04:49:21.875] iteration 9760 : loss : 0.240071, loss_ce: 0.012842
[04:49:25.988] iteration 9770 : loss : 0.253106, loss_ce: 0.016674
[04:49:30.089] iteration 9780 : loss : 0.276851, loss_ce: 0.030020
[04:49:34.202] iteration 9790 : loss : 0.244887, loss_ce: 0.031833
[04:49:38.302] iteration 9800 : loss : 0.103724, loss_ce: 0.011674
[04:49:42.412] iteration 9810 : loss : 0.274052, loss_ce: 0.004751
[04:49:46.515] iteration 9820 : loss : 0.220305, loss_ce: 0.006984
[04:49:50.626] iteration 9830 : loss : 0.243263, loss_ce: 0.025049
[04:49:54.727] iteration 9840 : loss : 0.227523, loss_ce: 0.015975
[04:49:58.838] iteration 9850 : loss : 0.324615, loss_ce: 0.008397
[04:50:02.940] iteration 9860 : loss : 0.206154, loss_ce: 0.013687
[04:50:07.051] iteration 9870 : loss : 0.326193, loss_ce: 0.011135
[04:50:11.155] iteration 9880 : loss : 0.190117, loss_ce: 0.008770
[04:50:15.269] iteration 9890 : loss : 0.232470, loss_ce: 0.022282
[04:50:19.371] iteration 9900 : loss : 0.306238, loss_ce: 0.015565
[04:50:23.481] iteration 9910 : loss : 0.306348, loss_ce: 0.006156
[04:52:12.301] iteration 9920 : loss : 0.232133, loss_ce: 0.010469
[04:52:16.399] iteration 9930 : loss : 0.198329, loss_ce: 0.018466
[04:52:20.487] iteration 9940 : loss : 0.223451, loss_ce: 0.072433
[04:52:24.592] iteration 9950 : loss : 0.181350, loss_ce: 0.015298
[04:52:28.690] iteration 9960 : loss : 0.167497, loss_ce: 0.005347
[04:52:32.800] iteration 9970 : loss : 0.092149, loss_ce: 0.020712
[04:52:36.895] iteration 9980 : loss : 0.276101, loss_ce: 0.005162
[04:52:41.002] iteration 9990 : loss : 0.275456, loss_ce: 0.017311
[04:52:45.097] iteration 10000 : loss : 0.331518, loss_ce: 0.010490
[04:52:49.203] iteration 10010 : loss : 0.138668, loss_ce: 0.016395
[04:52:53.302] iteration 10020 : loss : 0.198551, loss_ce: 0.010903
[04:52:57.411] iteration 10030 : loss : 0.216381, loss_ce: 0.008743
[04:53:01.513] iteration 10040 : loss : 0.258677, loss_ce: 0.012436
[04:53:05.620] iteration 10050 : loss : 0.220451, loss_ce: 0.007602
[04:53:09.718] iteration 10060 : loss : 0.314651, loss_ce: 0.011846
[04:53:13.828] iteration 10070 : loss : 0.243034, loss_ce: 0.018867
[04:53:17.929] iteration 10080 : loss : 0.198482, loss_ce: 0.014107
[04:53:22.042] iteration 10090 : loss : 0.030111, loss_ce: 0.005733
[04:53:26.143] iteration 10100 : loss : 0.261959, loss_ce: 0.019893
[04:53:30.257] iteration 10110 : loss : 0.248129, loss_ce: 0.009592
[04:53:34.360] iteration 10120 : loss : 0.314065, loss_ce: 0.008142
[04:53:38.482] iteration 10130 : loss : 0.047884, loss_ce: 0.007215
[04:53:42.584] iteration 10140 : loss : 0.202043, loss_ce: 0.010191
[04:53:46.696] iteration 10150 : loss : 0.150077, loss_ce: 0.008504
[04:53:50.799] iteration 10160 : loss : 0.316373, loss_ce: 0.005347
[04:53:54.913] iteration 10170 : loss : 0.210627, loss_ce: 0.005915
[04:53:59.016] iteration 10180 : loss : 0.188769, loss_ce: 0.005479
[04:55:37.275] iteration 10190 : loss : 0.249031, loss_ce: 0.033602
[04:55:41.366] iteration 10200 : loss : 0.371851, loss_ce: 0.053088
[04:55:45.467] iteration 10210 : loss : 0.093542, loss_ce: 0.019879
[04:55:49.559] iteration 10220 : loss : 0.316300, loss_ce: 0.011669
[04:55:53.659] iteration 10230 : loss : 0.333123, loss_ce: 0.026823
[04:55:57.754] iteration 10240 : loss : 0.130316, loss_ce: 0.011183
[04:56:01.862] iteration 10250 : loss : 0.310752, loss_ce: 0.008208
[04:56:05.961] iteration 10260 : loss : 0.219445, loss_ce: 0.019836
[04:56:10.070] iteration 10270 : loss : 0.324753, loss_ce: 0.017207
[04:56:14.170] iteration 10280 : loss : 0.048717, loss_ce: 0.012208
[04:56:18.282] iteration 10290 : loss : 0.320199, loss_ce: 0.022391
[04:56:22.382] iteration 10300 : loss : 0.337794, loss_ce: 0.021850
[04:56:26.495] iteration 10310 : loss : 0.134396, loss_ce: 0.022908
[04:56:30.595] iteration 10320 : loss : 0.068602, loss_ce: 0.014917
[04:56:34.709] iteration 10330 : loss : 0.067765, loss_ce: 0.010215
[04:56:38.810] iteration 10340 : loss : 0.050104, loss_ce: 0.008246
[04:56:42.921] iteration 10350 : loss : 0.253895, loss_ce: 0.011511
[04:56:47.026] iteration 10360 : loss : 0.295786, loss_ce: 0.024402
[04:56:51.136] iteration 10370 : loss : 0.160591, loss_ce: 0.003273
[04:56:55.238] iteration 10380 : loss : 0.080087, loss_ce: 0.008609
[04:56:59.349] iteration 10390 : loss : 0.278115, loss_ce: 0.025813
[04:57:03.452] iteration 10400 : loss : 0.054178, loss_ce: 0.006307
[04:57:07.565] iteration 10410 : loss : 0.024880, loss_ce: 0.004663
[04:57:11.670] iteration 10420 : loss : 0.227415, loss_ce: 0.020283
[04:57:15.783] iteration 10430 : loss : 0.323507, loss_ce: 0.009597
[04:57:19.885] iteration 10440 : loss : 0.256590, loss_ce: 0.010447
[04:57:23.993] iteration 10450 : loss : 0.203669, loss_ce: 0.006727
[04:59:02.418] iteration 10460 : loss : 0.258538, loss_ce: 0.013149
[04:59:06.517] iteration 10470 : loss : 0.245210, loss_ce: 0.017894
[04:59:10.608] iteration 10480 : loss : 0.263796, loss_ce: 0.011365
[04:59:14.712] iteration 10490 : loss : 0.253968, loss_ce: 0.030124
[04:59:18.806] iteration 10500 : loss : 0.185163, loss_ce: 0.025243
[04:59:22.918] iteration 10510 : loss : 0.107821, loss_ce: 0.021080
[04:59:27.015] iteration 10520 : loss : 0.236598, loss_ce: 0.020578
[04:59:31.126] iteration 10530 : loss : 0.260595, loss_ce: 0.009199
[04:59:35.228] iteration 10540 : loss : 0.228364, loss_ce: 0.010840
[04:59:39.335] iteration 10550 : loss : 0.313725, loss_ce: 0.007975
[04:59:43.437] iteration 10560 : loss : 0.118697, loss_ce: 0.015433
[04:59:47.550] iteration 10570 : loss : 0.335222, loss_ce: 0.014525
[04:59:51.650] iteration 10580 : loss : 0.261970, loss_ce: 0.013325
[04:59:55.764] iteration 10590 : loss : 0.242585, loss_ce: 0.003884
[04:59:59.866] iteration 10600 : loss : 0.239664, loss_ce: 0.006183
[05:00:03.982] iteration 10610 : loss : 0.234022, loss_ce: 0.030314
[05:00:08.083] iteration 10620 : loss : 0.064612, loss_ce: 0.010240
[05:00:12.199] iteration 10630 : loss : 0.298796, loss_ce: 0.006204
[05:00:16.300] iteration 10640 : loss : 0.130602, loss_ce: 0.017039
[05:00:20.414] iteration 10650 : loss : 0.231250, loss_ce: 0.016173
[05:00:24.519] iteration 10660 : loss : 0.220274, loss_ce: 0.008833
[05:00:28.636] iteration 10670 : loss : 0.249797, loss_ce: 0.016214
[05:00:32.741] iteration 10680 : loss : 0.187591, loss_ce: 0.008152
[05:00:36.856] iteration 10690 : loss : 0.126492, loss_ce: 0.008095
[05:00:40.959] iteration 10700 : loss : 0.189749, loss_ce: 0.006908
[05:00:45.078] iteration 10710 : loss : 0.280389, loss_ce: 0.032964
[05:00:49.083] iteration 10720 : loss : 0.246796, loss_ce: 0.023349
[05:02:24.110] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_39.pth
[05:02:37.982] iteration 10730 : loss : 0.326378, loss_ce: 0.011354
[05:02:42.071] iteration 10740 : loss : 0.266033, loss_ce: 0.023501
[05:02:46.172] iteration 10750 : loss : 0.313206, loss_ce: 0.031752
[05:02:50.266] iteration 10760 : loss : 0.304133, loss_ce: 0.091511
[05:02:54.373] iteration 10770 : loss : 0.314980, loss_ce: 0.034814
[05:02:58.470] iteration 10780 : loss : 0.216777, loss_ce: 0.008267
[05:03:02.578] iteration 10790 : loss : 0.210712, loss_ce: 0.012642
[05:03:06.675] iteration 10800 : loss : 0.275170, loss_ce: 0.020089
[05:03:10.785] iteration 10810 : loss : 0.196839, loss_ce: 0.007412
[05:03:14.884] iteration 10820 : loss : 0.255417, loss_ce: 0.011441
[05:03:18.995] iteration 10830 : loss : 0.284718, loss_ce: 0.010823
[05:03:23.097] iteration 10840 : loss : 0.321372, loss_ce: 0.006647
[05:03:27.209] iteration 10850 : loss : 0.331073, loss_ce: 0.016645
[05:03:31.309] iteration 10860 : loss : 0.191686, loss_ce: 0.012250
[05:03:35.425] iteration 10870 : loss : 0.205493, loss_ce: 0.008821
[05:03:39.528] iteration 10880 : loss : 0.057414, loss_ce: 0.004490
[05:03:43.637] iteration 10890 : loss : 0.222680, loss_ce: 0.018105
[05:03:47.740] iteration 10900 : loss : 0.311562, loss_ce: 0.007643
[05:03:51.854] iteration 10910 : loss : 0.220244, loss_ce: 0.009082
[05:03:55.957] iteration 10920 : loss : 0.200954, loss_ce: 0.007759
[05:04:00.067] iteration 10930 : loss : 0.173150, loss_ce: 0.002888
[05:04:04.173] iteration 10940 : loss : 0.210675, loss_ce: 0.025315
[05:04:08.284] iteration 10950 : loss : 0.244721, loss_ce: 0.018580
[05:04:12.387] iteration 10960 : loss : 0.207590, loss_ce: 0.016376
[05:04:16.501] iteration 10970 : loss : 0.242534, loss_ce: 0.009041
[05:04:20.604] iteration 10980 : loss : 0.273963, loss_ce: 0.013678
[05:05:58.845] iteration 10990 : loss : 0.289188, loss_ce: 0.028526
[05:06:02.934] iteration 11000 : loss : 0.356910, loss_ce: 0.047474
[05:06:07.033] iteration 11010 : loss : 0.341132, loss_ce: 0.044802
[05:06:11.126] iteration 11020 : loss : 0.336619, loss_ce: 0.020709
[05:06:15.232] iteration 11030 : loss : 0.333408, loss_ce: 0.032344
[05:06:19.331] iteration 11040 : loss : 0.336233, loss_ce: 0.049211
[05:06:23.439] iteration 11050 : loss : 0.341380, loss_ce: 0.041974
[05:06:27.542] iteration 11060 : loss : 0.198029, loss_ce: 0.065935
[05:06:31.654] iteration 11070 : loss : 0.190593, loss_ce: 0.025845
[05:06:35.753] iteration 11080 : loss : 0.304976, loss_ce: 0.009633
[05:06:39.863] iteration 11090 : loss : 0.278827, loss_ce: 0.020938
[05:06:43.964] iteration 11100 : loss : 0.028622, loss_ce: 0.005634
[05:06:48.075] iteration 11110 : loss : 0.214491, loss_ce: 0.013568
[05:06:52.178] iteration 11120 : loss : 0.195436, loss_ce: 0.006035
[05:06:56.287] iteration 11130 : loss : 0.157465, loss_ce: 0.012152
[05:07:00.390] iteration 11140 : loss : 0.196216, loss_ce: 0.008698
[05:07:04.503] iteration 11150 : loss : 0.303734, loss_ce: 0.010974
[05:07:08.604] iteration 11160 : loss : 0.270013, loss_ce: 0.016713
[05:07:12.713] iteration 11170 : loss : 0.099200, loss_ce: 0.010700
[05:07:16.813] iteration 11180 : loss : 0.225225, loss_ce: 0.009738
[05:07:20.925] iteration 11190 : loss : 0.071733, loss_ce: 0.014393
[05:07:25.027] iteration 11200 : loss : 0.247866, loss_ce: 0.013446
[05:07:29.139] iteration 11210 : loss : 0.334430, loss_ce: 0.039610
[05:07:33.243] iteration 11220 : loss : 0.215199, loss_ce: 0.011074
[05:07:37.357] iteration 11230 : loss : 0.129824, loss_ce: 0.005981
[05:07:41.459] iteration 11240 : loss : 0.308870, loss_ce: 0.019274
[05:07:45.570] iteration 11250 : loss : 0.325710, loss_ce: 0.010436
[05:09:23.999] iteration 11260 : loss : 0.344592, loss_ce: 0.029782
[05:09:28.098] iteration 11270 : loss : 0.254357, loss_ce: 0.033768
[05:09:32.192] iteration 11280 : loss : 0.226989, loss_ce: 0.039864
[05:09:36.302] iteration 11290 : loss : 0.312567, loss_ce: 0.004530
[05:09:40.398] iteration 11300 : loss : 0.178429, loss_ce: 0.008173
[05:09:44.504] iteration 11310 : loss : 0.037431, loss_ce: 0.004862
[05:09:48.601] iteration 11320 : loss : 0.230071, loss_ce: 0.020033
[05:09:52.712] iteration 11330 : loss : 0.267888, loss_ce: 0.025594
[05:09:56.813] iteration 11340 : loss : 0.326580, loss_ce: 0.012769
[05:10:00.925] iteration 11350 : loss : 0.057995, loss_ce: 0.007213
[05:10:05.029] iteration 11360 : loss : 0.191830, loss_ce: 0.009040
[05:10:09.142] iteration 11370 : loss : 0.251271, loss_ce: 0.015764
[05:10:13.247] iteration 11380 : loss : 0.319222, loss_ce: 0.006424
[05:10:17.363] iteration 11390 : loss : 0.106215, loss_ce: 0.010221
[05:10:21.470] iteration 11400 : loss : 0.174472, loss_ce: 0.008194
[05:10:25.588] iteration 11410 : loss : 0.335375, loss_ce: 0.016362
[05:10:29.689] iteration 11420 : loss : 0.207299, loss_ce: 0.007173
[05:10:33.805] iteration 11430 : loss : 0.119998, loss_ce: 0.005695
[05:10:37.906] iteration 11440 : loss : 0.044090, loss_ce: 0.007418
[05:10:42.017] iteration 11450 : loss : 0.206001, loss_ce: 0.017375
[05:10:46.119] iteration 11460 : loss : 0.184921, loss_ce: 0.006367
[05:10:50.233] iteration 11470 : loss : 0.200761, loss_ce: 0.008774
[05:10:54.335] iteration 11480 : loss : 0.231087, loss_ce: 0.012672
[05:10:58.450] iteration 11490 : loss : 0.074755, loss_ce: 0.009755
[05:11:02.553] iteration 11500 : loss : 0.237359, loss_ce: 0.012507
[05:11:06.667] iteration 11510 : loss : 0.322275, loss_ce: 0.008848
[05:11:10.770] iteration 11520 : loss : 0.196662, loss_ce: 0.005633
[05:12:59.550] iteration 11530 : loss : 0.326165, loss_ce: 0.008864
[05:13:03.638] iteration 11540 : loss : 0.322369, loss_ce: 0.015270
[05:13:07.737] iteration 11550 : loss : 0.265260, loss_ce: 0.024376
[05:13:11.827] iteration 11560 : loss : 0.315897, loss_ce: 0.007713
[05:13:15.933] iteration 11570 : loss : 0.083759, loss_ce: 0.021979
[05:13:20.031] iteration 11580 : loss : 0.281208, loss_ce: 0.015881
[05:13:24.140] iteration 11590 : loss : 0.293389, loss_ce: 0.008155
[05:13:28.239] iteration 11600 : loss : 0.203648, loss_ce: 0.017651
[05:13:32.346] iteration 11610 : loss : 0.217236, loss_ce: 0.022297
[05:13:36.446] iteration 11620 : loss : 0.077516, loss_ce: 0.007664
[05:13:40.558] iteration 11630 : loss : 0.233401, loss_ce: 0.014222
[05:13:44.657] iteration 11640 : loss : 0.285602, loss_ce: 0.024863
[05:13:48.764] iteration 11650 : loss : 0.326092, loss_ce: 0.027328
[05:13:52.864] iteration 11660 : loss : 0.216600, loss_ce: 0.008151
[05:13:56.975] iteration 11670 : loss : 0.260168, loss_ce: 0.010319
[05:14:01.075] iteration 11680 : loss : 0.334155, loss_ce: 0.013446
[05:14:05.186] iteration 11690 : loss : 0.242492, loss_ce: 0.017817
[05:14:09.288] iteration 11700 : loss : 0.244153, loss_ce: 0.013883
[05:14:13.398] iteration 11710 : loss : 0.117523, loss_ce: 0.003095
[05:14:17.498] iteration 11720 : loss : 0.196498, loss_ce: 0.012316
[05:14:21.609] iteration 11730 : loss : 0.217764, loss_ce: 0.010114
[05:14:25.714] iteration 11740 : loss : 0.252266, loss_ce: 0.012396
[05:14:29.824] iteration 11750 : loss : 0.186379, loss_ce: 0.012628
[05:14:33.927] iteration 11760 : loss : 0.059965, loss_ce: 0.007180
[05:14:38.039] iteration 11770 : loss : 0.111692, loss_ce: 0.021342
[05:14:42.141] iteration 11780 : loss : 0.172353, loss_ce: 0.003853
[05:14:46.254] iteration 11790 : loss : 0.204233, loss_ce: 0.008823
[05:16:25.711] iteration 11800 : loss : 0.286104, loss_ce: 0.014756
[05:16:29.810] iteration 11810 : loss : 0.261639, loss_ce: 0.020138
[05:16:33.902] iteration 11820 : loss : 0.326293, loss_ce: 0.019580
[05:16:38.004] iteration 11830 : loss : 0.313355, loss_ce: 0.012595
[05:16:42.097] iteration 11840 : loss : 0.224032, loss_ce: 0.011195
[05:16:46.201] iteration 11850 : loss : 0.340361, loss_ce: 0.023496
[05:16:50.298] iteration 11860 : loss : 0.242571, loss_ce: 0.023844
[05:16:54.410] iteration 11870 : loss : 0.272465, loss_ce: 0.013473
[05:16:58.513] iteration 11880 : loss : 0.175422, loss_ce: 0.011764
[05:17:02.629] iteration 11890 : loss : 0.320579, loss_ce: 0.007231
[05:17:06.732] iteration 11900 : loss : 0.249791, loss_ce: 0.019132
[05:17:10.845] iteration 11910 : loss : 0.220735, loss_ce: 0.009336
[05:17:14.948] iteration 11920 : loss : 0.220521, loss_ce: 0.017828
[05:17:19.059] iteration 11930 : loss : 0.198677, loss_ce: 0.008651
[05:17:23.163] iteration 11940 : loss : 0.235326, loss_ce: 0.017264
[05:17:27.279] iteration 11950 : loss : 0.273423, loss_ce: 0.012068
[05:17:31.383] iteration 11960 : loss : 0.324839, loss_ce: 0.020076
[05:17:35.501] iteration 11970 : loss : 0.202210, loss_ce: 0.014896
[05:17:39.603] iteration 11980 : loss : 0.217460, loss_ce: 0.015523
[05:17:43.715] iteration 11990 : loss : 0.280680, loss_ce: 0.055374
[05:17:47.820] iteration 12000 : loss : 0.039535, loss_ce: 0.009499
[05:17:51.933] iteration 12010 : loss : 0.237900, loss_ce: 0.008720
[05:17:56.040] iteration 12020 : loss : 0.246628, loss_ce: 0.012828
[05:18:00.154] iteration 12030 : loss : 0.270373, loss_ce: 0.010975
[05:18:04.259] iteration 12040 : loss : 0.212546, loss_ce: 0.010924
[05:18:08.370] iteration 12050 : loss : 0.270226, loss_ce: 0.020676
[05:18:12.375] iteration 12060 : loss : 0.241374, loss_ce: 0.013292
[05:19:50.869] iteration 12070 : loss : 0.378875, loss_ce: 0.055711
[05:19:54.960] iteration 12080 : loss : 0.266624, loss_ce: 0.028036
[05:19:59.062] iteration 12090 : loss : 0.325828, loss_ce: 0.006796
[05:20:03.158] iteration 12100 : loss : 0.310456, loss_ce: 0.055200
[05:20:07.266] iteration 12110 : loss : 0.240672, loss_ce: 0.022220
[05:20:11.361] iteration 12120 : loss : 0.318674, loss_ce: 0.024983
[05:20:15.470] iteration 12130 : loss : 0.208162, loss_ce: 0.016884
[05:20:19.567] iteration 12140 : loss : 0.314096, loss_ce: 0.012597
[05:20:23.675] iteration 12150 : loss : 0.219772, loss_ce: 0.014010
[05:20:27.777] iteration 12160 : loss : 0.324221, loss_ce: 0.007600
[05:20:31.883] iteration 12170 : loss : 0.244262, loss_ce: 0.015712
[05:20:35.982] iteration 12180 : loss : 0.313484, loss_ce: 0.018328
[05:20:40.094] iteration 12190 : loss : 0.248535, loss_ce: 0.020689
[05:20:44.191] iteration 12200 : loss : 0.187136, loss_ce: 0.019559
[05:20:48.301] iteration 12210 : loss : 0.066294, loss_ce: 0.007375
[05:20:52.401] iteration 12220 : loss : 0.215307, loss_ce: 0.013568
[05:20:56.510] iteration 12230 : loss : 0.227855, loss_ce: 0.005918
[05:21:00.611] iteration 12240 : loss : 0.225892, loss_ce: 0.021666
[05:21:04.721] iteration 12250 : loss : 0.052893, loss_ce: 0.007716
[05:21:08.821] iteration 12260 : loss : 0.200531, loss_ce: 0.011152
[05:21:12.931] iteration 12270 : loss : 0.324749, loss_ce: 0.016161
[05:21:17.033] iteration 12280 : loss : 0.275788, loss_ce: 0.011564
[05:21:21.142] iteration 12290 : loss : 0.202266, loss_ce: 0.011852
[05:21:25.246] iteration 12300 : loss : 0.226602, loss_ce: 0.010419
[05:21:29.354] iteration 12310 : loss : 0.278743, loss_ce: 0.009424
[05:21:33.457] iteration 12320 : loss : 0.329115, loss_ce: 0.012566
[05:23:23.145] iteration 12330 : loss : 0.389370, loss_ce: 0.066548
[05:23:27.233] iteration 12340 : loss : 0.306907, loss_ce: 0.036804
[05:23:31.329] iteration 12350 : loss : 0.322978, loss_ce: 0.029966
[05:23:35.423] iteration 12360 : loss : 0.232137, loss_ce: 0.020069
[05:23:39.529] iteration 12370 : loss : 0.212313, loss_ce: 0.011955
[05:23:43.624] iteration 12380 : loss : 0.263228, loss_ce: 0.019297
[05:23:47.731] iteration 12390 : loss : 0.337722, loss_ce: 0.019585
[05:23:51.828] iteration 12400 : loss : 0.207499, loss_ce: 0.004783
[05:23:55.934] iteration 12410 : loss : 0.207120, loss_ce: 0.011324
[05:24:00.036] iteration 12420 : loss : 0.321722, loss_ce: 0.020583
[05:24:04.144] iteration 12430 : loss : 0.206851, loss_ce: 0.018454
[05:24:08.244] iteration 12440 : loss : 0.210385, loss_ce: 0.011070
[05:24:12.355] iteration 12450 : loss : 0.271139, loss_ce: 0.029512
[05:24:16.456] iteration 12460 : loss : 0.245701, loss_ce: 0.009051
[05:24:20.566] iteration 12470 : loss : 0.210803, loss_ce: 0.009255
[05:24:24.668] iteration 12480 : loss : 0.276525, loss_ce: 0.017651
[05:24:28.784] iteration 12490 : loss : 0.301409, loss_ce: 0.016830
[05:24:32.887] iteration 12500 : loss : 0.281686, loss_ce: 0.028211
[05:24:36.999] iteration 12510 : loss : 0.326423, loss_ce: 0.016150
[05:24:41.100] iteration 12520 : loss : 0.243583, loss_ce: 0.017251
[05:24:45.212] iteration 12530 : loss : 0.290929, loss_ce: 0.030529
[05:24:49.315] iteration 12540 : loss : 0.323247, loss_ce: 0.010354
[05:24:53.427] iteration 12550 : loss : 0.236165, loss_ce: 0.011815
[05:24:57.526] iteration 12560 : loss : 0.217322, loss_ce: 0.016158
[05:25:01.638] iteration 12570 : loss : 0.325988, loss_ce: 0.016797
[05:25:05.740] iteration 12580 : loss : 0.206528, loss_ce: 0.006873
[05:25:09.851] iteration 12590 : loss : 0.304094, loss_ce: 0.020124
[05:26:48.293] iteration 12600 : loss : 0.428445, loss_ce: 0.052787
[05:26:52.393] iteration 12610 : loss : 0.324342, loss_ce: 0.009681
[05:26:56.480] iteration 12620 : loss : 0.248919, loss_ce: 0.023978
[05:27:00.581] iteration 12630 : loss : 0.304962, loss_ce: 0.041718
[05:27:04.678] iteration 12640 : loss : 0.236725, loss_ce: 0.023846
[05:27:08.787] iteration 12650 : loss : 0.224766, loss_ce: 0.017894
[05:27:12.886] iteration 12660 : loss : 0.246857, loss_ce: 0.025464
[05:27:16.995] iteration 12670 : loss : 0.252482, loss_ce: 0.032590
[05:27:21.093] iteration 12680 : loss : 0.319761, loss_ce: 0.007667
[05:27:25.203] iteration 12690 : loss : 0.235826, loss_ce: 0.016432
[05:27:29.304] iteration 12700 : loss : 0.221355, loss_ce: 0.014862
[05:27:33.418] iteration 12710 : loss : 0.196067, loss_ce: 0.016775
[05:27:37.520] iteration 12720 : loss : 0.240953, loss_ce: 0.018366
[05:27:41.629] iteration 12730 : loss : 0.234753, loss_ce: 0.021837
[05:27:45.735] iteration 12740 : loss : 0.333266, loss_ce: 0.012803
[05:27:49.847] iteration 12750 : loss : 0.314941, loss_ce: 0.005109
[05:27:53.948] iteration 12760 : loss : 0.215939, loss_ce: 0.024773
[05:27:58.060] iteration 12770 : loss : 0.258191, loss_ce: 0.025597
[05:28:02.162] iteration 12780 : loss : 0.213277, loss_ce: 0.058317
[05:28:06.275] iteration 12790 : loss : 0.264622, loss_ce: 0.023681
[05:28:10.381] iteration 12800 : loss : 0.259223, loss_ce: 0.012292
[05:28:14.489] iteration 12810 : loss : 0.145751, loss_ce: 0.009528
[05:28:18.590] iteration 12820 : loss : 0.283503, loss_ce: 0.020637
[05:28:22.705] iteration 12830 : loss : 0.302672, loss_ce: 0.028204
[05:28:26.809] iteration 12840 : loss : 0.192339, loss_ce: 0.012230
[05:28:30.922] iteration 12850 : loss : 0.234192, loss_ce: 0.006971
[05:28:35.023] iteration 12860 : loss : 0.193698, loss_ce: 0.013179
[05:30:13.440] iteration 12870 : loss : 0.430150, loss_ce: 0.096439
[05:30:17.528] iteration 12880 : loss : 0.415981, loss_ce: 0.068853
[05:30:21.626] iteration 12890 : loss : 0.352185, loss_ce: 0.019735
[05:30:25.723] iteration 12900 : loss : 0.354384, loss_ce: 0.041203
[05:30:29.829] iteration 12910 : loss : 0.352294, loss_ce: 0.062540
[05:30:33.927] iteration 12920 : loss : 0.317447, loss_ce: 0.009896
[05:30:38.034] iteration 12930 : loss : 0.279067, loss_ce: 0.037595
[05:30:42.133] iteration 12940 : loss : 0.335178, loss_ce: 0.017259
[05:30:46.241] iteration 12950 : loss : 0.271933, loss_ce: 0.026085
[05:30:50.340] iteration 12960 : loss : 0.308646, loss_ce: 0.034994
[05:30:54.450] iteration 12970 : loss : 0.235889, loss_ce: 0.010186
[05:30:58.549] iteration 12980 : loss : 0.323318, loss_ce: 0.030624
[05:31:02.661] iteration 12990 : loss : 0.308444, loss_ce: 0.024987
[05:31:06.761] iteration 13000 : loss : 0.299122, loss_ce: 0.018209
[05:31:10.872] iteration 13010 : loss : 0.248393, loss_ce: 0.022797
[05:31:14.973] iteration 13020 : loss : 0.234546, loss_ce: 0.019408
[05:31:19.085] iteration 13030 : loss : 0.273565, loss_ce: 0.019688
[05:31:23.185] iteration 13040 : loss : 0.337684, loss_ce: 0.030172
[05:31:27.297] iteration 13050 : loss : 0.243651, loss_ce: 0.019376
[05:31:31.401] iteration 13060 : loss : 0.258916, loss_ce: 0.037076
[05:31:35.509] iteration 13070 : loss : 0.243646, loss_ce: 0.015330
[05:31:39.610] iteration 13080 : loss : 0.331808, loss_ce: 0.030783
[05:31:43.721] iteration 13090 : loss : 0.301334, loss_ce: 0.027603
[05:31:47.823] iteration 13100 : loss : 0.204213, loss_ce: 0.017072
[05:31:51.936] iteration 13110 : loss : 0.333552, loss_ce: 0.026296
[05:31:56.036] iteration 13120 : loss : 0.240741, loss_ce: 0.022346
[05:32:00.145] iteration 13130 : loss : 0.346840, loss_ce: 0.008923
[05:33:49.206] iteration 13140 : loss : 0.440036, loss_ce: 0.056319
[05:33:53.304] iteration 13150 : loss : 0.422733, loss_ce: 0.080947
[05:33:57.395] iteration 13160 : loss : 0.403705, loss_ce: 0.028635
[05:34:01.496] iteration 13170 : loss : 0.383532, loss_ce: 0.029829
[05:34:05.589] iteration 13180 : loss : 0.396745, loss_ce: 0.045654
[05:34:09.693] iteration 13190 : loss : 0.361420, loss_ce: 0.039654
[05:34:13.790] iteration 13200 : loss : 0.379936, loss_ce: 0.085610
[05:34:17.895] iteration 13210 : loss : 0.371456, loss_ce: 0.072624
[05:34:21.993] iteration 13220 : loss : 0.360026, loss_ce: 0.028242
[05:34:26.100] iteration 13230 : loss : 0.348591, loss_ce: 0.028938
[05:34:30.203] iteration 13240 : loss : 0.332721, loss_ce: 0.017100
[05:34:34.314] iteration 13250 : loss : 0.355774, loss_ce: 0.017994
[05:34:38.414] iteration 13260 : loss : 0.365722, loss_ce: 0.054939
[05:34:42.527] iteration 13270 : loss : 0.353599, loss_ce: 0.031545
[05:34:46.627] iteration 13280 : loss : 0.342831, loss_ce: 0.076661
[05:34:50.739] iteration 13290 : loss : 0.338216, loss_ce: 0.013151
[05:34:54.839] iteration 13300 : loss : 0.356186, loss_ce: 0.034282
[05:34:58.948] iteration 13310 : loss : 0.338799, loss_ce: 0.026388
[05:35:03.049] iteration 13320 : loss : 0.356039, loss_ce: 0.024156
[05:35:07.163] iteration 13330 : loss : 0.327731, loss_ce: 0.023371
[05:35:11.264] iteration 13340 : loss : 0.327265, loss_ce: 0.005643
[05:35:15.376] iteration 13350 : loss : 0.355196, loss_ce: 0.038914
[05:35:19.478] iteration 13360 : loss : 0.351617, loss_ce: 0.052373
[05:35:23.591] iteration 13370 : loss : 0.391224, loss_ce: 0.085043
[05:35:27.697] iteration 13380 : loss : 0.333383, loss_ce: 0.019551
[05:35:31.809] iteration 13390 : loss : 0.368077, loss_ce: 0.093373
[05:35:35.814] iteration 13400 : loss : 0.348302, loss_ce: 0.039287
[05:37:00.442] save model to ./finetune_tpgm_kits23_continual\finetuned_epoch_49.pth
[05:37:00.655] save final model to ./finetune_tpgm_kits23_continual\finetuned_final.pth
