[14:03:45.514] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual_500iter', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=500, tpgm_exclude=[], gpu_id=1)
[14:03:45.539] Using 8569/95221 samples for finetuning
[14:03:45.540] Using 953/95221 samples for TPGM
[14:03:45.541] Model has 9 total classes, training on 4 classes
[14:03:55.533] 268 iterations per epoch. 13400 max iterations 
[14:04:09.920] iteration 10 : loss : 0.461870, loss_ce: 0.081058
[14:04:13.946] iteration 20 : loss : 0.447126, loss_ce: 0.067454
[14:04:18.006] iteration 30 : loss : 0.454064, loss_ce: 0.061676
[14:04:22.036] iteration 40 : loss : 0.438523, loss_ce: 0.037797
[14:04:26.085] iteration 50 : loss : 0.427871, loss_ce: 0.060777
[14:04:30.128] iteration 60 : loss : 0.289397, loss_ce: 0.043874
[14:04:34.182] iteration 70 : loss : 0.430093, loss_ce: 0.078371
[14:04:38.230] iteration 80 : loss : 0.432416, loss_ce: 0.045691
[14:04:42.288] iteration 90 : loss : 0.438128, loss_ce: 0.059353
[14:04:46.341] iteration 100 : loss : 0.424840, loss_ce: 0.065352
[14:04:50.402] iteration 110 : loss : 0.289580, loss_ce: 0.027083
[14:04:54.451] iteration 120 : loss : 0.420143, loss_ce: 0.054165
[14:04:58.512] iteration 130 : loss : 0.428500, loss_ce: 0.046873
[14:05:02.565] iteration 140 : loss : 0.401049, loss_ce: 0.039351
[14:05:06.625] iteration 150 : loss : 0.411577, loss_ce: 0.030802
[14:05:10.677] iteration 160 : loss : 0.382576, loss_ce: 0.048536
[14:05:14.743] iteration 170 : loss : 0.386560, loss_ce: 0.035044
[14:05:18.801] iteration 180 : loss : 0.363701, loss_ce: 0.024090
[14:05:22.866] iteration 190 : loss : 0.240404, loss_ce: 0.031053
[14:05:26.922] iteration 200 : loss : 0.373939, loss_ce: 0.052010
[14:05:30.987] iteration 210 : loss : 0.388509, loss_ce: 0.043876
[14:05:35.044] iteration 220 : loss : 0.379827, loss_ce: 0.024280
[14:05:39.115] iteration 230 : loss : 0.358815, loss_ce: 0.026818
[14:05:43.173] iteration 240 : loss : 0.366219, loss_ce: 0.032203
[14:05:47.241] iteration 250 : loss : 0.391504, loss_ce: 0.052399
[14:05:51.299] iteration 260 : loss : 0.350837, loss_ce: 0.040769
[14:07:42.893] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual_500iter', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=500, tpgm_exclude=[], gpu_id=1)
[14:07:42.920] Using 8569/95221 samples for finetuning
[14:07:42.920] Using 953/95221 samples for TPGM
[14:07:42.920] Model has 9 total classes, training on 4 classes
[14:07:52.788] 268 iterations per epoch. 13400 max iterations 
[14:08:07.290] iteration 10 : loss : 0.461870, loss_ce: 0.081058
[14:08:11.324] iteration 20 : loss : 0.447126, loss_ce: 0.067454
[14:08:15.382] iteration 30 : loss : 0.454064, loss_ce: 0.061675
[14:08:19.423] iteration 40 : loss : 0.438523, loss_ce: 0.037797
[14:08:23.478] iteration 50 : loss : 0.427871, loss_ce: 0.060777
[14:08:27.522] iteration 60 : loss : 0.289397, loss_ce: 0.043874
[14:08:31.630] iteration 70 : loss : 0.430093, loss_ce: 0.078371
[14:08:35.685] iteration 80 : loss : 0.432416, loss_ce: 0.045691
[14:08:39.745] iteration 90 : loss : 0.438128, loss_ce: 0.059353
[14:08:43.793] iteration 100 : loss : 0.424840, loss_ce: 0.065352
[14:08:47.854] iteration 110 : loss : 0.289580, loss_ce: 0.027083
[14:08:51.906] iteration 120 : loss : 0.420143, loss_ce: 0.054165
[14:08:55.971] iteration 130 : loss : 0.428500, loss_ce: 0.046873
[14:09:00.028] iteration 140 : loss : 0.401049, loss_ce: 0.039351
[14:09:04.094] iteration 150 : loss : 0.411577, loss_ce: 0.030802
[14:09:08.150] iteration 160 : loss : 0.382576, loss_ce: 0.048536
[14:09:12.220] iteration 170 : loss : 0.386560, loss_ce: 0.035044
[14:09:16.275] iteration 180 : loss : 0.363701, loss_ce: 0.024090
[14:09:20.345] iteration 190 : loss : 0.240404, loss_ce: 0.031053
[14:09:24.406] iteration 200 : loss : 0.373939, loss_ce: 0.052010
[14:09:28.474] iteration 210 : loss : 0.388509, loss_ce: 0.043876
[14:09:32.534] iteration 220 : loss : 0.379827, loss_ce: 0.024280
[14:09:36.602] iteration 230 : loss : 0.358815, loss_ce: 0.026818
[14:09:40.664] iteration 240 : loss : 0.366219, loss_ce: 0.032203
[14:09:44.732] iteration 250 : loss : 0.391504, loss_ce: 0.052399
[14:09:48.793] iteration 260 : loss : 0.350837, loss_ce: 0.040769
[14:17:31.932] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual_500iter', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=500, tpgm_exclude=[], gpu_id=1)
[14:17:31.956] Using 8569/95221 samples for finetuning
[14:17:31.956] Using 953/95221 samples for TPGM
[14:17:31.956] Model has 9 total classes, training on 4 classes
[14:17:41.808] 268 iterations per epoch. 13400 max iterations 
[14:17:56.659] iteration 10 : loss : 0.461870, loss_ce: 0.081058
[14:18:00.693] iteration 20 : loss : 0.447126, loss_ce: 0.067454
[14:18:04.756] iteration 30 : loss : 0.454064, loss_ce: 0.061675
[14:18:08.818] iteration 40 : loss : 0.438523, loss_ce: 0.037797
[14:18:12.874] iteration 50 : loss : 0.427871, loss_ce: 0.060777
[14:18:16.927] iteration 60 : loss : 0.289397, loss_ce: 0.043874
[14:18:20.991] iteration 70 : loss : 0.430093, loss_ce: 0.078371
[14:18:25.041] iteration 80 : loss : 0.432416, loss_ce: 0.045691
[14:18:29.104] iteration 90 : loss : 0.438128, loss_ce: 0.059353
[14:18:33.160] iteration 100 : loss : 0.424840, loss_ce: 0.065352
[14:18:37.226] iteration 110 : loss : 0.289580, loss_ce: 0.027083
[14:18:41.282] iteration 120 : loss : 0.420143, loss_ce: 0.054165
[14:18:45.349] iteration 130 : loss : 0.428500, loss_ce: 0.046873
[14:18:49.407] iteration 140 : loss : 0.401049, loss_ce: 0.039351
[14:18:53.472] iteration 150 : loss : 0.411577, loss_ce: 0.030802
[14:18:57.528] iteration 160 : loss : 0.382576, loss_ce: 0.048536
[14:19:01.598] iteration 170 : loss : 0.386560, loss_ce: 0.035044
[14:19:05.659] iteration 180 : loss : 0.363701, loss_ce: 0.024090
[14:19:09.726] iteration 190 : loss : 0.240404, loss_ce: 0.031053
[14:19:13.785] iteration 200 : loss : 0.373939, loss_ce: 0.052010
[14:19:17.853] iteration 210 : loss : 0.388509, loss_ce: 0.043876
[14:19:21.913] iteration 220 : loss : 0.379827, loss_ce: 0.024280
[14:19:25.986] iteration 230 : loss : 0.358815, loss_ce: 0.026818
[14:19:30.053] iteration 240 : loss : 0.366219, loss_ce: 0.032203
[14:19:34.125] iteration 250 : loss : 0.391504, loss_ce: 0.052399
[14:19:38.190] iteration 260 : loss : 0.350837, loss_ce: 0.040769
[14:26:47.751] iteration 270 : loss : 0.436003, loss_ce: 0.052128
[14:26:51.796] iteration 280 : loss : 0.479819, loss_ce: 0.086565
[14:26:55.849] iteration 290 : loss : 0.502037, loss_ce: 0.128396
[14:26:59.893] iteration 300 : loss : 0.460936, loss_ce: 0.072885
[14:27:03.956] iteration 310 : loss : 0.446667, loss_ce: 0.056358
[14:27:08.010] iteration 320 : loss : 0.450827, loss_ce: 0.049819
[14:27:12.068] iteration 330 : loss : 0.449753, loss_ce: 0.060368
[14:27:16.125] iteration 340 : loss : 0.445452, loss_ce: 0.049780
[14:27:20.193] iteration 350 : loss : 0.443438, loss_ce: 0.036185
[14:27:24.246] iteration 360 : loss : 0.450344, loss_ce: 0.070133
[14:27:28.314] iteration 370 : loss : 0.442362, loss_ce: 0.056655
[14:27:32.369] iteration 380 : loss : 0.456191, loss_ce: 0.080339
[14:27:36.441] iteration 390 : loss : 0.301420, loss_ce: 0.070091
[14:27:40.500] iteration 400 : loss : 0.287776, loss_ce: 0.030123
[14:27:44.570] iteration 410 : loss : 0.280881, loss_ce: 0.038001
[14:27:48.632] iteration 420 : loss : 0.440740, loss_ce: 0.054539
[14:27:52.711] iteration 430 : loss : 0.426631, loss_ce: 0.045856
[14:27:56.782] iteration 440 : loss : 0.426094, loss_ce: 0.048203
[14:28:00.870] iteration 450 : loss : 0.422490, loss_ce: 0.054536
[14:28:04.936] iteration 460 : loss : 0.265998, loss_ce: 0.047730
[14:28:09.018] iteration 470 : loss : 0.423755, loss_ce: 0.047633
[14:28:13.089] iteration 480 : loss : 0.420260, loss_ce: 0.033978
[14:28:17.170] iteration 490 : loss : 0.316263, loss_ce: 0.110860
[14:28:21.236] iteration 500 : loss : 0.422102, loss_ce: 0.044112
[14:28:25.327] iteration 510 : loss : 0.436678, loss_ce: 0.045469
[14:28:29.400] iteration 520 : loss : 0.436495, loss_ce: 0.059200
[14:28:33.489] iteration 530 : loss : 0.290972, loss_ce: 0.053557
[14:36:01.992] iteration 540 : loss : 0.490400, loss_ce: 0.151852
[14:36:06.050] iteration 550 : loss : 0.457049, loss_ce: 0.066233
[14:36:10.092] iteration 560 : loss : 0.470169, loss_ce: 0.074938
[14:36:14.149] iteration 570 : loss : 0.433252, loss_ce: 0.052771
[14:36:18.201] iteration 580 : loss : 0.438138, loss_ce: 0.058770
[14:36:22.261] iteration 590 : loss : 0.429063, loss_ce: 0.048599
[14:36:26.317] iteration 600 : loss : 0.281390, loss_ce: 0.028807
[14:36:30.382] iteration 610 : loss : 0.389734, loss_ce: 0.048119
[14:36:34.436] iteration 620 : loss : 0.442679, loss_ce: 0.051127
[14:36:38.501] iteration 630 : loss : 0.416677, loss_ce: 0.054463
[14:36:42.556] iteration 640 : loss : 0.377227, loss_ce: 0.060473
[14:36:46.625] iteration 650 : loss : 0.442189, loss_ce: 0.028361
[14:36:50.685] iteration 660 : loss : 0.393638, loss_ce: 0.022641
[14:36:54.754] iteration 670 : loss : 0.380822, loss_ce: 0.040107
[14:36:58.817] iteration 680 : loss : 0.376579, loss_ce: 0.025534
[14:37:02.890] iteration 690 : loss : 0.306058, loss_ce: 0.031699
[14:37:06.950] iteration 700 : loss : 0.349687, loss_ce: 0.012753
[14:37:11.027] iteration 710 : loss : 0.346890, loss_ce: 0.036849
[14:37:15.088] iteration 720 : loss : 0.182433, loss_ce: 0.006358
[14:37:19.166] iteration 730 : loss : 0.330692, loss_ce: 0.036099
[14:37:23.239] iteration 740 : loss : 0.332252, loss_ce: 0.020915
[14:37:27.318] iteration 750 : loss : 0.324239, loss_ce: 0.014222
[14:37:31.389] iteration 760 : loss : 0.315648, loss_ce: 0.032403
[14:37:35.463] iteration 770 : loss : 0.334340, loss_ce: 0.032301
[14:37:39.528] iteration 780 : loss : 0.272229, loss_ce: 0.034559
[14:37:43.605] iteration 790 : loss : 0.268998, loss_ce: 0.033672
[14:37:47.674] iteration 800 : loss : 0.410796, loss_ce: 0.050166
[14:45:02.635] iteration 810 : loss : 0.482835, loss_ce: 0.085298
[14:45:06.675] iteration 820 : loss : 0.336093, loss_ce: 0.073145
[14:45:10.727] iteration 830 : loss : 0.460767, loss_ce: 0.048262
[14:45:14.769] iteration 840 : loss : 0.453252, loss_ce: 0.050470
[14:45:18.826] iteration 850 : loss : 0.301881, loss_ce: 0.070288
[14:45:22.875] iteration 860 : loss : 0.285829, loss_ce: 0.039461
[14:45:26.934] iteration 870 : loss : 0.445856, loss_ce: 0.047535
[14:45:30.986] iteration 880 : loss : 0.452891, loss_ce: 0.072022
[14:45:35.187] iteration 890 : loss : 0.435854, loss_ce: 0.056521
[14:45:40.396] iteration 900 : loss : 0.444408, loss_ce: 0.037907
[14:45:44.463] iteration 910 : loss : 0.289730, loss_ce: 0.046719
[14:45:48.517] iteration 920 : loss : 0.436904, loss_ce: 0.048984
[14:45:52.584] iteration 930 : loss : 0.428155, loss_ce: 0.053205
[14:45:56.649] iteration 940 : loss : 0.285473, loss_ce: 0.051226
[14:46:00.728] iteration 950 : loss : 0.436438, loss_ce: 0.033989
[14:46:04.795] iteration 960 : loss : 0.438120, loss_ce: 0.052712
[14:46:08.874] iteration 970 : loss : 0.432510, loss_ce: 0.071769
[14:46:12.944] iteration 980 : loss : 0.269346, loss_ce: 0.039331
[14:46:17.019] iteration 990 : loss : 0.425912, loss_ce: 0.037524
[14:46:21.088] iteration 1000 : loss : 0.440624, loss_ce: 0.078313
[14:46:25.164] iteration 1010 : loss : 0.446610, loss_ce: 0.065805
[14:46:29.233] iteration 1020 : loss : 0.414480, loss_ce: 0.074522
[14:46:33.310] iteration 1030 : loss : 0.430755, loss_ce: 0.053724
[14:46:37.378] iteration 1040 : loss : 0.251317, loss_ce: 0.057692
[14:46:41.458] iteration 1050 : loss : 0.388678, loss_ce: 0.045458
[14:46:45.531] iteration 1060 : loss : 0.372244, loss_ce: 0.054792
[14:46:49.615] iteration 1070 : loss : 0.390507, loss_ce: 0.034878
[14:54:01.281] iteration 1080 : loss : 0.523085, loss_ce: 0.180840
[14:54:05.330] iteration 1090 : loss : 0.486596, loss_ce: 0.090072
[14:54:09.372] iteration 1100 : loss : 0.482216, loss_ce: 0.079978
[14:54:13.425] iteration 1110 : loss : 0.463827, loss_ce: 0.041854
[14:54:17.469] iteration 1120 : loss : 0.452973, loss_ce: 0.034531
[14:54:21.525] iteration 1130 : loss : 0.456875, loss_ce: 0.058582
[14:54:25.573] iteration 1140 : loss : 0.447255, loss_ce: 0.045994
[14:54:29.633] iteration 1150 : loss : 0.298251, loss_ce: 0.067359
[14:54:33.684] iteration 1160 : loss : 0.290639, loss_ce: 0.035417
[14:54:37.746] iteration 1170 : loss : 0.431356, loss_ce: 0.066647
[14:54:41.800] iteration 1180 : loss : 0.429324, loss_ce: 0.062456
[14:54:45.864] iteration 1190 : loss : 0.437602, loss_ce: 0.063400
[14:54:49.919] iteration 1200 : loss : 0.411405, loss_ce: 0.046859
[14:54:53.985] iteration 1210 : loss : 0.268233, loss_ce: 0.031844
[14:54:58.044] iteration 1220 : loss : 0.269522, loss_ce: 0.059684
[14:55:02.113] iteration 1230 : loss : 0.434924, loss_ce: 0.046270
[14:55:06.178] iteration 1240 : loss : 0.405076, loss_ce: 0.027170
[14:55:10.252] iteration 1250 : loss : 0.395541, loss_ce: 0.023068
[14:55:14.318] iteration 1260 : loss : 0.240487, loss_ce: 0.028497
[14:55:18.393] iteration 1270 : loss : 0.394893, loss_ce: 0.038230
[14:55:22.457] iteration 1280 : loss : 0.390366, loss_ce: 0.053560
[14:55:26.532] iteration 1290 : loss : 0.411291, loss_ce: 0.084865
[14:55:30.600] iteration 1300 : loss : 0.379703, loss_ce: 0.013491
[14:55:34.677] iteration 1310 : loss : 0.234306, loss_ce: 0.049392
[14:55:38.744] iteration 1320 : loss : 0.357531, loss_ce: 0.042387
[14:55:42.821] iteration 1330 : loss : 0.388763, loss_ce: 0.068312
[14:55:46.791] iteration 1340 : loss : 0.412023, loss_ce: 0.060713
[15:02:57.336] iteration 1350 : loss : 0.320432, loss_ce: 0.066168
[15:03:01.379] iteration 1360 : loss : 0.318690, loss_ce: 0.084169
[15:03:05.432] iteration 1370 : loss : 0.454444, loss_ce: 0.074457
[15:03:09.475] iteration 1380 : loss : 0.293350, loss_ce: 0.028451
[15:03:13.528] iteration 1390 : loss : 0.278993, loss_ce: 0.079611
[15:03:17.576] iteration 1400 : loss : 0.442076, loss_ce: 0.062893
[15:03:21.637] iteration 1410 : loss : 0.434040, loss_ce: 0.070822
[15:03:25.689] iteration 1420 : loss : 0.427300, loss_ce: 0.050938
[15:03:29.751] iteration 1430 : loss : 0.432972, loss_ce: 0.039914
[15:03:33.804] iteration 1440 : loss : 0.277198, loss_ce: 0.043429
[15:03:37.866] iteration 1450 : loss : 0.278129, loss_ce: 0.045477
[15:03:41.924] iteration 1460 : loss : 0.411761, loss_ce: 0.043327
[15:03:45.993] iteration 1470 : loss : 0.401211, loss_ce: 0.027085
[15:03:50.053] iteration 1480 : loss : 0.441384, loss_ce: 0.054641
[15:03:54.123] iteration 1490 : loss : 0.471440, loss_ce: 0.065225
[15:03:58.187] iteration 1500 : loss : 0.238690, loss_ce: 0.014545
[15:04:02.264] iteration 1510 : loss : 0.379262, loss_ce: 0.043056
[15:04:06.333] iteration 1520 : loss : 0.349381, loss_ce: 0.043572
[15:04:10.407] iteration 1530 : loss : 0.368633, loss_ce: 0.036409
[15:04:14.474] iteration 1540 : loss : 0.368272, loss_ce: 0.054510
[15:04:18.551] iteration 1550 : loss : 0.359366, loss_ce: 0.066716
[15:04:22.620] iteration 1560 : loss : 0.335488, loss_ce: 0.012249
[15:04:26.700] iteration 1570 : loss : 0.344276, loss_ce: 0.009572
[15:04:30.770] iteration 1580 : loss : 0.296018, loss_ce: 0.030858
[15:04:34.850] iteration 1590 : loss : 0.379514, loss_ce: 0.034483
[15:04:38.921] iteration 1600 : loss : 0.313079, loss_ce: 0.044217
[15:11:42.766] iteration 1610 : loss : 0.420178, loss_ce: 0.020710
[15:11:46.805] iteration 1620 : loss : 0.475819, loss_ce: 0.113639
[15:11:51.977] iteration 1630 : loss : 0.472493, loss_ce: 0.092239
[15:11:56.020] iteration 1640 : loss : 0.453055, loss_ce: 0.042978
[15:12:00.074] iteration 1650 : loss : 0.454437, loss_ce: 0.058337
[15:12:04.120] iteration 1660 : loss : 0.440726, loss_ce: 0.040778
[15:12:08.181] iteration 1670 : loss : 0.440815, loss_ce: 0.079844
[15:12:12.232] iteration 1680 : loss : 0.314328, loss_ce: 0.046919
[15:12:16.290] iteration 1690 : loss : 0.318828, loss_ce: 0.082884
[15:12:20.338] iteration 1700 : loss : 0.467664, loss_ce: 0.101598
[15:12:24.398] iteration 1710 : loss : 0.293163, loss_ce: 0.073018
[15:12:29.127] iteration 1720 : loss : 0.440731, loss_ce: 0.053740
[15:12:33.189] iteration 1730 : loss : 0.434065, loss_ce: 0.050828
[15:12:37.243] iteration 1740 : loss : 0.440901, loss_ce: 0.049105
[15:12:41.306] iteration 1750 : loss : 0.432892, loss_ce: 0.064828
[15:12:45.365] iteration 1760 : loss : 0.433244, loss_ce: 0.044936
[15:12:49.436] iteration 1770 : loss : 0.439721, loss_ce: 0.044826
[15:12:53.497] iteration 1780 : loss : 0.277585, loss_ce: 0.051057
[15:12:57.573] iteration 1790 : loss : 0.441788, loss_ce: 0.037562
[15:13:01.634] iteration 1800 : loss : 0.278217, loss_ce: 0.040683
[15:13:07.270] iteration 1810 : loss : 0.446035, loss_ce: 0.047105
[15:13:11.334] iteration 1820 : loss : 0.285826, loss_ce: 0.034429
[15:13:15.408] iteration 1830 : loss : 0.423328, loss_ce: 0.042139
[15:13:19.473] iteration 1840 : loss : 0.433277, loss_ce: 0.063065
[15:13:23.546] iteration 1850 : loss : 0.436450, loss_ce: 0.082361
[15:13:27.613] iteration 1860 : loss : 0.431047, loss_ce: 0.072392
[15:13:31.687] iteration 1870 : loss : 0.451075, loss_ce: 0.080635
[15:20:51.777] iteration 1880 : loss : 0.458159, loss_ce: 0.040784
[15:20:55.827] iteration 1890 : loss : 0.323158, loss_ce: 0.087576
[15:20:59.871] iteration 1900 : loss : 0.462237, loss_ce: 0.063514
[15:21:05.031] iteration 1910 : loss : 0.436331, loss_ce: 0.056984
[15:21:09.078] iteration 1920 : loss : 0.440016, loss_ce: 0.048372
[15:21:13.138] iteration 1930 : loss : 0.291382, loss_ce: 0.060047
[15:21:17.190] iteration 1940 : loss : 0.422148, loss_ce: 0.040444
[15:21:21.252] iteration 1950 : loss : 0.280532, loss_ce: 0.058783
[15:21:25.306] iteration 1960 : loss : 0.415474, loss_ce: 0.037122
[15:21:29.368] iteration 1970 : loss : 0.468051, loss_ce: 0.054853
[15:21:33.423] iteration 1980 : loss : 0.422277, loss_ce: 0.049212
[15:21:37.490] iteration 1990 : loss : 0.354987, loss_ce: 0.024401
[15:21:41.546] iteration 2000 : loss : 0.406174, loss_ce: 0.030929
[15:21:45.620] iteration 2010 : loss : 0.312074, loss_ce: 0.012221
[15:21:49.687] iteration 2020 : loss : 0.336287, loss_ce: 0.018780
[15:21:53.761] iteration 2030 : loss : 0.194890, loss_ce: 0.011554
[15:21:58.648] iteration 2040 : loss : 0.357018, loss_ce: 0.036011
[15:22:02.723] iteration 2050 : loss : 0.322022, loss_ce: 0.033681
[15:22:06.789] iteration 2060 : loss : 0.324745, loss_ce: 0.017824
[15:22:10.865] iteration 2070 : loss : 0.194809, loss_ce: 0.006479
[15:22:14.929] iteration 2080 : loss : 0.305493, loss_ce: 0.027192
[15:22:19.008] iteration 2090 : loss : 0.308792, loss_ce: 0.026664
[15:22:23.075] iteration 2100 : loss : 0.126039, loss_ce: 0.013836
[15:22:27.153] iteration 2110 : loss : 0.340039, loss_ce: 0.060131
[15:22:31.226] iteration 2120 : loss : 0.200666, loss_ce: 0.017660
[15:22:38.020] iteration 2130 : loss : 0.336080, loss_ce: 0.031583
[15:22:42.626] iteration 2140 : loss : 0.181147, loss_ce: 0.004648
[15:30:07.477] iteration 2150 : loss : 0.461321, loss_ce: 0.042197
[15:30:11.515] iteration 2160 : loss : 0.490865, loss_ce: 0.100718
[15:30:15.568] iteration 2170 : loss : 0.482760, loss_ce: 0.080589
[15:30:19.614] iteration 2180 : loss : 0.333226, loss_ce: 0.083137
[15:30:23.669] iteration 2190 : loss : 0.470209, loss_ce: 0.076265
[15:30:27.715] iteration 2200 : loss : 0.468747, loss_ce: 0.056329
[15:30:31.774] iteration 2210 : loss : 0.469849, loss_ce: 0.075621
[15:30:35.824] iteration 2220 : loss : 0.462503, loss_ce: 0.059658
[15:30:39.884] iteration 2230 : loss : 0.459442, loss_ce: 0.053300
[15:30:43.935] iteration 2240 : loss : 0.461377, loss_ce: 0.062115
[15:30:47.997] iteration 2250 : loss : 0.460072, loss_ce: 0.064614
[15:30:52.052] iteration 2260 : loss : 0.447308, loss_ce: 0.059396
[15:30:56.114] iteration 2270 : loss : 0.442003, loss_ce: 0.044522
[15:31:00.171] iteration 2280 : loss : 0.448190, loss_ce: 0.068597
[15:31:04.739] iteration 2290 : loss : 0.445008, loss_ce: 0.060422
[15:31:10.014] iteration 2300 : loss : 0.427384, loss_ce: 0.074692
[15:31:14.081] iteration 2310 : loss : 0.427121, loss_ce: 0.057984
[15:31:18.142] iteration 2320 : loss : 0.289321, loss_ce: 0.065211
[15:31:22.213] iteration 2330 : loss : 0.439242, loss_ce: 0.058960
[15:31:26.275] iteration 2340 : loss : 0.284387, loss_ce: 0.084126
[15:31:30.346] iteration 2350 : loss : 0.279723, loss_ce: 0.045420
[15:31:35.167] iteration 2360 : loss : 0.427435, loss_ce: 0.062476
[15:31:40.046] iteration 2370 : loss : 0.275541, loss_ce: 0.037283
[15:31:44.874] iteration 2380 : loss : 0.275178, loss_ce: 0.046650
[15:31:48.951] iteration 2390 : loss : 0.429847, loss_ce: 0.071945
[15:31:53.020] iteration 2400 : loss : 0.427948, loss_ce: 0.051559
[15:31:57.095] iteration 2410 : loss : 0.409638, loss_ce: 0.045207
[15:39:31.876] iteration 2420 : loss : 0.481085, loss_ce: 0.076411
[15:39:37.747] iteration 2430 : loss : 0.466638, loss_ce: 0.053484
[15:39:41.786] iteration 2440 : loss : 0.467078, loss_ce: 0.043687
[15:39:45.841] iteration 2450 : loss : 0.454592, loss_ce: 0.070629
[15:39:50.370] iteration 2460 : loss : 0.435520, loss_ce: 0.047665
[15:39:57.264] iteration 2470 : loss : 0.436253, loss_ce: 0.054190
[15:40:04.025] iteration 2480 : loss : 0.443177, loss_ce: 0.081221
[15:40:10.801] iteration 2490 : loss : 0.437955, loss_ce: 0.072152
[15:40:16.618] iteration 2500 : loss : 0.444276, loss_ce: 0.063874
[15:40:20.680] iteration 2510 : loss : 0.424620, loss_ce: 0.067057
[15:40:24.732] iteration 2520 : loss : 0.444983, loss_ce: 0.053809
[15:40:28.796] iteration 2530 : loss : 0.428407, loss_ce: 0.068576
[15:40:34.529] iteration 2540 : loss : 0.426762, loss_ce: 0.061531
[15:40:40.196] iteration 2550 : loss : 0.444554, loss_ce: 0.049174
[15:40:44.254] iteration 2560 : loss : 0.304857, loss_ce: 0.053369
[15:40:48.319] iteration 2570 : loss : 0.293174, loss_ce: 0.043488
[15:40:52.375] iteration 2580 : loss : 0.447946, loss_ce: 0.062199
[15:40:56.444] iteration 2590 : loss : 0.447172, loss_ce: 0.042879
[15:41:00.503] iteration 2600 : loss : 0.446124, loss_ce: 0.060730
[15:41:04.572] iteration 2610 : loss : 0.439003, loss_ce: 0.049436
[15:41:08.631] iteration 2620 : loss : 0.440751, loss_ce: 0.057057
[15:41:13.643] iteration 2630 : loss : 0.445395, loss_ce: 0.065252
[15:41:20.411] iteration 2640 : loss : 0.432074, loss_ce: 0.046580
[15:41:26.717] iteration 2650 : loss : 0.427436, loss_ce: 0.037744
[15:41:30.782] iteration 2660 : loss : 0.283992, loss_ce: 0.034027
[15:41:34.859] iteration 2670 : loss : 0.439289, loss_ce: 0.065506
[15:41:38.828] iteration 2680 : loss : 0.276323, loss_ce: 0.074534
[15:49:11.241] save model to ./finetune_tpgm_kits23_continual_500iter\finetuned_epoch_9.pth
[15:49:25.280] iteration 2690 : loss : 0.458123, loss_ce: 0.026321
[15:49:29.325] iteration 2700 : loss : 0.456918, loss_ce: 0.068991
[15:49:36.228] iteration 2710 : loss : 0.444046, loss_ce: 0.061985
[15:49:42.985] iteration 2720 : loss : 0.294514, loss_ce: 0.029961
[15:49:49.753] iteration 2730 : loss : 0.448967, loss_ce: 0.047966
[15:49:56.513] iteration 2740 : loss : 0.435350, loss_ce: 0.039601
[15:50:01.411] iteration 2750 : loss : 0.281426, loss_ce: 0.061520
[15:50:07.556] iteration 2760 : loss : 0.278556, loss_ce: 0.043424
[15:50:11.622] iteration 2770 : loss : 0.437730, loss_ce: 0.041680
[15:50:15.678] iteration 2780 : loss : 0.425269, loss_ce: 0.061673
[15:50:19.746] iteration 2790 : loss : 0.433565, loss_ce: 0.061946
[15:50:23.808] iteration 2800 : loss : 0.424635, loss_ce: 0.046234
[15:50:27.880] iteration 2810 : loss : 0.408993, loss_ce: 0.036013
[15:50:31.945] iteration 2820 : loss : 0.399682, loss_ce: 0.047516
[15:50:36.022] iteration 2830 : loss : 0.416684, loss_ce: 0.035177
[15:50:40.088] iteration 2840 : loss : 0.367271, loss_ce: 0.027597
[15:50:44.166] iteration 2850 : loss : 0.352784, loss_ce: 0.035307
[15:50:48.234] iteration 2860 : loss : 0.205662, loss_ce: 0.025858
[15:50:52.306] iteration 2870 : loss : 0.384439, loss_ce: 0.026537
[15:50:56.375] iteration 2880 : loss : 0.303694, loss_ce: 0.027731
[15:51:00.454] iteration 2890 : loss : 0.178137, loss_ce: 0.024391
[15:51:04.524] iteration 2900 : loss : 0.310054, loss_ce: 0.178613
[15:51:08.604] iteration 2910 : loss : 0.231606, loss_ce: 0.028706
[15:51:12.675] iteration 2920 : loss : 0.315108, loss_ce: 0.023456
[15:51:16.759] iteration 2930 : loss : 0.170842, loss_ce: 0.014640
[15:51:20.835] iteration 2940 : loss : 0.199051, loss_ce: 0.016627
[15:59:02.075] iteration 2950 : loss : 0.433659, loss_ce: 0.065863
[15:59:06.117] iteration 2960 : loss : 0.499641, loss_ce: 0.147090
[15:59:10.172] iteration 2970 : loss : 0.478496, loss_ce: 0.075068
[15:59:14.216] iteration 2980 : loss : 0.440318, loss_ce: 0.039777
[15:59:18.278] iteration 2990 : loss : 0.438877, loss_ce: 0.050522
[15:59:22.331] iteration 3000 : loss : 0.435749, loss_ce: 0.042507
[15:59:26.521] iteration 3010 : loss : 0.440099, loss_ce: 0.060012
[15:59:30.576] iteration 3020 : loss : 0.452857, loss_ce: 0.061260
[15:59:34.735] iteration 3030 : loss : 0.432145, loss_ce: 0.033541
[15:59:38.794] iteration 3040 : loss : 0.324990, loss_ce: 0.090963
[15:59:42.861] iteration 3050 : loss : 0.456825, loss_ce: 0.048600
[15:59:46.922] iteration 3060 : loss : 0.321993, loss_ce: 0.122301
[15:59:51.068] iteration 3070 : loss : 0.442335, loss_ce: 0.040263
[15:59:55.127] iteration 3080 : loss : 0.436351, loss_ce: 0.054507
[15:59:59.198] iteration 3090 : loss : 0.436015, loss_ce: 0.075233
[16:00:03.260] iteration 3100 : loss : 0.419973, loss_ce: 0.037244
[16:00:07.329] iteration 3110 : loss : 0.394866, loss_ce: 0.027079
[16:00:11.394] iteration 3120 : loss : 0.401421, loss_ce: 0.062819
[16:00:15.471] iteration 3130 : loss : 0.272246, loss_ce: 0.034304
[16:00:19.539] iteration 3140 : loss : 0.384693, loss_ce: 0.032033
[16:00:23.622] iteration 3150 : loss : 0.363876, loss_ce: 0.036316
[16:00:27.691] iteration 3160 : loss : 0.375167, loss_ce: 0.053225
[16:00:31.769] iteration 3170 : loss : 0.228411, loss_ce: 0.045201
[16:00:35.846] iteration 3180 : loss : 0.376944, loss_ce: 0.040127
[16:00:39.924] iteration 3190 : loss : 0.347876, loss_ce: 0.046472
[16:00:43.993] iteration 3200 : loss : 0.335372, loss_ce: 0.020750
[16:00:48.079] iteration 3210 : loss : 0.344397, loss_ce: 0.049406
[16:07:53.526] iteration 3220 : loss : 0.472385, loss_ce: 0.056153
[16:07:57.576] iteration 3230 : loss : 0.489986, loss_ce: 0.099994
[16:08:01.617] iteration 3240 : loss : 0.458891, loss_ce: 0.053063
[16:08:05.669] iteration 3250 : loss : 0.450031, loss_ce: 0.088375
[16:08:09.714] iteration 3260 : loss : 0.448942, loss_ce: 0.064122
[16:08:13.771] iteration 3270 : loss : 0.274725, loss_ce: 0.058403
[16:08:17.821] iteration 3280 : loss : 0.290741, loss_ce: 0.050236
[16:08:21.882] iteration 3290 : loss : 0.278915, loss_ce: 0.060135
[16:08:25.933] iteration 3300 : loss : 0.428656, loss_ce: 0.050367
[16:08:29.994] iteration 3310 : loss : 0.424698, loss_ce: 0.079068
[16:08:34.047] iteration 3320 : loss : 0.438030, loss_ce: 0.074932
[16:08:38.110] iteration 3330 : loss : 0.426639, loss_ce: 0.051792
[16:08:42.168] iteration 3340 : loss : 0.405390, loss_ce: 0.051664
[16:08:46.236] iteration 3350 : loss : 0.417489, loss_ce: 0.040769
[16:08:50.299] iteration 3360 : loss : 0.397371, loss_ce: 0.044213
[16:08:54.370] iteration 3370 : loss : 0.208884, loss_ce: 0.023070
[16:08:58.432] iteration 3380 : loss : 0.367780, loss_ce: 0.028414
[16:09:02.507] iteration 3390 : loss : 0.360540, loss_ce: 0.026549
[16:09:06.571] iteration 3400 : loss : 0.336740, loss_ce: 0.047124
[16:09:10.644] iteration 3410 : loss : 0.331772, loss_ce: 0.041805
[16:09:14.711] iteration 3420 : loss : 0.384172, loss_ce: 0.060978
[16:09:18.787] iteration 3430 : loss : 0.506756, loss_ce: 0.139324
[16:09:22.850] iteration 3440 : loss : 0.330191, loss_ce: 0.100691
[16:09:26.920] iteration 3450 : loss : 0.456999, loss_ce: 0.066398
[16:09:30.984] iteration 3460 : loss : 0.450958, loss_ce: 0.075462
[16:09:35.059] iteration 3470 : loss : 0.439704, loss_ce: 0.048047
[16:09:39.127] iteration 3480 : loss : 0.299954, loss_ce: 0.060343
[16:16:50.371] iteration 3490 : loss : 0.488879, loss_ce: 0.164049
[16:16:54.407] iteration 3500 : loss : 0.461724, loss_ce: 0.070553
[16:16:58.460] iteration 3510 : loss : 0.461970, loss_ce: 0.052044
[16:17:02.504] iteration 3520 : loss : 0.442057, loss_ce: 0.044085
[16:17:06.558] iteration 3530 : loss : 0.434133, loss_ce: 0.062402
[16:17:10.605] iteration 3540 : loss : 0.446859, loss_ce: 0.069112
[16:17:14.661] iteration 3550 : loss : 0.296236, loss_ce: 0.058276
[16:17:18.712] iteration 3560 : loss : 0.295501, loss_ce: 0.060720
[16:17:22.771] iteration 3570 : loss : 0.421852, loss_ce: 0.047278
[16:17:26.825] iteration 3580 : loss : 0.430882, loss_ce: 0.067212
[16:17:30.890] iteration 3590 : loss : 0.411676, loss_ce: 0.067421
[16:17:34.945] iteration 3600 : loss : 0.417508, loss_ce: 0.045472
[16:17:39.015] iteration 3610 : loss : 0.394484, loss_ce: 0.031868
[16:17:43.075] iteration 3620 : loss : 0.287223, loss_ce: 0.043016
[16:17:47.145] iteration 3630 : loss : 0.245121, loss_ce: 0.022605
[16:17:51.208] iteration 3640 : loss : 0.366505, loss_ce: 0.048366
[16:17:55.274] iteration 3650 : loss : 0.191997, loss_ce: 0.021370
[16:17:59.333] iteration 3660 : loss : 0.427966, loss_ce: 0.063965
[16:18:03.400] iteration 3670 : loss : 0.487760, loss_ce: 0.093676
[16:18:07.461] iteration 3680 : loss : 0.435015, loss_ce: 0.030042
[16:18:11.536] iteration 3690 : loss : 0.417499, loss_ce: 0.029337
[16:18:15.602] iteration 3700 : loss : 0.401548, loss_ce: 0.016811
[16:18:19.677] iteration 3710 : loss : 0.441972, loss_ce: 0.048083
[16:18:23.746] iteration 3720 : loss : 0.362920, loss_ce: 0.059789
[16:18:27.826] iteration 3730 : loss : 0.362793, loss_ce: 0.039985
[16:18:31.894] iteration 3740 : loss : 0.262092, loss_ce: 0.064278
[16:18:35.972] iteration 3750 : loss : 0.311314, loss_ce: 0.048586
[16:25:46.832] iteration 3760 : loss : 0.474390, loss_ce: 0.103750
[16:25:50.883] iteration 3770 : loss : 0.321032, loss_ce: 0.052652
[16:25:54.924] iteration 3780 : loss : 0.456589, loss_ce: 0.063607
[16:25:58.976] iteration 3790 : loss : 0.441376, loss_ce: 0.042247
[16:26:03.021] iteration 3800 : loss : 0.439268, loss_ce: 0.057789
[16:26:07.076] iteration 3810 : loss : 0.427874, loss_ce: 0.044200
[16:26:11.126] iteration 3820 : loss : 0.440881, loss_ce: 0.038551
[16:26:15.185] iteration 3830 : loss : 0.442090, loss_ce: 0.049405
[16:26:19.234] iteration 3840 : loss : 0.439010, loss_ce: 0.070420
[16:26:23.297] iteration 3850 : loss : 0.425273, loss_ce: 0.051611
[16:26:27.350] iteration 3860 : loss : 0.412212, loss_ce: 0.060935
[16:26:31.414] iteration 3870 : loss : 0.422399, loss_ce: 0.047167
[16:26:35.468] iteration 3880 : loss : 0.400195, loss_ce: 0.042757
[16:26:39.532] iteration 3890 : loss : 0.427391, loss_ce: 0.070655
[16:26:43.590] iteration 3900 : loss : 0.227257, loss_ce: 0.035474
[16:26:47.663] iteration 3910 : loss : 0.352709, loss_ce: 0.044212
[16:26:51.726] iteration 3920 : loss : 0.374981, loss_ce: 0.041201
[16:26:55.801] iteration 3930 : loss : 0.310841, loss_ce: 0.017338
[16:26:59.864] iteration 3940 : loss : 0.330010, loss_ce: 0.018885
[16:27:03.940] iteration 3950 : loss : 0.306322, loss_ce: 0.017799
[16:27:08.006] iteration 3960 : loss : 0.185888, loss_ce: 0.040594
[16:27:12.086] iteration 3970 : loss : 0.336077, loss_ce: 0.043969
[16:27:16.154] iteration 3980 : loss : 0.085520, loss_ce: 0.015581
[16:27:20.232] iteration 3990 : loss : 0.354370, loss_ce: 0.039044
[16:27:24.298] iteration 4000 : loss : 0.375160, loss_ce: 0.036144
[16:27:28.376] iteration 4010 : loss : 0.361896, loss_ce: 0.023202
[16:27:32.348] iteration 4020 : loss : 0.258018, loss_ce: 0.024866
[16:34:36.474] iteration 4030 : loss : 0.460345, loss_ce: 0.040810
[16:34:40.517] iteration 4040 : loss : 0.459516, loss_ce: 0.059093
[16:34:44.570] iteration 4050 : loss : 0.459778, loss_ce: 0.105640
[16:34:48.614] iteration 4060 : loss : 0.290414, loss_ce: 0.072671
[16:34:52.670] iteration 4070 : loss : 0.436119, loss_ce: 0.060104
[16:34:56.719] iteration 4080 : loss : 0.454446, loss_ce: 0.075611
[16:35:00.781] iteration 4090 : loss : 0.293051, loss_ce: 0.070215
[16:35:04.836] iteration 4100 : loss : 0.423789, loss_ce: 0.043416
[16:35:08.899] iteration 4110 : loss : 0.282188, loss_ce: 0.053947
[16:35:12.952] iteration 4120 : loss : 0.411698, loss_ce: 0.052703
[16:35:17.019] iteration 4130 : loss : 0.460448, loss_ce: 0.065153
[16:35:21.075] iteration 4140 : loss : 0.429457, loss_ce: 0.095802
[16:35:25.145] iteration 4150 : loss : 0.405410, loss_ce: 0.045416
[16:35:29.208] iteration 4160 : loss : 0.369739, loss_ce: 0.046313
[16:35:33.282] iteration 4170 : loss : 0.200854, loss_ce: 0.016124
[16:35:37.345] iteration 4180 : loss : 0.374570, loss_ce: 0.038097
[16:35:41.424] iteration 4190 : loss : 0.347809, loss_ce: 0.025877
[16:35:45.490] iteration 4200 : loss : 0.180773, loss_ce: 0.017359
[16:35:49.567] iteration 4210 : loss : 0.321268, loss_ce: 0.017483
[16:35:53.634] iteration 4220 : loss : 0.164935, loss_ce: 0.017250
[16:35:57.715] iteration 4230 : loss : 0.281703, loss_ce: 0.018088
[16:36:01.786] iteration 4240 : loss : 0.264860, loss_ce: 0.105752
[16:36:05.872] iteration 4250 : loss : 0.373904, loss_ce: 0.032505
[16:36:09.943] iteration 4260 : loss : 0.337652, loss_ce: 0.039885
[16:36:14.091] iteration 4270 : loss : 0.271567, loss_ce: 0.024928
[16:36:18.161] iteration 4280 : loss : 0.177804, loss_ce: 0.011998
[16:43:38.068] iteration 4290 : loss : 0.480404, loss_ce: 0.125011
[16:43:42.111] iteration 4300 : loss : 0.481811, loss_ce: 0.084297
[16:43:46.161] iteration 4310 : loss : 0.488261, loss_ce: 0.116292
[16:43:50.207] iteration 4320 : loss : 0.305806, loss_ce: 0.015392
[16:43:54.264] iteration 4330 : loss : 0.307138, loss_ce: 0.042761
[16:43:58.314] iteration 4340 : loss : 0.449656, loss_ce: 0.054123
[16:44:02.379] iteration 4350 : loss : 0.438645, loss_ce: 0.053712
[16:44:06.432] iteration 4360 : loss : 0.443074, loss_ce: 0.076276
[16:44:10.493] iteration 4370 : loss : 0.433390, loss_ce: 0.089216
[16:44:14.548] iteration 4380 : loss : 0.426583, loss_ce: 0.066120
[16:44:18.617] iteration 4390 : loss : 0.436612, loss_ce: 0.046725
[16:44:22.675] iteration 4400 : loss : 0.428769, loss_ce: 0.038566
[16:44:26.747] iteration 4410 : loss : 0.285354, loss_ce: 0.071991
[16:44:30.812] iteration 4420 : loss : 0.421734, loss_ce: 0.074841
[16:44:34.885] iteration 4430 : loss : 0.431071, loss_ce: 0.056272
[16:44:38.949] iteration 4440 : loss : 0.429615, loss_ce: 0.051104
[16:44:43.033] iteration 4450 : loss : 0.439132, loss_ce: 0.073796
[16:44:47.102] iteration 4460 : loss : 0.421768, loss_ce: 0.077983
[16:44:51.180] iteration 4470 : loss : 0.417938, loss_ce: 0.047115
[16:44:55.247] iteration 4480 : loss : 0.424646, loss_ce: 0.053774
[16:44:59.325] iteration 4490 : loss : 0.406760, loss_ce: 0.031306
[16:45:03.393] iteration 4500 : loss : 0.400011, loss_ce: 0.051306
[16:45:07.473] iteration 4510 : loss : 0.377612, loss_ce: 0.059686
[16:45:11.546] iteration 4520 : loss : 0.387566, loss_ce: 0.038206
[16:45:15.624] iteration 4530 : loss : 0.490292, loss_ce: 0.099098
[16:45:19.693] iteration 4540 : loss : 0.487117, loss_ce: 0.091524
[16:45:23.771] iteration 4550 : loss : 0.484506, loss_ce: 0.084912
[16:52:56.918] iteration 4560 : loss : 0.467824, loss_ce: 0.058261
[16:53:00.972] iteration 4570 : loss : 0.479685, loss_ce: 0.073674
[16:53:05.024] iteration 4580 : loss : 0.492031, loss_ce: 0.121424
[16:53:09.087] iteration 4590 : loss : 0.468352, loss_ce: 0.064064
[16:53:13.139] iteration 4600 : loss : 0.452542, loss_ce: 0.083630
[16:53:17.203] iteration 4610 : loss : 0.291068, loss_ce: 0.055246
[16:53:21.259] iteration 4620 : loss : 0.448265, loss_ce: 0.057610
[16:53:25.323] iteration 4630 : loss : 0.444419, loss_ce: 0.055526
[16:53:29.380] iteration 4640 : loss : 0.441649, loss_ce: 0.046433
[16:53:33.455] iteration 4650 : loss : 0.434931, loss_ce: 0.042455
[16:53:37.518] iteration 4660 : loss : 0.427188, loss_ce: 0.075423
[16:53:41.593] iteration 4670 : loss : 0.433814, loss_ce: 0.055550
[16:53:45.657] iteration 4680 : loss : 0.441300, loss_ce: 0.050276
[16:53:49.729] iteration 4690 : loss : 0.435599, loss_ce: 0.053615
[16:53:53.795] iteration 4700 : loss : 0.274456, loss_ce: 0.044174
[16:53:57.873] iteration 4710 : loss : 0.421959, loss_ce: 0.074537
[16:54:01.944] iteration 4720 : loss : 0.425059, loss_ce: 0.045936
[16:54:06.029] iteration 4730 : loss : 0.430369, loss_ce: 0.048102
[16:54:10.103] iteration 4740 : loss : 0.405190, loss_ce: 0.052044
[16:54:14.186] iteration 4750 : loss : 0.409840, loss_ce: 0.065304
[16:54:18.259] iteration 4760 : loss : 0.469890, loss_ce: 0.061999
[16:54:22.342] iteration 4770 : loss : 0.431728, loss_ce: 0.051315
[16:54:26.419] iteration 4780 : loss : 0.412599, loss_ce: 0.062633
[16:54:30.507] iteration 4790 : loss : 0.409385, loss_ce: 0.042767
[16:54:34.583] iteration 4800 : loss : 0.398118, loss_ce: 0.049017
[16:54:38.674] iteration 4810 : loss : 0.212869, loss_ce: 0.015642
[16:54:42.754] iteration 4820 : loss : 0.343840, loss_ce: 0.033020
[17:02:08.558] iteration 4830 : loss : 0.411840, loss_ce: 0.093905
[17:02:12.600] iteration 4840 : loss : 0.463107, loss_ce: 0.049501
[17:02:16.661] iteration 4850 : loss : 0.451313, loss_ce: 0.033262
[17:02:20.707] iteration 4860 : loss : 0.302564, loss_ce: 0.026444
[17:02:24.763] iteration 4870 : loss : 0.301094, loss_ce: 0.062653
[17:02:28.814] iteration 4880 : loss : 0.454763, loss_ce: 0.079835
[17:02:32.875] iteration 4890 : loss : 0.434905, loss_ce: 0.072453
[17:02:36.929] iteration 4900 : loss : 0.447212, loss_ce: 0.105956
[17:02:40.999] iteration 4910 : loss : 0.287795, loss_ce: 0.091776
[17:02:45.054] iteration 4920 : loss : 0.432539, loss_ce: 0.074949
[17:02:49.119] iteration 4930 : loss : 0.437370, loss_ce: 0.061764
[17:02:53.180] iteration 4940 : loss : 0.430350, loss_ce: 0.061730
[17:02:57.247] iteration 4950 : loss : 0.274051, loss_ce: 0.069119
[17:03:01.305] iteration 4960 : loss : 0.438777, loss_ce: 0.072486
[17:03:05.376] iteration 4970 : loss : 0.422427, loss_ce: 0.064694
[17:03:09.440] iteration 4980 : loss : 0.437436, loss_ce: 0.042629
[17:03:13.519] iteration 4990 : loss : 0.443438, loss_ce: 0.049440
[17:03:17.588] iteration 5000 : loss : 0.433239, loss_ce: 0.042104
[17:03:21.667] iteration 5010 : loss : 0.278252, loss_ce: 0.045081
[17:03:25.737] iteration 5020 : loss : 0.413772, loss_ce: 0.066016
[17:03:29.815] iteration 5030 : loss : 0.415206, loss_ce: 0.053062
[17:03:33.887] iteration 5040 : loss : 0.451756, loss_ce: 0.049213
[17:03:37.969] iteration 5050 : loss : 0.394191, loss_ce: 0.043597
[17:03:42.040] iteration 5060 : loss : 0.412717, loss_ce: 0.087128
[17:03:46.126] iteration 5070 : loss : 0.369174, loss_ce: 0.036057
[17:03:50.201] iteration 5080 : loss : 0.397017, loss_ce: 0.042432
[17:03:54.286] iteration 5090 : loss : 0.296171, loss_ce: 0.023304
[17:11:32.426] iteration 5100 : loss : 0.318454, loss_ce: 0.077099
[17:11:36.486] iteration 5110 : loss : 0.482093, loss_ce: 0.087221
[17:11:40.537] iteration 5120 : loss : 0.455516, loss_ce: 0.046718
[17:11:44.596] iteration 5130 : loss : 0.291405, loss_ce: 0.039816
[17:11:48.647] iteration 5140 : loss : 0.446462, loss_ce: 0.059027
[17:11:52.714] iteration 5150 : loss : 0.431370, loss_ce: 0.054098
[17:11:56.774] iteration 5160 : loss : 0.285718, loss_ce: 0.058463
[17:12:00.840] iteration 5170 : loss : 0.441891, loss_ce: 0.058007
[17:12:04.918] iteration 5180 : loss : 0.438547, loss_ce: 0.045418
[17:12:08.995] iteration 5190 : loss : 0.428146, loss_ce: 0.041354
[17:12:13.055] iteration 5200 : loss : 0.422620, loss_ce: 0.058327
[17:12:17.127] iteration 5210 : loss : 0.422946, loss_ce: 0.033228
[17:12:21.191] iteration 5220 : loss : 0.421708, loss_ce: 0.078635
[17:12:25.271] iteration 5230 : loss : 0.425183, loss_ce: 0.042847
[17:12:29.340] iteration 5240 : loss : 0.408126, loss_ce: 0.038784
[17:12:33.421] iteration 5250 : loss : 0.418170, loss_ce: 0.033916
[17:12:37.497] iteration 5260 : loss : 0.427107, loss_ce: 0.077630
[17:12:41.579] iteration 5270 : loss : 0.400422, loss_ce: 0.035577
[17:12:45.651] iteration 5280 : loss : 0.382604, loss_ce: 0.048269
[17:12:49.736] iteration 5290 : loss : 0.374766, loss_ce: 0.032965
[17:12:53.813] iteration 5300 : loss : 0.375826, loss_ce: 0.035309
[17:12:57.902] iteration 5310 : loss : 0.362422, loss_ce: 0.077577
[17:13:01.979] iteration 5320 : loss : 0.296803, loss_ce: 0.043655
[17:13:06.068] iteration 5330 : loss : 0.356010, loss_ce: 0.030964
[17:13:10.144] iteration 5340 : loss : 0.312202, loss_ce: 0.022433
[17:13:14.233] iteration 5350 : loss : 0.302705, loss_ce: 0.028305
[17:13:18.216] iteration 5360 : loss : 0.380048, loss_ce: 0.050533
[17:20:42.661] save model to ./finetune_tpgm_kits23_continual_500iter\finetuned_epoch_19.pth
[17:20:57.847] iteration 5370 : loss : 0.472152, loss_ce: 0.058635
[17:21:01.894] iteration 5380 : loss : 0.456842, loss_ce: 0.040043
[17:21:05.950] iteration 5390 : loss : 0.473494, loss_ce: 0.075602
[17:21:10.000] iteration 5400 : loss : 0.299789, loss_ce: 0.059074
[17:21:14.062] iteration 5410 : loss : 0.291720, loss_ce: 0.053935
[17:21:18.118] iteration 5420 : loss : 0.308312, loss_ce: 0.099946
[17:21:22.182] iteration 5430 : loss : 0.439656, loss_ce: 0.065993
[17:21:26.239] iteration 5440 : loss : 0.445774, loss_ce: 0.075465
[17:21:30.307] iteration 5450 : loss : 0.418748, loss_ce: 0.054591
[17:21:34.366] iteration 5460 : loss : 0.405562, loss_ce: 0.053109
[17:21:38.438] iteration 5470 : loss : 0.412436, loss_ce: 0.024425
[17:21:42.502] iteration 5480 : loss : 0.245889, loss_ce: 0.014152
[17:21:46.574] iteration 5490 : loss : 0.325830, loss_ce: 0.030605
[17:21:50.637] iteration 5500 : loss : 0.164832, loss_ce: 0.026489
[17:21:54.713] iteration 5510 : loss : 0.346459, loss_ce: 0.040341
[17:21:58.782] iteration 5520 : loss : 0.188574, loss_ce: 0.028520
[17:22:02.862] iteration 5530 : loss : 0.302762, loss_ce: 0.025269
[17:22:06.929] iteration 5540 : loss : 0.356590, loss_ce: 0.030755
[17:22:11.005] iteration 5550 : loss : 0.346821, loss_ce: 0.021013
[17:22:15.073] iteration 5560 : loss : 0.355625, loss_ce: 0.045448
[17:22:19.151] iteration 5570 : loss : 0.330047, loss_ce: 0.023014
[17:22:23.220] iteration 5580 : loss : 0.337960, loss_ce: 0.026034
[17:22:27.306] iteration 5590 : loss : 0.293833, loss_ce: 0.024165
[17:22:31.381] iteration 5600 : loss : 0.210557, loss_ce: 0.008880
[17:22:35.467] iteration 5610 : loss : 0.341335, loss_ce: 0.023918
[17:22:39.538] iteration 5620 : loss : 0.300678, loss_ce: 0.016243
[17:30:06.430] iteration 5630 : loss : 0.505345, loss_ce: 0.171307
[17:30:10.475] iteration 5640 : loss : 0.503155, loss_ce: 0.167983
[17:30:14.531] iteration 5650 : loss : 0.469477, loss_ce: 0.102108
[17:30:18.576] iteration 5660 : loss : 0.318448, loss_ce: 0.074455
[17:30:22.631] iteration 5670 : loss : 0.459191, loss_ce: 0.036415
[17:30:26.684] iteration 5680 : loss : 0.304960, loss_ce: 0.065985
[17:30:30.745] iteration 5690 : loss : 0.449622, loss_ce: 0.058207
[17:30:34.795] iteration 5700 : loss : 0.435546, loss_ce: 0.058693
[17:30:38.858] iteration 5710 : loss : 0.281671, loss_ce: 0.062415
[17:30:42.908] iteration 5720 : loss : 0.280518, loss_ce: 0.049697
[17:30:46.972] iteration 5730 : loss : 0.447186, loss_ce: 0.041840
[17:30:51.027] iteration 5740 : loss : 0.436659, loss_ce: 0.052087
[17:30:55.096] iteration 5750 : loss : 0.439458, loss_ce: 0.048106
[17:30:59.151] iteration 5760 : loss : 0.276356, loss_ce: 0.046374
[17:31:03.223] iteration 5770 : loss : 0.419055, loss_ce: 0.062124
[17:31:07.286] iteration 5780 : loss : 0.426275, loss_ce: 0.031753
[17:31:11.360] iteration 5790 : loss : 0.422458, loss_ce: 0.084024
[17:31:15.427] iteration 5800 : loss : 0.431825, loss_ce: 0.056539
[17:31:19.505] iteration 5810 : loss : 0.428691, loss_ce: 0.041918
[17:31:23.571] iteration 5820 : loss : 0.412277, loss_ce: 0.057255
[17:31:27.651] iteration 5830 : loss : 0.243604, loss_ce: 0.030613
[17:31:31.720] iteration 5840 : loss : 0.382332, loss_ce: 0.021860
[17:31:35.799] iteration 5850 : loss : 0.262556, loss_ce: 0.058838
[17:31:39.867] iteration 5860 : loss : 0.476538, loss_ce: 0.129597
[17:31:43.942] iteration 5870 : loss : 0.467615, loss_ce: 0.043960
[17:31:48.011] iteration 5880 : loss : 0.467554, loss_ce: 0.043682
[17:31:52.089] iteration 5890 : loss : 0.436861, loss_ce: 0.046762
[17:39:29.683] iteration 5900 : loss : 0.506182, loss_ce: 0.138262
[17:39:33.729] iteration 5910 : loss : 0.477262, loss_ce: 0.076602
[17:39:37.768] iteration 5920 : loss : 0.457241, loss_ce: 0.065976
[17:39:41.819] iteration 5930 : loss : 0.441131, loss_ce: 0.042568
[17:39:45.862] iteration 5940 : loss : 0.292032, loss_ce: 0.066541
[17:39:49.918] iteration 5950 : loss : 0.435213, loss_ce: 0.043343
[17:39:53.971] iteration 5960 : loss : 0.430972, loss_ce: 0.046907
[17:39:58.038] iteration 5970 : loss : 0.427073, loss_ce: 0.080444
[17:40:02.090] iteration 5980 : loss : 0.436037, loss_ce: 0.060442
[17:40:06.154] iteration 5990 : loss : 0.418838, loss_ce: 0.044552
[17:40:10.206] iteration 6000 : loss : 0.400453, loss_ce: 0.056013
[17:40:14.272] iteration 6010 : loss : 0.399788, loss_ce: 0.035149
[17:40:18.332] iteration 6020 : loss : 0.384986, loss_ce: 0.041694
[17:40:22.405] iteration 6030 : loss : 0.401366, loss_ce: 0.039218
[17:40:26.463] iteration 6040 : loss : 0.199555, loss_ce: 0.019322
[17:40:30.532] iteration 6050 : loss : 0.400914, loss_ce: 0.015327
[17:40:34.593] iteration 6060 : loss : 0.346465, loss_ce: 0.020886
[17:40:38.665] iteration 6070 : loss : 0.368139, loss_ce: 0.047631
[17:40:42.729] iteration 6080 : loss : 0.200217, loss_ce: 0.014109
[17:40:46.802] iteration 6090 : loss : 0.396475, loss_ce: 0.052097
[17:40:50.867] iteration 6100 : loss : 0.330417, loss_ce: 0.030046
[17:40:54.946] iteration 6110 : loss : 0.333184, loss_ce: 0.012053
[17:40:59.017] iteration 6120 : loss : 0.331346, loss_ce: 0.009723
[17:41:03.094] iteration 6130 : loss : 0.314165, loss_ce: 0.055199
[17:41:07.161] iteration 6140 : loss : 0.344588, loss_ce: 0.010948
[17:41:11.241] iteration 6150 : loss : 0.230598, loss_ce: 0.015068
[17:41:15.309] iteration 6160 : loss : 0.296079, loss_ce: 0.012566
[17:48:35.317] iteration 6170 : loss : 0.463873, loss_ce: 0.044451
[17:48:39.352] iteration 6180 : loss : 0.449746, loss_ce: 0.066473
[17:48:43.404] iteration 6190 : loss : 0.450547, loss_ce: 0.052294
[17:48:47.450] iteration 6200 : loss : 0.442135, loss_ce: 0.047925
[17:48:51.508] iteration 6210 : loss : 0.281585, loss_ce: 0.055504
[17:48:55.557] iteration 6220 : loss : 0.273217, loss_ce: 0.066384
[17:48:59.624] iteration 6230 : loss : 0.399270, loss_ce: 0.033636
[17:49:03.684] iteration 6240 : loss : 0.370847, loss_ce: 0.032342
[17:49:07.749] iteration 6250 : loss : 0.348188, loss_ce: 0.042760
[17:49:11.805] iteration 6260 : loss : 0.350987, loss_ce: 0.035799
[17:49:15.872] iteration 6270 : loss : 0.360291, loss_ce: 0.057942
[17:49:19.933] iteration 6280 : loss : 0.321836, loss_ce: 0.007559
[17:49:24.001] iteration 6290 : loss : 0.310794, loss_ce: 0.013291
[17:49:28.060] iteration 6300 : loss : 0.295688, loss_ce: 0.039609
[17:49:32.128] iteration 6310 : loss : 0.297210, loss_ce: 0.028148
[17:49:36.191] iteration 6320 : loss : 0.408589, loss_ce: 0.033797
[17:49:40.263] iteration 6330 : loss : 0.254073, loss_ce: 0.051510
[17:49:44.325] iteration 6340 : loss : 0.331608, loss_ce: 0.021664
[17:49:48.400] iteration 6350 : loss : 0.273618, loss_ce: 0.024567
[17:49:52.465] iteration 6360 : loss : 0.294257, loss_ce: 0.027045
[17:49:56.539] iteration 6370 : loss : 0.278371, loss_ce: 0.025189
[17:50:00.604] iteration 6380 : loss : 0.255580, loss_ce: 0.020783
[17:50:04.681] iteration 6390 : loss : 0.352396, loss_ce: 0.036768
[17:50:08.756] iteration 6400 : loss : 0.299200, loss_ce: 0.011936
[17:50:12.837] iteration 6410 : loss : 0.158019, loss_ce: 0.026526
[17:50:16.911] iteration 6420 : loss : 0.308966, loss_ce: 0.022470
[17:50:20.990] iteration 6430 : loss : 0.153663, loss_ce: 0.015175
[17:57:31.171] iteration 6440 : loss : 0.461173, loss_ce: 0.040516
[17:57:35.229] iteration 6450 : loss : 0.455398, loss_ce: 0.083179
[17:57:39.278] iteration 6460 : loss : 0.447107, loss_ce: 0.084595
[17:57:43.337] iteration 6470 : loss : 0.443448, loss_ce: 0.045166
[17:57:47.386] iteration 6480 : loss : 0.453270, loss_ce: 0.082794
[17:57:51.445] iteration 6490 : loss : 0.428530, loss_ce: 0.073202
[17:57:55.496] iteration 6500 : loss : 0.286427, loss_ce: 0.028050
[17:57:59.565] iteration 6510 : loss : 0.289807, loss_ce: 0.029212
[17:58:03.633] iteration 6520 : loss : 0.436428, loss_ce: 0.028242
[17:58:07.699] iteration 6530 : loss : 0.415238, loss_ce: 0.042381
[17:58:11.759] iteration 6540 : loss : 0.412430, loss_ce: 0.054153
[17:58:15.829] iteration 6550 : loss : 0.401106, loss_ce: 0.028547
[17:58:19.890] iteration 6560 : loss : 0.424050, loss_ce: 0.061988
[17:58:23.964] iteration 6570 : loss : 0.348300, loss_ce: 0.016328
[17:58:28.030] iteration 6580 : loss : 0.366402, loss_ce: 0.027717
[17:58:32.107] iteration 6590 : loss : 0.344033, loss_ce: 0.036894
[17:58:36.177] iteration 6600 : loss : 0.348901, loss_ce: 0.036808
[17:58:40.258] iteration 6610 : loss : 0.272372, loss_ce: 0.017999
[17:58:44.326] iteration 6620 : loss : 0.283065, loss_ce: 0.021560
[17:58:48.456] iteration 6630 : loss : 0.359005, loss_ce: 0.020372
[17:58:52.526] iteration 6640 : loss : 0.322167, loss_ce: 0.021462
[17:58:56.606] iteration 6650 : loss : 0.334929, loss_ce: 0.013402
[17:59:00.678] iteration 6660 : loss : 0.314441, loss_ce: 0.013062
[17:59:04.762] iteration 6670 : loss : 0.289107, loss_ce: 0.010598
[17:59:08.836] iteration 6680 : loss : 0.377471, loss_ce: 0.090523
[17:59:12.921] iteration 6690 : loss : 0.311416, loss_ce: 0.029606
[17:59:16.913] iteration 6700 : loss : 0.313519, loss_ce: 0.020721
[18:06:32.814] iteration 6710 : loss : 0.481895, loss_ce: 0.081554
[18:06:36.863] iteration 6720 : loss : 0.472148, loss_ce: 0.064111
[18:06:40.917] iteration 6730 : loss : 0.488474, loss_ce: 0.109389
[18:06:44.967] iteration 6740 : loss : 0.463802, loss_ce: 0.062325
[18:06:49.027] iteration 6750 : loss : 0.432490, loss_ce: 0.048745
[18:06:53.075] iteration 6760 : loss : 0.435376, loss_ce: 0.050830
[18:06:57.134] iteration 6770 : loss : 0.419119, loss_ce: 0.059516
[18:07:01.185] iteration 6780 : loss : 0.424157, loss_ce: 0.065649
[18:07:05.250] iteration 6790 : loss : 0.269792, loss_ce: 0.056660
[18:07:09.305] iteration 6800 : loss : 0.407035, loss_ce: 0.049710
[18:07:13.371] iteration 6810 : loss : 0.438424, loss_ce: 0.025406
[18:07:17.428] iteration 6820 : loss : 0.421985, loss_ce: 0.056069
[18:07:21.503] iteration 6830 : loss : 0.259671, loss_ce: 0.055285
[18:07:25.581] iteration 6840 : loss : 0.237890, loss_ce: 0.046826
[18:07:29.653] iteration 6850 : loss : 0.162528, loss_ce: 0.016308
[18:07:33.721] iteration 6860 : loss : 0.307852, loss_ce: 0.024540
[18:07:37.795] iteration 6870 : loss : 0.310983, loss_ce: 0.028739
[18:07:41.862] iteration 6880 : loss : 0.260623, loss_ce: 0.022927
[18:07:45.939] iteration 6890 : loss : 0.348593, loss_ce: 0.020797
[18:07:50.005] iteration 6900 : loss : 0.317691, loss_ce: 0.006831
[18:07:54.082] iteration 6910 : loss : 0.321323, loss_ce: 0.008428
[18:07:58.150] iteration 6920 : loss : 0.329671, loss_ce: 0.013940
[18:08:02.232] iteration 6930 : loss : 0.092863, loss_ce: 0.016349
[18:08:06.305] iteration 6940 : loss : 0.250946, loss_ce: 0.013133
[18:08:10.389] iteration 6950 : loss : 0.244041, loss_ce: 0.015897
[18:08:14.463] iteration 6960 : loss : 0.347068, loss_ce: 0.013813
[18:15:29.212] iteration 6970 : loss : 0.425363, loss_ce: 0.044214
[18:15:33.248] iteration 6980 : loss : 0.453489, loss_ce: 0.058466
[18:15:37.297] iteration 6990 : loss : 0.307398, loss_ce: 0.070533
[18:15:41.341] iteration 7000 : loss : 0.298555, loss_ce: 0.030213
[18:15:45.401] iteration 7010 : loss : 0.286699, loss_ce: 0.073381
[18:15:49.451] iteration 7020 : loss : 0.279114, loss_ce: 0.043187
[18:15:53.511] iteration 7030 : loss : 0.431739, loss_ce: 0.045163
[18:15:57.560] iteration 7040 : loss : 0.411112, loss_ce: 0.050680
[18:16:01.621] iteration 7050 : loss : 0.396059, loss_ce: 0.067647
[18:16:05.676] iteration 7060 : loss : 0.406787, loss_ce: 0.062690
[18:16:09.740] iteration 7070 : loss : 0.376269, loss_ce: 0.048139
[18:16:13.797] iteration 7080 : loss : 0.297100, loss_ce: 0.018719
[18:16:17.862] iteration 7090 : loss : 0.128256, loss_ce: 0.021249
[18:16:21.919] iteration 7100 : loss : 0.132252, loss_ce: 0.017334
[18:16:25.986] iteration 7110 : loss : 0.356264, loss_ce: 0.035315
[18:16:30.047] iteration 7120 : loss : 0.082633, loss_ce: 0.015281
[18:16:34.117] iteration 7130 : loss : 0.120049, loss_ce: 0.014526
[18:16:38.176] iteration 7140 : loss : 0.134472, loss_ce: 0.032815
[18:16:42.248] iteration 7150 : loss : 0.252629, loss_ce: 0.013336
[18:16:46.315] iteration 7160 : loss : 0.320500, loss_ce: 0.020762
[18:16:50.393] iteration 7170 : loss : 0.257854, loss_ce: 0.017129
[18:16:54.460] iteration 7180 : loss : 0.270931, loss_ce: 0.006925
[18:16:58.536] iteration 7190 : loss : 0.226793, loss_ce: 0.016786
[18:17:02.604] iteration 7200 : loss : 0.059845, loss_ce: 0.005952
[18:17:06.683] iteration 7210 : loss : 0.255685, loss_ce: 0.017781
[18:17:10.752] iteration 7220 : loss : 0.340455, loss_ce: 0.023979
[18:17:14.831] iteration 7230 : loss : 0.280303, loss_ce: 0.010451
[18:24:20.875] iteration 7240 : loss : 0.483514, loss_ce: 0.082564
[18:24:24.924] iteration 7250 : loss : 0.457983, loss_ce: 0.030193
[18:24:28.967] iteration 7260 : loss : 0.435655, loss_ce: 0.042516
[18:24:33.025] iteration 7270 : loss : 0.423643, loss_ce: 0.040326
[18:24:37.070] iteration 7280 : loss : 0.415702, loss_ce: 0.040441
[18:24:41.132] iteration 7290 : loss : 0.329992, loss_ce: 0.082356
[18:24:45.180] iteration 7300 : loss : 0.439525, loss_ce: 0.050441
[18:24:49.244] iteration 7310 : loss : 0.446487, loss_ce: 0.039733
[18:24:53.294] iteration 7320 : loss : 0.333283, loss_ce: 0.025868
[18:24:57.357] iteration 7330 : loss : 0.407329, loss_ce: 0.037625
[18:25:01.413] iteration 7340 : loss : 0.193689, loss_ce: 0.026581
[18:25:05.482] iteration 7350 : loss : 0.328284, loss_ce: 0.035759
[18:25:09.538] iteration 7360 : loss : 0.359420, loss_ce: 0.012230
[18:25:13.606] iteration 7370 : loss : 0.302200, loss_ce: 0.015519
[18:25:17.672] iteration 7380 : loss : 0.234635, loss_ce: 0.023295
[18:25:21.748] iteration 7390 : loss : 0.249320, loss_ce: 0.013622
[18:25:25.808] iteration 7400 : loss : 0.267569, loss_ce: 0.021293
[18:25:29.882] iteration 7410 : loss : 0.297339, loss_ce: 0.016608
[18:25:33.944] iteration 7420 : loss : 0.250987, loss_ce: 0.022071
[18:25:38.095] iteration 7430 : loss : 0.244864, loss_ce: 0.021355
[18:25:42.161] iteration 7440 : loss : 0.199044, loss_ce: 0.015943
[18:25:46.236] iteration 7450 : loss : 0.208335, loss_ce: 0.010630
[18:25:50.304] iteration 7460 : loss : 0.321116, loss_ce: 0.008984
[18:25:54.379] iteration 7470 : loss : 0.266194, loss_ce: 0.029907
[18:25:58.447] iteration 7480 : loss : 0.218045, loss_ce: 0.016253
[18:26:02.526] iteration 7490 : loss : 0.240730, loss_ce: 0.013440
[18:26:06.593] iteration 7500 : loss : 0.408396, loss_ce: 0.037424
[18:33:21.311] iteration 7510 : loss : 0.470346, loss_ce: 0.050139
[18:33:25.353] iteration 7520 : loss : 0.476365, loss_ce: 0.073035
[18:33:29.409] iteration 7530 : loss : 0.460274, loss_ce: 0.053001
[18:33:33.456] iteration 7540 : loss : 0.441888, loss_ce: 0.062512
[18:33:37.515] iteration 7550 : loss : 0.445234, loss_ce: 0.040414
[18:33:41.560] iteration 7560 : loss : 0.442538, loss_ce: 0.039655
[18:33:45.621] iteration 7570 : loss : 0.429126, loss_ce: 0.029723
[18:33:49.674] iteration 7580 : loss : 0.289743, loss_ce: 0.066886
[18:33:53.737] iteration 7590 : loss : 0.439522, loss_ce: 0.078794
[18:33:57.796] iteration 7600 : loss : 0.276549, loss_ce: 0.048979
[18:34:01.868] iteration 7610 : loss : 0.270081, loss_ce: 0.059183
[18:34:05.929] iteration 7620 : loss : 0.400064, loss_ce: 0.037181
[18:34:10.001] iteration 7630 : loss : 0.413963, loss_ce: 0.056446
[18:34:14.062] iteration 7640 : loss : 0.394711, loss_ce: 0.062072
[18:34:18.139] iteration 7650 : loss : 0.380922, loss_ce: 0.051820
[18:34:22.204] iteration 7660 : loss : 0.370071, loss_ce: 0.031323
[18:34:26.283] iteration 7670 : loss : 0.363344, loss_ce: 0.036849
[18:34:30.350] iteration 7680 : loss : 0.315944, loss_ce: 0.013842
[18:34:34.431] iteration 7690 : loss : 0.280620, loss_ce: 0.048058
[18:34:38.500] iteration 7700 : loss : 0.301892, loss_ce: 0.023349
[18:34:42.580] iteration 7710 : loss : 0.258311, loss_ce: 0.017606
[18:34:46.652] iteration 7720 : loss : 0.349702, loss_ce: 0.030892
[18:34:50.734] iteration 7730 : loss : 0.356595, loss_ce: 0.046184
[18:34:54.804] iteration 7740 : loss : 0.270749, loss_ce: 0.021443
[18:34:58.886] iteration 7750 : loss : 0.257791, loss_ce: 0.012331
[18:35:02.957] iteration 7760 : loss : 0.098367, loss_ce: 0.015250
[18:35:07.037] iteration 7770 : loss : 0.347454, loss_ce: 0.019904
[18:42:23.434] iteration 7780 : loss : 0.463339, loss_ce: 0.075705
[18:42:27.483] iteration 7790 : loss : 0.448068, loss_ce: 0.051711
[18:42:31.525] iteration 7800 : loss : 0.449001, loss_ce: 0.067386
[18:42:35.581] iteration 7810 : loss : 0.446514, loss_ce: 0.050636
[18:42:39.629] iteration 7820 : loss : 0.425984, loss_ce: 0.064948
[18:42:43.686] iteration 7830 : loss : 0.429869, loss_ce: 0.080347
[18:42:47.737] iteration 7840 : loss : 0.442917, loss_ce: 0.045098
[18:42:51.797] iteration 7850 : loss : 0.279830, loss_ce: 0.045411
[18:42:55.850] iteration 7860 : loss : 0.434897, loss_ce: 0.064262
[18:42:59.915] iteration 7870 : loss : 0.426743, loss_ce: 0.047940
[18:43:03.971] iteration 7880 : loss : 0.265524, loss_ce: 0.033567
[18:43:08.040] iteration 7890 : loss : 0.242235, loss_ce: 0.044060
[18:43:12.102] iteration 7900 : loss : 0.239133, loss_ce: 0.042406
[18:43:16.174] iteration 7910 : loss : 0.251491, loss_ce: 0.029551
[18:43:20.239] iteration 7920 : loss : 0.191901, loss_ce: 0.014492
[18:43:24.313] iteration 7930 : loss : 0.351191, loss_ce: 0.029984
[18:43:28.379] iteration 7940 : loss : 0.333713, loss_ce: 0.023161
[18:43:32.456] iteration 7950 : loss : 0.182189, loss_ce: 0.032669
[18:43:36.521] iteration 7960 : loss : 0.347621, loss_ce: 0.024503
[18:43:40.596] iteration 7970 : loss : 0.096529, loss_ce: 0.006940
[18:43:44.664] iteration 7980 : loss : 0.315858, loss_ce: 0.021611
[18:43:48.742] iteration 7990 : loss : 0.339767, loss_ce: 0.026495
[18:43:52.812] iteration 8000 : loss : 0.355086, loss_ce: 0.018326
[18:43:56.887] iteration 8010 : loss : 0.348110, loss_ce: 0.028679
[18:44:00.956] iteration 8020 : loss : 0.146630, loss_ce: 0.011015
[18:44:05.036] iteration 8030 : loss : 0.317356, loss_ce: 0.018894
[18:44:09.014] iteration 8040 : loss : 0.175484, loss_ce: 0.011164
[18:51:02.934] save model to ./finetune_tpgm_kits23_continual_500iter\finetuned_epoch_29.pth
[18:51:17.205] iteration 8050 : loss : 0.314794, loss_ce: 0.038172
[18:51:21.253] iteration 8060 : loss : 0.463403, loss_ce: 0.054595
[18:51:25.307] iteration 8070 : loss : 0.452097, loss_ce: 0.055270
[18:51:29.351] iteration 8080 : loss : 0.298622, loss_ce: 0.058859
[18:51:33.410] iteration 8090 : loss : 0.289482, loss_ce: 0.063772
[18:51:37.460] iteration 8100 : loss : 0.438894, loss_ce: 0.058445
[18:51:41.520] iteration 8110 : loss : 0.285317, loss_ce: 0.051945
[18:51:45.572] iteration 8120 : loss : 0.435900, loss_ce: 0.036984
[18:51:49.632] iteration 8130 : loss : 0.289986, loss_ce: 0.075760
[18:51:53.688] iteration 8140 : loss : 0.424805, loss_ce: 0.041125
[18:51:57.753] iteration 8150 : loss : 0.429457, loss_ce: 0.044846
[18:52:01.810] iteration 8160 : loss : 0.407944, loss_ce: 0.055802
[18:52:05.878] iteration 8170 : loss : 0.420513, loss_ce: 0.038009
[18:52:09.937] iteration 8180 : loss : 0.421307, loss_ce: 0.047451
[18:52:14.014] iteration 8190 : loss : 0.403112, loss_ce: 0.048710
[18:52:18.079] iteration 8200 : loss : 0.388279, loss_ce: 0.040542
[18:52:22.157] iteration 8210 : loss : 0.223628, loss_ce: 0.032702
[18:52:26.223] iteration 8220 : loss : 0.396805, loss_ce: 0.021413
[18:52:30.300] iteration 8230 : loss : 0.226209, loss_ce: 0.044706
[18:52:34.369] iteration 8240 : loss : 0.355823, loss_ce: 0.048349
[18:52:38.448] iteration 8250 : loss : 0.327009, loss_ce: 0.054881
[18:52:42.519] iteration 8260 : loss : 0.370330, loss_ce: 0.036135
[18:52:46.596] iteration 8270 : loss : 0.343669, loss_ce: 0.028608
[18:52:50.669] iteration 8280 : loss : 0.282795, loss_ce: 0.015375
[18:52:54.749] iteration 8290 : loss : 0.111079, loss_ce: 0.016353
[18:52:58.820] iteration 8300 : loss : 0.145163, loss_ce: 0.019038
[19:00:21.405] iteration 8310 : loss : 0.444792, loss_ce: 0.107126
[19:00:25.446] iteration 8320 : loss : 0.472452, loss_ce: 0.061069
[19:00:29.499] iteration 8330 : loss : 0.471889, loss_ce: 0.059393
[19:00:33.543] iteration 8340 : loss : 0.319213, loss_ce: 0.069867
[19:00:37.601] iteration 8350 : loss : 0.450224, loss_ce: 0.045236
[19:00:41.651] iteration 8360 : loss : 0.452540, loss_ce: 0.080632
[19:00:45.714] iteration 8370 : loss : 0.442148, loss_ce: 0.055224
[19:00:49.767] iteration 8380 : loss : 0.445191, loss_ce: 0.068597
[19:00:53.839] iteration 8390 : loss : 0.442710, loss_ce: 0.068840
[19:00:57.908] iteration 8400 : loss : 0.432282, loss_ce: 0.042440
[19:01:01.979] iteration 8410 : loss : 0.424915, loss_ce: 0.049981
[19:01:06.039] iteration 8420 : loss : 0.446156, loss_ce: 0.073683
[19:01:10.107] iteration 8430 : loss : 0.274025, loss_ce: 0.066401
[19:01:14.171] iteration 8440 : loss : 0.274216, loss_ce: 0.061274
[19:01:18.247] iteration 8450 : loss : 0.430014, loss_ce: 0.037110
[19:01:22.315] iteration 8460 : loss : 0.421730, loss_ce: 0.031824
[19:01:26.391] iteration 8470 : loss : 0.430979, loss_ce: 0.049591
[19:01:30.457] iteration 8480 : loss : 0.401079, loss_ce: 0.058168
[19:01:34.533] iteration 8490 : loss : 0.417013, loss_ce: 0.054523
[19:01:38.603] iteration 8500 : loss : 0.436881, loss_ce: 0.057662
[19:01:42.680] iteration 8510 : loss : 0.399948, loss_ce: 0.033878
[19:01:46.753] iteration 8520 : loss : 0.384316, loss_ce: 0.030749
[19:01:50.836] iteration 8530 : loss : 0.363284, loss_ce: 0.028512
[19:01:54.908] iteration 8540 : loss : 0.180316, loss_ce: 0.012658
[19:01:58.992] iteration 8550 : loss : 0.317784, loss_ce: 0.015266
[19:02:03.065] iteration 8560 : loss : 0.312009, loss_ce: 0.023790
[19:02:07.148] iteration 8570 : loss : 0.338971, loss_ce: 0.032442
[19:09:35.078] iteration 8580 : loss : 0.329970, loss_ce: 0.036616
[19:09:39.127] iteration 8590 : loss : 0.476945, loss_ce: 0.066182
[19:09:43.174] iteration 8600 : loss : 0.466385, loss_ce: 0.071912
[19:09:47.235] iteration 8610 : loss : 0.306898, loss_ce: 0.080324
[19:09:51.283] iteration 8620 : loss : 0.373339, loss_ce: 0.039002
[19:09:55.345] iteration 8630 : loss : 0.387215, loss_ce: 0.034261
[19:09:59.400] iteration 8640 : loss : 0.391573, loss_ce: 0.159672
[19:10:03.467] iteration 8650 : loss : 0.351886, loss_ce: 0.040553
[19:10:07.523] iteration 8660 : loss : 0.339437, loss_ce: 0.049091
[19:10:11.594] iteration 8670 : loss : 0.322916, loss_ce: 0.015344
[19:10:15.652] iteration 8680 : loss : 0.196315, loss_ce: 0.055725
[19:10:19.725] iteration 8690 : loss : 0.164487, loss_ce: 0.002670
[19:10:23.793] iteration 8700 : loss : 0.322394, loss_ce: 0.007986
[19:10:27.866] iteration 8710 : loss : 0.302975, loss_ce: 0.008357
[19:10:31.928] iteration 8720 : loss : 0.254598, loss_ce: 0.017259
[19:10:36.002] iteration 8730 : loss : 0.267683, loss_ce: 0.040860
[19:10:40.066] iteration 8740 : loss : 0.368448, loss_ce: 0.029249
[19:10:44.139] iteration 8750 : loss : 0.197760, loss_ce: 0.063536
[19:10:48.205] iteration 8760 : loss : 0.311730, loss_ce: 0.056774
[19:10:52.284] iteration 8770 : loss : 0.196219, loss_ce: 0.015023
[19:10:56.356] iteration 8780 : loss : 0.302264, loss_ce: 0.016496
[19:11:00.439] iteration 8790 : loss : 0.289974, loss_ce: 0.039450
[19:11:04.510] iteration 8800 : loss : 0.317452, loss_ce: 0.023523
[19:11:08.594] iteration 8810 : loss : 0.266654, loss_ce: 0.016145
[19:11:12.664] iteration 8820 : loss : 0.299013, loss_ce: 0.005946
[19:11:16.746] iteration 8830 : loss : 0.233961, loss_ce: 0.011821
[19:11:20.821] iteration 8840 : loss : 0.282127, loss_ce: 0.012594
[19:18:22.502] iteration 8850 : loss : 0.468310, loss_ce: 0.063552
[19:18:26.543] iteration 8860 : loss : 0.480925, loss_ce: 0.082679
[19:18:30.594] iteration 8870 : loss : 0.440519, loss_ce: 0.067452
[19:18:34.638] iteration 8880 : loss : 0.209166, loss_ce: 0.020576
[19:18:38.696] iteration 8890 : loss : 0.329480, loss_ce: 0.009503
[19:18:42.743] iteration 8900 : loss : 0.326172, loss_ce: 0.009883
[19:18:46.807] iteration 8910 : loss : 0.336546, loss_ce: 0.025427
[19:18:50.860] iteration 8920 : loss : 0.310340, loss_ce: 0.014732
[19:18:54.923] iteration 8930 : loss : 0.293435, loss_ce: 0.024372
[19:18:58.973] iteration 8940 : loss : 0.307968, loss_ce: 0.011755
[19:19:03.040] iteration 8950 : loss : 0.183917, loss_ce: 0.014416
[19:19:07.096] iteration 8960 : loss : 0.334950, loss_ce: 0.015972
[19:19:11.163] iteration 8970 : loss : 0.256617, loss_ce: 0.020553
[19:19:15.222] iteration 8980 : loss : 0.317303, loss_ce: 0.013040
[19:19:19.289] iteration 8990 : loss : 0.223295, loss_ce: 0.015606
[19:19:23.350] iteration 9000 : loss : 0.158556, loss_ce: 0.020150
[19:19:27.420] iteration 9010 : loss : 0.333707, loss_ce: 0.046382
[19:19:31.483] iteration 9020 : loss : 0.328637, loss_ce: 0.023145
[19:19:35.556] iteration 9030 : loss : 0.116333, loss_ce: 0.013053
[19:19:39.621] iteration 9040 : loss : 0.246822, loss_ce: 0.016917
[19:19:43.696] iteration 9050 : loss : 0.206740, loss_ce: 0.007834
[19:19:47.765] iteration 9060 : loss : 0.119645, loss_ce: 0.012499
[19:19:51.843] iteration 9070 : loss : 0.251446, loss_ce: 0.011893
[19:19:55.912] iteration 9080 : loss : 0.059495, loss_ce: 0.003183
[19:19:59.989] iteration 9090 : loss : 0.320689, loss_ce: 0.019616
[19:20:04.058] iteration 9100 : loss : 0.174069, loss_ce: 0.018414
[19:20:08.139] iteration 9110 : loss : 0.298653, loss_ce: 0.011963
[19:27:20.139] iteration 9120 : loss : 0.444576, loss_ce: 0.028827
[19:27:24.195] iteration 9130 : loss : 0.384913, loss_ce: 0.064223
[19:27:28.239] iteration 9140 : loss : 0.141908, loss_ce: 0.028219
[19:27:32.295] iteration 9150 : loss : 0.331591, loss_ce: 0.011160
[19:27:36.342] iteration 9160 : loss : 0.200659, loss_ce: 0.043880
[19:27:40.400] iteration 9170 : loss : 0.316257, loss_ce: 0.052253
[19:27:44.453] iteration 9180 : loss : 0.319641, loss_ce: 0.014755
[19:27:48.512] iteration 9190 : loss : 0.289520, loss_ce: 0.013212
[19:27:52.565] iteration 9200 : loss : 0.325554, loss_ce: 0.037455
[19:27:56.632] iteration 9210 : loss : 0.143087, loss_ce: 0.015188
[19:28:00.685] iteration 9220 : loss : 0.167384, loss_ce: 0.027028
[19:28:04.749] iteration 9230 : loss : 0.310011, loss_ce: 0.005431
[19:28:08.808] iteration 9240 : loss : 0.323775, loss_ce: 0.004713
[19:28:12.872] iteration 9250 : loss : 0.294163, loss_ce: 0.011902
[19:28:16.930] iteration 9260 : loss : 0.175964, loss_ce: 0.011360
[19:28:21.004] iteration 9270 : loss : 0.276366, loss_ce: 0.012518
[19:28:25.067] iteration 9280 : loss : 0.378781, loss_ce: 0.034956
[19:28:29.140] iteration 9290 : loss : 0.309415, loss_ce: 0.009439
[19:28:33.205] iteration 9300 : loss : 0.329130, loss_ce: 0.009761
[19:28:37.278] iteration 9310 : loss : 0.102246, loss_ce: 0.015014
[19:28:41.344] iteration 9320 : loss : 0.268746, loss_ce: 0.036611
[19:28:45.421] iteration 9330 : loss : 0.214299, loss_ce: 0.010730
[19:28:49.487] iteration 9340 : loss : 0.309148, loss_ce: 0.013064
[19:28:53.567] iteration 9350 : loss : 0.085603, loss_ce: 0.008215
[19:28:57.634] iteration 9360 : loss : 0.299194, loss_ce: 0.012998
[19:29:01.712] iteration 9370 : loss : 0.132885, loss_ce: 0.012851
[19:29:05.685] iteration 9380 : loss : 0.267459, loss_ce: 0.005545
[19:36:19.985] iteration 9390 : loss : 0.338958, loss_ce: 0.084637
[19:36:24.027] iteration 9400 : loss : 0.310474, loss_ce: 0.056998
[19:36:28.080] iteration 9410 : loss : 0.462673, loss_ce: 0.076992
[19:36:32.127] iteration 9420 : loss : 0.453149, loss_ce: 0.062895
[19:36:36.184] iteration 9430 : loss : 0.449155, loss_ce: 0.054106
[19:36:40.232] iteration 9440 : loss : 0.456877, loss_ce: 0.058075
[19:36:44.295] iteration 9450 : loss : 0.436305, loss_ce: 0.062291
[19:36:48.346] iteration 9460 : loss : 0.440559, loss_ce: 0.059388
[19:36:52.408] iteration 9470 : loss : 0.448649, loss_ce: 0.080006
[19:36:56.461] iteration 9480 : loss : 0.425546, loss_ce: 0.046991
[19:37:00.525] iteration 9490 : loss : 0.426748, loss_ce: 0.059912
[19:37:04.585] iteration 9500 : loss : 0.294148, loss_ce: 0.073406
[19:37:08.649] iteration 9510 : loss : 0.414358, loss_ce: 0.040373
[19:37:12.710] iteration 9520 : loss : 0.242515, loss_ce: 0.036043
[19:37:16.782] iteration 9530 : loss : 0.409169, loss_ce: 0.050736
[19:37:20.848] iteration 9540 : loss : 0.238801, loss_ce: 0.037419
[19:37:24.922] iteration 9550 : loss : 0.365106, loss_ce: 0.031565
[19:37:28.989] iteration 9560 : loss : 0.347547, loss_ce: 0.031153
[19:37:33.065] iteration 9570 : loss : 0.382432, loss_ce: 0.026068
[19:37:37.133] iteration 9580 : loss : 0.388657, loss_ce: 0.053207
[19:37:41.210] iteration 9590 : loss : 0.329130, loss_ce: 0.033413
[19:37:45.280] iteration 9600 : loss : 0.212795, loss_ce: 0.038483
[19:37:49.360] iteration 9610 : loss : 0.364595, loss_ce: 0.026834
[19:37:53.433] iteration 9620 : loss : 0.304877, loss_ce: 0.019856
[19:37:57.516] iteration 9630 : loss : 0.327318, loss_ce: 0.034737
[19:38:01.587] iteration 9640 : loss : 0.312681, loss_ce: 0.008432
[19:45:12.215] iteration 9650 : loss : 0.445507, loss_ce: 0.038272
[19:45:16.258] iteration 9660 : loss : 0.377320, loss_ce: 0.073384
[19:45:20.312] iteration 9670 : loss : 0.401239, loss_ce: 0.092984
[19:45:24.356] iteration 9680 : loss : 0.348199, loss_ce: 0.024791
[19:45:28.417] iteration 9690 : loss : 0.221926, loss_ce: 0.056771
[19:45:32.474] iteration 9700 : loss : 0.176606, loss_ce: 0.012868
[19:45:36.540] iteration 9710 : loss : 0.329795, loss_ce: 0.038850
[19:45:40.592] iteration 9720 : loss : 0.330085, loss_ce: 0.025578
[19:45:44.657] iteration 9730 : loss : 0.295039, loss_ce: 0.015469
[19:45:48.714] iteration 9740 : loss : 0.325149, loss_ce: 0.010758
[19:45:52.782] iteration 9750 : loss : 0.307180, loss_ce: 0.029655
[19:45:56.845] iteration 9760 : loss : 0.295939, loss_ce: 0.024480
[19:46:00.917] iteration 9770 : loss : 0.279665, loss_ce: 0.018590
[19:46:04.978] iteration 9780 : loss : 0.149820, loss_ce: 0.008254
[19:46:09.291] iteration 9790 : loss : 0.245256, loss_ce: 0.015412
[19:46:13.351] iteration 9800 : loss : 0.245416, loss_ce: 0.017121
[19:46:17.426] iteration 9810 : loss : 0.289242, loss_ce: 0.017081
[19:46:21.491] iteration 9820 : loss : 0.310953, loss_ce: 0.005750
[19:46:25.567] iteration 9830 : loss : 0.330957, loss_ce: 0.032085
[19:46:29.634] iteration 9840 : loss : 0.171638, loss_ce: 0.010388
[19:46:33.711] iteration 9850 : loss : 0.258984, loss_ce: 0.010982
[19:46:37.776] iteration 9860 : loss : 0.324337, loss_ce: 0.029045
[19:46:41.858] iteration 9870 : loss : 0.280990, loss_ce: 0.008247
[19:46:45.928] iteration 9880 : loss : 0.253810, loss_ce: 0.015143
[19:46:50.011] iteration 9890 : loss : 0.077801, loss_ce: 0.013421
[19:46:54.082] iteration 9900 : loss : 0.328624, loss_ce: 0.022019
[19:46:58.165] iteration 9910 : loss : 0.104553, loss_ce: 0.012262
[19:54:21.635] iteration 9920 : loss : 0.368924, loss_ce: 0.037168
[19:54:25.692] iteration 9930 : loss : 0.417725, loss_ce: 0.110032
[19:54:29.738] iteration 9940 : loss : 0.366365, loss_ce: 0.041660
[19:54:33.795] iteration 9950 : loss : 0.338858, loss_ce: 0.020702
[19:54:37.844] iteration 9960 : loss : 0.353708, loss_ce: 0.045936
[19:54:41.929] iteration 9970 : loss : 0.154725, loss_ce: 0.026102
[19:54:45.981] iteration 9980 : loss : 0.302190, loss_ce: 0.036168
[19:54:50.044] iteration 9990 : loss : 0.359693, loss_ce: 0.083579
[19:54:54.098] iteration 10000 : loss : 0.241292, loss_ce: 0.014556
[19:54:58.167] iteration 10010 : loss : 0.263501, loss_ce: 0.022900
[19:55:02.223] iteration 10020 : loss : 0.272422, loss_ce: 0.019479
[19:55:06.302] iteration 10030 : loss : 0.324265, loss_ce: 0.007872
[19:55:10.365] iteration 10040 : loss : 0.314152, loss_ce: 0.003396
[19:55:14.439] iteration 10050 : loss : 0.066501, loss_ce: 0.018468
[19:55:18.505] iteration 10060 : loss : 0.249700, loss_ce: 0.015432
[19:55:22.583] iteration 10070 : loss : 0.170683, loss_ce: 0.009807
[19:55:26.652] iteration 10080 : loss : 0.240640, loss_ce: 0.052470
[19:55:30.732] iteration 10090 : loss : 0.167272, loss_ce: 0.012763
[19:55:34.797] iteration 10100 : loss : 0.330939, loss_ce: 0.015715
[19:55:38.876] iteration 10110 : loss : 0.283011, loss_ce: 0.018219
[19:55:42.946] iteration 10120 : loss : 0.270743, loss_ce: 0.042640
[19:55:47.022] iteration 10130 : loss : 0.155292, loss_ce: 0.017586
[19:55:51.093] iteration 10140 : loss : 0.104647, loss_ce: 0.014087
[19:55:55.175] iteration 10150 : loss : 0.214075, loss_ce: 0.022051
[19:55:59.244] iteration 10160 : loss : 0.254871, loss_ce: 0.013385
[19:56:03.325] iteration 10170 : loss : 0.247275, loss_ce: 0.013036
[19:56:07.399] iteration 10180 : loss : 0.272233, loss_ce: 0.023949
[20:03:27.676] iteration 10190 : loss : 0.357707, loss_ce: 0.047488
[20:03:31.716] iteration 10200 : loss : 0.366097, loss_ce: 0.070511
[20:03:35.774] iteration 10210 : loss : 0.383882, loss_ce: 0.065024
[20:03:39.824] iteration 10220 : loss : 0.345665, loss_ce: 0.043259
[20:03:43.885] iteration 10230 : loss : 0.176979, loss_ce: 0.014123
[20:03:47.939] iteration 10240 : loss : 0.311752, loss_ce: 0.007706
[20:03:52.008] iteration 10250 : loss : 0.316493, loss_ce: 0.013002
[20:03:56.070] iteration 10260 : loss : 0.335744, loss_ce: 0.024053
[20:04:00.137] iteration 10270 : loss : 0.325060, loss_ce: 0.028625
[20:04:04.200] iteration 10280 : loss : 0.294010, loss_ce: 0.015245
[20:04:08.276] iteration 10290 : loss : 0.293980, loss_ce: 0.018440
[20:04:12.335] iteration 10300 : loss : 0.283976, loss_ce: 0.013251
[20:04:16.410] iteration 10310 : loss : 0.182926, loss_ce: 0.021036
[20:04:20.475] iteration 10320 : loss : 0.241467, loss_ce: 0.021240
[20:04:24.548] iteration 10330 : loss : 0.235979, loss_ce: 0.012094
[20:04:28.617] iteration 10340 : loss : 0.128314, loss_ce: 0.018817
[20:04:32.690] iteration 10350 : loss : 0.159936, loss_ce: 0.007708
[20:04:36.778] iteration 10360 : loss : 0.270923, loss_ce: 0.017760
[20:04:40.866] iteration 10370 : loss : 0.319562, loss_ce: 0.023934
[20:04:44.940] iteration 10380 : loss : 0.314234, loss_ce: 0.006870
[20:04:49.023] iteration 10390 : loss : 0.246708, loss_ce: 0.013331
[20:04:53.123] iteration 10400 : loss : 0.198517, loss_ce: 0.006645
[20:04:57.206] iteration 10410 : loss : 0.163577, loss_ce: 0.003844
[20:05:01.278] iteration 10420 : loss : 0.275733, loss_ce: 0.019517
[20:05:05.360] iteration 10430 : loss : 0.261222, loss_ce: 0.023871
[20:05:09.436] iteration 10440 : loss : 0.321641, loss_ce: 0.011727
[20:05:13.519] iteration 10450 : loss : 0.242689, loss_ce: 0.015861
[20:12:41.724] iteration 10460 : loss : 0.351897, loss_ce: 0.051943
[20:12:45.780] iteration 10470 : loss : 0.373463, loss_ce: 0.063101
[20:12:49.831] iteration 10480 : loss : 0.353697, loss_ce: 0.050979
[20:12:53.893] iteration 10490 : loss : 0.328691, loss_ce: 0.023730
[20:12:57.947] iteration 10500 : loss : 0.180103, loss_ce: 0.026051
[20:13:02.014] iteration 10510 : loss : 0.367873, loss_ce: 0.095396
[20:13:06.068] iteration 10520 : loss : 0.317940, loss_ce: 0.007480
[20:13:10.141] iteration 10530 : loss : 0.325412, loss_ce: 0.020083
[20:13:14.198] iteration 10540 : loss : 0.342222, loss_ce: 0.042962
[20:13:18.268] iteration 10550 : loss : 0.314373, loss_ce: 0.005867
[20:13:22.331] iteration 10560 : loss : 0.328354, loss_ce: 0.010498
[20:13:26.404] iteration 10570 : loss : 0.314617, loss_ce: 0.006262
[20:13:30.465] iteration 10580 : loss : 0.234058, loss_ce: 0.010521
[20:13:34.538] iteration 10590 : loss : 0.288262, loss_ce: 0.100479
[20:13:38.602] iteration 10600 : loss : 0.149180, loss_ce: 0.013793
[20:13:42.679] iteration 10610 : loss : 0.198385, loss_ce: 0.009217
[20:13:46.745] iteration 10620 : loss : 0.210034, loss_ce: 0.007816
[20:13:50.822] iteration 10630 : loss : 0.328472, loss_ce: 0.017193
[20:13:54.889] iteration 10640 : loss : 0.203444, loss_ce: 0.024690
[20:13:58.971] iteration 10650 : loss : 0.327056, loss_ce: 0.029647
[20:14:03.041] iteration 10660 : loss : 0.300776, loss_ce: 0.040390
[20:14:07.121] iteration 10670 : loss : 0.263902, loss_ce: 0.031724
[20:14:11.195] iteration 10680 : loss : 0.309332, loss_ce: 0.011087
[20:14:15.280] iteration 10690 : loss : 0.220514, loss_ce: 0.008047
[20:14:19.356] iteration 10700 : loss : 0.085050, loss_ce: 0.010204
[20:14:23.449] iteration 10710 : loss : 0.215832, loss_ce: 0.009262
[20:14:27.437] iteration 10720 : loss : 0.311217, loss_ce: 0.004516
[20:21:43.013] save model to ./finetune_tpgm_kits23_continual_500iter\finetuned_epoch_39.pth
[20:21:57.555] iteration 10730 : loss : 0.366416, loss_ce: 0.050063
[20:22:01.596] iteration 10740 : loss : 0.353421, loss_ce: 0.056892
[20:22:05.652] iteration 10750 : loss : 0.170729, loss_ce: 0.015816
[20:22:09.699] iteration 10760 : loss : 0.173446, loss_ce: 0.018247
[20:22:13.761] iteration 10770 : loss : 0.329667, loss_ce: 0.024508
[20:22:17.813] iteration 10780 : loss : 0.332218, loss_ce: 0.011912
[20:22:21.877] iteration 10790 : loss : 0.316798, loss_ce: 0.007546
[20:22:25.932] iteration 10800 : loss : 0.320930, loss_ce: 0.016430
[20:22:29.998] iteration 10810 : loss : 0.315087, loss_ce: 0.007371
[20:22:34.056] iteration 10820 : loss : 0.171143, loss_ce: 0.005818
[20:22:38.127] iteration 10830 : loss : 0.336612, loss_ce: 0.037438
[20:22:42.188] iteration 10840 : loss : 0.316881, loss_ce: 0.015737
[20:22:46.258] iteration 10850 : loss : 0.319605, loss_ce: 0.021342
[20:22:50.323] iteration 10860 : loss : 0.318082, loss_ce: 0.009094
[20:22:54.399] iteration 10870 : loss : 0.305536, loss_ce: 0.014976
[20:22:58.465] iteration 10880 : loss : 0.338687, loss_ce: 0.006150
[20:23:02.540] iteration 10890 : loss : 0.283486, loss_ce: 0.039069
[20:23:06.610] iteration 10900 : loss : 0.283298, loss_ce: 0.025416
[20:23:10.685] iteration 10910 : loss : 0.303103, loss_ce: 0.009871
[20:23:14.756] iteration 10920 : loss : 0.228200, loss_ce: 0.042607
[20:23:18.834] iteration 10930 : loss : 0.315567, loss_ce: 0.010292
[20:23:22.903] iteration 10940 : loss : 0.318806, loss_ce: 0.021175
[20:23:26.984] iteration 10950 : loss : 0.230384, loss_ce: 0.015075
[20:23:31.052] iteration 10960 : loss : 0.172458, loss_ce: 0.006102
[20:23:35.135] iteration 10970 : loss : 0.228531, loss_ce: 0.015024
[20:23:39.205] iteration 10980 : loss : 0.239841, loss_ce: 0.014571
[20:31:07.163] iteration 10990 : loss : 0.463122, loss_ce: 0.060251
[20:31:11.205] iteration 11000 : loss : 0.341602, loss_ce: 0.041375
[20:31:15.262] iteration 11010 : loss : 0.344316, loss_ce: 0.042325
[20:31:19.309] iteration 11020 : loss : 0.355286, loss_ce: 0.053879
[20:31:23.371] iteration 11030 : loss : 0.173397, loss_ce: 0.013763
[20:31:27.432] iteration 11040 : loss : 0.326600, loss_ce: 0.027745
[20:31:31.495] iteration 11050 : loss : 0.185555, loss_ce: 0.018182
[20:31:35.549] iteration 11060 : loss : 0.339990, loss_ce: 0.045050
[20:31:39.613] iteration 11070 : loss : 0.171905, loss_ce: 0.019588
[20:31:43.667] iteration 11080 : loss : 0.325087, loss_ce: 0.021895
[20:31:47.732] iteration 11090 : loss : 0.267642, loss_ce: 0.032552
[20:31:51.791] iteration 11100 : loss : 0.318846, loss_ce: 0.015743
[20:31:55.861] iteration 11110 : loss : 0.288498, loss_ce: 0.022078
[20:31:59.919] iteration 11120 : loss : 0.319600, loss_ce: 0.020558
[20:32:03.992] iteration 11130 : loss : 0.269226, loss_ce: 0.014330
[20:32:08.053] iteration 11140 : loss : 0.291267, loss_ce: 0.014663
[20:32:12.126] iteration 11150 : loss : 0.314830, loss_ce: 0.020264
[20:32:16.190] iteration 11160 : loss : 0.220243, loss_ce: 0.010331
[20:32:20.261] iteration 11170 : loss : 0.148425, loss_ce: 0.005324
[20:32:24.326] iteration 11180 : loss : 0.074251, loss_ce: 0.010364
[20:32:28.406] iteration 11190 : loss : 0.334046, loss_ce: 0.015171
[20:32:32.471] iteration 11200 : loss : 0.251071, loss_ce: 0.005653
[20:32:36.550] iteration 11210 : loss : 0.320078, loss_ce: 0.012691
[20:32:40.618] iteration 11220 : loss : 0.310038, loss_ce: 0.015645
[20:32:44.696] iteration 11230 : loss : 0.262111, loss_ce: 0.012882
[20:32:48.765] iteration 11240 : loss : 0.232262, loss_ce: 0.009698
[20:32:52.842] iteration 11250 : loss : 0.162973, loss_ce: 0.005400
[20:40:11.192] iteration 11260 : loss : 0.401696, loss_ce: 0.055383
[20:40:15.251] iteration 11270 : loss : 0.368638, loss_ce: 0.080748
[20:40:19.297] iteration 11280 : loss : 0.377070, loss_ce: 0.106528
[20:40:23.355] iteration 11290 : loss : 0.332449, loss_ce: 0.034683
[20:40:27.406] iteration 11300 : loss : 0.049007, loss_ce: 0.016048
[20:40:31.471] iteration 11310 : loss : 0.338518, loss_ce: 0.035771
[20:40:35.525] iteration 11320 : loss : 0.351772, loss_ce: 0.076340
[20:40:39.591] iteration 11330 : loss : 0.326667, loss_ce: 0.021990
[20:40:43.646] iteration 11340 : loss : 0.332168, loss_ce: 0.034095
[20:40:47.717] iteration 11350 : loss : 0.179510, loss_ce: 0.020049
[20:40:51.775] iteration 11360 : loss : 0.356909, loss_ce: 0.083650
[20:40:55.846] iteration 11370 : loss : 0.369175, loss_ce: 0.067591
[20:40:59.906] iteration 11380 : loss : 0.321258, loss_ce: 0.030377
[20:41:03.975] iteration 11390 : loss : 0.329872, loss_ce: 0.029488
[20:41:08.036] iteration 11400 : loss : 0.183430, loss_ce: 0.010328
[20:41:12.111] iteration 11410 : loss : 0.336631, loss_ce: 0.008637
[20:41:16.177] iteration 11420 : loss : 0.335297, loss_ce: 0.018065
[20:41:20.255] iteration 11430 : loss : 0.355059, loss_ce: 0.090995
[20:41:24.323] iteration 11440 : loss : 0.312590, loss_ce: 0.008754
[20:41:28.402] iteration 11450 : loss : 0.335964, loss_ce: 0.040848
[20:41:32.470] iteration 11460 : loss : 0.325210, loss_ce: 0.020775
[20:41:36.553] iteration 11470 : loss : 0.170360, loss_ce: 0.022032
[20:41:40.625] iteration 11480 : loss : 0.344693, loss_ce: 0.027070
[20:41:44.706] iteration 11490 : loss : 0.158557, loss_ce: 0.002114
[20:41:48.774] iteration 11500 : loss : 0.325036, loss_ce: 0.007984
[20:41:52.856] iteration 11510 : loss : 0.323495, loss_ce: 0.031894
[20:41:56.925] iteration 11520 : loss : 0.165465, loss_ce: 0.012687
[20:49:26.974] iteration 11530 : loss : 0.377498, loss_ce: 0.041683
[20:49:31.019] iteration 11540 : loss : 0.410162, loss_ce: 0.084205
[20:49:35.076] iteration 11550 : loss : 0.335834, loss_ce: 0.027568
[20:49:39.127] iteration 11560 : loss : 0.357219, loss_ce: 0.019557
[20:49:43.182] iteration 11570 : loss : 0.255168, loss_ce: 0.020717
[20:49:47.235] iteration 11580 : loss : 0.323054, loss_ce: 0.011717
[20:49:51.296] iteration 11590 : loss : 0.290621, loss_ce: 0.057237
[20:49:55.350] iteration 11600 : loss : 0.268308, loss_ce: 0.024145
[20:49:59.416] iteration 11610 : loss : 0.302578, loss_ce: 0.044311
[20:50:03.473] iteration 11620 : loss : 0.262680, loss_ce: 0.026396
[20:50:07.539] iteration 11630 : loss : 0.246700, loss_ce: 0.018459
[20:50:11.598] iteration 11640 : loss : 0.240716, loss_ce: 0.023064
[20:50:15.668] iteration 11650 : loss : 0.139061, loss_ce: 0.011590
[20:50:19.731] iteration 11660 : loss : 0.195326, loss_ce: 0.009688
[20:50:23.804] iteration 11670 : loss : 0.243706, loss_ce: 0.007809
[20:50:27.870] iteration 11680 : loss : 0.300865, loss_ce: 0.020020
[20:50:31.948] iteration 11690 : loss : 0.276010, loss_ce: 0.015255
[20:50:36.015] iteration 11700 : loss : 0.209636, loss_ce: 0.006578
[20:50:40.100] iteration 11710 : loss : 0.170561, loss_ce: 0.014450
[20:50:44.168] iteration 11720 : loss : 0.212131, loss_ce: 0.015731
[20:50:48.248] iteration 11730 : loss : 0.175338, loss_ce: 0.023816
[20:50:52.315] iteration 11740 : loss : 0.318280, loss_ce: 0.008826
[20:50:56.398] iteration 11750 : loss : 0.267715, loss_ce: 0.020628
[20:51:00.473] iteration 11760 : loss : 0.097376, loss_ce: 0.022944
[20:51:04.558] iteration 11770 : loss : 0.226089, loss_ce: 0.021576
[20:51:08.631] iteration 11780 : loss : 0.236530, loss_ce: 0.021705
[20:51:12.716] iteration 11790 : loss : 0.221091, loss_ce: 0.017713
[20:58:41.840] iteration 11800 : loss : 0.438902, loss_ce: 0.177882
[20:58:45.893] iteration 11810 : loss : 0.358358, loss_ce: 0.057989
[20:58:49.940] iteration 11820 : loss : 0.335354, loss_ce: 0.017474
[20:58:53.999] iteration 11830 : loss : 0.228904, loss_ce: 0.051806
[20:58:58.050] iteration 11840 : loss : 0.319045, loss_ce: 0.008651
[20:59:02.124] iteration 11850 : loss : 0.356785, loss_ce: 0.065347
[20:59:06.178] iteration 11860 : loss : 0.321386, loss_ce: 0.015362
[20:59:10.245] iteration 11870 : loss : 0.321473, loss_ce: 0.022532
[20:59:14.301] iteration 11880 : loss : 0.323056, loss_ce: 0.029945
[20:59:18.369] iteration 11890 : loss : 0.335782, loss_ce: 0.035385
[20:59:22.430] iteration 11900 : loss : 0.174590, loss_ce: 0.011751
[20:59:26.499] iteration 11910 : loss : 0.329737, loss_ce: 0.024600
[20:59:30.562] iteration 11920 : loss : 0.316926, loss_ce: 0.004100
[20:59:34.627] iteration 11930 : loss : 0.331977, loss_ce: 0.013983
[20:59:38.688] iteration 11940 : loss : 0.322227, loss_ce: 0.024743
[20:59:42.762] iteration 11950 : loss : 0.329798, loss_ce: 0.019021
[20:59:46.825] iteration 11960 : loss : 0.328643, loss_ce: 0.036975
[20:59:50.899] iteration 11970 : loss : 0.326061, loss_ce: 0.019387
[20:59:54.962] iteration 11980 : loss : 0.182416, loss_ce: 0.030097
[20:59:59.038] iteration 11990 : loss : 0.312704, loss_ce: 0.008932
[21:00:03.104] iteration 12000 : loss : 0.309795, loss_ce: 0.026133
[21:00:07.179] iteration 12010 : loss : 0.132163, loss_ce: 0.026387
[21:00:11.244] iteration 12020 : loss : 0.313128, loss_ce: 0.017771
[21:00:15.324] iteration 12030 : loss : 0.319049, loss_ce: 0.006607
[21:00:19.394] iteration 12040 : loss : 0.286170, loss_ce: 0.016229
[21:00:23.471] iteration 12050 : loss : 0.326546, loss_ce: 0.015452
[21:00:27.443] iteration 12060 : loss : 0.213665, loss_ce: 0.008166
[21:07:46.068] iteration 12070 : loss : 0.395172, loss_ce: 0.095566
[21:07:50.120] iteration 12080 : loss : 0.289040, loss_ce: 0.013012
[21:07:54.178] iteration 12090 : loss : 0.333823, loss_ce: 0.018723
[21:07:58.229] iteration 12100 : loss : 0.346040, loss_ce: 0.007783
[21:08:02.291] iteration 12110 : loss : 0.352454, loss_ce: 0.052295
[21:08:06.343] iteration 12120 : loss : 0.362774, loss_ce: 0.071050
[21:08:10.406] iteration 12130 : loss : 0.347903, loss_ce: 0.073899
[21:08:14.458] iteration 12140 : loss : 0.359705, loss_ce: 0.076500
[21:08:18.525] iteration 12150 : loss : 0.346782, loss_ce: 0.035343
[21:08:22.583] iteration 12160 : loss : 0.320024, loss_ce: 0.008621
[21:08:26.650] iteration 12170 : loss : 0.365859, loss_ce: 0.076070
[21:08:30.709] iteration 12180 : loss : 0.344561, loss_ce: 0.070651
[21:08:34.777] iteration 12190 : loss : 0.316698, loss_ce: 0.006893
[21:08:38.840] iteration 12200 : loss : 0.194818, loss_ce: 0.007578
[21:08:42.914] iteration 12210 : loss : 0.332500, loss_ce: 0.028975
[21:08:46.979] iteration 12220 : loss : 0.183479, loss_ce: 0.025005
[21:08:51.057] iteration 12230 : loss : 0.345310, loss_ce: 0.074221
[21:08:55.124] iteration 12240 : loss : 0.200803, loss_ce: 0.018692
[21:08:59.203] iteration 12250 : loss : 0.175940, loss_ce: 0.027310
[21:09:03.275] iteration 12260 : loss : 0.180556, loss_ce: 0.040159
[21:09:07.350] iteration 12270 : loss : 0.355915, loss_ce: 0.070110
[21:09:11.418] iteration 12280 : loss : 0.332863, loss_ce: 0.028700
[21:09:15.498] iteration 12290 : loss : 0.316907, loss_ce: 0.010175
[21:09:19.568] iteration 12300 : loss : 0.330423, loss_ce: 0.020231
[21:09:23.649] iteration 12310 : loss : 0.331753, loss_ce: 0.035091
[21:09:27.718] iteration 12320 : loss : 0.173252, loss_ce: 0.024238
[21:16:57.069] iteration 12330 : loss : 0.463450, loss_ce: 0.071469
[21:17:01.114] iteration 12340 : loss : 0.372435, loss_ce: 0.070293
[21:17:05.174] iteration 12350 : loss : 0.323898, loss_ce: 0.010480
[21:17:09.223] iteration 12360 : loss : 0.367392, loss_ce: 0.054297
[21:17:13.283] iteration 12370 : loss : 0.386587, loss_ce: 0.099105
[21:17:17.333] iteration 12380 : loss : 0.206611, loss_ce: 0.050224
[21:17:21.398] iteration 12390 : loss : 0.345942, loss_ce: 0.040736
[21:17:25.454] iteration 12400 : loss : 0.329417, loss_ce: 0.037771
[21:17:29.518] iteration 12410 : loss : 0.351270, loss_ce: 0.043528
[21:17:33.574] iteration 12420 : loss : 0.328928, loss_ce: 0.022405
[21:17:37.639] iteration 12430 : loss : 0.356533, loss_ce: 0.103494
[21:17:41.697] iteration 12440 : loss : 0.345984, loss_ce: 0.056683
[21:17:45.772] iteration 12450 : loss : 0.323170, loss_ce: 0.018192
[21:17:49.831] iteration 12460 : loss : 0.328852, loss_ce: 0.022841
[21:17:53.905] iteration 12470 : loss : 0.171303, loss_ce: 0.015922
[21:17:57.968] iteration 12480 : loss : 0.335558, loss_ce: 0.046066
[21:18:02.044] iteration 12490 : loss : 0.329354, loss_ce: 0.008684
[21:18:06.113] iteration 12500 : loss : 0.172898, loss_ce: 0.024963
[21:18:10.189] iteration 12510 : loss : 0.337592, loss_ce: 0.021368
[21:18:14.259] iteration 12520 : loss : 0.361292, loss_ce: 0.088590
[21:18:18.337] iteration 12530 : loss : 0.354724, loss_ce: 0.067825
[21:18:22.406] iteration 12540 : loss : 0.344142, loss_ce: 0.031059
[21:18:26.487] iteration 12550 : loss : 0.336523, loss_ce: 0.025255
[21:18:30.556] iteration 12560 : loss : 0.327213, loss_ce: 0.021536
[21:18:34.638] iteration 12570 : loss : 0.329073, loss_ce: 0.014449
[21:18:38.708] iteration 12580 : loss : 0.313282, loss_ce: 0.006382
[21:18:42.789] iteration 12590 : loss : 0.187686, loss_ce: 0.054481
[21:26:12.507] iteration 12600 : loss : 0.449063, loss_ce: 0.059581
[21:26:16.561] iteration 12610 : loss : 0.388914, loss_ce: 0.017982
[21:26:20.611] iteration 12620 : loss : 0.390346, loss_ce: 0.104290
[21:26:24.670] iteration 12630 : loss : 0.328403, loss_ce: 0.033068
[21:26:28.719] iteration 12640 : loss : 0.363424, loss_ce: 0.011508
[21:26:32.780] iteration 12650 : loss : 0.323979, loss_ce: 0.010080
[21:26:36.834] iteration 12660 : loss : 0.412158, loss_ce: 0.122250
[21:26:40.902] iteration 12670 : loss : 0.323517, loss_ce: 0.020041
[21:26:44.956] iteration 12680 : loss : 0.329971, loss_ce: 0.043557
[21:26:49.023] iteration 12690 : loss : 0.345807, loss_ce: 0.032014
[21:26:53.081] iteration 12700 : loss : 0.346266, loss_ce: 0.042467
[21:26:57.153] iteration 12710 : loss : 0.319164, loss_ce: 0.011083
[21:27:01.213] iteration 12720 : loss : 0.369956, loss_ce: 0.087660
[21:27:05.285] iteration 12730 : loss : 0.326421, loss_ce: 0.022412
[21:27:09.348] iteration 12740 : loss : 0.328683, loss_ce: 0.021910
[21:27:13.422] iteration 12750 : loss : 0.346128, loss_ce: 0.053930
[21:27:17.485] iteration 12760 : loss : 0.354139, loss_ce: 0.060620
[21:27:21.560] iteration 12770 : loss : 0.337461, loss_ce: 0.034661
[21:27:25.626] iteration 12780 : loss : 0.312433, loss_ce: 0.004512
[21:27:29.702] iteration 12790 : loss : 0.337369, loss_ce: 0.053482
[21:27:33.769] iteration 12800 : loss : 0.333482, loss_ce: 0.027900
[21:27:37.845] iteration 12810 : loss : 0.311423, loss_ce: 0.005513
[21:27:41.917] iteration 12820 : loss : 0.355188, loss_ce: 0.059665
[21:27:45.997] iteration 12830 : loss : 0.313372, loss_ce: 0.005039
[21:27:50.071] iteration 12840 : loss : 0.321983, loss_ce: 0.023226
[21:27:54.153] iteration 12850 : loss : 0.279441, loss_ce: 0.016170
[21:27:58.227] iteration 12860 : loss : 0.338397, loss_ce: 0.037724
[21:35:15.158] iteration 12870 : loss : 0.455944, loss_ce: 0.063987
[21:35:19.205] iteration 12880 : loss : 0.422405, loss_ce: 0.041985
[21:35:23.259] iteration 12890 : loss : 0.368135, loss_ce: 0.020408
[21:35:27.308] iteration 12900 : loss : 0.334251, loss_ce: 0.021846
[21:35:31.370] iteration 12910 : loss : 0.366201, loss_ce: 0.033634
[21:35:35.421] iteration 12920 : loss : 0.337301, loss_ce: 0.017318
[21:35:39.484] iteration 12930 : loss : 0.332935, loss_ce: 0.025732
[21:35:43.538] iteration 12940 : loss : 0.334320, loss_ce: 0.018012
[21:35:47.600] iteration 12950 : loss : 0.328303, loss_ce: 0.039836
[21:35:51.655] iteration 12960 : loss : 0.327809, loss_ce: 0.021007
[21:35:55.721] iteration 12970 : loss : 0.318800, loss_ce: 0.014425
[21:35:59.781] iteration 12980 : loss : 0.333962, loss_ce: 0.035882
[21:36:03.851] iteration 12990 : loss : 0.327527, loss_ce: 0.029238
[21:36:07.913] iteration 13000 : loss : 0.349235, loss_ce: 0.052944
[21:36:11.984] iteration 13010 : loss : 0.338530, loss_ce: 0.018520
[21:36:16.043] iteration 13020 : loss : 0.323117, loss_ce: 0.012578
[21:36:20.117] iteration 13030 : loss : 0.351280, loss_ce: 0.064010
[21:36:24.179] iteration 13040 : loss : 0.321721, loss_ce: 0.014336
[21:36:28.255] iteration 13050 : loss : 0.327789, loss_ce: 0.014534
[21:36:32.319] iteration 13060 : loss : 0.362288, loss_ce: 0.059375
[21:36:36.392] iteration 13070 : loss : 0.330904, loss_ce: 0.029214
[21:36:40.458] iteration 13080 : loss : 0.337433, loss_ce: 0.015619
[21:36:44.533] iteration 13090 : loss : 0.324029, loss_ce: 0.014749
[21:36:48.602] iteration 13100 : loss : 0.346029, loss_ce: 0.047645
[21:36:52.681] iteration 13110 : loss : 0.339223, loss_ce: 0.043519
[21:36:56.750] iteration 13120 : loss : 0.352036, loss_ce: 0.009606
[21:37:00.836] iteration 13130 : loss : 0.354937, loss_ce: 0.091391
[21:44:29.398] iteration 13140 : loss : 0.466086, loss_ce: 0.062852
[21:44:33.452] iteration 13150 : loss : 0.446882, loss_ce: 0.060895
[21:44:37.500] iteration 13160 : loss : 0.426831, loss_ce: 0.054274
[21:44:41.559] iteration 13170 : loss : 0.483081, loss_ce: 0.109135
[21:44:45.609] iteration 13180 : loss : 0.401260, loss_ce: 0.047454
[21:44:49.676] iteration 13190 : loss : 0.373683, loss_ce: 0.021679
[21:44:53.729] iteration 13200 : loss : 0.375342, loss_ce: 0.044780
[21:44:57.794] iteration 13210 : loss : 0.383051, loss_ce: 0.093704
[21:45:01.853] iteration 13220 : loss : 0.349234, loss_ce: 0.018344
[21:45:05.919] iteration 13230 : loss : 0.357136, loss_ce: 0.044413
[21:45:09.978] iteration 13240 : loss : 0.397288, loss_ce: 0.125177
[21:45:14.047] iteration 13250 : loss : 0.344577, loss_ce: 0.017964
[21:45:18.109] iteration 13260 : loss : 0.343235, loss_ce: 0.022131
[21:45:22.187] iteration 13270 : loss : 0.345546, loss_ce: 0.011837
[21:45:26.253] iteration 13280 : loss : 0.354209, loss_ce: 0.072480
[21:45:30.331] iteration 13290 : loss : 0.346141, loss_ce: 0.016647
[21:45:34.399] iteration 13300 : loss : 0.334705, loss_ce: 0.020191
[21:45:38.479] iteration 13310 : loss : 0.339349, loss_ce: 0.025526
[21:45:42.547] iteration 13320 : loss : 0.340154, loss_ce: 0.032646
[21:45:46.626] iteration 13330 : loss : 0.329526, loss_ce: 0.010636
[21:45:50.697] iteration 13340 : loss : 0.337139, loss_ce: 0.043815
[21:45:54.776] iteration 13350 : loss : 0.368040, loss_ce: 0.052936
[21:45:58.845] iteration 13360 : loss : 0.322032, loss_ce: 0.006743
[21:46:02.927] iteration 13370 : loss : 0.358945, loss_ce: 0.048812
[21:46:06.998] iteration 13380 : loss : 0.332251, loss_ce: 0.044726
[21:46:11.083] iteration 13390 : loss : 0.364065, loss_ce: 0.079402
[21:46:15.059] iteration 13400 : loss : 0.382576, loss_ce: 0.122868
[21:53:30.790] save model to ./finetune_tpgm_kits23_continual_500iter\finetuned_epoch_49.pth
[21:53:30.960] save final model to ./finetune_tpgm_kits23_continual_500iter\finetuned_final.pth
