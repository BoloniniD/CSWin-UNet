[13:33:26.697] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', output_dir='./finetune_continual_kits23', num_classes_new=4, num_classes_old=9, max_iterations=10000, max_epochs=30, batch_size=48, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.05, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, num_classes=13)
[13:33:26.704] Using 4761/95221 samples (5.0%) for continual learning.
[13:33:26.708] 100 iterations per epoch.
[13:36:19.097] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', output_dir='./finetune_continual_kits23', num_classes_new=4, num_classes_old=9, max_iterations=10000, max_epochs=30, batch_size=48, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.05, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, num_classes=13)
[13:36:19.104] Using 4761/95221 samples (5.0%) for continual learning.
[13:36:19.108] 100 iterations per epoch.
[13:37:14.474] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', output_dir='./finetune_continual_kits23', num_classes_new=4, num_classes_old=9, max_iterations=10000, max_epochs=30, batch_size=48, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.05, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, num_classes=13)
[13:37:14.481] Using 4761/95221 samples (5.0%) for continual learning.
[13:37:14.486] 100 iterations per epoch.
[13:37:41.025] iteration 20 : loss : 1.2496, loss_ce: 1.6812
[13:37:55.737] iteration 40 : loss : 0.7607, loss_ce: 0.5138
[13:38:10.367] iteration 60 : loss : 0.6210, loss_ce: 0.1691
[13:38:24.951] iteration 80 : loss : 0.5974, loss_ce: 0.1096
[13:38:38.477] iteration 100 : loss : 0.6071, loss_ce: 0.1333
[13:39:03.665] iteration 120 : loss : 0.5819, loss_ce: 0.0706
[13:39:17.706] iteration 140 : loss : 0.5855, loss_ce: 0.0796
[13:39:31.960] iteration 160 : loss : 0.5839, loss_ce: 0.0758
[13:39:46.197] iteration 180 : loss : 0.5722, loss_ce: 0.0470
[13:39:59.547] iteration 200 : loss : 0.5755, loss_ce: 0.0556
[13:40:24.476] iteration 220 : loss : 0.5819, loss_ce: 0.0715
[13:40:38.457] iteration 240 : loss : 0.5731, loss_ce: 0.0498
[13:40:52.445] iteration 260 : loss : 0.5737, loss_ce: 0.0516
[13:41:06.444] iteration 280 : loss : 0.5757, loss_ce: 0.0565
[13:41:19.869] iteration 300 : loss : 0.5782, loss_ce: 0.0634
[13:41:45.074] iteration 320 : loss : 0.5730, loss_ce: 0.0504
[13:41:59.049] iteration 340 : loss : 0.5676, loss_ce: 0.0369
[13:42:13.509] iteration 360 : loss : 0.5689, loss_ce: 0.0404
[13:42:27.614] iteration 380 : loss : 0.5628, loss_ce: 0.0393
[13:42:41.137] iteration 400 : loss : 0.4831, loss_ce: 0.0555
[13:43:06.532] iteration 420 : loss : 0.4773, loss_ce: 0.0409
[13:43:20.829] iteration 440 : loss : 0.4779, loss_ce: 0.0436
[13:43:35.179] iteration 460 : loss : 0.4835, loss_ce: 0.0615
[13:43:49.257] iteration 480 : loss : 0.4340, loss_ce: 0.0493
[13:44:02.788] iteration 500 : loss : 0.3299, loss_ce: 0.0117
[13:44:28.540] iteration 520 : loss : 0.3410, loss_ce: 0.0444
[13:44:42.860] iteration 540 : loss : 0.3393, loss_ce: 0.0429
[13:44:57.233] iteration 560 : loss : 0.3407, loss_ce: 0.0468
[13:45:11.362] iteration 580 : loss : 0.3396, loss_ce: 0.0448
[13:45:24.763] iteration 600 : loss : 0.3127, loss_ce: 0.0857
[13:45:50.340] iteration 620 : loss : 0.2998, loss_ce: 0.0597
[13:46:04.540] iteration 640 : loss : 0.2573, loss_ce: 0.0716
[13:46:19.159] iteration 660 : loss : 0.1943, loss_ce: 0.0439
[13:46:33.230] iteration 680 : loss : 0.1520, loss_ce: 0.0353
[13:46:46.761] iteration 700 : loss : 0.1051, loss_ce: 0.0344
[13:47:11.716] iteration 720 : loss : 0.1525, loss_ce: 0.0384
[13:47:25.902] iteration 740 : loss : 0.1595, loss_ce: 0.0561
[13:47:39.943] iteration 760 : loss : 0.1599, loss_ce: 0.0579
[13:47:54.238] iteration 780 : loss : 0.1493, loss_ce: 0.0305
[13:48:07.655] iteration 800 : loss : 0.1592, loss_ce: 0.0557
[13:48:32.208] iteration 820 : loss : 0.1554, loss_ce: 0.0471
[13:48:46.045] iteration 840 : loss : 0.1537, loss_ce: 0.0440
[13:48:59.912] iteration 860 : loss : 0.1485, loss_ce: 0.0328
[13:49:14.142] iteration 880 : loss : 0.1500, loss_ce: 0.0399
[13:49:28.210] iteration 900 : loss : 0.1548, loss_ce: 0.0555
[13:49:53.555] iteration 920 : loss : 0.1522, loss_ce: 0.0531
[13:50:08.388] iteration 940 : loss : 0.1484, loss_ce: 0.0455
[13:50:23.011] iteration 960 : loss : 0.1456, loss_ce: 0.0372
[13:50:37.853] iteration 980 : loss : 0.1460, loss_ce: 0.0406
[13:50:52.018] iteration 1000 : loss : 0.0985, loss_ce: 0.0348
[13:50:52.729] Saved model to ./finetune_continual_kits23\continual_epoch_9.pth
[13:51:16.929] iteration 1020 : loss : 0.1434, loss_ce: 0.0426
[13:51:30.884] iteration 1040 : loss : 0.1499, loss_ce: 0.0520
[13:51:44.898] iteration 1060 : loss : 0.1443, loss_ce: 0.0428
[13:51:58.811] iteration 1080 : loss : 0.1488, loss_ce: 0.0547
[13:52:12.188] iteration 1100 : loss : 0.0962, loss_ce: 0.0233
[13:52:36.956] iteration 1120 : loss : 0.1467, loss_ce: 0.0515
[13:52:51.038] iteration 1140 : loss : 0.1462, loss_ce: 0.0576
[13:53:05.842] iteration 1160 : loss : 0.1395, loss_ce: 0.0358
[13:53:20.033] iteration 1180 : loss : 0.1469, loss_ce: 0.0334
[13:53:33.529] iteration 1200 : loss : 0.1520, loss_ce: 0.0612
[13:53:58.394] iteration 1220 : loss : 0.1453, loss_ce: 0.0450
[13:54:12.423] iteration 1240 : loss : 0.1444, loss_ce: 0.0412
[13:54:26.585] iteration 1260 : loss : 0.1408, loss_ce: 0.0360
[13:54:40.632] iteration 1280 : loss : 0.1464, loss_ce: 0.0584
[13:54:54.091] iteration 1300 : loss : 0.1669, loss_ce: 0.0922
[13:55:18.955] iteration 1320 : loss : 0.1351, loss_ce: 0.0459
[13:55:33.303] iteration 1340 : loss : 0.0842, loss_ce: 0.0299
[13:55:47.688] iteration 1360 : loss : 0.1275, loss_ce: 0.0289
[13:56:02.170] iteration 1380 : loss : 0.1231, loss_ce: 0.0311
[13:56:16.076] iteration 1400 : loss : 0.1462, loss_ce: 0.0190
[13:56:42.877] iteration 1420 : loss : 0.1231, loss_ce: 0.0189
[13:56:57.679] iteration 1440 : loss : 0.1195, loss_ce: 0.0410
[13:57:11.874] iteration 1460 : loss : 0.1110, loss_ce: 0.0207
[13:57:26.182] iteration 1480 : loss : 0.1118, loss_ce: 0.0206
[13:57:40.357] iteration 1500 : loss : 0.1143, loss_ce: 0.0287
[13:58:05.675] iteration 1520 : loss : 0.1098, loss_ce: 0.0255
[13:58:19.621] iteration 1540 : loss : 0.1109, loss_ce: 0.0240
[13:58:33.540] iteration 1560 : loss : 0.1181, loss_ce: 0.0349
[13:58:47.704] iteration 1580 : loss : 0.1151, loss_ce: 0.0192
[13:59:01.301] iteration 1600 : loss : 0.1302, loss_ce: 0.0440
[13:59:26.319] iteration 1620 : loss : 0.1090, loss_ce: 0.0297
[13:59:40.253] iteration 1640 : loss : 0.1060, loss_ce: 0.0236
[13:59:54.297] iteration 1660 : loss : 0.1129, loss_ce: 0.0311
[14:00:08.259] iteration 1680 : loss : 0.1044, loss_ce: 0.0221
[14:00:21.656] iteration 1700 : loss : 0.0607, loss_ce: 0.0164
[14:00:46.547] iteration 1720 : loss : 0.1085, loss_ce: 0.0217
[14:01:00.550] iteration 1740 : loss : 0.1029, loss_ce: 0.0252
[14:01:14.599] iteration 1760 : loss : 0.1015, loss_ce: 0.0162
[14:01:28.837] iteration 1780 : loss : 0.0941, loss_ce: 0.0197
[14:01:42.686] iteration 1800 : loss : 0.1156, loss_ce: 0.0275
[14:02:07.908] iteration 1820 : loss : 0.0912, loss_ce: 0.0194
[14:02:21.953] iteration 1840 : loss : 0.1012, loss_ce: 0.0252
[14:02:35.884] iteration 1860 : loss : 0.0888, loss_ce: 0.0163
[14:02:50.184] iteration 1880 : loss : 0.0957, loss_ce: 0.0217
[14:03:04.275] iteration 1900 : loss : 0.0534, loss_ce: 0.0120
[14:03:29.449] iteration 1920 : loss : 0.0559, loss_ce: 0.0045
[14:03:43.761] iteration 1940 : loss : 0.0835, loss_ce: 0.0094
[14:03:57.997] iteration 1960 : loss : 0.0982, loss_ce: 0.0129
[14:04:12.414] iteration 1980 : loss : 0.0906, loss_ce: 0.0127
[14:04:26.111] iteration 2000 : loss : 0.0557, loss_ce: 0.0080
[14:04:26.815] Saved model to ./finetune_continual_kits23\continual_epoch_19.pth
[14:04:51.019] iteration 2020 : loss : 0.0700, loss_ce: 0.0127
[14:05:05.149] iteration 2040 : loss : 0.0776, loss_ce: 0.0113
[14:05:19.230] iteration 2060 : loss : 0.0854, loss_ce: 0.0155
[14:05:33.152] iteration 2080 : loss : 0.0943, loss_ce: 0.0167
[14:05:46.553] iteration 2100 : loss : 0.0788, loss_ce: 0.0246
[14:06:11.141] iteration 2120 : loss : 0.0606, loss_ce: 0.0073
[14:06:25.026] iteration 2140 : loss : 0.0832, loss_ce: 0.0117
[14:06:38.892] iteration 2160 : loss : 0.0752, loss_ce: 0.0159
[14:06:52.843] iteration 2180 : loss : 0.0862, loss_ce: 0.0151
[14:07:06.293] iteration 2200 : loss : 0.0599, loss_ce: 0.0140
[14:07:31.097] iteration 2220 : loss : 0.0754, loss_ce: 0.0139
[14:07:45.088] iteration 2240 : loss : 0.0701, loss_ce: 0.0061
[14:07:58.975] iteration 2260 : loss : 0.0649, loss_ce: 0.0074
[14:08:12.918] iteration 2280 : loss : 0.0739, loss_ce: 0.0092
[14:08:26.340] iteration 2300 : loss : 0.0117, loss_ce: 0.0070
[14:08:51.149] iteration 2320 : loss : 0.0704, loss_ce: 0.0136
[14:09:05.166] iteration 2340 : loss : 0.0979, loss_ce: 0.0107
[14:09:19.102] iteration 2360 : loss : 0.0638, loss_ce: 0.0122
[14:09:33.514] iteration 2380 : loss : 0.0794, loss_ce: 0.0151
[14:09:46.915] iteration 2400 : loss : 0.0533, loss_ce: 0.0027
[14:10:11.703] iteration 2420 : loss : 0.0830, loss_ce: 0.0149
[14:10:25.568] iteration 2440 : loss : 0.0794, loss_ce: 0.0147
[14:10:39.509] iteration 2460 : loss : 0.0722, loss_ce: 0.0115
[14:10:53.407] iteration 2480 : loss : 0.0683, loss_ce: 0.0100
[14:11:07.621] iteration 2500 : loss : 0.0279, loss_ce: 0.0223
[14:11:32.429] iteration 2520 : loss : 0.0653, loss_ce: 0.0076
[14:11:46.409] iteration 2540 : loss : 0.0712, loss_ce: 0.0126
[14:12:00.461] iteration 2560 : loss : 0.0899, loss_ce: 0.0073
[14:12:14.449] iteration 2580 : loss : 0.0700, loss_ce: 0.0091
[14:12:28.185] iteration 2600 : loss : 0.0590, loss_ce: 0.0040
[14:12:52.874] iteration 2620 : loss : 0.0761, loss_ce: 0.0053
[14:13:06.906] iteration 2640 : loss : 0.0771, loss_ce: 0.0071
[14:13:20.845] iteration 2660 : loss : 0.0639, loss_ce: 0.0085
[14:13:34.863] iteration 2680 : loss : 0.0616, loss_ce: 0.0117
[14:13:48.414] iteration 2700 : loss : 0.0158, loss_ce: 0.0138
[14:14:13.289] iteration 2720 : loss : 0.0659, loss_ce: 0.0073
[14:14:27.670] iteration 2740 : loss : 0.0843, loss_ce: 0.0111
[14:14:41.912] iteration 2760 : loss : 0.0819, loss_ce: 0.0065
[14:14:56.162] iteration 2780 : loss : 0.0604, loss_ce: 0.0125
[14:15:09.693] iteration 2800 : loss : 0.0617, loss_ce: 0.0108
[14:15:34.532] iteration 2820 : loss : 0.0573, loss_ce: 0.0043
[14:15:48.386] iteration 2840 : loss : 0.0222, loss_ce: 0.0060
[14:16:02.243] iteration 2860 : loss : 0.0747, loss_ce: 0.0049
[14:16:16.107] iteration 2880 : loss : 0.0488, loss_ce: 0.0038
[14:16:29.454] iteration 2900 : loss : 0.0780, loss_ce: 0.0136
[14:16:54.455] iteration 2920 : loss : 0.0672, loss_ce: 0.0120
[14:17:08.724] iteration 2940 : loss : 0.0679, loss_ce: 0.0149
[14:17:22.986] iteration 2960 : loss : 0.0693, loss_ce: 0.0091
[14:17:37.248] iteration 2980 : loss : 0.0657, loss_ce: 0.0080
[14:17:50.914] iteration 3000 : loss : 0.0476, loss_ce: 0.0005
[14:17:51.621] Saved model to ./finetune_continual_kits23\continual_epoch_29.pth
