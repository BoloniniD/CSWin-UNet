[14:30:48.393] Namespace(root_path='./datasets/lits17/train_npz', dataset='lits17', list_dir='./lists/lits17', stage=2, num_classes_old=12, num_classes_new=3, num_classes_lits17=3, output_dir='./simple_finetuning_lits17', max_iterations=10000, max_epochs=15, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./debug_simple/continual_surgical_tpgm_final.pth', data_fraction=0.5, kd_temperature=3.0, kd_weight=0.2, freeze_old_classes=False, auto_tune='none', gradient_batches=5, tpgm_norm_mode='l2', tpgm_lr=0.01, tpgm_iters=200, tpgm_exclude=[], tpgm_frequency=5, tpgm_start_epoch=10, disable_tpgm=True, tpgm_data_fraction=0.1, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[14:30:48.397] Stage 2: Using 7421/14843 samples (50.0%) for continual learning
[14:30:48.397] Old classes: 12, New classes: 3, Total: 14
[14:30:48.397] Dataset: lits17
[14:30:48.397] TPGM enabled: False
[14:30:48.397] Surgical fine-tuning method: none
[14:31:36.148] Combined Continual Learning Stage 2 + Surgical + TPGM Configuration:
[14:31:36.148] KD Temperature: 3.0
[14:31:36.148] KD Weight: 0.2
[14:31:36.148] Auto-tune method: none
[14:31:36.148] TPGM start epoch: 10
[14:31:36.148] TPGM frequency: 5
[14:31:36.149] 232 iterations per epoch. 3480 max iterations 
[14:31:52.955] iteration 10 : loss : 894.000488, loss_ce: 0.512778, loss_kd: 4466.819336
[14:31:58.570] iteration 20 : loss : 377.813080, loss_ce: 0.509655, loss_kd: 1885.876953
[14:32:04.211] iteration 30 : loss : 264.885925, loss_ce: 0.436368, loss_kd: 1321.305908
[14:32:09.847] iteration 40 : loss : 138.111206, loss_ce: 0.426412, loss_kd: 687.434753
[14:32:15.496] iteration 50 : loss : 124.122185, loss_ce: 0.706507, loss_kd: 617.262207
[14:32:21.141] iteration 60 : loss : 104.575134, loss_ce: 0.496659, loss_kd: 519.697144
[14:32:26.793] iteration 70 : loss : 109.465569, loss_ce: 0.397253, loss_kd: 544.216431
[14:32:32.440] iteration 80 : loss : 75.769417, loss_ce: 0.441663, loss_kd: 375.718201
[14:32:38.099] iteration 90 : loss : 57.362541, loss_ce: 0.386764, loss_kd: 283.727875
[14:32:43.749] iteration 100 : loss : 68.861023, loss_ce: 0.458601, loss_kd: 341.158997
[14:32:49.412] iteration 110 : loss : 60.060860, loss_ce: 0.439671, loss_kd: 297.163330
[14:32:55.063] iteration 120 : loss : 59.106144, loss_ce: 0.373446, loss_kd: 292.459106
[14:33:00.728] iteration 130 : loss : 105.802948, loss_ce: 0.404861, loss_kd: 525.909058
[14:33:06.385] iteration 140 : loss : 79.450043, loss_ce: 0.348110, loss_kd: 394.193115
[14:33:12.055] iteration 150 : loss : 67.427696, loss_ce: 0.361832, loss_kd: 334.073578
[14:33:17.716] iteration 160 : loss : 144.350220, loss_ce: 0.356646, loss_kd: 718.685486
[14:33:23.388] iteration 170 : loss : 122.519455, loss_ce: 0.291967, loss_kd: 609.582458
[14:33:29.045] iteration 180 : loss : 137.165451, loss_ce: 0.311106, loss_kd: 682.798279
[14:33:34.721] iteration 190 : loss : 84.056030, loss_ce: 0.334627, loss_kd: 417.228027
[14:33:40.388] iteration 200 : loss : 92.833664, loss_ce: 0.185337, loss_kd: 461.241425
[14:33:46.071] iteration 210 : loss : 90.097328, loss_ce: 0.252171, loss_kd: 447.505127
[14:33:51.740] iteration 220 : loss : 58.912331, loss_ce: 0.202323, loss_kd: 291.622528
[14:33:57.416] iteration 230 : loss : 58.378529, loss_ce: 0.232561, loss_kd: 288.927704
[14:34:13.954] iteration 240 : loss : 136.423828, loss_ce: 0.156674, loss_kd: 679.215271
[14:34:19.620] iteration 250 : loss : 225.090439, loss_ce: 0.183871, loss_kd: 1122.547119
[14:34:25.278] iteration 260 : loss : 123.534264, loss_ce: 0.178035, loss_kd: 614.763184
[14:34:30.951] iteration 270 : loss : 115.737289, loss_ce: 0.077451, loss_kd: 575.876099
[14:34:36.619] iteration 280 : loss : 84.403229, loss_ce: 0.065016, loss_kd: 419.243500
[14:34:42.303] iteration 290 : loss : 83.155396, loss_ce: 0.061764, loss_kd: 413.051819
[14:34:47.972] iteration 300 : loss : 106.971344, loss_ce: 0.082772, loss_kd: 532.148071
[14:34:53.648] iteration 310 : loss : 84.539360, loss_ce: 0.040450, loss_kd: 420.065613
[14:34:59.321] iteration 320 : loss : 126.701424, loss_ce: 0.091183, loss_kd: 630.842163
[14:35:05.001] iteration 330 : loss : 89.239616, loss_ce: 0.040669, loss_kd: 443.586212
[14:35:10.673] iteration 340 : loss : 164.391144, loss_ce: 0.038441, loss_kd: 819.333435
[14:35:16.357] iteration 350 : loss : 193.506302, loss_ce: 0.026173, loss_kd: 964.934631
[14:35:22.033] iteration 360 : loss : 123.660713, loss_ce: 0.046859, loss_kd: 615.698120
[14:35:27.720] iteration 370 : loss : 84.596893, loss_ce: 0.041166, loss_kd: 420.361328
[14:35:33.395] iteration 380 : loss : 85.602051, loss_ce: 0.026839, loss_kd: 425.426392
[14:35:39.086] iteration 390 : loss : 110.484215, loss_ce: 0.017541, loss_kd: 549.838867
[14:35:44.764] iteration 400 : loss : 57.322571, loss_ce: 0.030188, loss_kd: 284.014038
[14:35:50.461] iteration 410 : loss : 73.386086, loss_ce: 0.052585, loss_kd: 364.316528
[14:35:56.146] iteration 420 : loss : 92.059509, loss_ce: 0.020651, loss_kd: 457.725464
[14:36:01.839] iteration 430 : loss : 59.603951, loss_ce: 0.036148, loss_kd: 295.409485
[14:36:07.527] iteration 440 : loss : 58.374912, loss_ce: 0.032268, loss_kd: 289.294647
[14:36:13.221] iteration 450 : loss : 79.350067, loss_ce: 0.033197, loss_kd: 394.178955
[14:36:18.910] iteration 460 : loss : 52.263176, loss_ce: 0.035715, loss_kd: 258.733887
[14:36:35.380] iteration 470 : loss : 62.337688, loss_ce: 0.041586, loss_kd: 309.083557
[14:36:41.046] iteration 480 : loss : 136.060013, loss_ce: 0.038367, loss_kd: 677.698486
[14:36:46.729] iteration 490 : loss : 73.405083, loss_ce: 0.014898, loss_kd: 364.458679
[14:36:52.403] iteration 500 : loss : 101.433311, loss_ce: 0.012726, loss_kd: 504.614197
[14:36:58.093] iteration 510 : loss : 64.159042, loss_ce: 0.026976, loss_kd: 318.222534
[14:37:03.767] iteration 520 : loss : 231.871902, loss_ce: 0.027662, loss_kd: 1156.794189
[14:37:09.454] iteration 530 : loss : 118.682915, loss_ce: 0.021687, loss_kd: 590.846069
[14:37:15.136] iteration 540 : loss : 118.183144, loss_ce: 0.022158, loss_kd: 588.350891
[14:37:20.827] iteration 550 : loss : 113.315918, loss_ce: 0.023501, loss_kd: 564.044861
[14:37:26.514] iteration 560 : loss : 113.623238, loss_ce: 0.013119, loss_kd: 565.590271
[14:37:32.212] iteration 570 : loss : 83.416618, loss_ce: 0.019683, loss_kd: 414.559875
[14:37:37.905] iteration 580 : loss : 80.293053, loss_ce: 0.016906, loss_kd: 398.890503
[14:37:43.608] iteration 590 : loss : 75.779556, loss_ce: 0.034624, loss_kd: 376.340759
[14:37:49.305] iteration 600 : loss : 125.280769, loss_ce: 0.031290, loss_kd: 623.840637
[14:37:55.015] iteration 610 : loss : 95.312805, loss_ce: 0.011816, loss_kd: 474.050385
[14:38:00.714] iteration 620 : loss : 133.662857, loss_ce: 0.026140, loss_kd: 665.790894
[14:38:06.420] iteration 630 : loss : 95.559952, loss_ce: 0.026338, loss_kd: 475.244476
[14:38:12.122] iteration 640 : loss : 79.008614, loss_ce: 0.027136, loss_kd: 392.502686
[14:38:17.840] iteration 650 : loss : 75.638100, loss_ce: 0.019393, loss_kd: 375.631287
[14:38:23.544] iteration 660 : loss : 94.728195, loss_ce: 0.016185, loss_kd: 471.085266
[14:38:29.257] iteration 670 : loss : 226.615250, loss_ce: 0.022148, loss_kd: 1130.518433
[14:38:34.968] iteration 680 : loss : 103.915123, loss_ce: 0.014911, loss_kd: 517.054504
[14:38:40.686] iteration 690 : loss : 75.114952, loss_ce: 0.016686, loss_kd: 373.077484
[14:38:57.028] iteration 700 : loss : 74.610542, loss_ce: 0.023690, loss_kd: 370.520782
[14:39:02.716] iteration 710 : loss : 71.725128, loss_ce: 0.031216, loss_kd: 356.095734
[14:39:08.388] iteration 720 : loss : 59.374783, loss_ce: 0.012068, loss_kd: 294.377441
[14:39:14.078] iteration 730 : loss : 68.701279, loss_ce: 0.032301, loss_kd: 340.981293
[14:39:19.767] iteration 740 : loss : 71.539215, loss_ce: 0.016061, loss_kd: 355.172668
[14:39:25.464] iteration 750 : loss : 66.431908, loss_ce: 0.020265, loss_kd: 329.655426
[14:39:31.151] iteration 760 : loss : 104.497238, loss_ce: 0.026931, loss_kd: 519.983459
[14:39:36.853] iteration 770 : loss : 151.414001, loss_ce: 0.028781, loss_kd: 754.520203
[14:39:42.542] iteration 780 : loss : 100.736237, loss_ce: 0.028277, loss_kd: 501.111359
[14:39:48.251] iteration 790 : loss : 113.461281, loss_ce: 0.012660, loss_kd: 564.777832
[14:39:53.952] iteration 800 : loss : 81.225929, loss_ce: 0.057733, loss_kd: 403.560455
[14:39:59.673] iteration 810 : loss : 56.008228, loss_ce: 0.024774, loss_kd: 277.525970
[14:40:05.383] iteration 820 : loss : 55.062946, loss_ce: 0.027646, loss_kd: 272.806152
[14:40:11.103] iteration 830 : loss : 85.147507, loss_ce: 0.027875, loss_kd: 423.152435
[14:40:16.810] iteration 840 : loss : 84.558327, loss_ce: 0.011425, loss_kd: 420.289978
[14:40:22.523] iteration 850 : loss : 81.842667, loss_ce: 0.031958, loss_kd: 406.689240
[14:40:28.236] iteration 860 : loss : 87.217110, loss_ce: 0.021054, loss_kd: 433.563232
[14:40:33.954] iteration 870 : loss : 98.470116, loss_ce: 0.016429, loss_kd: 489.833099
[14:40:39.667] iteration 880 : loss : 93.318138, loss_ce: 0.036677, loss_kd: 464.031067
[14:40:45.389] iteration 890 : loss : 89.271935, loss_ce: 0.025768, loss_kd: 443.788574
[14:40:51.111] iteration 900 : loss : 86.246704, loss_ce: 0.024407, loss_kd: 428.735535
[14:40:56.843] iteration 910 : loss : 115.490349, loss_ce: 0.019452, loss_kd: 574.965027
[14:41:02.569] iteration 920 : loss : 149.132492, loss_ce: 0.020449, loss_kd: 743.104187
[14:41:19.425] iteration 930 : loss : 99.792480, loss_ce: 0.012443, loss_kd: 496.459320
[14:41:25.092] iteration 940 : loss : 137.273941, loss_ce: 0.015991, loss_kd: 683.874878
[14:41:30.782] iteration 950 : loss : 109.701859, loss_ce: 0.024336, loss_kd: 546.001343
[14:41:36.469] iteration 960 : loss : 82.839211, loss_ce: 0.025014, loss_kd: 411.678619
[14:41:42.164] iteration 970 : loss : 123.137665, loss_ce: 0.021125, loss_kd: 613.160706
[14:41:47.855] iteration 980 : loss : 92.932503, loss_ce: 0.039297, loss_kd: 462.146362
[14:41:53.562] iteration 990 : loss : 131.600433, loss_ce: 0.018223, loss_kd: 655.501404
[14:41:59.254] iteration 1000 : loss : 86.688187, loss_ce: 0.021301, loss_kd: 430.944733
[14:42:04.967] iteration 1010 : loss : 87.702614, loss_ce: 0.030036, loss_kd: 435.971649
[14:42:10.671] iteration 1020 : loss : 69.772774, loss_ce: 0.014449, loss_kd: 346.337128
[14:42:16.388] iteration 1030 : loss : 78.425766, loss_ce: 0.024515, loss_kd: 389.632935
[14:42:22.094] iteration 1040 : loss : 63.799427, loss_ce: 0.016071, loss_kd: 316.460449
[14:42:27.819] iteration 1050 : loss : 68.701126, loss_ce: 0.012261, loss_kd: 340.994720
[14:42:33.536] iteration 1060 : loss : 53.634384, loss_ce: 0.021993, loss_kd: 265.620422
[14:42:39.270] iteration 1070 : loss : 57.921680, loss_ce: 0.036286, loss_kd: 287.040314
[14:42:44.996] iteration 1080 : loss : 46.297493, loss_ce: 0.037302, loss_kd: 228.941925
[14:42:50.740] iteration 1090 : loss : 48.604809, loss_ce: 0.036116, loss_kd: 240.457214
[14:42:56.462] iteration 1100 : loss : 50.220287, loss_ce: 0.024159, loss_kd: 248.587341
[14:43:02.204] iteration 1110 : loss : 49.153099, loss_ce: 0.023858, loss_kd: 243.214615
[14:43:07.932] iteration 1120 : loss : 46.965298, loss_ce: 0.018666, loss_kd: 232.251389
[14:43:13.673] iteration 1130 : loss : 41.302044, loss_ce: 0.016698, loss_kd: 203.983093
[14:43:19.405] iteration 1140 : loss : 57.331364, loss_ce: 0.025028, loss_kd: 284.127930
[14:43:25.154] iteration 1150 : loss : 41.182877, loss_ce: 0.038828, loss_kd: 203.380280
[14:43:30.829] iteration 1160 : loss : 43.454536, loss_ce: 0.030690, loss_kd: 214.720428
[14:43:31.555] save model to ./simple_finetuning_lits17\continual_surgical_tpgm_stage2_epoch_4.pth
[14:43:47.345] iteration 1170 : loss : 47.097809, loss_ce: 0.036248, loss_kd: 232.943542
[14:43:53.025] iteration 1180 : loss : 61.943493, loss_ce: 0.031577, loss_kd: 307.182129
[14:43:58.725] iteration 1190 : loss : 49.558270, loss_ce: 0.015491, loss_kd: 245.316483
[14:44:04.418] iteration 1200 : loss : 49.077793, loss_ce: 0.013504, loss_kd: 242.890091
[14:44:10.129] iteration 1210 : loss : 44.062092, loss_ce: 0.029150, loss_kd: 217.810181
[14:44:15.829] iteration 1220 : loss : 41.573704, loss_ce: 0.019496, loss_kd: 205.346176
[14:44:21.543] iteration 1230 : loss : 48.162815, loss_ce: 0.011358, loss_kd: 238.297516
[14:44:27.251] iteration 1240 : loss : 47.709602, loss_ce: 0.024552, loss_kd: 236.062027
[14:44:32.966] iteration 1250 : loss : 39.838913, loss_ce: 0.032744, loss_kd: 196.643875
[14:44:38.681] iteration 1260 : loss : 53.316612, loss_ce: 0.042629, loss_kd: 264.055176
[14:44:44.404] iteration 1270 : loss : 50.947422, loss_ce: 0.021696, loss_kd: 252.200165
[14:44:50.119] iteration 1280 : loss : 42.940712, loss_ce: 0.025479, loss_kd: 212.177628
[14:44:55.847] iteration 1290 : loss : 107.151031, loss_ce: 0.032324, loss_kd: 533.219360
[14:45:01.570] iteration 1300 : loss : 57.323845, loss_ce: 0.020490, loss_kd: 284.091522
[14:45:07.310] iteration 1310 : loss : 79.447441, loss_ce: 0.019637, loss_kd: 394.749512
[14:45:13.036] iteration 1320 : loss : 60.564438, loss_ce: 0.022953, loss_kd: 300.325531
[14:45:18.776] iteration 1330 : loss : 62.152008, loss_ce: 0.010235, loss_kd: 308.253601
[14:45:24.504] iteration 1340 : loss : 83.468933, loss_ce: 0.011182, loss_kd: 414.827850
[14:45:30.240] iteration 1350 : loss : 82.351433, loss_ce: 0.027568, loss_kd: 409.242188
[14:45:35.967] iteration 1360 : loss : 108.552933, loss_ce: 0.008134, loss_kd: 540.210083
[14:45:41.704] iteration 1370 : loss : 82.505661, loss_ce: 0.024637, loss_kd: 410.038940
[14:45:47.430] iteration 1380 : loss : 79.212234, loss_ce: 0.015700, loss_kd: 393.560577
[14:45:53.172] iteration 1390 : loss : 58.108322, loss_ce: 0.028559, loss_kd: 288.029175
[14:46:09.547] iteration 1400 : loss : 76.659851, loss_ce: 0.022753, loss_kd: 380.751251
[14:46:15.241] iteration 1410 : loss : 66.246300, loss_ce: 0.030397, loss_kd: 328.717865
[14:46:20.928] iteration 1420 : loss : 170.258911, loss_ce: 0.019016, loss_kd: 848.816345
[14:46:26.639] iteration 1430 : loss : 111.007576, loss_ce: 0.013957, loss_kd: 552.502258
[14:46:32.331] iteration 1440 : loss : 89.643326, loss_ce: 0.016696, loss_kd: 445.696930
[14:46:38.050] iteration 1450 : loss : 78.611778, loss_ce: 0.013728, loss_kd: 390.571045
[14:46:43.757] iteration 1460 : loss : 78.406914, loss_ce: 0.038486, loss_kd: 389.466675
[14:46:49.475] iteration 1470 : loss : 58.269993, loss_ce: 0.016021, loss_kd: 288.844543
[14:46:55.188] iteration 1480 : loss : 43.138519, loss_ce: 0.027394, loss_kd: 213.222275
[14:47:00.913] iteration 1490 : loss : 117.863686, loss_ce: 0.012962, loss_kd: 586.846680
[14:47:06.643] iteration 1500 : loss : 73.100677, loss_ce: 0.031309, loss_kd: 362.936188
[14:47:12.383] iteration 1510 : loss : 78.671486, loss_ce: 0.016618, loss_kd: 390.833130
[14:47:18.111] iteration 1520 : loss : 80.403656, loss_ce: 0.016595, loss_kd: 399.536743
[14:47:23.849] iteration 1530 : loss : 87.244850, loss_ce: 0.025367, loss_kd: 433.665466
[14:47:29.569] iteration 1540 : loss : 54.509930, loss_ce: 0.020769, loss_kd: 270.037140
[14:47:35.311] iteration 1550 : loss : 75.566002, loss_ce: 0.008833, loss_kd: 375.334290
[14:47:41.041] iteration 1560 : loss : 61.081867, loss_ce: 0.013108, loss_kd: 302.938995
[14:47:46.779] iteration 1570 : loss : 80.589729, loss_ce: 0.024275, loss_kd: 400.466675
[14:47:52.511] iteration 1580 : loss : 202.171463, loss_ce: 0.019516, loss_kd: 1008.333130
[14:47:58.255] iteration 1590 : loss : 94.891640, loss_ce: 0.025680, loss_kd: 471.924500
[14:48:03.989] iteration 1600 : loss : 83.287704, loss_ce: 0.023572, loss_kd: 413.924072
[14:48:09.744] iteration 1610 : loss : 97.610130, loss_ce: 0.032203, loss_kd: 485.534790
[14:48:15.480] iteration 1620 : loss : 73.695229, loss_ce: 0.038838, loss_kd: 365.949524
[14:48:31.790] iteration 1630 : loss : 73.367058, loss_ce: 0.022311, loss_kd: 364.325500
[14:48:37.468] iteration 1640 : loss : 67.129105, loss_ce: 0.023999, loss_kd: 333.146362
[14:48:43.170] iteration 1650 : loss : 73.386314, loss_ce: 0.009460, loss_kd: 364.435486
[14:48:48.867] iteration 1660 : loss : 72.326492, loss_ce: 0.009681, loss_kd: 359.155884
[14:48:54.577] iteration 1670 : loss : 133.787125, loss_ce: 0.023081, loss_kd: 666.399902
[14:49:00.275] iteration 1680 : loss : 84.908005, loss_ce: 0.022782, loss_kd: 422.006775
[14:49:05.988] iteration 1690 : loss : 89.088997, loss_ce: 0.019585, loss_kd: 442.926575
[14:49:11.693] iteration 1700 : loss : 95.315613, loss_ce: 0.015124, loss_kd: 474.108337
[14:49:17.418] iteration 1710 : loss : 60.655396, loss_ce: 0.029660, loss_kd: 300.754913
[14:49:23.132] iteration 1720 : loss : 51.060040, loss_ce: 0.009035, loss_kd: 252.815186
[14:49:28.860] iteration 1730 : loss : 60.590313, loss_ce: 0.020505, loss_kd: 300.439697
[14:49:34.580] iteration 1740 : loss : 44.363026, loss_ce: 0.015660, loss_kd: 219.283981
[14:49:40.314] iteration 1750 : loss : 72.696030, loss_ce: 0.042914, loss_kd: 360.928101
[14:49:46.048] iteration 1760 : loss : 59.214066, loss_ce: 0.037966, loss_kd: 293.515411
[14:49:51.785] iteration 1770 : loss : 62.139343, loss_ce: 0.008066, loss_kd: 308.241028
[14:49:57.517] iteration 1780 : loss : 46.196651, loss_ce: 0.022035, loss_kd: 228.513672
[14:50:03.261] iteration 1790 : loss : 67.406830, loss_ce: 0.025660, loss_kd: 334.505127
[14:50:08.994] iteration 1800 : loss : 61.106831, loss_ce: 0.021234, loss_kd: 303.036316
[14:50:14.734] iteration 1810 : loss : 75.181252, loss_ce: 0.020739, loss_kd: 373.355499
[14:50:20.477] iteration 1820 : loss : 57.892361, loss_ce: 0.011611, loss_kd: 286.960175
[14:50:26.228] iteration 1830 : loss : 51.017780, loss_ce: 0.022256, loss_kd: 252.550446
[14:50:31.973] iteration 1840 : loss : 46.772354, loss_ce: 0.011400, loss_kd: 231.383606
[14:50:37.721] iteration 1850 : loss : 49.837788, loss_ce: 0.017553, loss_kd: 246.713104
[14:50:54.216] iteration 1860 : loss : 53.707745, loss_ce: 0.019280, loss_kd: 266.034058
[14:50:59.909] iteration 1870 : loss : 41.334824, loss_ce: 0.026809, loss_kd: 204.170898
[14:51:05.597] iteration 1880 : loss : 46.477837, loss_ce: 0.011982, loss_kd: 229.905350
[14:51:11.298] iteration 1890 : loss : 34.922226, loss_ce: 0.023075, loss_kd: 172.138077
[14:51:17.000] iteration 1900 : loss : 40.448521, loss_ce: 0.014645, loss_kd: 199.746918
[14:51:22.719] iteration 1910 : loss : 37.560207, loss_ce: 0.017471, loss_kd: 185.313049
[14:51:28.430] iteration 1920 : loss : 40.021801, loss_ce: 0.024971, loss_kd: 197.639786
[14:51:34.152] iteration 1930 : loss : 42.945034, loss_ce: 0.024689, loss_kd: 212.175552
[14:51:39.863] iteration 1940 : loss : 34.073105, loss_ce: 0.018785, loss_kd: 167.845856
[14:51:45.588] iteration 1950 : loss : 40.975193, loss_ce: 0.011178, loss_kd: 202.383698
[14:51:51.305] iteration 1960 : loss : 43.559597, loss_ce: 0.039316, loss_kd: 215.294189
[14:51:57.035] iteration 1970 : loss : 38.124794, loss_ce: 0.019254, loss_kd: 188.131531
[14:52:02.756] iteration 1980 : loss : 37.253246, loss_ce: 0.027465, loss_kd: 183.776245
[14:52:08.484] iteration 1990 : loss : 40.546280, loss_ce: 0.027135, loss_kd: 200.176376
[14:52:14.210] iteration 2000 : loss : 33.666553, loss_ce: 0.015973, loss_kd: 165.829407
[14:52:19.948] iteration 2010 : loss : 36.888851, loss_ce: 0.027087, loss_kd: 181.942795
[14:52:25.679] iteration 2020 : loss : 42.417904, loss_ce: 0.020550, loss_kd: 209.574890
[14:52:31.424] iteration 2030 : loss : 38.655998, loss_ce: 0.018469, loss_kd: 190.771561
[14:52:37.155] iteration 2040 : loss : 42.616894, loss_ce: 0.031342, loss_kd: 210.575836
[14:52:42.901] iteration 2050 : loss : 34.435417, loss_ce: 0.023374, loss_kd: 169.619308
[14:52:48.651] iteration 2060 : loss : 36.713894, loss_ce: 0.025018, loss_kd: 181.073776
[14:52:54.399] iteration 2070 : loss : 40.866039, loss_ce: 0.020719, loss_kd: 201.837738
[14:53:00.135] iteration 2080 : loss : 43.378380, loss_ce: 0.020277, loss_kd: 214.338867
[14:53:17.015] iteration 2090 : loss : 54.735813, loss_ce: 0.015373, loss_kd: 271.172760
[14:53:22.692] iteration 2100 : loss : 52.573498, loss_ce: 0.014187, loss_kd: 260.402954
[14:53:28.392] iteration 2110 : loss : 32.887993, loss_ce: 0.021472, loss_kd: 161.961105
[14:53:34.084] iteration 2120 : loss : 65.463829, loss_ce: 0.023774, loss_kd: 324.815521
[14:53:39.794] iteration 2130 : loss : 55.701733, loss_ce: 0.017252, loss_kd: 276.003235
[14:53:45.501] iteration 2140 : loss : 64.929703, loss_ce: 0.040578, loss_kd: 322.133301
[14:53:51.215] iteration 2150 : loss : 96.660393, loss_ce: 0.013231, loss_kd: 480.821869
[14:53:56.927] iteration 2160 : loss : 112.783875, loss_ce: 0.019768, loss_kd: 561.439514
[14:54:02.652] iteration 2170 : loss : 70.291214, loss_ce: 0.035933, loss_kd: 348.899292
[14:54:08.363] iteration 2180 : loss : 86.565689, loss_ce: 0.014256, loss_kd: 430.301239
[14:54:14.092] iteration 2190 : loss : 92.498825, loss_ce: 0.023382, loss_kd: 460.008453
[14:54:19.817] iteration 2200 : loss : 149.427017, loss_ce: 0.023593, loss_kd: 744.594849
[14:54:25.547] iteration 2210 : loss : 71.316109, loss_ce: 0.010368, loss_kd: 354.099487
[14:54:31.276] iteration 2220 : loss : 74.248276, loss_ce: 0.022409, loss_kd: 368.694702
[14:54:37.018] iteration 2230 : loss : 88.267761, loss_ce: 0.032556, loss_kd: 438.794617
[14:54:42.747] iteration 2240 : loss : 89.767143, loss_ce: 0.034982, loss_kd: 446.283966
[14:54:48.487] iteration 2250 : loss : 63.251034, loss_ce: 0.032543, loss_kd: 313.701996
[14:54:54.213] iteration 2260 : loss : 86.025124, loss_ce: 0.019478, loss_kd: 427.637512
[14:54:59.954] iteration 2270 : loss : 75.707123, loss_ce: 0.024259, loss_kd: 375.993469
[14:55:05.682] iteration 2280 : loss : 69.558090, loss_ce: 0.014578, loss_kd: 345.239807
[14:55:11.434] iteration 2290 : loss : 80.938683, loss_ce: 0.016230, loss_kd: 402.170654
[14:55:17.176] iteration 2300 : loss : 76.734619, loss_ce: 0.018690, loss_kd: 381.126434
[14:55:22.934] iteration 2310 : loss : 52.146446, loss_ce: 0.030417, loss_kd: 258.219940
[14:55:28.619] iteration 2320 : loss : 87.850296, loss_ce: 0.026866, loss_kd: 436.739594
[14:55:29.327] save model to ./simple_finetuning_lits17\continual_surgical_tpgm_stage2_epoch_9.pth
[14:55:45.134] iteration 2330 : loss : 55.670269, loss_ce: 0.027148, loss_kd: 275.833038
[14:55:50.819] iteration 2340 : loss : 65.838326, loss_ce: 0.033193, loss_kd: 326.652527
[14:55:56.518] iteration 2350 : loss : 61.325283, loss_ce: 0.021201, loss_kd: 304.131042
[14:56:02.213] iteration 2360 : loss : 66.229729, loss_ce: 0.011562, loss_kd: 328.670532
[14:56:07.921] iteration 2370 : loss : 63.254936, loss_ce: 0.028862, loss_kd: 313.770020
[14:56:13.627] iteration 2380 : loss : 51.900673, loss_ce: 0.014925, loss_kd: 257.027100
[14:56:19.343] iteration 2390 : loss : 65.976921, loss_ce: 0.011655, loss_kd: 327.383881
[14:56:25.047] iteration 2400 : loss : 79.263130, loss_ce: 0.030389, loss_kd: 393.819824
[14:56:30.773] iteration 2410 : loss : 50.822758, loss_ce: 0.029596, loss_kd: 251.570572
[14:56:36.487] iteration 2420 : loss : 64.365120, loss_ce: 0.040810, loss_kd: 319.295837
[14:56:42.215] iteration 2430 : loss : 70.934898, loss_ce: 0.022554, loss_kd: 352.167847
[14:56:47.942] iteration 2440 : loss : 83.461632, loss_ce: 0.019391, loss_kd: 414.806671
[14:56:53.674] iteration 2450 : loss : 72.159737, loss_ce: 0.023867, loss_kd: 358.305847
[14:56:59.406] iteration 2460 : loss : 94.105865, loss_ce: 0.020646, loss_kd: 467.999115
[14:57:05.156] iteration 2470 : loss : 67.903488, loss_ce: 0.018076, loss_kd: 337.044495
[14:57:10.893] iteration 2480 : loss : 61.234856, loss_ce: 0.022658, loss_kd: 303.674011
[14:57:16.644] iteration 2490 : loss : 105.922440, loss_ce: 0.009187, loss_kd: 527.144226
[14:57:22.379] iteration 2500 : loss : 99.808769, loss_ce: 0.011456, loss_kd: 496.520569
[14:57:28.132] iteration 2510 : loss : 70.987518, loss_ce: 0.029649, loss_kd: 352.414581
[14:57:33.868] iteration 2520 : loss : 68.037354, loss_ce: 0.008069, loss_kd: 337.638367
[14:57:39.614] iteration 2530 : loss : 69.227081, loss_ce: 0.023707, loss_kd: 343.641571
[14:57:45.361] iteration 2540 : loss : 61.146370, loss_ce: 0.014767, loss_kd: 303.234497
[14:57:51.108] iteration 2550 : loss : 110.421806, loss_ce: 0.019414, loss_kd: 549.604004
[14:58:07.550] iteration 2560 : loss : 79.726196, loss_ce: 0.025185, loss_kd: 396.077911
[14:58:13.247] iteration 2570 : loss : 68.774055, loss_ce: 0.026515, loss_kd: 341.374420
[14:58:18.930] iteration 2580 : loss : 170.076126, loss_ce: 0.019470, loss_kd: 847.906067
[14:58:24.633] iteration 2590 : loss : 95.320305, loss_ce: 0.012512, loss_kd: 474.090057
[14:58:30.334] iteration 2600 : loss : 62.357544, loss_ce: 0.012007, loss_kd: 309.284332
[14:58:36.055] iteration 2610 : loss : 109.888641, loss_ce: 0.014085, loss_kd: 546.966248
[14:58:41.763] iteration 2620 : loss : 79.676025, loss_ce: 0.034065, loss_kd: 395.828552
[14:58:47.476] iteration 2630 : loss : 85.331879, loss_ce: 0.013703, loss_kd: 424.180298
[14:58:53.187] iteration 2640 : loss : 162.571381, loss_ce: 0.025826, loss_kd: 810.397949
[14:58:58.913] iteration 2650 : loss : 136.168701, loss_ce: 0.010696, loss_kd: 678.384399
[14:59:04.639] iteration 2660 : loss : 64.602173, loss_ce: 0.023653, loss_kd: 320.469421
[14:59:10.374] iteration 2670 : loss : 66.216537, loss_ce: 0.017415, loss_kd: 328.576843
[14:59:16.102] iteration 2680 : loss : 52.760708, loss_ce: 0.011826, loss_kd: 261.360016
[14:59:21.844] iteration 2690 : loss : 126.747482, loss_ce: 0.019398, loss_kd: 631.204224
[14:59:27.582] iteration 2700 : loss : 60.573990, loss_ce: 0.016362, loss_kd: 300.387421
[14:59:33.325] iteration 2710 : loss : 63.410328, loss_ce: 0.008972, loss_kd: 314.543182
[14:59:39.063] iteration 2720 : loss : 64.551880, loss_ce: 0.012218, loss_kd: 320.294067
[14:59:44.817] iteration 2730 : loss : 55.590157, loss_ce: 0.020873, loss_kd: 275.488342
[14:59:50.554] iteration 2740 : loss : 94.507828, loss_ce: 0.019028, loss_kd: 470.016602
[14:59:56.317] iteration 2750 : loss : 52.690922, loss_ce: 0.021914, loss_kd: 260.948975
[15:00:02.061] iteration 2760 : loss : 54.269947, loss_ce: 0.021053, loss_kd: 268.848755
[15:00:07.818] iteration 2770 : loss : 79.784660, loss_ce: 0.026163, loss_kd: 396.417572
[15:00:13.560] iteration 2780 : loss : 68.519058, loss_ce: 0.032512, loss_kd: 340.084564
[15:00:29.978] iteration 2790 : loss : 112.046989, loss_ce: 0.018655, loss_kd: 557.755005
[15:00:35.660] iteration 2800 : loss : 55.081341, loss_ce: 0.022092, loss_kd: 272.918060
[15:00:41.368] iteration 2810 : loss : 93.050110, loss_ce: 0.010794, loss_kd: 462.741852
[15:00:47.063] iteration 2820 : loss : 105.199623, loss_ce: 0.012168, loss_kd: 523.519043
[15:00:52.776] iteration 2830 : loss : 131.740662, loss_ce: 0.019830, loss_kd: 656.197571
[15:00:58.481] iteration 2840 : loss : 84.451126, loss_ce: 0.025763, loss_kd: 419.711975
[15:01:04.197] iteration 2850 : loss : 115.188850, loss_ce: 0.019750, loss_kd: 573.422058
[15:01:09.903] iteration 2860 : loss : 56.273918, loss_ce: 0.017000, loss_kd: 278.898926
[15:01:15.625] iteration 2870 : loss : 46.708263, loss_ce: 0.024158, loss_kd: 231.047287
[15:01:21.348] iteration 2880 : loss : 57.545269, loss_ce: 0.011730, loss_kd: 285.236176
[15:01:27.077] iteration 2890 : loss : 62.530521, loss_ce: 0.015130, loss_kd: 310.176636
[15:01:32.792] iteration 2900 : loss : 70.199165, loss_ce: 0.014213, loss_kd: 348.465668
[15:01:38.539] iteration 2910 : loss : 59.090248, loss_ce: 0.033821, loss_kd: 292.927490
[15:01:44.266] iteration 2920 : loss : 44.514545, loss_ce: 0.036249, loss_kd: 220.008102
[15:01:50.007] iteration 2930 : loss : 60.771332, loss_ce: 0.007875, loss_kd: 301.385315
[15:01:55.739] iteration 2940 : loss : 44.276535, loss_ce: 0.022143, loss_kd: 218.908218
[15:02:01.484] iteration 2950 : loss : 63.889870, loss_ce: 0.021910, loss_kd: 316.937164
[15:02:07.233] iteration 2960 : loss : 82.770790, loss_ce: 0.021047, loss_kd: 411.340637
[15:02:12.983] iteration 2970 : loss : 68.767998, loss_ce: 0.019156, loss_kd: 341.297852
[15:02:18.734] iteration 2980 : loss : 50.104122, loss_ce: 0.016988, loss_kd: 248.009933
[15:02:24.483] iteration 2990 : loss : 69.863998, loss_ce: 0.020627, loss_kd: 346.789032
[15:02:30.233] iteration 3000 : loss : 75.677322, loss_ce: 0.011648, loss_kd: 375.897003
[15:02:35.996] iteration 3010 : loss : 58.630890, loss_ce: 0.016380, loss_kd: 290.671631
[15:02:52.799] iteration 3020 : loss : 47.496971, loss_ce: 0.016104, loss_kd: 234.987717
[15:02:58.492] iteration 3030 : loss : 47.539623, loss_ce: 0.025347, loss_kd: 235.197784
[15:03:04.185] iteration 3040 : loss : 48.548512, loss_ce: 0.009669, loss_kd: 240.267929
[15:03:09.888] iteration 3050 : loss : 44.368462, loss_ce: 0.026760, loss_kd: 219.350937
[15:03:15.585] iteration 3060 : loss : 38.916443, loss_ce: 0.013366, loss_kd: 192.095886
[15:03:21.299] iteration 3070 : loss : 147.588379, loss_ce: 0.015999, loss_kd: 735.464111
[15:03:27.005] iteration 3080 : loss : 168.263870, loss_ce: 0.018154, loss_kd: 838.865723
[15:03:32.725] iteration 3090 : loss : 157.476273, loss_ce: 0.024156, loss_kd: 784.823120
[15:03:38.435] iteration 3100 : loss : 113.182449, loss_ce: 0.019125, loss_kd: 563.395752
[15:03:44.159] iteration 3110 : loss : 62.507282, loss_ce: 0.011268, loss_kd: 310.040100
[15:03:49.880] iteration 3120 : loss : 80.668556, loss_ce: 0.037791, loss_kd: 400.842194
[15:03:55.612] iteration 3130 : loss : 80.570816, loss_ce: 0.016282, loss_kd: 400.375580
[15:04:01.337] iteration 3140 : loss : 69.888596, loss_ce: 0.023157, loss_kd: 346.966919
[15:04:07.070] iteration 3150 : loss : 62.925072, loss_ce: 0.019703, loss_kd: 312.102417
[15:04:12.804] iteration 3160 : loss : 67.430214, loss_ce: 0.008292, loss_kd: 334.693268
[15:04:18.560] iteration 3170 : loss : 78.779709, loss_ce: 0.023178, loss_kd: 391.423309
[15:04:24.298] iteration 3180 : loss : 109.098183, loss_ce: 0.019485, loss_kd: 542.975281
[15:04:30.042] iteration 3190 : loss : 55.732079, loss_ce: 0.017947, loss_kd: 276.153168
[15:04:35.778] iteration 3200 : loss : 55.966232, loss_ce: 0.029287, loss_kd: 277.336731
[15:04:41.534] iteration 3210 : loss : 112.589363, loss_ce: 0.022049, loss_kd: 560.398438
[15:04:47.271] iteration 3220 : loss : 50.647434, loss_ce: 0.022762, loss_kd: 250.749802
[15:04:53.017] iteration 3230 : loss : 46.969589, loss_ce: 0.018912, loss_kd: 232.373749
[15:04:58.753] iteration 3240 : loss : 67.503693, loss_ce: 0.020829, loss_kd: 334.972351
[15:05:15.393] iteration 3250 : loss : 67.688820, loss_ce: 0.009381, loss_kd: 335.964172
[15:05:21.082] iteration 3260 : loss : 65.736313, loss_ce: 0.013177, loss_kd: 326.214630
[15:05:26.778] iteration 3270 : loss : 58.626965, loss_ce: 0.017393, loss_kd: 290.671600
[15:05:32.469] iteration 3280 : loss : 48.036724, loss_ce: 0.021296, loss_kd: 237.699600
[15:05:38.182] iteration 3290 : loss : 50.147762, loss_ce: 0.014709, loss_kd: 248.243729
[15:05:43.890] iteration 3300 : loss : 47.842186, loss_ce: 0.033214, loss_kd: 236.710007
[15:05:49.610] iteration 3310 : loss : 73.534828, loss_ce: 0.014140, loss_kd: 365.194946
[15:05:55.324] iteration 3320 : loss : 65.521919, loss_ce: 0.018191, loss_kd: 325.128510
[15:06:01.049] iteration 3330 : loss : 58.322895, loss_ce: 0.024420, loss_kd: 289.104950
[15:06:06.770] iteration 3340 : loss : 115.400795, loss_ce: 0.013114, loss_kd: 574.493225
[15:06:12.504] iteration 3350 : loss : 52.207634, loss_ce: 0.021487, loss_kd: 258.560333
[15:06:18.232] iteration 3360 : loss : 75.914642, loss_ce: 0.011318, loss_kd: 377.035339
[15:06:23.975] iteration 3370 : loss : 66.319527, loss_ce: 0.009960, loss_kd: 329.095581
[15:06:29.703] iteration 3380 : loss : 56.559250, loss_ce: 0.017548, loss_kd: 280.254974
[15:06:35.447] iteration 3390 : loss : 57.695770, loss_ce: 0.029904, loss_kd: 285.947571
[15:06:41.178] iteration 3400 : loss : 58.442577, loss_ce: 0.033763, loss_kd: 289.666077
[15:06:46.927] iteration 3410 : loss : 49.965073, loss_ce: 0.031466, loss_kd: 247.280762
[15:06:52.663] iteration 3420 : loss : 46.790943, loss_ce: 0.013962, loss_kd: 231.493179
[15:06:58.417] iteration 3430 : loss : 41.989082, loss_ce: 0.019465, loss_kd: 207.441635
[15:07:04.160] iteration 3440 : loss : 49.672115, loss_ce: 0.018072, loss_kd: 245.807251
[15:07:09.910] iteration 3450 : loss : 50.563072, loss_ce: 0.013869, loss_kd: 250.313736
[15:07:15.655] iteration 3460 : loss : 63.320831, loss_ce: 0.014053, loss_kd: 314.103638
[15:07:21.407] iteration 3470 : loss : 72.483650, loss_ce: 0.028198, loss_kd: 359.926208
[15:07:27.087] iteration 3480 : loss : 49.313602, loss_ce: 0.020449, loss_kd: 244.084198
[15:07:27.816] save model to ./simple_finetuning_lits17\continual_surgical_tpgm_stage2_epoch_14.pth
[15:07:27.893] save final model to ./simple_finetuning_lits17\continual_surgical_tpgm_stage2_final.pth
[04:37:20.787] Namespace(root_path='./datasets/lits17/train_npz', dataset='lits17', list_dir='./lists/lits17', stage=2, num_classes_old=12, num_classes_new=3, num_classes_lits17=3, output_dir='./simple_finetuning_lits17', max_iterations=10000, max_epochs=30, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./debug_simple/continual_surgical_tpgm_final.pth', data_fraction=0.45, kd_temperature=3.0, kd_weight=0.2, freeze_old_classes=False, auto_tune='none', gradient_batches=5, tpgm_norm_mode='l2', tpgm_lr=0.01, tpgm_iters=200, tpgm_exclude=[], tpgm_frequency=5, tpgm_start_epoch=10, disable_tpgm=True, tpgm_data_fraction=0.1, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False)
[04:37:20.790] Stage 2: Using 6679/14843 samples (45.0%) for continual learning
[04:37:20.791] Old classes: 12, New classes: 3, Total: 14
[04:37:20.791] Dataset: lits17
[04:37:20.791] TPGM enabled: False
[04:37:20.791] Surgical fine-tuning method: none
[04:38:02.288] Combined Continual Learning Stage 2 + Surgical + TPGM Configuration:
[04:38:02.288] KD Temperature: 3.0
[04:38:02.288] KD Weight: 0.2
[04:38:02.288] Auto-tune method: none
[04:38:02.288] TPGM start epoch: 10
[04:38:02.288] TPGM frequency: 5
[04:38:02.289] 209 iterations per epoch. 6270 max iterations 
[04:38:18.542] iteration 10 : loss : 1106.397217, loss_ce: 0.615130, loss_kd: 5528.717773
[04:38:24.155] iteration 20 : loss : 414.305908, loss_ce: 0.501986, loss_kd: 2068.358887
[04:38:29.794] iteration 30 : loss : 236.055893, loss_ce: 0.553606, loss_kd: 1177.050293
[04:38:35.426] iteration 40 : loss : 171.134171, loss_ce: 0.434557, loss_kd: 852.542419
[04:38:41.074] iteration 50 : loss : 178.659073, loss_ce: 0.520648, loss_kd: 890.093384
[04:38:46.716] iteration 60 : loss : 114.342949, loss_ce: 0.419278, loss_kd: 568.599731
[04:38:52.367] iteration 70 : loss : 104.830124, loss_ce: 0.493956, loss_kd: 520.971741
[04:38:58.008] iteration 80 : loss : 171.846481, loss_ce: 0.463556, loss_kd: 856.081177
[04:39:03.666] iteration 90 : loss : 148.821793, loss_ce: 0.479063, loss_kd: 740.941895
[04:39:09.314] iteration 100 : loss : 115.798492, loss_ce: 0.407577, loss_kd: 575.884216
[04:39:14.975] iteration 110 : loss : 108.730827, loss_ce: 0.353080, loss_kd: 540.595703
[04:39:20.622] iteration 120 : loss : 91.855301, loss_ce: 0.390359, loss_kd: 456.189484
[04:39:26.284] iteration 130 : loss : 82.428024, loss_ce: 0.384585, loss_kd: 409.048828
[04:39:31.933] iteration 140 : loss : 69.131958, loss_ce: 0.353609, loss_kd: 342.602539
[04:39:37.595] iteration 150 : loss : 96.453903, loss_ce: 0.370562, loss_kd: 479.192810
[04:39:43.253] iteration 160 : loss : 135.629395, loss_ce: 0.321692, loss_kd: 675.101501
[04:39:48.917] iteration 170 : loss : 116.859947, loss_ce: 0.269628, loss_kd: 581.304871
[04:39:54.574] iteration 180 : loss : 146.053711, loss_ce: 0.322618, loss_kd: 727.236816
[04:40:00.238] iteration 190 : loss : 114.197594, loss_ce: 0.199599, loss_kd: 568.043823
[04:40:05.897] iteration 200 : loss : 91.363907, loss_ce: 0.190611, loss_kd: 453.888947
[04:40:22.923] iteration 210 : loss : 86.019371, loss_ce: 0.185733, loss_kd: 427.195312
[04:40:28.572] iteration 220 : loss : 242.716080, loss_ce: 0.215371, loss_kd: 1210.629395
[04:40:34.238] iteration 230 : loss : 122.800377, loss_ce: 0.103505, loss_kd: 611.171753
[04:40:39.895] iteration 240 : loss : 113.431038, loss_ce: 0.110716, loss_kd: 564.329895
[04:40:45.564] iteration 250 : loss : 144.409500, loss_ce: 0.070401, loss_kd: 719.274353
[04:40:51.226] iteration 260 : loss : 170.217590, loss_ce: 0.065658, loss_kd: 848.371948
[04:40:56.898] iteration 270 : loss : 101.271019, loss_ce: 0.081265, loss_kd: 503.642670
[04:41:02.563] iteration 280 : loss : 94.263611, loss_ce: 0.064721, loss_kd: 468.682251
[04:41:08.265] iteration 290 : loss : 116.106972, loss_ce: 0.024525, loss_kd: 577.941711
[04:41:13.930] iteration 300 : loss : 81.760254, loss_ce: 0.049971, loss_kd: 406.184143
[04:41:19.613] iteration 310 : loss : 102.838875, loss_ce: 0.042045, loss_kd: 511.581421
[04:41:25.280] iteration 320 : loss : 99.007500, loss_ce: 0.027830, loss_kd: 492.438110
[04:41:30.959] iteration 330 : loss : 74.874222, loss_ce: 0.030786, loss_kd: 371.790344
[04:41:36.629] iteration 340 : loss : 73.373932, loss_ce: 0.024763, loss_kd: 364.270844
[04:41:42.315] iteration 350 : loss : 72.035698, loss_ce: 0.024028, loss_kd: 357.613861
[04:41:47.985] iteration 360 : loss : 70.430954, loss_ce: 0.027538, loss_kd: 349.573608
[04:41:53.667] iteration 370 : loss : 49.552624, loss_ce: 0.030169, loss_kd: 245.203491
[04:41:59.340] iteration 380 : loss : 68.894951, loss_ce: 0.052853, loss_kd: 341.879395
[04:42:05.029] iteration 390 : loss : 122.446808, loss_ce: 0.015960, loss_kd: 609.670044
[04:42:10.705] iteration 400 : loss : 78.961372, loss_ce: 0.014391, loss_kd: 392.249237
[04:42:16.391] iteration 410 : loss : 233.897171, loss_ce: 0.036783, loss_kd: 1166.911987
[04:42:33.052] iteration 420 : loss : 114.898872, loss_ce: 0.022403, loss_kd: 571.953674
[04:42:38.722] iteration 430 : loss : 91.909309, loss_ce: 0.029273, loss_kd: 456.997528
[04:42:44.388] iteration 440 : loss : 69.166893, loss_ce: 0.027457, loss_kd: 343.257446
[04:42:50.066] iteration 450 : loss : 56.858650, loss_ce: 0.023867, loss_kd: 281.734009
[04:42:55.732] iteration 460 : loss : 60.180897, loss_ce: 0.019512, loss_kd: 298.342072
[04:43:01.410] iteration 470 : loss : 51.004677, loss_ce: 0.022755, loss_kd: 252.459946
[04:43:07.078] iteration 480 : loss : 53.940918, loss_ce: 0.020726, loss_kd: 267.177185
[04:43:12.772] iteration 490 : loss : 58.726627, loss_ce: 0.024926, loss_kd: 291.077606
[04:43:18.445] iteration 500 : loss : 51.778854, loss_ce: 0.016179, loss_kd: 256.363312
[04:43:24.135] iteration 510 : loss : 44.376194, loss_ce: 0.048502, loss_kd: 219.321152
[04:43:29.808] iteration 520 : loss : 48.627880, loss_ce: 0.023357, loss_kd: 240.601547
[04:43:35.487] iteration 530 : loss : 58.607430, loss_ce: 0.030366, loss_kd: 290.487366
[04:43:41.161] iteration 540 : loss : 58.341843, loss_ce: 0.019903, loss_kd: 289.189545
[04:43:46.844] iteration 550 : loss : 40.437145, loss_ce: 0.022997, loss_kd: 199.647491
[04:43:52.524] iteration 560 : loss : 46.380970, loss_ce: 0.022074, loss_kd: 229.373825
[04:43:58.211] iteration 570 : loss : 46.895451, loss_ce: 0.025196, loss_kd: 231.907379
[04:44:03.888] iteration 580 : loss : 51.620609, loss_ce: 0.030342, loss_kd: 255.544601
[04:44:09.576] iteration 590 : loss : 49.655453, loss_ce: 0.041192, loss_kd: 245.731308
[04:44:15.256] iteration 600 : loss : 55.413822, loss_ce: 0.017569, loss_kd: 274.520111
[04:44:20.949] iteration 610 : loss : 45.406101, loss_ce: 0.016588, loss_kd: 224.516815
[04:44:26.632] iteration 620 : loss : 46.424999, loss_ce: 0.025194, loss_kd: 229.568771
[04:44:42.804] iteration 630 : loss : 54.132053, loss_ce: 0.028071, loss_kd: 268.123596
[04:44:48.468] iteration 640 : loss : 57.252110, loss_ce: 0.028929, loss_kd: 283.712341
[04:44:54.142] iteration 650 : loss : 58.032936, loss_ce: 0.037988, loss_kd: 287.616364
[04:44:59.815] iteration 660 : loss : 61.495663, loss_ce: 0.014067, loss_kd: 304.961670
[04:45:05.492] iteration 670 : loss : 52.495708, loss_ce: 0.026730, loss_kd: 259.909576
[04:45:11.165] iteration 680 : loss : 62.400311, loss_ce: 0.033503, loss_kd: 309.457703
[04:45:16.846] iteration 690 : loss : 87.416367, loss_ce: 0.031506, loss_kd: 434.550354
[04:45:22.523] iteration 700 : loss : 83.066254, loss_ce: 0.023812, loss_kd: 412.822357
[04:45:28.214] iteration 710 : loss : 59.570061, loss_ce: 0.038724, loss_kd: 295.286560
[04:45:33.893] iteration 720 : loss : 70.924088, loss_ce: 0.013048, loss_kd: 352.135559
[04:45:39.580] iteration 730 : loss : 94.518654, loss_ce: 0.012663, loss_kd: 470.073792
[04:45:45.257] iteration 740 : loss : 267.624664, loss_ce: 0.037680, loss_kd: 1335.578125
[04:45:50.949] iteration 750 : loss : 171.905029, loss_ce: 0.021754, loss_kd: 857.027954
[04:45:56.633] iteration 760 : loss : 122.100410, loss_ce: 0.024422, loss_kd: 607.952148
[04:46:02.331] iteration 770 : loss : 110.486565, loss_ce: 0.023841, loss_kd: 549.899902
[04:46:08.019] iteration 780 : loss : 89.516808, loss_ce: 0.023533, loss_kd: 445.079681
[04:46:13.719] iteration 790 : loss : 65.056656, loss_ce: 0.023142, loss_kd: 322.730438
[04:46:19.411] iteration 800 : loss : 109.164856, loss_ce: 0.020670, loss_kd: 543.251648
[04:46:25.118] iteration 810 : loss : 170.657623, loss_ce: 0.036098, loss_kd: 850.776855
[04:46:30.808] iteration 820 : loss : 99.119759, loss_ce: 0.010598, loss_kd: 493.100037
[04:46:36.515] iteration 830 : loss : 81.436768, loss_ce: 0.014205, loss_kd: 404.660248
[04:46:52.728] iteration 840 : loss : 86.897194, loss_ce: 0.019113, loss_kd: 431.967651
[04:46:58.403] iteration 850 : loss : 69.417953, loss_ce: 0.020773, loss_kd: 344.549805
[04:47:04.074] iteration 860 : loss : 92.165344, loss_ce: 0.015434, loss_kd: 458.280334
[04:47:09.752] iteration 870 : loss : 84.010002, loss_ce: 0.026791, loss_kd: 417.536560
[04:47:15.430] iteration 880 : loss : 151.152145, loss_ce: 0.021033, loss_kd: 753.247681
[04:47:21.115] iteration 890 : loss : 111.741943, loss_ce: 0.019292, loss_kd: 556.189148
[04:47:26.792] iteration 900 : loss : 89.112640, loss_ce: 0.015566, loss_kd: 443.036591
[04:47:32.490] iteration 910 : loss : 100.944664, loss_ce: 0.023758, loss_kd: 502.216309
[04:47:38.171] iteration 920 : loss : 119.620102, loss_ce: 0.024106, loss_kd: 595.571045
[04:47:43.868] iteration 930 : loss : 89.456284, loss_ce: 0.033799, loss_kd: 444.764252
[04:47:49.554] iteration 940 : loss : 93.348869, loss_ce: 0.018945, loss_kd: 464.231262
[04:47:55.261] iteration 950 : loss : 69.586922, loss_ce: 0.015799, loss_kd: 345.418823
[04:48:00.955] iteration 960 : loss : 87.615791, loss_ce: 0.025604, loss_kd: 435.548645
[04:48:06.655] iteration 970 : loss : 70.920731, loss_ce: 0.020057, loss_kd: 352.102814
[04:48:12.349] iteration 980 : loss : 72.382545, loss_ce: 0.010625, loss_kd: 359.412659
[04:48:18.064] iteration 990 : loss : 133.325836, loss_ce: 0.016435, loss_kd: 664.137390
[04:48:23.760] iteration 1000 : loss : 85.958519, loss_ce: 0.018850, loss_kd: 427.257233
[04:48:29.467] iteration 1010 : loss : 96.674698, loss_ce: 0.018985, loss_kd: 480.859436
[04:48:35.168] iteration 1020 : loss : 70.102524, loss_ce: 0.020644, loss_kd: 348.002075
[04:48:40.876] iteration 1030 : loss : 67.735725, loss_ce: 0.023848, loss_kd: 336.165741
[04:48:46.573] iteration 1040 : loss : 77.513466, loss_ce: 0.016669, loss_kd: 385.085419
[04:48:50.016] save model to ./simple_finetuning_lits17\continual_surgical_tpgm_stage2_epoch_4.pth
[04:49:03.442] iteration 1050 : loss : 56.892323, loss_ce: 0.012647, loss_kd: 281.982849
[04:49:09.111] iteration 1060 : loss : 69.100830, loss_ce: 0.031493, loss_kd: 342.954926
[04:49:14.792] iteration 1070 : loss : 79.025261, loss_ce: 0.017515, loss_kd: 392.600952
[04:49:20.463] iteration 1080 : loss : 105.958038, loss_ce: 0.019346, loss_kd: 527.237610
[04:49:26.149] iteration 1090 : loss : 64.002892, loss_ce: 0.018256, loss_kd: 317.503204
[04:49:31.825] iteration 1100 : loss : 88.306931, loss_ce: 0.051329, loss_kd: 438.973083
[04:49:37.522] iteration 1110 : loss : 87.499619, loss_ce: 0.028235, loss_kd: 434.935547
[04:49:43.204] iteration 1120 : loss : 57.073494, loss_ce: 0.030501, loss_kd: 282.829651
[04:49:48.898] iteration 1130 : loss : 51.856339, loss_ce: 0.015283, loss_kd: 256.768799
[04:49:54.589] iteration 1140 : loss : 59.292244, loss_ce: 0.011840, loss_kd: 293.989014
[04:50:00.291] iteration 1150 : loss : 98.052727, loss_ce: 0.015405, loss_kd: 487.750549
[04:50:05.986] iteration 1160 : loss : 179.707520, loss_ce: 0.019296, loss_kd: 896.043701
[04:50:11.694] iteration 1170 : loss : 67.997566, loss_ce: 0.025489, loss_kd: 337.472168
[04:50:17.389] iteration 1180 : loss : 66.615349, loss_ce: 0.025822, loss_kd: 330.584198
[04:50:23.095] iteration 1190 : loss : 51.263508, loss_ce: 0.030639, loss_kd: 253.796692
[04:50:28.794] iteration 1200 : loss : 73.068016, loss_ce: 0.032765, loss_kd: 362.739807
[04:50:34.509] iteration 1210 : loss : 63.592243, loss_ce: 0.031945, loss_kd: 315.445312
[04:50:40.208] iteration 1220 : loss : 51.585148, loss_ce: 0.014429, loss_kd: 255.434708
[04:50:45.918] iteration 1230 : loss : 52.199242, loss_ce: 0.039209, loss_kd: 258.476898
[04:50:51.618] iteration 1240 : loss : 118.298691, loss_ce: 0.025303, loss_kd: 588.961243
[04:50:57.327] iteration 1250 : loss : 148.154968, loss_ce: 0.015158, loss_kd: 738.311951
[04:51:13.757] iteration 1260 : loss : 58.826019, loss_ce: 0.035117, loss_kd: 291.621094
[04:51:19.437] iteration 1270 : loss : 200.557434, loss_ce: 0.021149, loss_kd: 1000.283936
[04:51:25.110] iteration 1280 : loss : 80.415588, loss_ce: 0.009949, loss_kd: 399.588562
[04:51:30.798] iteration 1290 : loss : 77.156563, loss_ce: 0.015325, loss_kd: 383.198730
[04:51:36.479] iteration 1300 : loss : 80.916191, loss_ce: 0.018736, loss_kd: 402.052948
[04:51:42.178] iteration 1310 : loss : 76.712929, loss_ce: 0.029825, loss_kd: 381.052368
[04:51:47.862] iteration 1320 : loss : 104.096626, loss_ce: 0.020586, loss_kd: 517.987610
[04:51:53.559] iteration 1330 : loss : 100.648186, loss_ce: 0.019595, loss_kd: 500.711487
[04:51:59.246] iteration 1340 : loss : 88.067680, loss_ce: 0.035535, loss_kd: 437.816986
[04:52:04.949] iteration 1350 : loss : 80.613869, loss_ce: 0.028456, loss_kd: 400.562744
[04:52:10.648] iteration 1360 : loss : 60.575699, loss_ce: 0.013043, loss_kd: 300.362793
[04:52:16.354] iteration 1370 : loss : 47.857430, loss_ce: 0.017259, loss_kd: 236.739410
[04:52:22.045] iteration 1380 : loss : 158.388931, loss_ce: 0.018151, loss_kd: 789.430054
[04:52:27.757] iteration 1390 : loss : 86.811386, loss_ce: 0.014398, loss_kd: 431.590332
[04:52:33.456] iteration 1400 : loss : 70.101868, loss_ce: 0.033378, loss_kd: 348.029266
[04:52:39.163] iteration 1410 : loss : 68.328735, loss_ce: 0.025013, loss_kd: 339.125153
[04:52:44.862] iteration 1420 : loss : 53.562260, loss_ce: 0.042008, loss_kd: 265.255280
[04:52:50.574] iteration 1430 : loss : 54.644196, loss_ce: 0.014985, loss_kd: 270.752014
[04:52:56.279] iteration 1440 : loss : 68.053619, loss_ce: 0.012334, loss_kd: 337.769836
[04:53:01.999] iteration 1450 : loss : 107.081367, loss_ce: 0.013807, loss_kd: 532.872314
[04:53:07.707] iteration 1460 : loss : 81.085037, loss_ce: 0.021033, loss_kd: 402.929901
[04:53:24.090] iteration 1470 : loss : 65.052498, loss_ce: 0.021626, loss_kd: 322.728363
[04:53:29.760] iteration 1480 : loss : 51.733917, loss_ce: 0.019945, loss_kd: 256.177917
[04:53:35.447] iteration 1490 : loss : 65.695869, loss_ce: 0.023331, loss_kd: 325.988892
[04:53:41.133] iteration 1500 : loss : 94.950142, loss_ce: 0.015416, loss_kd: 472.209473
[04:53:46.828] iteration 1510 : loss : 115.637512, loss_ce: 0.017470, loss_kd: 575.700928
[04:53:52.513] iteration 1520 : loss : 92.336845, loss_ce: 0.016588, loss_kd: 459.164337
[04:53:58.208] iteration 1530 : loss : 98.249832, loss_ce: 0.048430, loss_kd: 488.690826
[04:54:03.898] iteration 1540 : loss : 76.280220, loss_ce: 0.010605, loss_kd: 378.851196
[04:54:09.595] iteration 1550 : loss : 69.805710, loss_ce: 0.019941, loss_kd: 346.524506
[04:54:15.292] iteration 1560 : loss : 77.466682, loss_ce: 0.033123, loss_kd: 384.813843
[04:54:20.999] iteration 1570 : loss : 65.798752, loss_ce: 0.011887, loss_kd: 326.487030
[04:54:26.696] iteration 1580 : loss : 59.653248, loss_ce: 0.009848, loss_kd: 295.775085
[04:54:32.406] iteration 1590 : loss : 53.593315, loss_ce: 0.020818, loss_kd: 265.453735
[04:54:38.107] iteration 1600 : loss : 52.609539, loss_ce: 0.023198, loss_kd: 260.546356
[04:54:43.829] iteration 1610 : loss : 84.262085, loss_ce: 0.015389, loss_kd: 418.789978
[04:54:49.527] iteration 1620 : loss : 98.725876, loss_ce: 0.028304, loss_kd: 491.104980
[04:54:55.246] iteration 1630 : loss : 92.395775, loss_ce: 0.029204, loss_kd: 459.462250
[04:55:00.949] iteration 1640 : loss : 44.277599, loss_ce: 0.011398, loss_kd: 218.895889
[04:55:06.669] iteration 1650 : loss : 62.973248, loss_ce: 0.021165, loss_kd: 312.347412
[04:55:12.375] iteration 1660 : loss : 57.741978, loss_ce: 0.023853, loss_kd: 286.188019
[04:55:18.094] iteration 1670 : loss : 43.123421, loss_ce: 0.022530, loss_kd: 213.092789
[04:55:34.418] iteration 1680 : loss : 50.165661, loss_ce: 0.017063, loss_kd: 248.299652
[04:55:40.105] iteration 1690 : loss : 36.951466, loss_ce: 0.041398, loss_kd: 182.252426
[04:55:45.782] iteration 1700 : loss : 45.086002, loss_ce: 0.014883, loss_kd: 222.900497
[04:55:51.473] iteration 1710 : loss : 48.743576, loss_ce: 0.010307, loss_kd: 241.222183
[04:55:57.156] iteration 1720 : loss : 46.450935, loss_ce: 0.016831, loss_kd: 229.721130
[04:56:02.852] iteration 1730 : loss : 80.386772, loss_ce: 0.018489, loss_kd: 399.427521
[04:56:08.539] iteration 1740 : loss : 53.808765, loss_ce: 0.026405, loss_kd: 266.479401
[04:56:14.238] iteration 1750 : loss : 51.210262, loss_ce: 0.017896, loss_kd: 253.515839
[04:56:19.934] iteration 1760 : loss : 49.552582, loss_ce: 0.031991, loss_kd: 245.201172
[04:56:25.643] iteration 1770 : loss : 43.936996, loss_ce: 0.012157, loss_kd: 217.205750
[04:56:31.343] iteration 1780 : loss : 49.700520, loss_ce: 0.024682, loss_kd: 245.995605
[04:56:37.051] iteration 1790 : loss : 49.984779, loss_ce: 0.015304, loss_kd: 247.432068
[04:56:42.749] iteration 1800 : loss : 39.746712, loss_ce: 0.020186, loss_kd: 196.213486
[04:56:48.468] iteration 1810 : loss : 51.734856, loss_ce: 0.026928, loss_kd: 256.147888
[04:56:54.172] iteration 1820 : loss : 54.400604, loss_ce: 0.016351, loss_kd: 269.476776
[04:56:59.885] iteration 1830 : loss : 86.417542, loss_ce: 0.033258, loss_kd: 429.568970
[04:57:05.589] iteration 1840 : loss : 118.461365, loss_ce: 0.025107, loss_kd: 589.822876
[04:57:11.299] iteration 1850 : loss : 109.291016, loss_ce: 0.022340, loss_kd: 543.954590
[04:57:17.016] iteration 1860 : loss : 111.313087, loss_ce: 0.028730, loss_kd: 554.055542
[04:57:22.733] iteration 1870 : loss : 105.788704, loss_ce: 0.031461, loss_kd: 526.426208
[04:57:28.438] iteration 1880 : loss : 75.924225, loss_ce: 0.032527, loss_kd: 377.101349
[04:57:44.755] iteration 1890 : loss : 79.227699, loss_ce: 0.014786, loss_kd: 393.648193
[04:57:50.431] iteration 1900 : loss : 52.310310, loss_ce: 0.021486, loss_kd: 259.050293
[04:57:56.119] iteration 1910 : loss : 65.662071, loss_ce: 0.021777, loss_kd: 325.819244
[04:58:01.795] iteration 1920 : loss : 105.839951, loss_ce: 0.023308, loss_kd: 526.652466
[04:58:07.483] iteration 1930 : loss : 75.208855, loss_ce: 0.023084, loss_kd: 373.510986
[04:58:13.165] iteration 1940 : loss : 46.078690, loss_ce: 0.021089, loss_kd: 227.902771
[04:58:18.865] iteration 1950 : loss : 61.067352, loss_ce: 0.014816, loss_kd: 302.833252
[04:58:24.555] iteration 1960 : loss : 56.955799, loss_ce: 0.025285, loss_kd: 282.267151
[04:58:30.260] iteration 1970 : loss : 55.564434, loss_ce: 0.016717, loss_kd: 275.292053
[04:58:35.953] iteration 1980 : loss : 78.017365, loss_ce: 0.034496, loss_kd: 387.525421
[04:58:41.663] iteration 1990 : loss : 75.912773, loss_ce: 0.021903, loss_kd: 377.054596
[04:58:47.363] iteration 2000 : loss : 60.643936, loss_ce: 0.049412, loss_kd: 300.656525
[04:58:53.079] iteration 2010 : loss : 50.824501, loss_ce: 0.016796, loss_kd: 251.577713
[04:58:58.778] iteration 2020 : loss : 40.161114, loss_ce: 0.028924, loss_kd: 198.284378
[04:59:04.492] iteration 2030 : loss : 52.230938, loss_ce: 0.027228, loss_kd: 258.645782
[04:59:10.196] iteration 2040 : loss : 48.123299, loss_ce: 0.029568, loss_kd: 238.098236
[04:59:15.917] iteration 2050 : loss : 42.089779, loss_ce: 0.023131, loss_kd: 207.938568
[04:59:21.628] iteration 2060 : loss : 48.808338, loss_ce: 0.011041, loss_kd: 241.480560
[04:59:27.353] iteration 2070 : loss : 74.569115, loss_ce: 0.019364, loss_kd: 370.339294
[04:59:33.064] iteration 2080 : loss : 53.917690, loss_ce: 0.016039, loss_kd: 267.061676
[04:59:38.619] iteration 2090 : loss : 59.588085, loss_ce: 0.031864, loss_kd: 295.458160
[04:59:39.355] save model to ./simple_finetuning_lits17\continual_surgical_tpgm_stage2_epoch_9.pth
[04:59:55.280] iteration 2100 : loss : 67.771027, loss_ce: 0.021560, loss_kd: 336.342499
[05:00:00.969] iteration 2110 : loss : 54.894737, loss_ce: 0.015773, loss_kd: 271.987640
[05:00:06.646] iteration 2120 : loss : 87.595512, loss_ce: 0.017736, loss_kd: 435.494263
[05:00:12.337] iteration 2130 : loss : 66.985901, loss_ce: 0.015156, loss_kd: 332.391724
[05:00:18.020] iteration 2140 : loss : 104.430679, loss_ce: 0.013213, loss_kd: 519.626709
[05:00:23.720] iteration 2150 : loss : 83.165871, loss_ce: 0.016051, loss_kd: 413.334656
[05:00:29.409] iteration 2160 : loss : 76.085846, loss_ce: 0.023098, loss_kd: 377.925293
[05:00:35.113] iteration 2170 : loss : 69.610901, loss_ce: 0.017171, loss_kd: 345.543610
[05:00:40.805] iteration 2180 : loss : 68.794670, loss_ce: 0.031358, loss_kd: 341.456177
[05:00:46.519] iteration 2190 : loss : 53.372894, loss_ce: 0.025140, loss_kd: 264.343323
[05:00:52.215] iteration 2200 : loss : 72.275803, loss_ce: 0.016984, loss_kd: 358.905884
[05:00:57.933] iteration 2210 : loss : 53.088615, loss_ce: 0.021540, loss_kd: 262.971191
[05:01:03.635] iteration 2220 : loss : 119.734276, loss_ce: 0.017273, loss_kd: 596.173950
[05:01:09.342] iteration 2230 : loss : 218.667709, loss_ce: 0.027915, loss_kd: 1090.785767
[05:01:15.040] iteration 2240 : loss : 92.607376, loss_ce: 0.032571, loss_kd: 460.499146
[05:01:20.752] iteration 2250 : loss : 69.965401, loss_ce: 0.030964, loss_kd: 347.325958
[05:01:26.464] iteration 2260 : loss : 47.799690, loss_ce: 0.021302, loss_kd: 236.491745
[05:01:32.188] iteration 2270 : loss : 58.759769, loss_ce: 0.037197, loss_kd: 291.293274
[05:01:37.898] iteration 2280 : loss : 47.884979, loss_ce: 0.011668, loss_kd: 236.912415
[05:01:43.617] iteration 2290 : loss : 58.357021, loss_ce: 0.009882, loss_kd: 289.324951
[05:01:59.752] iteration 2300 : loss : 150.321793, loss_ce: 0.025298, loss_kd: 749.097229
[05:02:05.437] iteration 2310 : loss : 74.440895, loss_ce: 0.038392, loss_kd: 369.685974
[05:02:11.117] iteration 2320 : loss : 70.409782, loss_ce: 0.017447, loss_kd: 349.553223
[05:02:16.817] iteration 2330 : loss : 62.386047, loss_ce: 0.030203, loss_kd: 309.421356
[05:02:22.502] iteration 2340 : loss : 65.703125, loss_ce: 0.024613, loss_kd: 325.981750
[05:02:28.205] iteration 2350 : loss : 78.976768, loss_ce: 0.024643, loss_kd: 392.393768
[05:02:33.900] iteration 2360 : loss : 85.017113, loss_ce: 0.018168, loss_kd: 422.621552
[05:02:39.606] iteration 2370 : loss : 114.589996, loss_ce: 0.030043, loss_kd: 570.444580
[05:02:45.309] iteration 2380 : loss : 80.133789, loss_ce: 0.012302, loss_kd: 398.172882
[05:02:51.021] iteration 2390 : loss : 98.286079, loss_ce: 0.033587, loss_kd: 488.905823
[05:02:56.723] iteration 2400 : loss : 87.679405, loss_ce: 0.030989, loss_kd: 435.882782
[05:03:02.438] iteration 2410 : loss : 69.143837, loss_ce: 0.022116, loss_kd: 343.168671
[05:03:08.142] iteration 2420 : loss : 68.010208, loss_ce: 0.021157, loss_kd: 337.547791
[05:03:13.852] iteration 2430 : loss : 59.364380, loss_ce: 0.018369, loss_kd: 294.291901
[05:03:19.562] iteration 2440 : loss : 84.385536, loss_ce: 0.021610, loss_kd: 419.418945
[05:03:25.279] iteration 2450 : loss : 62.744900, loss_ce: 0.021316, loss_kd: 311.197205
[05:03:30.986] iteration 2460 : loss : 56.695225, loss_ce: 0.022838, loss_kd: 280.985657
[05:03:36.708] iteration 2470 : loss : 65.600410, loss_ce: 0.044916, loss_kd: 325.468506
[05:03:42.416] iteration 2480 : loss : 66.683746, loss_ce: 0.016243, loss_kd: 330.877777
[05:03:48.131] iteration 2490 : loss : 156.691406, loss_ce: 0.009286, loss_kd: 780.950806
[05:03:53.844] iteration 2500 : loss : 126.872093, loss_ce: 0.026533, loss_kd: 631.859619
[05:04:10.456] iteration 2510 : loss : 91.889061, loss_ce: 0.014203, loss_kd: 456.965881
[05:04:16.129] iteration 2520 : loss : 81.918495, loss_ce: 0.029454, loss_kd: 407.075867
[05:04:21.820] iteration 2530 : loss : 58.456081, loss_ce: 0.016276, loss_kd: 289.786285
[05:04:27.500] iteration 2540 : loss : 50.650597, loss_ce: 0.024693, loss_kd: 250.729767
[05:04:33.191] iteration 2550 : loss : 90.131477, loss_ce: 0.018836, loss_kd: 448.125671
[05:04:38.880] iteration 2560 : loss : 95.498688, loss_ce: 0.022740, loss_kd: 474.953278
[05:04:44.580] iteration 2570 : loss : 64.393089, loss_ce: 0.017393, loss_kd: 319.495605
[05:04:50.274] iteration 2580 : loss : 58.338787, loss_ce: 0.018346, loss_kd: 289.205566
[05:04:55.978] iteration 2590 : loss : 55.831215, loss_ce: 0.011670, loss_kd: 276.682404
[05:05:01.675] iteration 2600 : loss : 73.708244, loss_ce: 0.031456, loss_kd: 366.058685
[05:05:07.387] iteration 2610 : loss : 91.974243, loss_ce: 0.021218, loss_kd: 457.368317
[05:05:13.084] iteration 2620 : loss : 94.756676, loss_ce: 0.027559, loss_kd: 471.299835
[05:05:18.796] iteration 2630 : loss : 100.229630, loss_ce: 0.015310, loss_kd: 498.692627
[05:05:24.492] iteration 2640 : loss : 72.980644, loss_ce: 0.023622, loss_kd: 362.379822
[05:05:30.207] iteration 2650 : loss : 68.694260, loss_ce: 0.019124, loss_kd: 340.967010
[05:05:35.912] iteration 2660 : loss : 59.123131, loss_ce: 0.021090, loss_kd: 293.075195
[05:05:41.628] iteration 2670 : loss : 74.030014, loss_ce: 0.027310, loss_kd: 367.622375
[05:05:47.338] iteration 2680 : loss : 49.886848, loss_ce: 0.042141, loss_kd: 246.911499
[05:05:53.064] iteration 2690 : loss : 94.187943, loss_ce: 0.017182, loss_kd: 468.404083
[05:05:58.771] iteration 2700 : loss : 44.250130, loss_ce: 0.013167, loss_kd: 218.792389
[05:06:04.493] iteration 2710 : loss : 57.453049, loss_ce: 0.017699, loss_kd: 284.748657
[05:06:21.412] iteration 2720 : loss : 61.304588, loss_ce: 0.019689, loss_kd: 304.007172
[05:06:27.093] iteration 2730 : loss : 78.533585, loss_ce: 0.021986, loss_kd: 390.158081
[05:06:32.767] iteration 2740 : loss : 80.433105, loss_ce: 0.027812, loss_kd: 399.661804
[05:06:38.459] iteration 2750 : loss : 74.092926, loss_ce: 0.011492, loss_kd: 367.963562
[05:06:44.141] iteration 2760 : loss : 54.460476, loss_ce: 0.018707, loss_kd: 269.767609
[05:06:49.835] iteration 2770 : loss : 79.033173, loss_ce: 0.026180, loss_kd: 392.660492
[05:06:55.531] iteration 2780 : loss : 49.287170, loss_ce: 0.029559, loss_kd: 243.933533
[05:07:01.232] iteration 2790 : loss : 77.912895, loss_ce: 0.021553, loss_kd: 387.066956
[05:07:06.924] iteration 2800 : loss : 105.600296, loss_ce: 0.024231, loss_kd: 525.507507
[05:07:12.633] iteration 2810 : loss : 54.148808, loss_ce: 0.011064, loss_kd: 268.264465
[05:07:18.329] iteration 2820 : loss : 53.017056, loss_ce: 0.010403, loss_kd: 262.597961
[05:07:24.043] iteration 2830 : loss : 84.606918, loss_ce: 0.038317, loss_kd: 420.503357
[05:07:29.743] iteration 2840 : loss : 55.400532, loss_ce: 0.018844, loss_kd: 274.538208
[05:07:35.456] iteration 2850 : loss : 57.214855, loss_ce: 0.021807, loss_kd: 283.549896
[05:07:41.166] iteration 2860 : loss : 60.399441, loss_ce: 0.015018, loss_kd: 299.510437
[05:07:46.880] iteration 2870 : loss : 54.600163, loss_ce: 0.024642, loss_kd: 270.504333
[05:07:52.584] iteration 2880 : loss : 47.990517, loss_ce: 0.014566, loss_kd: 237.442612
[05:07:58.303] iteration 2890 : loss : 59.082870, loss_ce: 0.021873, loss_kd: 292.874146
[05:08:04.015] iteration 2900 : loss : 42.470600, loss_ce: 0.028407, loss_kd: 209.865219
[05:08:09.744] iteration 2910 : loss : 37.881950, loss_ce: 0.009378, loss_kd: 186.915039
[05:08:15.457] iteration 2920 : loss : 74.633064, loss_ce: 0.013367, loss_kd: 370.664398
[05:08:31.930] iteration 2930 : loss : 71.449387, loss_ce: 0.021297, loss_kd: 354.737976
[05:08:37.603] iteration 2940 : loss : 50.187946, loss_ce: 0.016186, loss_kd: 248.432571
[05:08:43.291] iteration 2950 : loss : 50.364471, loss_ce: 0.012385, loss_kd: 249.293549
[05:08:48.969] iteration 2960 : loss : 48.294815, loss_ce: 0.027273, loss_kd: 238.975204
[05:08:54.665] iteration 2970 : loss : 47.456310, loss_ce: 0.019369, loss_kd: 234.790253
[05:09:00.352] iteration 2980 : loss : 48.297249, loss_ce: 0.015775, loss_kd: 239.004044
[05:09:06.049] iteration 2990 : loss : 67.047157, loss_ce: 0.012586, loss_kd: 332.735901
[05:09:11.743] iteration 3000 : loss : 44.611076, loss_ce: 0.022417, loss_kd: 220.568176
[05:09:17.455] iteration 3010 : loss : 49.716858, loss_ce: 0.020481, loss_kd: 246.078644
[05:09:23.153] iteration 3020 : loss : 75.779411, loss_ce: 0.027263, loss_kd: 376.398804
[05:09:28.857] iteration 3030 : loss : 57.020222, loss_ce: 0.016449, loss_kd: 282.606781
[05:09:34.559] iteration 3040 : loss : 43.727947, loss_ce: 0.015558, loss_kd: 216.133652
[05:09:40.270] iteration 3050 : loss : 50.660519, loss_ce: 0.026555, loss_kd: 250.790268
[05:09:45.973] iteration 3060 : loss : 77.356400, loss_ce: 0.014469, loss_kd: 384.313873
[05:09:51.694] iteration 3070 : loss : 65.412415, loss_ce: 0.010578, loss_kd: 324.559723
[05:09:57.401] iteration 3080 : loss : 100.870392, loss_ce: 0.012470, loss_kd: 501.886230
[05:10:03.120] iteration 3090 : loss : 46.596584, loss_ce: 0.021237, loss_kd: 230.451080
[05:10:08.834] iteration 3100 : loss : 49.854904, loss_ce: 0.016260, loss_kd: 246.773834
[05:10:14.562] iteration 3110 : loss : 45.210922, loss_ce: 0.018277, loss_kd: 223.556519
[05:10:20.278] iteration 3120 : loss : 72.616852, loss_ce: 0.022665, loss_kd: 360.581055
[05:10:26.008] iteration 3130 : loss : 52.104641, loss_ce: 0.011259, loss_kd: 258.087189
[05:10:29.440] save model to ./simple_finetuning_lits17\continual_surgical_tpgm_stage2_epoch_14.pth
[05:10:43.038] iteration 3140 : loss : 49.669491, loss_ce: 0.014006, loss_kd: 245.867096
[05:10:48.719] iteration 3150 : loss : 53.493843, loss_ce: 0.026020, loss_kd: 264.940979
[05:10:54.398] iteration 3160 : loss : 46.476383, loss_ce: 0.023063, loss_kd: 229.868927
[05:11:00.093] iteration 3170 : loss : 47.792274, loss_ce: 0.016668, loss_kd: 236.440506
[05:11:05.773] iteration 3180 : loss : 48.852139, loss_ce: 0.015194, loss_kd: 241.761154
[05:11:11.466] iteration 3190 : loss : 81.280228, loss_ce: 0.033219, loss_kd: 403.909027
[05:11:17.157] iteration 3200 : loss : 64.710541, loss_ce: 0.022628, loss_kd: 320.999084
[05:11:22.863] iteration 3210 : loss : 58.365017, loss_ce: 0.032518, loss_kd: 289.268951
[05:11:28.552] iteration 3220 : loss : 53.095535, loss_ce: 0.012570, loss_kd: 262.971130
[05:11:34.259] iteration 3230 : loss : 71.116699, loss_ce: 0.010777, loss_kd: 353.119537
[05:11:39.958] iteration 3240 : loss : 80.685379, loss_ce: 0.013112, loss_kd: 400.930664
[05:11:45.666] iteration 3250 : loss : 55.616467, loss_ce: 0.019124, loss_kd: 275.587036
[05:11:51.372] iteration 3260 : loss : 103.425354, loss_ce: 0.019317, loss_kd: 514.647827
[05:11:57.081] iteration 3270 : loss : 74.240883, loss_ce: 0.021301, loss_kd: 368.708496
[05:12:02.787] iteration 3280 : loss : 76.326004, loss_ce: 0.033339, loss_kd: 379.111877
[05:12:08.510] iteration 3290 : loss : 66.109299, loss_ce: 0.022394, loss_kd: 328.005524
[05:12:14.215] iteration 3300 : loss : 57.832855, loss_ce: 0.023724, loss_kd: 286.666504
[05:12:19.940] iteration 3310 : loss : 45.360992, loss_ce: 0.016863, loss_kd: 224.327423
[05:12:25.649] iteration 3320 : loss : 39.271809, loss_ce: 0.031620, loss_kd: 193.870270
[05:12:31.370] iteration 3330 : loss : 56.985195, loss_ce: 0.023494, loss_kd: 282.423279
[05:12:37.090] iteration 3340 : loss : 40.462868, loss_ce: 0.013559, loss_kd: 199.871262
[05:12:53.569] iteration 3350 : loss : 44.834915, loss_ce: 0.035195, loss_kd: 221.664307
[05:12:59.242] iteration 3360 : loss : 40.278206, loss_ce: 0.017967, loss_kd: 198.908356
[05:13:04.928] iteration 3370 : loss : 32.126480, loss_ce: 0.008846, loss_kd: 158.158737
[05:13:10.607] iteration 3380 : loss : 37.702133, loss_ce: 0.014296, loss_kd: 185.932129
[05:13:16.305] iteration 3390 : loss : 34.963829, loss_ce: 0.015859, loss_kd: 172.302567
[05:13:21.988] iteration 3400 : loss : 42.155807, loss_ce: 0.024546, loss_kd: 208.301361
[05:13:27.685] iteration 3410 : loss : 31.762533, loss_ce: 0.015060, loss_kd: 156.343735
[05:13:33.379] iteration 3420 : loss : 36.568035, loss_ce: 0.019043, loss_kd: 180.328812
[05:13:39.089] iteration 3430 : loss : 37.395344, loss_ce: 0.029881, loss_kd: 184.489227
[05:13:44.791] iteration 3440 : loss : 38.605835, loss_ce: 0.025450, loss_kd: 190.534561
[05:13:50.499] iteration 3450 : loss : 37.358334, loss_ce: 0.011460, loss_kd: 184.294022
[05:13:56.199] iteration 3460 : loss : 37.746380, loss_ce: 0.016142, loss_kd: 186.189377
[05:14:01.915] iteration 3470 : loss : 34.029316, loss_ce: 0.013826, loss_kd: 167.655243
[05:14:07.622] iteration 3480 : loss : 37.686577, loss_ce: 0.008789, loss_kd: 186.009232
[05:14:13.342] iteration 3490 : loss : 38.839828, loss_ce: 0.033411, loss_kd: 191.708160
[05:14:19.049] iteration 3500 : loss : 30.508915, loss_ce: 0.021711, loss_kd: 150.036148
[05:14:24.774] iteration 3510 : loss : 36.275059, loss_ce: 0.031594, loss_kd: 178.857788
[05:14:30.479] iteration 3520 : loss : 32.837830, loss_ce: 0.014577, loss_kd: 161.735199
[05:14:36.203] iteration 3530 : loss : 31.974255, loss_ce: 0.010315, loss_kd: 157.392578
[05:14:41.917] iteration 3540 : loss : 56.326275, loss_ce: 0.011107, loss_kd: 279.117584
[05:14:47.643] iteration 3550 : loss : 44.047691, loss_ce: 0.015658, loss_kd: 217.762451
[05:15:04.096] iteration 3560 : loss : 39.388611, loss_ce: 0.018005, loss_kd: 194.423737
[05:15:09.777] iteration 3570 : loss : 41.008553, loss_ce: 0.012992, loss_kd: 202.522400
[05:15:15.452] iteration 3580 : loss : 33.347069, loss_ce: 0.024353, loss_kd: 164.250595
[05:15:21.144] iteration 3590 : loss : 46.578289, loss_ce: 0.012812, loss_kd: 230.386459
[05:15:26.829] iteration 3600 : loss : 38.530807, loss_ce: 0.014377, loss_kd: 190.187851
[05:15:32.524] iteration 3610 : loss : 34.577557, loss_ce: 0.018389, loss_kd: 170.375763
[05:15:38.213] iteration 3620 : loss : 68.367340, loss_ce: 0.036519, loss_kd: 339.302826
[05:15:43.923] iteration 3630 : loss : 84.303459, loss_ce: 0.012922, loss_kd: 418.974335
[05:15:49.618] iteration 3640 : loss : 54.606571, loss_ce: 0.018988, loss_kd: 270.526306
[05:15:55.324] iteration 3650 : loss : 82.280838, loss_ce: 0.025570, loss_kd: 408.914886
[05:16:01.026] iteration 3660 : loss : 52.114716, loss_ce: 0.012287, loss_kd: 258.061035
[05:16:06.737] iteration 3670 : loss : 89.984322, loss_ce: 0.011952, loss_kd: 447.423981
[05:16:12.444] iteration 3680 : loss : 58.768902, loss_ce: 0.018439, loss_kd: 291.350250
[05:16:18.156] iteration 3690 : loss : 114.377419, loss_ce: 0.017862, loss_kd: 569.411377
[05:16:23.862] iteration 3700 : loss : 56.380981, loss_ce: 0.017143, loss_kd: 279.365112
[05:16:29.583] iteration 3710 : loss : 61.523563, loss_ce: 0.019711, loss_kd: 305.117859
[05:16:35.290] iteration 3720 : loss : 81.842499, loss_ce: 0.016333, loss_kd: 406.716187
[05:16:41.010] iteration 3730 : loss : 54.356171, loss_ce: 0.010501, loss_kd: 269.297852
[05:16:46.720] iteration 3740 : loss : 41.899239, loss_ce: 0.022192, loss_kd: 206.973907
[05:16:52.445] iteration 3750 : loss : 55.630657, loss_ce: 0.017297, loss_kd: 275.670410
[05:16:58.160] iteration 3760 : loss : 72.197586, loss_ce: 0.021286, loss_kd: 358.477295
[05:17:14.416] iteration 3770 : loss : 80.945343, loss_ce: 0.011048, loss_kd: 402.241608
[05:17:20.093] iteration 3780 : loss : 63.806610, loss_ce: 0.035715, loss_kd: 316.546631
[05:17:25.781] iteration 3790 : loss : 78.072746, loss_ce: 0.009883, loss_kd: 387.867126
[05:17:31.462] iteration 3800 : loss : 55.966763, loss_ce: 0.008735, loss_kd: 277.330048
[05:17:37.164] iteration 3810 : loss : 40.419861, loss_ce: 0.013236, loss_kd: 199.585266
[05:17:42.855] iteration 3820 : loss : 76.719818, loss_ce: 0.018756, loss_kd: 381.117249
[05:17:48.557] iteration 3830 : loss : 129.013474, loss_ce: 0.026354, loss_kd: 642.500183
[05:17:54.252] iteration 3840 : loss : 73.072037, loss_ce: 0.011390, loss_kd: 362.878052
[05:17:59.962] iteration 3850 : loss : 146.569153, loss_ce: 0.031755, loss_kd: 730.317505
[05:18:05.669] iteration 3860 : loss : 62.739544, loss_ce: 0.012409, loss_kd: 311.237396
[05:18:11.381] iteration 3870 : loss : 56.952995, loss_ce: 0.019536, loss_kd: 282.294495
[05:18:17.085] iteration 3880 : loss : 58.375790, loss_ce: 0.013987, loss_kd: 289.396179
[05:18:22.803] iteration 3890 : loss : 40.473083, loss_ce: 0.012282, loss_kd: 199.884430
[05:18:28.510] iteration 3900 : loss : 51.150272, loss_ce: 0.021637, loss_kd: 253.246078
[05:18:34.230] iteration 3910 : loss : 57.370029, loss_ce: 0.020179, loss_kd: 284.315460
[05:18:39.939] iteration 3920 : loss : 46.010193, loss_ce: 0.019649, loss_kd: 227.591919
[05:18:45.657] iteration 3930 : loss : 66.288910, loss_ce: 0.027804, loss_kd: 328.957458
[05:18:51.372] iteration 3940 : loss : 87.792519, loss_ce: 0.021432, loss_kd: 436.453247
[05:18:57.093] iteration 3950 : loss : 80.899719, loss_ce: 0.024174, loss_kd: 401.992920
[05:19:02.819] iteration 3960 : loss : 91.216064, loss_ce: 0.024555, loss_kd: 453.570496
[05:19:08.547] iteration 3970 : loss : 63.428596, loss_ce: 0.033361, loss_kd: 314.611084
[05:19:24.766] iteration 3980 : loss : 60.836563, loss_ce: 0.016861, loss_kd: 301.701508
[05:19:30.452] iteration 3990 : loss : 63.118568, loss_ce: 0.017470, loss_kd: 313.121765
[05:19:36.132] iteration 4000 : loss : 96.043701, loss_ce: 0.020828, loss_kd: 477.737152
[05:19:41.830] iteration 4010 : loss : 75.486000, loss_ce: 0.018263, loss_kd: 374.894043
[05:19:47.514] iteration 4020 : loss : 55.556946, loss_ce: 0.019972, loss_kd: 275.273621
[05:19:53.214] iteration 4030 : loss : 54.703156, loss_ce: 0.018216, loss_kd: 271.029114
[05:19:58.909] iteration 4040 : loss : 49.696251, loss_ce: 0.012254, loss_kd: 245.998978
[05:20:04.613] iteration 4050 : loss : 46.439701, loss_ce: 0.026002, loss_kd: 229.697525
[05:20:10.310] iteration 4060 : loss : 44.575462, loss_ce: 0.013469, loss_kd: 220.373016
[05:20:16.020] iteration 4070 : loss : 58.060608, loss_ce: 0.028548, loss_kd: 287.755524
[05:20:21.722] iteration 4080 : loss : 49.103493, loss_ce: 0.019291, loss_kd: 243.020462
[05:20:27.438] iteration 4090 : loss : 85.840813, loss_ce: 0.036168, loss_kd: 426.661560
[05:20:33.146] iteration 4100 : loss : 64.611771, loss_ce: 0.015609, loss_kd: 320.510925
[05:20:38.865] iteration 4110 : loss : 158.992920, loss_ce: 0.021672, loss_kd: 792.461182
[05:20:44.578] iteration 4120 : loss : 108.735519, loss_ce: 0.015845, loss_kd: 541.204346
[05:20:50.296] iteration 4130 : loss : 74.077309, loss_ce: 0.019826, loss_kd: 367.921448
[05:20:56.009] iteration 4140 : loss : 50.933182, loss_ce: 0.014207, loss_kd: 252.148438
[05:21:01.731] iteration 4150 : loss : 63.283390, loss_ce: 0.007548, loss_kd: 313.872040
[05:21:07.444] iteration 4160 : loss : 63.467587, loss_ce: 0.015295, loss_kd: 314.860809
[05:21:13.167] iteration 4170 : loss : 50.946903, loss_ce: 0.015762, loss_kd: 252.224960
[05:21:18.717] iteration 4180 : loss : 36.441483, loss_ce: 0.025898, loss_kd: 179.750992
[05:21:19.458] save model to ./simple_finetuning_lits17\continual_surgical_tpgm_stage2_epoch_19.pth
[05:21:35.098] iteration 4190 : loss : 72.688805, loss_ce: 0.023194, loss_kd: 360.921021
[05:21:40.774] iteration 4200 : loss : 51.919304, loss_ce: 0.018839, loss_kd: 257.097778
[05:21:46.466] iteration 4210 : loss : 65.435158, loss_ce: 0.014428, loss_kd: 324.714722
[05:21:52.147] iteration 4220 : loss : 49.546810, loss_ce: 0.013745, loss_kd: 245.214081
[05:21:57.843] iteration 4230 : loss : 31.829308, loss_ce: 0.012101, loss_kd: 156.630386
[05:22:03.530] iteration 4240 : loss : 42.991112, loss_ce: 0.009076, loss_kd: 212.434021
[05:22:09.236] iteration 4250 : loss : 41.919601, loss_ce: 0.019274, loss_kd: 207.115845
[05:22:14.928] iteration 4260 : loss : 36.037678, loss_ce: 0.014645, loss_kd: 177.697571
[05:22:20.637] iteration 4270 : loss : 33.912762, loss_ce: 0.024232, loss_kd: 167.068832
[05:22:26.341] iteration 4280 : loss : 37.839626, loss_ce: 0.021794, loss_kd: 186.702179
[05:22:32.051] iteration 4290 : loss : 56.679611, loss_ce: 0.016296, loss_kd: 280.908600
[05:22:37.753] iteration 4300 : loss : 49.437016, loss_ce: 0.021846, loss_kd: 244.728760
[05:22:43.471] iteration 4310 : loss : 50.827461, loss_ce: 0.018747, loss_kd: 251.661316
[05:22:49.179] iteration 4320 : loss : 51.634556, loss_ce: 0.030126, loss_kd: 255.611145
[05:22:54.902] iteration 4330 : loss : 68.475449, loss_ce: 0.026431, loss_kd: 339.867767
[05:23:00.613] iteration 4340 : loss : 57.243156, loss_ce: 0.026783, loss_kd: 283.723236
[05:23:06.334] iteration 4350 : loss : 42.401432, loss_ce: 0.016439, loss_kd: 209.524124
[05:23:12.053] iteration 4360 : loss : 72.888985, loss_ce: 0.028373, loss_kd: 361.960754
[05:23:17.781] iteration 4370 : loss : 76.985291, loss_ce: 0.014054, loss_kd: 382.418243
[05:23:23.497] iteration 4380 : loss : 57.327511, loss_ce: 0.008782, loss_kd: 284.179688
[05:23:42.741] iteration 4390 : loss : 55.045383, loss_ce: 0.020326, loss_kd: 272.734589
[05:23:48.409] iteration 4400 : loss : 71.158684, loss_ce: 0.023624, loss_kd: 353.312836
[05:23:54.104] iteration 4410 : loss : 66.395691, loss_ce: 0.014127, loss_kd: 329.497223
[05:23:59.786] iteration 4420 : loss : 42.658226, loss_ce: 0.020946, loss_kd: 210.815598
[05:24:05.485] iteration 4430 : loss : 54.951065, loss_ce: 0.017941, loss_kd: 272.229980
[05:24:11.175] iteration 4440 : loss : 36.537189, loss_ce: 0.021683, loss_kd: 180.202087
[05:24:16.886] iteration 4450 : loss : 59.726570, loss_ce: 0.017633, loss_kd: 296.163452
[05:24:22.588] iteration 4460 : loss : 94.749725, loss_ce: 0.022446, loss_kd: 471.264893
[05:24:28.301] iteration 4470 : loss : 73.856407, loss_ce: 0.011816, loss_kd: 366.776917
[05:24:34.007] iteration 4480 : loss : 97.434204, loss_ce: 0.030368, loss_kd: 484.668335
[05:24:39.713] iteration 4490 : loss : 64.451775, loss_ce: 0.023327, loss_kd: 319.755890
[05:24:45.420] iteration 4500 : loss : 65.862190, loss_ce: 0.017805, loss_kd: 326.792175
[05:24:51.144] iteration 4510 : loss : 54.401573, loss_ce: 0.019109, loss_kd: 269.534393
[05:24:56.857] iteration 4520 : loss : 64.396027, loss_ce: 0.017135, loss_kd: 319.460571
[05:25:02.586] iteration 4530 : loss : 51.937386, loss_ce: 0.020325, loss_kd: 257.189728
[05:25:08.309] iteration 4540 : loss : 50.230652, loss_ce: 0.018435, loss_kd: 248.655426
[05:25:14.045] iteration 4550 : loss : 47.243809, loss_ce: 0.019136, loss_kd: 233.754791
[05:25:19.763] iteration 4560 : loss : 61.863350, loss_ce: 0.042825, loss_kd: 306.786682
[05:25:25.485] iteration 4570 : loss : 58.490044, loss_ce: 0.017017, loss_kd: 289.907501
[05:25:31.202] iteration 4580 : loss : 40.340427, loss_ce: 0.009900, loss_kd: 199.190353
[05:25:36.937] iteration 4590 : loss : 78.021126, loss_ce: 0.022556, loss_kd: 387.635620
[05:25:53.472] iteration 4600 : loss : 76.867653, loss_ce: 0.014844, loss_kd: 381.864899
[05:25:59.154] iteration 4610 : loss : 58.581661, loss_ce: 0.024324, loss_kd: 290.429199
[05:26:04.840] iteration 4620 : loss : 47.460907, loss_ce: 0.015973, loss_kd: 234.816818
[05:26:10.542] iteration 4630 : loss : 55.358871, loss_ce: 0.024409, loss_kd: 274.300537
[05:26:16.230] iteration 4640 : loss : 69.552742, loss_ce: 0.023552, loss_kd: 345.222870
[05:26:21.929] iteration 4650 : loss : 63.956253, loss_ce: 0.016922, loss_kd: 317.260925
[05:26:27.622] iteration 4660 : loss : 54.314243, loss_ce: 0.015006, loss_kd: 269.127716
[05:26:33.333] iteration 4670 : loss : 58.866909, loss_ce: 0.014030, loss_kd: 291.862335
[05:26:39.033] iteration 4680 : loss : 53.745117, loss_ce: 0.011200, loss_kd: 266.259430
[05:26:44.741] iteration 4690 : loss : 48.282055, loss_ce: 0.029287, loss_kd: 238.944839
[05:26:50.450] iteration 4700 : loss : 51.661163, loss_ce: 0.023487, loss_kd: 255.798309
[05:26:56.170] iteration 4710 : loss : 46.184937, loss_ce: 0.021455, loss_kd: 228.459427
[05:27:01.882] iteration 4720 : loss : 40.298317, loss_ce: 0.017278, loss_kd: 199.006775
[05:27:07.611] iteration 4730 : loss : 42.726974, loss_ce: 0.021683, loss_kd: 211.116241
[05:27:13.322] iteration 4740 : loss : 35.943336, loss_ce: 0.017743, loss_kd: 177.219330
[05:27:19.049] iteration 4750 : loss : 49.338448, loss_ce: 0.014959, loss_kd: 244.194351
[05:27:24.757] iteration 4760 : loss : 58.326611, loss_ce: 0.024339, loss_kd: 289.119537
[05:27:30.484] iteration 4770 : loss : 46.306049, loss_ce: 0.036906, loss_kd: 229.010452
[05:27:36.195] iteration 4780 : loss : 78.948433, loss_ce: 0.014448, loss_kd: 392.214050
[05:27:41.920] iteration 4790 : loss : 43.890125, loss_ce: 0.015145, loss_kd: 216.982620
[05:27:47.629] iteration 4800 : loss : 63.015072, loss_ce: 0.019649, loss_kd: 312.547516
[05:28:04.831] iteration 4810 : loss : 65.839081, loss_ce: 0.011247, loss_kd: 326.752594
[05:28:10.515] iteration 4820 : loss : 45.603168, loss_ce: 0.018102, loss_kd: 225.528320
[05:28:16.207] iteration 4830 : loss : 42.515491, loss_ce: 0.026696, loss_kd: 210.084351
[05:28:21.881] iteration 4840 : loss : 56.451984, loss_ce: 0.011793, loss_kd: 279.758423
[05:28:27.577] iteration 4850 : loss : 33.890736, loss_ce: 0.019005, loss_kd: 166.963867
[05:28:33.264] iteration 4860 : loss : 39.462372, loss_ce: 0.019045, loss_kd: 194.824646
[05:28:38.965] iteration 4870 : loss : 34.161060, loss_ce: 0.026079, loss_kd: 168.315781
[05:28:44.654] iteration 4880 : loss : 49.002762, loss_ce: 0.021360, loss_kd: 242.517746
[05:28:50.360] iteration 4890 : loss : 63.736141, loss_ce: 0.026572, loss_kd: 316.174561
[05:28:56.057] iteration 4900 : loss : 41.715069, loss_ce: 0.010936, loss_kd: 206.099716
[05:29:01.768] iteration 4910 : loss : 32.244766, loss_ce: 0.008681, loss_kd: 158.754684
[05:29:07.480] iteration 4920 : loss : 36.473167, loss_ce: 0.028065, loss_kd: 179.860046
[05:29:13.203] iteration 4930 : loss : 34.664803, loss_ce: 0.016906, loss_kd: 170.869705
[05:29:18.916] iteration 4940 : loss : 45.464622, loss_ce: 0.019544, loss_kd: 224.819580
[05:29:24.644] iteration 4950 : loss : 41.565590, loss_ce: 0.013621, loss_kd: 205.345291
[05:29:30.357] iteration 4960 : loss : 38.654675, loss_ce: 0.020753, loss_kd: 190.783813
[05:29:36.083] iteration 4970 : loss : 90.423981, loss_ce: 0.016601, loss_kd: 449.614380
[05:29:41.796] iteration 4980 : loss : 55.771030, loss_ce: 0.018354, loss_kd: 276.313232
[05:29:47.526] iteration 4990 : loss : 36.726532, loss_ce: 0.026192, loss_kd: 181.152191
[05:29:53.241] iteration 5000 : loss : 43.469395, loss_ce: 0.008814, loss_kd: 214.863434
[05:29:58.967] iteration 5010 : loss : 40.285572, loss_ce: 0.011569, loss_kd: 198.930435
[05:30:15.335] iteration 5020 : loss : 41.950005, loss_ce: 0.015030, loss_kd: 207.283188
[05:30:21.016] iteration 5030 : loss : 53.855938, loss_ce: 0.013896, loss_kd: 266.792755
[05:30:26.693] iteration 5040 : loss : 41.165554, loss_ce: 0.010405, loss_kd: 203.312225
[05:30:32.380] iteration 5050 : loss : 54.065434, loss_ce: 0.022943, loss_kd: 267.848206
[05:30:38.068] iteration 5060 : loss : 69.933640, loss_ce: 0.018461, loss_kd: 347.182465
[05:30:43.770] iteration 5070 : loss : 96.519424, loss_ce: 0.013411, loss_kd: 480.119141
[05:30:49.459] iteration 5080 : loss : 97.728027, loss_ce: 0.011725, loss_kd: 486.134857
[05:30:55.166] iteration 5090 : loss : 118.341225, loss_ce: 0.021255, loss_kd: 589.224243
[05:31:00.860] iteration 5100 : loss : 80.072655, loss_ce: 0.018319, loss_kd: 397.866028
[05:31:06.572] iteration 5110 : loss : 56.176487, loss_ce: 0.019577, loss_kd: 278.430023
[05:31:12.277] iteration 5120 : loss : 55.082642, loss_ce: 0.014450, loss_kd: 272.936584
[05:31:17.989] iteration 5130 : loss : 47.307636, loss_ce: 0.014928, loss_kd: 234.027054
[05:31:23.688] iteration 5140 : loss : 35.981476, loss_ce: 0.018880, loss_kd: 177.432449
[05:31:29.407] iteration 5150 : loss : 51.572342, loss_ce: 0.013860, loss_kd: 255.399658
[05:31:35.110] iteration 5160 : loss : 59.527878, loss_ce: 0.010687, loss_kd: 295.145538
[05:31:40.835] iteration 5170 : loss : 47.011620, loss_ce: 0.012071, loss_kd: 232.590576
[05:31:46.551] iteration 5180 : loss : 44.348755, loss_ce: 0.014991, loss_kd: 219.241928
[05:31:52.275] iteration 5190 : loss : 59.020996, loss_ce: 0.014878, loss_kd: 292.601044
[05:31:57.993] iteration 5200 : loss : 59.485535, loss_ce: 0.016489, loss_kd: 294.934753
[05:32:03.719] iteration 5210 : loss : 63.480667, loss_ce: 0.021308, loss_kd: 314.919617
[05:32:09.435] iteration 5220 : loss : 46.734360, loss_ce: 0.019365, loss_kd: 231.210678
[05:32:12.826] save model to ./simple_finetuning_lits17\continual_surgical_tpgm_stage2_epoch_24.pth
[05:32:25.849] iteration 5230 : loss : 57.912041, loss_ce: 0.009759, loss_kd: 287.100830
[05:32:31.517] iteration 5240 : loss : 42.805080, loss_ce: 0.027290, loss_kd: 211.490631
[05:32:37.205] iteration 5250 : loss : 37.545944, loss_ce: 0.014958, loss_kd: 185.221008
[05:32:42.889] iteration 5260 : loss : 42.545120, loss_ce: 0.015358, loss_kd: 210.202560
[05:32:48.589] iteration 5270 : loss : 65.589081, loss_ce: 0.015518, loss_kd: 325.463593
[05:32:54.283] iteration 5280 : loss : 56.615597, loss_ce: 0.034985, loss_kd: 280.583099
[05:32:59.986] iteration 5290 : loss : 57.346313, loss_ce: 0.021773, loss_kd: 284.187775
[05:33:05.677] iteration 5300 : loss : 38.605473, loss_ce: 0.021414, loss_kd: 190.522583
[05:33:11.382] iteration 5310 : loss : 42.462624, loss_ce: 0.012367, loss_kd: 209.828400
[05:33:17.088] iteration 5320 : loss : 43.126713, loss_ce: 0.009789, loss_kd: 213.179352
[05:33:22.806] iteration 5330 : loss : 51.599812, loss_ce: 0.012904, loss_kd: 255.517395
[05:33:28.511] iteration 5340 : loss : 46.764023, loss_ce: 0.021336, loss_kd: 231.336090
[05:33:34.231] iteration 5350 : loss : 38.869553, loss_ce: 0.020788, loss_kd: 191.864960
[05:33:39.941] iteration 5360 : loss : 39.084335, loss_ce: 0.019321, loss_kd: 192.948624
[05:33:45.657] iteration 5370 : loss : 46.195835, loss_ce: 0.026044, loss_kd: 228.485718
[05:33:51.369] iteration 5380 : loss : 140.576355, loss_ce: 0.019792, loss_kd: 700.338440
[05:33:57.092] iteration 5390 : loss : 66.955162, loss_ce: 0.020681, loss_kd: 332.303040
[05:34:02.804] iteration 5400 : loss : 46.788624, loss_ce: 0.013565, loss_kd: 231.477951
[05:34:08.541] iteration 5410 : loss : 52.867710, loss_ce: 0.029323, loss_kd: 261.872833
[05:34:14.251] iteration 5420 : loss : 50.030663, loss_ce: 0.017985, loss_kd: 247.679260
[05:34:19.986] iteration 5430 : loss : 55.656498, loss_ce: 0.014265, loss_kd: 275.837646
[05:34:36.570] iteration 5440 : loss : 46.333214, loss_ce: 0.031173, loss_kd: 229.175613
[05:34:42.258] iteration 5450 : loss : 38.865986, loss_ce: 0.015936, loss_kd: 191.858353
[05:34:47.935] iteration 5460 : loss : 40.717793, loss_ce: 0.008405, loss_kd: 201.115692
[05:34:53.627] iteration 5470 : loss : 54.908276, loss_ce: 0.011757, loss_kd: 271.994202
[05:34:59.314] iteration 5480 : loss : 53.744129, loss_ce: 0.013660, loss_kd: 266.211243
[05:35:05.014] iteration 5490 : loss : 63.947971, loss_ce: 0.023058, loss_kd: 317.270538
[05:35:10.706] iteration 5500 : loss : 47.909428, loss_ce: 0.015450, loss_kd: 237.081299
[05:35:16.415] iteration 5510 : loss : 39.659161, loss_ce: 0.013846, loss_kd: 195.787628
[05:35:22.113] iteration 5520 : loss : 40.842640, loss_ce: 0.023441, loss_kd: 201.741989
[05:35:27.826] iteration 5530 : loss : 39.143070, loss_ce: 0.021943, loss_kd: 193.240952
[05:35:33.531] iteration 5540 : loss : 42.064060, loss_ce: 0.012278, loss_kd: 207.809723
[05:35:39.242] iteration 5550 : loss : 46.627453, loss_ce: 0.017241, loss_kd: 230.592804
[05:35:44.957] iteration 5560 : loss : 33.104218, loss_ce: 0.011060, loss_kd: 163.042221
[05:35:50.678] iteration 5570 : loss : 56.784107, loss_ce: 0.008317, loss_kd: 281.506561
[05:35:56.385] iteration 5580 : loss : 45.697655, loss_ce: 0.030051, loss_kd: 226.023682
[05:36:02.106] iteration 5590 : loss : 56.258568, loss_ce: 0.019460, loss_kd: 278.807861
[05:36:07.813] iteration 5600 : loss : 64.435875, loss_ce: 0.028221, loss_kd: 319.680450
[05:36:13.539] iteration 5610 : loss : 47.072018, loss_ce: 0.010714, loss_kd: 232.916733
[05:36:19.254] iteration 5620 : loss : 33.538769, loss_ce: 0.010305, loss_kd: 165.219452
[05:36:24.980] iteration 5630 : loss : 42.539543, loss_ce: 0.010853, loss_kd: 210.196167
[05:36:30.707] iteration 5640 : loss : 35.110100, loss_ce: 0.017486, loss_kd: 173.065872
[05:36:47.298] iteration 5650 : loss : 34.221798, loss_ce: 0.017977, loss_kd: 168.592239
[05:36:52.973] iteration 5660 : loss : 44.716232, loss_ce: 0.011437, loss_kd: 221.084961
[05:36:58.664] iteration 5670 : loss : 40.007408, loss_ce: 0.016788, loss_kd: 197.576385
[05:37:04.346] iteration 5680 : loss : 40.737270, loss_ce: 0.010974, loss_kd: 201.195099
[05:37:10.045] iteration 5690 : loss : 32.662651, loss_ce: 0.014265, loss_kd: 160.841782
[05:37:15.737] iteration 5700 : loss : 36.917213, loss_ce: 0.014258, loss_kd: 182.075287
[05:37:21.446] iteration 5710 : loss : 37.092587, loss_ce: 0.034752, loss_kd: 182.955872
[05:37:27.140] iteration 5720 : loss : 38.157089, loss_ce: 0.010630, loss_kd: 188.237579
[05:37:32.850] iteration 5730 : loss : 34.481197, loss_ce: 0.016557, loss_kd: 169.912689
[05:37:38.551] iteration 5740 : loss : 35.111675, loss_ce: 0.019935, loss_kd: 173.084900
[05:37:44.269] iteration 5750 : loss : 29.800667, loss_ce: 0.011155, loss_kd: 146.505493
[05:37:49.980] iteration 5760 : loss : 33.377975, loss_ce: 0.009258, loss_kd: 164.402817
[05:37:55.696] iteration 5770 : loss : 147.736069, loss_ce: 0.017513, loss_kd: 736.199463
[05:38:01.412] iteration 5780 : loss : 77.781433, loss_ce: 0.016537, loss_kd: 386.446167
[05:38:07.130] iteration 5790 : loss : 60.939129, loss_ce: 0.013668, loss_kd: 302.171997
[05:38:12.842] iteration 5800 : loss : 57.809071, loss_ce: 0.017794, loss_kd: 286.560364
[05:38:18.571] iteration 5810 : loss : 39.411221, loss_ce: 0.016099, loss_kd: 194.562897
[05:38:24.286] iteration 5820 : loss : 31.741693, loss_ce: 0.009613, loss_kd: 156.240524
[05:38:30.013] iteration 5830 : loss : 54.692829, loss_ce: 0.019357, loss_kd: 270.965973
[05:38:35.732] iteration 5840 : loss : 50.171001, loss_ce: 0.018060, loss_kd: 248.361588
[05:38:41.462] iteration 5850 : loss : 40.388168, loss_ce: 0.018499, loss_kd: 199.449341
[05:38:57.838] iteration 5860 : loss : 41.751942, loss_ce: 0.012379, loss_kd: 206.277649
[05:39:03.523] iteration 5870 : loss : 69.745880, loss_ce: 0.038345, loss_kd: 346.221069
[05:39:09.201] iteration 5880 : loss : 71.251549, loss_ce: 0.010262, loss_kd: 353.760040
[05:39:14.895] iteration 5890 : loss : 50.569332, loss_ce: 0.009062, loss_kd: 250.351440
[05:39:20.587] iteration 5900 : loss : 70.413902, loss_ce: 0.013688, loss_kd: 349.549927
[05:39:26.287] iteration 5910 : loss : 50.938698, loss_ce: 0.016584, loss_kd: 252.179413
[05:39:31.980] iteration 5920 : loss : 47.553585, loss_ce: 0.025475, loss_kd: 235.218964
[05:39:37.687] iteration 5930 : loss : 50.665794, loss_ce: 0.012696, loss_kd: 250.853439
[05:39:43.388] iteration 5940 : loss : 46.714226, loss_ce: 0.024372, loss_kd: 231.065414
[05:39:49.097] iteration 5950 : loss : 41.924774, loss_ce: 0.010795, loss_kd: 207.161377
[05:39:54.805] iteration 5960 : loss : 92.457962, loss_ce: 0.017653, loss_kd: 459.844757
[05:40:00.524] iteration 5970 : loss : 56.255379, loss_ce: 0.012436, loss_kd: 278.813416
[05:40:06.236] iteration 5980 : loss : 62.464809, loss_ce: 0.011315, loss_kd: 309.849030
[05:40:11.956] iteration 5990 : loss : 56.757000, loss_ce: 0.023081, loss_kd: 281.262970
[05:40:17.670] iteration 6000 : loss : 55.857666, loss_ce: 0.013516, loss_kd: 276.773132
[05:40:23.395] iteration 6010 : loss : 38.655331, loss_ce: 0.026016, loss_kd: 190.797150
[05:40:29.110] iteration 6020 : loss : 45.605114, loss_ce: 0.019604, loss_kd: 225.575653
[05:40:34.840] iteration 6030 : loss : 44.301273, loss_ce: 0.020175, loss_kd: 219.025513
[05:40:40.560] iteration 6040 : loss : 63.721157, loss_ce: 0.020263, loss_kd: 316.119202
[05:40:46.286] iteration 6050 : loss : 59.857258, loss_ce: 0.016495, loss_kd: 296.837341
[05:40:52.001] iteration 6060 : loss : 72.978928, loss_ce: 0.022543, loss_kd: 362.413422
[05:41:08.356] iteration 6070 : loss : 56.090862, loss_ce: 0.013368, loss_kd: 277.986328
[05:41:14.032] iteration 6080 : loss : 52.304844, loss_ce: 0.019282, loss_kd: 259.037750
[05:41:19.719] iteration 6090 : loss : 49.323734, loss_ce: 0.022196, loss_kd: 244.142181
[05:41:25.403] iteration 6100 : loss : 56.736061, loss_ce: 0.018037, loss_kd: 281.154541
[05:41:31.098] iteration 6110 : loss : 58.339485, loss_ce: 0.020281, loss_kd: 289.197571
[05:41:36.790] iteration 6120 : loss : 39.046143, loss_ce: 0.012572, loss_kd: 192.782486
[05:41:42.498] iteration 6130 : loss : 36.997944, loss_ce: 0.016849, loss_kd: 182.488159
[05:41:48.194] iteration 6140 : loss : 42.142647, loss_ce: 0.022632, loss_kd: 208.221146
[05:41:53.908] iteration 6150 : loss : 42.950619, loss_ce: 0.012380, loss_kd: 212.257019
[05:41:59.612] iteration 6160 : loss : 85.535332, loss_ce: 0.027030, loss_kd: 425.125488
[05:42:05.328] iteration 6170 : loss : 45.657589, loss_ce: 0.018083, loss_kd: 225.799973
[05:42:11.034] iteration 6180 : loss : 47.026981, loss_ce: 0.030364, loss_kd: 232.611359
[05:42:16.750] iteration 6190 : loss : 34.061176, loss_ce: 0.015054, loss_kd: 167.760406
[05:42:22.459] iteration 6200 : loss : 38.804352, loss_ce: 0.020399, loss_kd: 191.523941
[05:42:28.185] iteration 6210 : loss : 39.444912, loss_ce: 0.016188, loss_kd: 194.753510
[05:42:33.899] iteration 6220 : loss : 33.517815, loss_ce: 0.019956, loss_kd: 165.118744
[05:42:39.617] iteration 6230 : loss : 32.603977, loss_ce: 0.012162, loss_kd: 160.527802
[05:42:45.338] iteration 6240 : loss : 29.791281, loss_ce: 0.007139, loss_kd: 146.413986
[05:42:51.071] iteration 6250 : loss : 44.229973, loss_ce: 0.015014, loss_kd: 218.677490
[05:42:56.796] iteration 6260 : loss : 37.886650, loss_ce: 0.011150, loss_kd: 186.969971
[05:43:02.352] iteration 6270 : loss : 45.181305, loss_ce: 0.024874, loss_kd: 223.446091
[05:43:03.073] save model to ./simple_finetuning_lits17\continual_surgical_tpgm_stage2_epoch_29.pth
[05:43:03.158] save final model to ./simple_finetuning_lits17\continual_surgical_tpgm_stage2_final.pth
