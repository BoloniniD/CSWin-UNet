[03:32:55.915] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual_50iter', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=50, tpgm_exclude=[], gpu_id=1)
[03:32:55.942] Using 8569/95221 samples for finetuning
[03:32:55.942] Using 953/95221 samples for TPGM
[03:32:55.943] Model has 9 total classes, training on 4 classes
[03:33:06.371] 268 iterations per epoch. 13400 max iterations 
[03:33:21.045] iteration 10 : loss : 0.461870, loss_ce: 0.081058
[03:33:25.073] iteration 20 : loss : 0.447126, loss_ce: 0.067454
[03:33:29.132] iteration 30 : loss : 0.454064, loss_ce: 0.061675
[03:33:33.166] iteration 40 : loss : 0.438523, loss_ce: 0.037797
[03:33:37.214] iteration 50 : loss : 0.427871, loss_ce: 0.060777
[03:33:41.254] iteration 60 : loss : 0.289397, loss_ce: 0.043874
[03:33:45.306] iteration 70 : loss : 0.430093, loss_ce: 0.078371
[03:33:49.351] iteration 80 : loss : 0.432416, loss_ce: 0.045691
[03:33:53.406] iteration 90 : loss : 0.438128, loss_ce: 0.059353
[03:33:57.455] iteration 100 : loss : 0.424840, loss_ce: 0.065352
[03:34:01.511] iteration 110 : loss : 0.289580, loss_ce: 0.027083
[03:34:05.563] iteration 120 : loss : 0.420143, loss_ce: 0.054165
[03:34:09.624] iteration 130 : loss : 0.428500, loss_ce: 0.046873
[03:34:13.680] iteration 140 : loss : 0.401049, loss_ce: 0.039351
[03:34:17.742] iteration 150 : loss : 0.411577, loss_ce: 0.030802
[03:34:21.796] iteration 160 : loss : 0.382576, loss_ce: 0.048536
[03:34:25.860] iteration 170 : loss : 0.386560, loss_ce: 0.035044
[03:34:29.917] iteration 180 : loss : 0.363701, loss_ce: 0.024090
[03:34:33.981] iteration 190 : loss : 0.240404, loss_ce: 0.031053
[03:34:38.038] iteration 200 : loss : 0.373939, loss_ce: 0.052010
[03:34:42.105] iteration 210 : loss : 0.388509, loss_ce: 0.043876
[03:34:46.162] iteration 220 : loss : 0.379827, loss_ce: 0.024280
[03:34:50.233] iteration 230 : loss : 0.358815, loss_ce: 0.026818
[03:34:54.296] iteration 240 : loss : 0.366219, loss_ce: 0.032203
[03:34:58.371] iteration 250 : loss : 0.391504, loss_ce: 0.052399
[03:35:02.434] iteration 260 : loss : 0.350837, loss_ce: 0.040769
[03:35:52.156] iteration 270 : loss : 0.437757, loss_ce: 0.090415
[03:35:56.203] iteration 280 : loss : 0.482379, loss_ce: 0.086296
[03:36:00.257] iteration 290 : loss : 0.446053, loss_ce: 0.044972
[03:36:04.306] iteration 300 : loss : 0.443130, loss_ce: 0.074765
[03:36:08.369] iteration 310 : loss : 0.287579, loss_ce: 0.054042
[03:36:12.423] iteration 320 : loss : 0.295280, loss_ce: 0.055167
[03:36:16.485] iteration 330 : loss : 0.432801, loss_ce: 0.055010
[03:36:20.540] iteration 340 : loss : 0.437042, loss_ce: 0.064224
[03:36:24.608] iteration 350 : loss : 0.289341, loss_ce: 0.065110
[03:36:28.669] iteration 360 : loss : 0.435019, loss_ce: 0.067948
[03:36:32.739] iteration 370 : loss : 0.434119, loss_ce: 0.043354
[03:36:36.801] iteration 380 : loss : 0.435268, loss_ce: 0.059927
[03:36:40.873] iteration 390 : loss : 0.441429, loss_ce: 0.062321
[03:36:44.934] iteration 400 : loss : 0.438816, loss_ce: 0.059918
[03:36:49.009] iteration 410 : loss : 0.289778, loss_ce: 0.040623
[03:36:53.072] iteration 420 : loss : 0.443022, loss_ce: 0.036963
[03:36:57.149] iteration 430 : loss : 0.447863, loss_ce: 0.071450
[03:37:01.215] iteration 440 : loss : 0.426907, loss_ce: 0.059775
[03:37:05.285] iteration 450 : loss : 0.454591, loss_ce: 0.073450
[03:37:09.350] iteration 460 : loss : 0.289865, loss_ce: 0.050836
[03:37:13.427] iteration 470 : loss : 0.449740, loss_ce: 0.072793
[03:37:17.493] iteration 480 : loss : 0.448631, loss_ce: 0.052236
[03:37:21.565] iteration 490 : loss : 0.450563, loss_ce: 0.071377
[03:37:25.628] iteration 500 : loss : 0.453443, loss_ce: 0.067674
[03:37:29.701] iteration 510 : loss : 0.439430, loss_ce: 0.068596
[03:37:33.766] iteration 520 : loss : 0.436792, loss_ce: 0.053489
[03:37:37.842] iteration 530 : loss : 0.435386, loss_ce: 0.044175
[03:38:37.932] iteration 540 : loss : 0.500809, loss_ce: 0.125347
[03:38:41.984] iteration 550 : loss : 0.664207, loss_ce: 0.514940
[03:38:46.025] iteration 560 : loss : 0.485560, loss_ce: 0.087558
[03:38:50.086] iteration 570 : loss : 0.472920, loss_ce: 0.061735
[03:38:54.139] iteration 580 : loss : 0.472192, loss_ce: 0.054573
[03:38:58.201] iteration 590 : loss : 0.469058, loss_ce: 0.047199
[03:39:02.253] iteration 600 : loss : 0.465275, loss_ce: 0.054720
[03:39:06.316] iteration 610 : loss : 0.461927, loss_ce: 0.065714
[03:39:10.373] iteration 620 : loss : 0.459591, loss_ce: 0.077879
[03:39:14.439] iteration 630 : loss : 0.447481, loss_ce: 0.053785
[03:39:18.502] iteration 640 : loss : 0.448892, loss_ce: 0.045312
[03:39:22.571] iteration 650 : loss : 0.444598, loss_ce: 0.040962
[03:39:26.636] iteration 660 : loss : 0.431894, loss_ce: 0.056568
[03:39:30.708] iteration 670 : loss : 0.439556, loss_ce: 0.068322
[03:39:34.771] iteration 680 : loss : 0.433411, loss_ce: 0.078198
[03:39:38.846] iteration 690 : loss : 0.443821, loss_ce: 0.044511
[03:39:42.912] iteration 700 : loss : 0.436691, loss_ce: 0.057065
[03:39:46.993] iteration 710 : loss : 0.288466, loss_ce: 0.049178
[03:39:51.062] iteration 720 : loss : 0.429939, loss_ce: 0.058518
[03:39:55.140] iteration 730 : loss : 0.435361, loss_ce: 0.054742
[03:39:59.210] iteration 740 : loss : 0.282112, loss_ce: 0.063373
[03:40:03.289] iteration 750 : loss : 0.452943, loss_ce: 0.045456
[03:40:07.356] iteration 760 : loss : 0.308564, loss_ce: 0.058772
[03:40:11.431] iteration 770 : loss : 0.302926, loss_ce: 0.075606
[03:40:15.500] iteration 780 : loss : 0.444659, loss_ce: 0.043390
[03:40:19.577] iteration 790 : loss : 0.437382, loss_ce: 0.062825
[03:40:23.648] iteration 800 : loss : 0.452565, loss_ce: 0.062769
[03:41:13.074] iteration 810 : loss : 0.490153, loss_ce: 0.113385
[03:41:17.125] iteration 820 : loss : 0.476108, loss_ce: 0.087700
[03:41:21.189] iteration 830 : loss : 0.448559, loss_ce: 0.046345
[03:41:25.248] iteration 840 : loss : 0.432640, loss_ce: 0.042133
[03:41:29.318] iteration 850 : loss : 0.434134, loss_ce: 0.088064
[03:41:33.379] iteration 860 : loss : 0.444617, loss_ce: 0.055701
[03:41:37.453] iteration 870 : loss : 0.436311, loss_ce: 0.056317
[03:41:41.514] iteration 880 : loss : 0.442438, loss_ce: 0.056943
[03:41:45.588] iteration 890 : loss : 0.285832, loss_ce: 0.069332
[03:41:49.656] iteration 900 : loss : 0.284308, loss_ce: 0.052600
[03:41:53.735] iteration 910 : loss : 0.431714, loss_ce: 0.071056
[03:41:57.804] iteration 920 : loss : 0.279323, loss_ce: 0.048133
[03:42:01.886] iteration 930 : loss : 0.425371, loss_ce: 0.057778
[03:42:05.958] iteration 940 : loss : 0.416259, loss_ce: 0.049141
[03:42:10.040] iteration 950 : loss : 0.404634, loss_ce: 0.047492
[03:42:14.110] iteration 960 : loss : 0.426760, loss_ce: 0.047736
[03:42:18.196] iteration 970 : loss : 0.392857, loss_ce: 0.046966
[03:42:22.269] iteration 980 : loss : 0.276971, loss_ce: 0.071774
[03:42:26.355] iteration 990 : loss : 0.406106, loss_ce: 0.035441
[03:42:30.430] iteration 1000 : loss : 0.428841, loss_ce: 0.045222
[03:42:34.518] iteration 1010 : loss : 0.417951, loss_ce: 0.023627
[03:42:38.593] iteration 1020 : loss : 0.398836, loss_ce: 0.072570
[03:42:42.681] iteration 1030 : loss : 0.409127, loss_ce: 0.038996
[03:42:46.758] iteration 1040 : loss : 0.383324, loss_ce: 0.045816
[03:42:50.847] iteration 1050 : loss : 0.412576, loss_ce: 0.042505
[03:42:54.923] iteration 1060 : loss : 0.366485, loss_ce: 0.043265
[03:42:59.013] iteration 1070 : loss : 0.355922, loss_ce: 0.053662
[03:43:59.432] iteration 1080 : loss : 0.417823, loss_ce: 0.077174
[03:44:03.490] iteration 1090 : loss : 0.444337, loss_ce: 0.054335
[03:44:07.547] iteration 1100 : loss : 0.289383, loss_ce: 0.031196
[03:44:11.614] iteration 1110 : loss : 0.438430, loss_ce: 0.048731
[03:44:15.670] iteration 1120 : loss : 0.276842, loss_ce: 0.047379
[03:44:19.740] iteration 1130 : loss : 0.423737, loss_ce: 0.052718
[03:44:23.801] iteration 1140 : loss : 0.262326, loss_ce: 0.053752
[03:44:27.872] iteration 1150 : loss : 0.249836, loss_ce: 0.039723
[03:44:31.934] iteration 1160 : loss : 0.380574, loss_ce: 0.060929
[03:44:36.008] iteration 1170 : loss : 0.404629, loss_ce: 0.031426
[03:44:40.076] iteration 1180 : loss : 0.386221, loss_ce: 0.052295
[03:44:44.152] iteration 1190 : loss : 0.354703, loss_ce: 0.020014
[03:44:48.221] iteration 1200 : loss : 0.449252, loss_ce: 0.035363
[03:44:52.311] iteration 1210 : loss : 0.370078, loss_ce: 0.033148
[03:44:56.381] iteration 1220 : loss : 0.217745, loss_ce: 0.028623
[03:45:00.462] iteration 1230 : loss : 0.391001, loss_ce: 0.027678
[03:45:04.533] iteration 1240 : loss : 0.307222, loss_ce: 0.020775
[03:45:08.614] iteration 1250 : loss : 0.276671, loss_ce: 0.023511
[03:45:12.686] iteration 1260 : loss : 0.071241, loss_ce: 0.013386
[03:45:16.768] iteration 1270 : loss : 0.341271, loss_ce: 0.020331
[03:45:20.841] iteration 1280 : loss : 0.318131, loss_ce: 0.018360
[03:45:24.924] iteration 1290 : loss : 0.348509, loss_ce: 0.019362
[03:45:28.998] iteration 1300 : loss : 0.235090, loss_ce: 0.047402
[03:45:33.083] iteration 1310 : loss : 0.349089, loss_ce: 0.032485
[03:45:37.158] iteration 1320 : loss : 0.292975, loss_ce: 0.030398
[03:45:41.242] iteration 1330 : loss : 0.185612, loss_ce: 0.025470
[03:45:45.219] iteration 1340 : loss : 0.212490, loss_ce: 0.018895
[03:46:45.587] iteration 1350 : loss : 0.525422, loss_ce: 0.186157
[03:46:49.638] iteration 1360 : loss : 0.487891, loss_ce: 0.097655
[03:46:53.700] iteration 1370 : loss : 0.471230, loss_ce: 0.067552
[03:46:57.756] iteration 1380 : loss : 0.446589, loss_ce: 0.069938
[03:47:01.818] iteration 1390 : loss : 0.445173, loss_ce: 0.080580
[03:47:05.872] iteration 1400 : loss : 0.288819, loss_ce: 0.031909
[03:47:09.942] iteration 1410 : loss : 0.437782, loss_ce: 0.061036
[03:47:14.007] iteration 1420 : loss : 0.429935, loss_ce: 0.058741
[03:47:18.084] iteration 1430 : loss : 0.432355, loss_ce: 0.051542
[03:47:22.151] iteration 1440 : loss : 0.288740, loss_ce: 0.040669
[03:47:26.228] iteration 1450 : loss : 0.419251, loss_ce: 0.065674
[03:47:30.297] iteration 1460 : loss : 0.425483, loss_ce: 0.063047
[03:47:34.374] iteration 1470 : loss : 0.415259, loss_ce: 0.066693
[03:47:38.443] iteration 1480 : loss : 0.408346, loss_ce: 0.040417
[03:47:42.526] iteration 1490 : loss : 0.263979, loss_ce: 0.046986
[03:47:46.598] iteration 1500 : loss : 0.447216, loss_ce: 0.057002
[03:47:50.677] iteration 1510 : loss : 0.400120, loss_ce: 0.066508
[03:47:54.748] iteration 1520 : loss : 0.366696, loss_ce: 0.050511
[03:47:58.830] iteration 1530 : loss : 0.224363, loss_ce: 0.022263
[03:48:02.904] iteration 1540 : loss : 0.469108, loss_ce: 0.053388
[03:48:06.990] iteration 1550 : loss : 0.429920, loss_ce: 0.051161
[03:48:11.065] iteration 1560 : loss : 0.260175, loss_ce: 0.064611
[03:48:15.153] iteration 1570 : loss : 0.328246, loss_ce: 0.030304
[03:48:19.231] iteration 1580 : loss : 0.357133, loss_ce: 0.018465
[03:48:23.316] iteration 1590 : loss : 0.166330, loss_ce: 0.029906
[03:48:27.395] iteration 1600 : loss : 0.329724, loss_ce: 0.013209
[03:49:17.110] iteration 1610 : loss : 0.471647, loss_ce: 0.133699
[03:49:21.165] iteration 1620 : loss : 0.477346, loss_ce: 0.092959
[03:49:25.228] iteration 1630 : loss : 0.314867, loss_ce: 0.052670
[03:49:29.286] iteration 1640 : loss : 0.446530, loss_ce: 0.058858
[03:49:33.353] iteration 1650 : loss : 0.283268, loss_ce: 0.046378
[03:49:37.418] iteration 1660 : loss : 0.282156, loss_ce: 0.051802
[03:49:41.493] iteration 1670 : loss : 0.422087, loss_ce: 0.045183
[03:49:45.559] iteration 1680 : loss : 0.416496, loss_ce: 0.043681
[03:49:49.636] iteration 1690 : loss : 0.414413, loss_ce: 0.044610
[03:49:53.703] iteration 1700 : loss : 0.418805, loss_ce: 0.048847
[03:49:57.783] iteration 1710 : loss : 0.410025, loss_ce: 0.073756
[03:50:01.853] iteration 1720 : loss : 0.357071, loss_ce: 0.032452
[03:50:05.933] iteration 1730 : loss : 0.351496, loss_ce: 0.040840
[03:50:10.003] iteration 1740 : loss : 0.317173, loss_ce: 0.038247
[03:50:14.084] iteration 1750 : loss : 0.389753, loss_ce: 0.137884
[03:50:18.157] iteration 1760 : loss : 0.358160, loss_ce: 0.062406
[03:50:22.238] iteration 1770 : loss : 0.317888, loss_ce: 0.012453
[03:50:26.312] iteration 1780 : loss : 0.344968, loss_ce: 0.030183
[03:50:30.397] iteration 1790 : loss : 0.197475, loss_ce: 0.026659
[03:50:34.474] iteration 1800 : loss : 0.182137, loss_ce: 0.023363
[03:50:38.559] iteration 1810 : loss : 0.287107, loss_ce: 0.024759
[03:50:42.635] iteration 1820 : loss : 0.292574, loss_ce: 0.016484
[03:50:46.721] iteration 1830 : loss : 0.282310, loss_ce: 0.019340
[03:50:50.809] iteration 1840 : loss : 0.349755, loss_ce: 0.025823
[03:50:54.903] iteration 1850 : loss : 0.274143, loss_ce: 0.020240
[03:50:58.985] iteration 1860 : loss : 0.250780, loss_ce: 0.006274
[03:51:03.081] iteration 1870 : loss : 0.448227, loss_ce: 0.175044
[03:52:03.669] iteration 1880 : loss : 0.487726, loss_ce: 0.092888
[03:52:07.730] iteration 1890 : loss : 0.468738, loss_ce: 0.047974
[03:52:11.778] iteration 1900 : loss : 0.478957, loss_ce: 0.116823
[03:52:15.842] iteration 1910 : loss : 0.468263, loss_ce: 0.051389
[03:52:19.896] iteration 1920 : loss : 0.313958, loss_ce: 0.055578
[03:52:23.962] iteration 1930 : loss : 0.469820, loss_ce: 0.073389
[03:52:28.018] iteration 1940 : loss : 0.456830, loss_ce: 0.050667
[03:52:32.086] iteration 1950 : loss : 0.449313, loss_ce: 0.052968
[03:52:36.147] iteration 1960 : loss : 0.440270, loss_ce: 0.063724
[03:52:40.221] iteration 1970 : loss : 0.293569, loss_ce: 0.056479
[03:52:44.283] iteration 1980 : loss : 0.447995, loss_ce: 0.058254
[03:52:48.359] iteration 1990 : loss : 0.295136, loss_ce: 0.040303
[03:52:52.424] iteration 2000 : loss : 0.292534, loss_ce: 0.054940
[03:52:56.501] iteration 2010 : loss : 0.435332, loss_ce: 0.065072
[03:53:00.569] iteration 2020 : loss : 0.288832, loss_ce: 0.036867
[03:53:04.648] iteration 2030 : loss : 0.285837, loss_ce: 0.048393
[03:53:08.715] iteration 2040 : loss : 0.435725, loss_ce: 0.053180
[03:53:12.792] iteration 2050 : loss : 0.435115, loss_ce: 0.056066
[03:53:16.864] iteration 2060 : loss : 0.447382, loss_ce: 0.097651
[03:53:20.943] iteration 2070 : loss : 0.428482, loss_ce: 0.068774
[03:53:25.015] iteration 2080 : loss : 0.441555, loss_ce: 0.058672
[03:53:29.100] iteration 2090 : loss : 0.430793, loss_ce: 0.056096
[03:53:33.173] iteration 2100 : loss : 0.425684, loss_ce: 0.054656
[03:53:37.260] iteration 2110 : loss : 0.424214, loss_ce: 0.068069
[03:53:41.339] iteration 2120 : loss : 0.291244, loss_ce: 0.029572
[03:53:45.427] iteration 2130 : loss : 0.430393, loss_ce: 0.052073
[03:53:49.504] iteration 2140 : loss : 0.446592, loss_ce: 0.056426
[03:54:49.830] iteration 2150 : loss : 0.461470, loss_ce: 0.029374
[03:54:53.879] iteration 2160 : loss : 0.288487, loss_ce: 0.076173
[03:54:57.939] iteration 2170 : loss : 0.432026, loss_ce: 0.099641
[03:55:01.993] iteration 2180 : loss : 0.439337, loss_ce: 0.051724
[03:55:06.060] iteration 2190 : loss : 0.275898, loss_ce: 0.049873
[03:55:10.113] iteration 2200 : loss : 0.430006, loss_ce: 0.046923
[03:55:14.182] iteration 2210 : loss : 0.446373, loss_ce: 0.055816
[03:55:18.241] iteration 2220 : loss : 0.411290, loss_ce: 0.038596
[03:55:22.311] iteration 2230 : loss : 0.420954, loss_ce: 0.053490
[03:55:26.377] iteration 2240 : loss : 0.259853, loss_ce: 0.044370
[03:55:30.452] iteration 2250 : loss : 0.388593, loss_ce: 0.058149
[03:55:34.521] iteration 2260 : loss : 0.402516, loss_ce: 0.032382
[03:55:38.600] iteration 2270 : loss : 0.415625, loss_ce: 0.092403
[03:55:42.673] iteration 2280 : loss : 0.363099, loss_ce: 0.042976
[03:55:46.751] iteration 2290 : loss : 0.331407, loss_ce: 0.043067
[03:55:50.823] iteration 2300 : loss : 0.381411, loss_ce: 0.022346
[03:55:54.904] iteration 2310 : loss : 0.363219, loss_ce: 0.024843
[03:55:58.975] iteration 2320 : loss : 0.346794, loss_ce: 0.019126
[03:56:03.062] iteration 2330 : loss : 0.391795, loss_ce: 0.041701
[03:56:07.135] iteration 2340 : loss : 0.320937, loss_ce: 0.027922
[03:56:11.217] iteration 2350 : loss : 0.190826, loss_ce: 0.008862
[03:56:15.290] iteration 2360 : loss : 0.175732, loss_ce: 0.015825
[03:56:19.376] iteration 2370 : loss : 0.327134, loss_ce: 0.028555
[03:56:23.452] iteration 2380 : loss : 0.236230, loss_ce: 0.013188
[03:56:27.537] iteration 2390 : loss : 0.364987, loss_ce: 0.030731
[03:56:31.612] iteration 2400 : loss : 0.288570, loss_ce: 0.020971
[03:56:35.703] iteration 2410 : loss : 0.318084, loss_ce: 0.058147
[03:57:25.596] iteration 2420 : loss : 0.554757, loss_ce: 0.268287
[03:57:29.658] iteration 2430 : loss : 0.476498, loss_ce: 0.065547
[03:57:33.718] iteration 2440 : loss : 0.358923, loss_ce: 0.145311
[03:57:37.791] iteration 2450 : loss : 0.317921, loss_ce: 0.044680
[03:57:41.853] iteration 2460 : loss : 0.476525, loss_ce: 0.073106
[03:57:45.923] iteration 2470 : loss : 0.467476, loss_ce: 0.045912
[03:57:49.984] iteration 2480 : loss : 0.310635, loss_ce: 0.056644
[03:57:54.057] iteration 2490 : loss : 0.318519, loss_ce: 0.090655
[03:57:58.121] iteration 2500 : loss : 0.290827, loss_ce: 0.054531
[03:58:02.197] iteration 2510 : loss : 0.433348, loss_ce: 0.045483
[03:58:06.263] iteration 2520 : loss : 0.444826, loss_ce: 0.086308
[03:58:10.340] iteration 2530 : loss : 0.433804, loss_ce: 0.040963
[03:58:14.411] iteration 2540 : loss : 0.440650, loss_ce: 0.049199
[03:58:18.493] iteration 2550 : loss : 0.281706, loss_ce: 0.032065
[03:58:22.563] iteration 2560 : loss : 0.262342, loss_ce: 0.033537
[03:58:26.645] iteration 2570 : loss : 0.264608, loss_ce: 0.044966
[03:58:30.720] iteration 2580 : loss : 0.430543, loss_ce: 0.039993
[03:58:34.805] iteration 2590 : loss : 0.426437, loss_ce: 0.042893
[03:58:38.880] iteration 2600 : loss : 0.425693, loss_ce: 0.092205
[03:58:42.964] iteration 2610 : loss : 0.413025, loss_ce: 0.069799
[03:58:47.045] iteration 2620 : loss : 0.387011, loss_ce: 0.046110
[03:58:51.139] iteration 2630 : loss : 0.228672, loss_ce: 0.026293
[03:58:55.232] iteration 2640 : loss : 0.462139, loss_ce: 0.081456
[03:58:59.331] iteration 2650 : loss : 0.354043, loss_ce: 0.015092
[03:59:03.421] iteration 2660 : loss : 0.364904, loss_ce: 0.014806
[03:59:07.523] iteration 2670 : loss : 0.306100, loss_ce: 0.030323
[03:59:11.518] iteration 2680 : loss : 0.464532, loss_ce: 0.051172
[03:59:57.920] save model to ./finetune_tpgm_kits23_continual_50iter\finetuned_epoch_9.pth
[04:00:12.153] iteration 2690 : loss : 0.480395, loss_ce: 0.097274
[04:00:16.205] iteration 2700 : loss : 0.491505, loss_ce: 0.104461
[04:00:20.268] iteration 2710 : loss : 0.470264, loss_ce: 0.079216
[04:00:24.323] iteration 2720 : loss : 0.475410, loss_ce: 0.080611
[04:00:28.384] iteration 2730 : loss : 0.459478, loss_ce: 0.053509
[04:00:32.437] iteration 2740 : loss : 0.451679, loss_ce: 0.049259
[04:00:36.509] iteration 2750 : loss : 0.442035, loss_ce: 0.098607
[04:00:40.566] iteration 2760 : loss : 0.451175, loss_ce: 0.074550
[04:00:44.639] iteration 2770 : loss : 0.441395, loss_ce: 0.061981
[04:00:48.706] iteration 2780 : loss : 0.439691, loss_ce: 0.064788
[04:00:52.784] iteration 2790 : loss : 0.443155, loss_ce: 0.053771
[04:00:56.850] iteration 2800 : loss : 0.443343, loss_ce: 0.076263
[04:01:00.929] iteration 2810 : loss : 0.436696, loss_ce: 0.062170
[04:01:04.994] iteration 2820 : loss : 0.439325, loss_ce: 0.051161
[04:01:09.074] iteration 2830 : loss : 0.443939, loss_ce: 0.073747
[04:01:13.143] iteration 2840 : loss : 0.431325, loss_ce: 0.042982
[04:01:17.226] iteration 2850 : loss : 0.441846, loss_ce: 0.050156
[04:01:21.298] iteration 2860 : loss : 0.438834, loss_ce: 0.038850
[04:01:25.381] iteration 2870 : loss : 0.428828, loss_ce: 0.062035
[04:01:29.456] iteration 2880 : loss : 0.434757, loss_ce: 0.050837
[04:01:33.543] iteration 2890 : loss : 0.434038, loss_ce: 0.050666
[04:01:37.618] iteration 2900 : loss : 0.438654, loss_ce: 0.030605
[04:01:41.704] iteration 2910 : loss : 0.441795, loss_ce: 0.038521
[04:01:45.782] iteration 2920 : loss : 0.426304, loss_ce: 0.070350
[04:01:49.875] iteration 2930 : loss : 0.425002, loss_ce: 0.059630
[04:01:53.958] iteration 2940 : loss : 0.425798, loss_ce: 0.053056
[04:02:54.271] iteration 2950 : loss : 0.450246, loss_ce: 0.051024
[04:02:58.321] iteration 2960 : loss : 0.486808, loss_ce: 0.107484
[04:03:02.379] iteration 2970 : loss : 0.502365, loss_ce: 0.134220
[04:03:06.428] iteration 2980 : loss : 0.334129, loss_ce: 0.053544
[04:03:10.487] iteration 2990 : loss : 0.470953, loss_ce: 0.051730
[04:03:14.539] iteration 3000 : loss : 0.492736, loss_ce: 0.105956
[04:03:18.600] iteration 3010 : loss : 0.333415, loss_ce: 0.094532
[04:03:22.654] iteration 3020 : loss : 0.474002, loss_ce: 0.071288
[04:03:26.721] iteration 3030 : loss : 0.464719, loss_ce: 0.059375
[04:03:30.783] iteration 3040 : loss : 0.458118, loss_ce: 0.048306
[04:03:34.853] iteration 3050 : loss : 0.465416, loss_ce: 0.068606
[04:03:38.915] iteration 3060 : loss : 0.470085, loss_ce: 0.084655
[04:03:42.986] iteration 3070 : loss : 0.463145, loss_ce: 0.075992
[04:03:47.049] iteration 3080 : loss : 0.459140, loss_ce: 0.051043
[04:03:51.124] iteration 3090 : loss : 0.460943, loss_ce: 0.063414
[04:03:55.188] iteration 3100 : loss : 0.460655, loss_ce: 0.055871
[04:03:59.264] iteration 3110 : loss : 0.457828, loss_ce: 0.059450
[04:04:03.329] iteration 3120 : loss : 0.454742, loss_ce: 0.054354
[04:04:07.411] iteration 3130 : loss : 0.344975, loss_ce: 0.158566
[04:04:11.478] iteration 3140 : loss : 0.470926, loss_ce: 0.054840
[04:04:15.552] iteration 3150 : loss : 0.313055, loss_ce: 0.049498
[04:04:19.621] iteration 3160 : loss : 0.316900, loss_ce: 0.063073
[04:04:23.698] iteration 3170 : loss : 0.463748, loss_ce: 0.074442
[04:04:27.771] iteration 3180 : loss : 0.299435, loss_ce: 0.034674
[04:04:31.854] iteration 3190 : loss : 0.291350, loss_ce: 0.052846
[04:04:35.927] iteration 3200 : loss : 0.442536, loss_ce: 0.054805
[04:04:40.011] iteration 3210 : loss : 0.441646, loss_ce: 0.053883
[04:05:29.618] iteration 3220 : loss : 0.518922, loss_ce: 0.170529
[04:05:33.683] iteration 3230 : loss : 0.497972, loss_ce: 0.118424
[04:05:37.742] iteration 3240 : loss : 0.480093, loss_ce: 0.078670
[04:05:41.810] iteration 3250 : loss : 0.462523, loss_ce: 0.055514
[04:05:45.868] iteration 3260 : loss : 0.471054, loss_ce: 0.059421
[04:05:49.939] iteration 3270 : loss : 0.450083, loss_ce: 0.055489
[04:05:54.002] iteration 3280 : loss : 0.450402, loss_ce: 0.071225
[04:05:58.074] iteration 3290 : loss : 0.448629, loss_ce: 0.070290
[04:06:02.137] iteration 3300 : loss : 0.446714, loss_ce: 0.059421
[04:06:06.213] iteration 3310 : loss : 0.435935, loss_ce: 0.060658
[04:06:10.280] iteration 3320 : loss : 0.444878, loss_ce: 0.056507
[04:06:14.357] iteration 3330 : loss : 0.426381, loss_ce: 0.065891
[04:06:18.426] iteration 3340 : loss : 0.446868, loss_ce: 0.062044
[04:06:22.505] iteration 3350 : loss : 0.443447, loss_ce: 0.054100
[04:06:26.576] iteration 3360 : loss : 0.441751, loss_ce: 0.038283
[04:06:30.658] iteration 3370 : loss : 0.432979, loss_ce: 0.049101
[04:06:34.733] iteration 3380 : loss : 0.423181, loss_ce: 0.051590
[04:06:38.818] iteration 3390 : loss : 0.417320, loss_ce: 0.048727
[04:06:42.893] iteration 3400 : loss : 0.429271, loss_ce: 0.060625
[04:06:46.981] iteration 3410 : loss : 0.420772, loss_ce: 0.038897
[04:06:51.060] iteration 3420 : loss : 0.427924, loss_ce: 0.057729
[04:06:55.151] iteration 3430 : loss : 0.403341, loss_ce: 0.046243
[04:06:59.227] iteration 3440 : loss : 0.394478, loss_ce: 0.055275
[04:07:03.312] iteration 3450 : loss : 0.403300, loss_ce: 0.045925
[04:07:07.393] iteration 3460 : loss : 0.374796, loss_ce: 0.041610
[04:07:11.487] iteration 3470 : loss : 0.379883, loss_ce: 0.029090
[04:07:15.579] iteration 3480 : loss : 0.322837, loss_ce: 0.022673
[04:08:16.185] iteration 3490 : loss : 0.467724, loss_ce: 0.043835
[04:08:20.234] iteration 3500 : loss : 0.459396, loss_ce: 0.048238
[04:08:24.297] iteration 3510 : loss : 0.434551, loss_ce: 0.046234
[04:08:28.355] iteration 3520 : loss : 0.292902, loss_ce: 0.050735
[04:08:32.423] iteration 3530 : loss : 0.297713, loss_ce: 0.049604
[04:08:36.485] iteration 3540 : loss : 0.427373, loss_ce: 0.064401
[04:08:40.557] iteration 3550 : loss : 0.279170, loss_ce: 0.018326
[04:08:44.625] iteration 3560 : loss : 0.396346, loss_ce: 0.046715
[04:08:48.700] iteration 3570 : loss : 0.352027, loss_ce: 0.037949
[04:08:52.769] iteration 3580 : loss : 0.221472, loss_ce: 0.039561
[04:08:56.846] iteration 3590 : loss : 0.391446, loss_ce: 0.054533
[04:09:00.914] iteration 3600 : loss : 0.440549, loss_ce: 0.037088
[04:09:04.991] iteration 3610 : loss : 0.422939, loss_ce: 0.059027
[04:09:09.059] iteration 3620 : loss : 0.392495, loss_ce: 0.030546
[04:09:13.137] iteration 3630 : loss : 0.234833, loss_ce: 0.024791
[04:09:17.209] iteration 3640 : loss : 0.388706, loss_ce: 0.056843
[04:09:21.291] iteration 3650 : loss : 0.187212, loss_ce: 0.010129
[04:09:25.367] iteration 3660 : loss : 0.328965, loss_ce: 0.024137
[04:09:29.452] iteration 3670 : loss : 0.351987, loss_ce: 0.032311
[04:09:33.531] iteration 3680 : loss : 0.316523, loss_ce: 0.026453
[04:09:37.618] iteration 3690 : loss : 0.238417, loss_ce: 0.022054
[04:09:41.694] iteration 3700 : loss : 0.370903, loss_ce: 0.023468
[04:09:45.781] iteration 3710 : loss : 0.101797, loss_ce: 0.020881
[04:09:49.857] iteration 3720 : loss : 0.454978, loss_ce: 0.082118
[04:09:53.948] iteration 3730 : loss : 0.315571, loss_ce: 0.048682
[04:09:58.026] iteration 3740 : loss : 0.446797, loss_ce: 0.071687
[04:10:02.118] iteration 3750 : loss : 0.398221, loss_ce: 0.072172
[04:11:02.345] iteration 3760 : loss : 0.541186, loss_ce: 0.225630
[04:11:06.405] iteration 3770 : loss : 0.478599, loss_ce: 0.116638
[04:11:10.458] iteration 3780 : loss : 0.381231, loss_ce: 0.035768
[04:11:14.522] iteration 3790 : loss : 0.474835, loss_ce: 0.061021
[04:11:18.581] iteration 3800 : loss : 0.452836, loss_ce: 0.056182
[04:11:22.649] iteration 3810 : loss : 0.453168, loss_ce: 0.071631
[04:11:26.706] iteration 3820 : loss : 0.304860, loss_ce: 0.078586
[04:11:30.776] iteration 3830 : loss : 0.439632, loss_ce: 0.060438
[04:11:34.837] iteration 3840 : loss : 0.434898, loss_ce: 0.052345
[04:11:38.908] iteration 3850 : loss : 0.439407, loss_ce: 0.051392
[04:11:42.971] iteration 3860 : loss : 0.443441, loss_ce: 0.054969
[04:11:47.047] iteration 3870 : loss : 0.429447, loss_ce: 0.051053
[04:11:51.115] iteration 3880 : loss : 0.421500, loss_ce: 0.056724
[04:11:55.193] iteration 3890 : loss : 0.412475, loss_ce: 0.049172
[04:11:59.262] iteration 3900 : loss : 0.419706, loss_ce: 0.042289
[04:12:03.353] iteration 3910 : loss : 0.421840, loss_ce: 0.038417
[04:12:07.425] iteration 3920 : loss : 0.398644, loss_ce: 0.034739
[04:12:11.505] iteration 3930 : loss : 0.432541, loss_ce: 0.054917
[04:12:15.575] iteration 3940 : loss : 0.280422, loss_ce: 0.046204
[04:12:19.656] iteration 3950 : loss : 0.352804, loss_ce: 0.018534
[04:12:23.733] iteration 3960 : loss : 0.309388, loss_ce: 0.030059
[04:12:27.817] iteration 3970 : loss : 0.326678, loss_ce: 0.034827
[04:12:31.891] iteration 3980 : loss : 0.294606, loss_ce: 0.030680
[04:12:35.976] iteration 3990 : loss : 0.382644, loss_ce: 0.089535
[04:12:40.055] iteration 4000 : loss : 0.342144, loss_ce: 0.024121
[04:12:44.140] iteration 4010 : loss : 0.299313, loss_ce: 0.024626
[04:12:48.123] iteration 4020 : loss : 0.280961, loss_ce: 0.025546
[04:13:37.732] iteration 4030 : loss : 0.486495, loss_ce: 0.089840
[04:13:41.787] iteration 4040 : loss : 0.487861, loss_ce: 0.103596
[04:13:45.855] iteration 4050 : loss : 0.473658, loss_ce: 0.093655
[04:13:49.912] iteration 4060 : loss : 0.446564, loss_ce: 0.047281
[04:13:53.986] iteration 4070 : loss : 0.283534, loss_ce: 0.072676
[04:13:58.048] iteration 4080 : loss : 0.448964, loss_ce: 0.061486
[04:14:02.121] iteration 4090 : loss : 0.286720, loss_ce: 0.046726
[04:14:06.187] iteration 4100 : loss : 0.442043, loss_ce: 0.039017
[04:14:10.262] iteration 4110 : loss : 0.424234, loss_ce: 0.046207
[04:14:14.328] iteration 4120 : loss : 0.409269, loss_ce: 0.070610
[04:14:18.406] iteration 4130 : loss : 0.418097, loss_ce: 0.041160
[04:14:22.475] iteration 4140 : loss : 0.395235, loss_ce: 0.032953
[04:14:26.553] iteration 4150 : loss : 0.427435, loss_ce: 0.026838
[04:14:30.626] iteration 4160 : loss : 0.143390, loss_ce: 0.018400
[04:14:34.706] iteration 4170 : loss : 0.363529, loss_ce: 0.051005
[04:14:38.779] iteration 4180 : loss : 0.358964, loss_ce: 0.019271
[04:14:42.864] iteration 4190 : loss : 0.334481, loss_ce: 0.047693
[04:14:46.950] iteration 4200 : loss : 0.293260, loss_ce: 0.026024
[04:14:51.044] iteration 4210 : loss : 0.349703, loss_ce: 0.028550
[04:14:55.122] iteration 4220 : loss : 0.342887, loss_ce: 0.036065
[04:14:59.208] iteration 4230 : loss : 0.186812, loss_ce: 0.024940
[04:15:03.287] iteration 4240 : loss : 0.316331, loss_ce: 0.036703
[04:15:07.383] iteration 4250 : loss : 0.340073, loss_ce: 0.016019
[04:15:11.475] iteration 4260 : loss : 0.382498, loss_ce: 0.034907
[04:15:15.579] iteration 4270 : loss : 0.410357, loss_ce: 0.059236
[04:15:19.679] iteration 4280 : loss : 0.349221, loss_ce: 0.053104
[04:16:20.042] iteration 4290 : loss : 0.426391, loss_ce: 0.064348
[04:16:24.099] iteration 4300 : loss : 0.469225, loss_ce: 0.048836
[04:16:28.159] iteration 4310 : loss : 0.459075, loss_ce: 0.084672
[04:16:32.213] iteration 4320 : loss : 0.456647, loss_ce: 0.074719
[04:16:36.278] iteration 4330 : loss : 0.449255, loss_ce: 0.066209
[04:16:40.336] iteration 4340 : loss : 0.299344, loss_ce: 0.040999
[04:16:44.405] iteration 4350 : loss : 0.446391, loss_ce: 0.073260
[04:16:48.466] iteration 4360 : loss : 0.306401, loss_ce: 0.088734
[04:16:52.537] iteration 4370 : loss : 0.297247, loss_ce: 0.060311
[04:16:56.601] iteration 4380 : loss : 0.458702, loss_ce: 0.086540
[04:17:00.677] iteration 4390 : loss : 0.281269, loss_ce: 0.048628
[04:17:04.746] iteration 4400 : loss : 0.438449, loss_ce: 0.052251
[04:17:08.826] iteration 4410 : loss : 0.286728, loss_ce: 0.033399
[04:17:12.894] iteration 4420 : loss : 0.446061, loss_ce: 0.088593
[04:17:16.969] iteration 4430 : loss : 0.436968, loss_ce: 0.053202
[04:17:21.040] iteration 4440 : loss : 0.448575, loss_ce: 0.077845
[04:17:25.121] iteration 4450 : loss : 0.274765, loss_ce: 0.048135
[04:17:29.193] iteration 4460 : loss : 0.404209, loss_ce: 0.050591
[04:17:33.274] iteration 4470 : loss : 0.410432, loss_ce: 0.063217
[04:17:37.349] iteration 4480 : loss : 0.358470, loss_ce: 0.026843
[04:17:41.440] iteration 4490 : loss : 0.304656, loss_ce: 0.039817
[04:17:45.514] iteration 4500 : loss : 0.387836, loss_ce: 0.028530
[04:17:49.599] iteration 4510 : loss : 0.330861, loss_ce: 0.027358
[04:17:53.677] iteration 4520 : loss : 0.380306, loss_ce: 0.055852
[04:17:57.771] iteration 4530 : loss : 0.347412, loss_ce: 0.036483
[04:18:01.858] iteration 4540 : loss : 0.400725, loss_ce: 0.042719
[04:18:05.949] iteration 4550 : loss : 0.331989, loss_ce: 0.024643
[04:19:06.179] iteration 4560 : loss : 0.484963, loss_ce: 0.085956
[04:19:10.241] iteration 4570 : loss : 0.464309, loss_ce: 0.040080
[04:19:14.294] iteration 4580 : loss : 0.445359, loss_ce: 0.063899
[04:19:18.357] iteration 4590 : loss : 0.435200, loss_ce: 0.067700
[04:19:22.417] iteration 4600 : loss : 0.276963, loss_ce: 0.058185
[04:19:26.490] iteration 4610 : loss : 0.423064, loss_ce: 0.044010
[04:19:30.553] iteration 4620 : loss : 0.426225, loss_ce: 0.057579
[04:19:34.625] iteration 4630 : loss : 0.409433, loss_ce: 0.054974
[04:19:38.688] iteration 4640 : loss : 0.380218, loss_ce: 0.029111
[04:19:42.769] iteration 4650 : loss : 0.221992, loss_ce: 0.021564
[04:19:46.838] iteration 4660 : loss : 0.348939, loss_ce: 0.035722
[04:19:50.916] iteration 4670 : loss : 0.383837, loss_ce: 0.070434
[04:19:54.986] iteration 4680 : loss : 0.399022, loss_ce: 0.018543
[04:19:59.070] iteration 4690 : loss : 0.327665, loss_ce: 0.024244
[04:20:03.144] iteration 4700 : loss : 0.357735, loss_ce: 0.038552
[04:20:07.230] iteration 4710 : loss : 0.190295, loss_ce: 0.009978
[04:20:11.302] iteration 4720 : loss : 0.227306, loss_ce: 0.062722
[04:20:15.387] iteration 4730 : loss : 0.271559, loss_ce: 0.030036
[04:20:19.465] iteration 4740 : loss : 0.360422, loss_ce: 0.035119
[04:20:23.550] iteration 4750 : loss : 0.321967, loss_ce: 0.019836
[04:20:27.628] iteration 4760 : loss : 0.222701, loss_ce: 0.011784
[04:20:31.715] iteration 4770 : loss : 0.341436, loss_ce: 0.034097
[04:20:35.796] iteration 4780 : loss : 0.305756, loss_ce: 0.020207
[04:20:39.884] iteration 4790 : loss : 0.168614, loss_ce: 0.013045
[04:20:43.962] iteration 4800 : loss : 0.178674, loss_ce: 0.010385
[04:20:48.052] iteration 4810 : loss : 0.305567, loss_ce: 0.016442
[04:20:52.137] iteration 4820 : loss : 0.323108, loss_ce: 0.013914
[04:21:41.944] iteration 4830 : loss : 0.476041, loss_ce: 0.073334
[04:21:45.998] iteration 4840 : loss : 0.453415, loss_ce: 0.058118
[04:21:50.061] iteration 4850 : loss : 0.461554, loss_ce: 0.051636
[04:21:54.118] iteration 4860 : loss : 0.449053, loss_ce: 0.058465
[04:21:58.184] iteration 4870 : loss : 0.442689, loss_ce: 0.061260
[04:22:02.247] iteration 4880 : loss : 0.277744, loss_ce: 0.047692
[04:22:06.322] iteration 4890 : loss : 0.450680, loss_ce: 0.045960
[04:22:10.384] iteration 4900 : loss : 0.427132, loss_ce: 0.055205
[04:22:14.460] iteration 4910 : loss : 0.430488, loss_ce: 0.077584
[04:22:18.528] iteration 4920 : loss : 0.422448, loss_ce: 0.030081
[04:22:22.606] iteration 4930 : loss : 0.425542, loss_ce: 0.037465
[04:22:26.676] iteration 4940 : loss : 0.411243, loss_ce: 0.051772
[04:22:30.759] iteration 4950 : loss : 0.253958, loss_ce: 0.058103
[04:22:34.831] iteration 4960 : loss : 0.384740, loss_ce: 0.031698
[04:22:38.912] iteration 4970 : loss : 0.369350, loss_ce: 0.034445
[04:22:42.984] iteration 4980 : loss : 0.383620, loss_ce: 0.066625
[04:22:47.070] iteration 4990 : loss : 0.367689, loss_ce: 0.029539
[04:22:51.144] iteration 5000 : loss : 0.188693, loss_ce: 0.028489
[04:22:55.233] iteration 5010 : loss : 0.163527, loss_ce: 0.043952
[04:22:59.309] iteration 5020 : loss : 0.352219, loss_ce: 0.042710
[04:23:03.397] iteration 5030 : loss : 0.344164, loss_ce: 0.015049
[04:23:07.477] iteration 5040 : loss : 0.332825, loss_ce: 0.020760
[04:23:11.562] iteration 5050 : loss : 0.175730, loss_ce: 0.012574
[04:23:15.640] iteration 5060 : loss : 0.266772, loss_ce: 0.026258
[04:23:19.738] iteration 5070 : loss : 0.280570, loss_ce: 0.025738
[04:23:23.830] iteration 5080 : loss : 0.215159, loss_ce: 0.031863
[04:23:27.929] iteration 5090 : loss : 0.313506, loss_ce: 0.012189
[04:24:28.501] iteration 5100 : loss : 0.453927, loss_ce: 0.037877
[04:24:32.565] iteration 5110 : loss : 0.472000, loss_ce: 0.057026
[04:24:36.619] iteration 5120 : loss : 0.462449, loss_ce: 0.056096
[04:24:40.688] iteration 5130 : loss : 0.294746, loss_ce: 0.046755
[04:24:44.747] iteration 5140 : loss : 0.283808, loss_ce: 0.054836
[04:24:48.819] iteration 5150 : loss : 0.414632, loss_ce: 0.051704
[04:24:52.881] iteration 5160 : loss : 0.413166, loss_ce: 0.029889
[04:24:56.956] iteration 5170 : loss : 0.269541, loss_ce: 0.040188
[04:25:01.021] iteration 5180 : loss : 0.252643, loss_ce: 0.028483
[04:25:05.098] iteration 5190 : loss : 0.243423, loss_ce: 0.039083
[04:25:09.164] iteration 5200 : loss : 0.360639, loss_ce: 0.050682
[04:25:13.243] iteration 5210 : loss : 0.252333, loss_ce: 0.019852
[04:25:17.317] iteration 5220 : loss : 0.358446, loss_ce: 0.015576
[04:25:21.398] iteration 5230 : loss : 0.336811, loss_ce: 0.031450
[04:25:25.469] iteration 5240 : loss : 0.198776, loss_ce: 0.021926
[04:25:29.552] iteration 5250 : loss : 0.297433, loss_ce: 0.021076
[04:25:33.625] iteration 5260 : loss : 0.320787, loss_ce: 0.042920
[04:25:37.709] iteration 5270 : loss : 0.253277, loss_ce: 0.022703
[04:25:41.781] iteration 5280 : loss : 0.323341, loss_ce: 0.025797
[04:25:45.866] iteration 5290 : loss : 0.328036, loss_ce: 0.019748
[04:25:49.943] iteration 5300 : loss : 0.340974, loss_ce: 0.038768
[04:25:54.031] iteration 5310 : loss : 0.116116, loss_ce: 0.014611
[04:25:58.104] iteration 5320 : loss : 0.319532, loss_ce: 0.005483
[04:26:02.197] iteration 5330 : loss : 0.164929, loss_ce: 0.003057
[04:26:06.291] iteration 5340 : loss : 0.332106, loss_ce: 0.033315
[04:26:10.389] iteration 5350 : loss : 0.254317, loss_ce: 0.014546
[04:26:14.384] iteration 5360 : loss : 0.271630, loss_ce: 0.016191
[04:27:00.746] save model to ./finetune_tpgm_kits23_continual_50iter\finetuned_epoch_19.pth
[04:27:14.880] iteration 5370 : loss : 0.332464, loss_ce: 0.079737
[04:27:18.935] iteration 5380 : loss : 0.451328, loss_ce: 0.065700
[04:27:22.993] iteration 5390 : loss : 0.444363, loss_ce: 0.069068
[04:27:27.045] iteration 5400 : loss : 0.433657, loss_ce: 0.054776
[04:27:31.108] iteration 5410 : loss : 0.438343, loss_ce: 0.045574
[04:27:35.166] iteration 5420 : loss : 0.283644, loss_ce: 0.055394
[04:27:39.237] iteration 5430 : loss : 0.442817, loss_ce: 0.063173
[04:27:43.302] iteration 5440 : loss : 0.429767, loss_ce: 0.062182
[04:27:47.376] iteration 5450 : loss : 0.281867, loss_ce: 0.043662
[04:27:51.440] iteration 5460 : loss : 0.412824, loss_ce: 0.042902
[04:27:55.517] iteration 5470 : loss : 0.429319, loss_ce: 0.095409
[04:27:59.583] iteration 5480 : loss : 0.389464, loss_ce: 0.035685
[04:28:03.660] iteration 5490 : loss : 0.326316, loss_ce: 0.021367
[04:28:07.728] iteration 5500 : loss : 0.337034, loss_ce: 0.030457
[04:28:11.805] iteration 5510 : loss : 0.304252, loss_ce: 0.024002
[04:28:15.875] iteration 5520 : loss : 0.314188, loss_ce: 0.014819
[04:28:19.957] iteration 5530 : loss : 0.176483, loss_ce: 0.028593
[04:28:24.029] iteration 5540 : loss : 0.303121, loss_ce: 0.021279
[04:28:28.113] iteration 5550 : loss : 0.346022, loss_ce: 0.019720
[04:28:32.187] iteration 5560 : loss : 0.340250, loss_ce: 0.019009
[04:28:36.272] iteration 5570 : loss : 0.310889, loss_ce: 0.021962
[04:28:40.349] iteration 5580 : loss : 0.239615, loss_ce: 0.015109
[04:28:44.432] iteration 5590 : loss : 0.337451, loss_ce: 0.045830
[04:28:48.508] iteration 5600 : loss : 0.346877, loss_ce: 0.023095
[04:28:52.596] iteration 5610 : loss : 0.337562, loss_ce: 0.031146
[04:28:56.681] iteration 5620 : loss : 0.331223, loss_ce: 0.021638
[04:29:46.196] iteration 5630 : loss : 0.474594, loss_ce: 0.125232
[04:29:50.245] iteration 5640 : loss : 0.450335, loss_ce: 0.030823
[04:29:54.310] iteration 5650 : loss : 0.477769, loss_ce: 0.070361
[04:29:58.364] iteration 5660 : loss : 0.460146, loss_ce: 0.059450
[04:30:02.430] iteration 5670 : loss : 0.456717, loss_ce: 0.070160
[04:30:06.489] iteration 5680 : loss : 0.446020, loss_ce: 0.059968
[04:30:10.562] iteration 5690 : loss : 0.448973, loss_ce: 0.041941
[04:30:14.627] iteration 5700 : loss : 0.424466, loss_ce: 0.043254
[04:30:18.701] iteration 5710 : loss : 0.417491, loss_ce: 0.056384
[04:30:22.766] iteration 5720 : loss : 0.417324, loss_ce: 0.025312
[04:30:26.843] iteration 5730 : loss : 0.379224, loss_ce: 0.049262
[04:30:30.911] iteration 5740 : loss : 0.171933, loss_ce: 0.018056
[04:30:34.989] iteration 5750 : loss : 0.391710, loss_ce: 0.122992
[04:30:39.061] iteration 5760 : loss : 0.322615, loss_ce: 0.019852
[04:30:43.144] iteration 5770 : loss : 0.152806, loss_ce: 0.018954
[04:30:47.216] iteration 5780 : loss : 0.319970, loss_ce: 0.047626
[04:30:51.298] iteration 5790 : loss : 0.314832, loss_ce: 0.021454
[04:30:55.373] iteration 5800 : loss : 0.275253, loss_ce: 0.039456
[04:30:59.460] iteration 5810 : loss : 0.246709, loss_ce: 0.019087
[04:31:03.538] iteration 5820 : loss : 0.281323, loss_ce: 0.025193
[04:31:07.625] iteration 5830 : loss : 0.091407, loss_ce: 0.012709
[04:31:11.705] iteration 5840 : loss : 0.252746, loss_ce: 0.029240
[04:31:15.795] iteration 5850 : loss : 0.291723, loss_ce: 0.012369
[04:31:19.872] iteration 5860 : loss : 0.216877, loss_ce: 0.012873
[04:31:23.966] iteration 5870 : loss : 0.266063, loss_ce: 0.014289
[04:31:28.051] iteration 5880 : loss : 0.320643, loss_ce: 0.024010
[04:31:32.142] iteration 5890 : loss : 0.300501, loss_ce: 0.016117
[04:32:32.529] iteration 5900 : loss : 0.515718, loss_ce: 0.161845
[04:32:36.592] iteration 5910 : loss : 0.487881, loss_ce: 0.093507
[04:32:40.643] iteration 5920 : loss : 0.343175, loss_ce: 0.106459
[04:32:44.707] iteration 5930 : loss : 0.491727, loss_ce: 0.102921
[04:32:48.766] iteration 5940 : loss : 0.480930, loss_ce: 0.105856
[04:32:52.836] iteration 5950 : loss : 0.465831, loss_ce: 0.048231
[04:32:56.896] iteration 5960 : loss : 0.310331, loss_ce: 0.061001
[04:33:00.971] iteration 5970 : loss : 0.458730, loss_ce: 0.045066
[04:33:05.034] iteration 5980 : loss : 0.309280, loss_ce: 0.068494
[04:33:09.110] iteration 5990 : loss : 0.446070, loss_ce: 0.042984
[04:33:13.179] iteration 6000 : loss : 0.448941, loss_ce: 0.071448
[04:33:17.257] iteration 6010 : loss : 0.455264, loss_ce: 0.063387
[04:33:21.327] iteration 6020 : loss : 0.439069, loss_ce: 0.046179
[04:33:25.405] iteration 6030 : loss : 0.275652, loss_ce: 0.052218
[04:33:29.474] iteration 6040 : loss : 0.416207, loss_ce: 0.053840
[04:33:33.555] iteration 6050 : loss : 0.410514, loss_ce: 0.051215
[04:33:37.625] iteration 6060 : loss : 0.297748, loss_ce: 0.028020
[04:33:41.707] iteration 6070 : loss : 0.479176, loss_ce: 0.072580
[04:33:45.776] iteration 6080 : loss : 0.476943, loss_ce: 0.069689
[04:33:49.856] iteration 6090 : loss : 0.466229, loss_ce: 0.086599
[04:33:53.927] iteration 6100 : loss : 0.448856, loss_ce: 0.054300
[04:33:58.014] iteration 6110 : loss : 0.449828, loss_ce: 0.077830
[04:34:02.086] iteration 6120 : loss : 0.440305, loss_ce: 0.054372
[04:34:06.173] iteration 6130 : loss : 0.432830, loss_ce: 0.061608
[04:34:10.246] iteration 6140 : loss : 0.423792, loss_ce: 0.072148
[04:34:14.337] iteration 6150 : loss : 0.439864, loss_ce: 0.051432
[04:34:18.421] iteration 6160 : loss : 0.441546, loss_ce: 0.058394
[04:35:18.691] iteration 6170 : loss : 0.441109, loss_ce: 0.053875
[04:35:22.742] iteration 6180 : loss : 0.445515, loss_ce: 0.090180
[04:35:26.806] iteration 6190 : loss : 0.435353, loss_ce: 0.065399
[04:35:30.862] iteration 6200 : loss : 0.390936, loss_ce: 0.059112
[04:35:34.932] iteration 6210 : loss : 0.363230, loss_ce: 0.057422
[04:35:38.995] iteration 6220 : loss : 0.315385, loss_ce: 0.024399
[04:35:43.070] iteration 6230 : loss : 0.352455, loss_ce: 0.026501
[04:35:47.137] iteration 6240 : loss : 0.327663, loss_ce: 0.035680
[04:35:51.213] iteration 6250 : loss : 0.358102, loss_ce: 0.050128
[04:35:55.281] iteration 6260 : loss : 0.346103, loss_ce: 0.024134
[04:35:59.361] iteration 6270 : loss : 0.189708, loss_ce: 0.033683
[04:36:03.432] iteration 6280 : loss : 0.162001, loss_ce: 0.009695
[04:36:07.511] iteration 6290 : loss : 0.297219, loss_ce: 0.014521
[04:36:11.584] iteration 6300 : loss : 0.302694, loss_ce: 0.029925
[04:36:15.665] iteration 6310 : loss : 0.225060, loss_ce: 0.008186
[04:36:19.737] iteration 6320 : loss : 0.311589, loss_ce: 0.010847
[04:36:23.820] iteration 6330 : loss : 0.338871, loss_ce: 0.044446
[04:36:27.894] iteration 6340 : loss : 0.323245, loss_ce: 0.017549
[04:36:31.981] iteration 6350 : loss : 0.177077, loss_ce: 0.003504
[04:36:36.057] iteration 6360 : loss : 0.219802, loss_ce: 0.009091
[04:36:40.154] iteration 6370 : loss : 0.338517, loss_ce: 0.039752
[04:36:44.232] iteration 6380 : loss : 0.149031, loss_ce: 0.023374
[04:36:48.320] iteration 6390 : loss : 0.207851, loss_ce: 0.010228
[04:36:52.400] iteration 6400 : loss : 0.232828, loss_ce: 0.016495
[04:36:56.488] iteration 6410 : loss : 0.283188, loss_ce: 0.034174
[04:37:00.573] iteration 6420 : loss : 0.228477, loss_ce: 0.016985
[04:37:04.669] iteration 6430 : loss : 0.321431, loss_ce: 0.007587
[04:37:54.653] iteration 6440 : loss : 0.461185, loss_ce: 0.038367
[04:37:58.718] iteration 6450 : loss : 0.310335, loss_ce: 0.041401
[04:38:02.777] iteration 6460 : loss : 0.309630, loss_ce: 0.059702
[04:38:06.849] iteration 6470 : loss : 0.302336, loss_ce: 0.059175
[04:38:10.914] iteration 6480 : loss : 0.449476, loss_ce: 0.074213
[04:38:14.986] iteration 6490 : loss : 0.442034, loss_ce: 0.044037
[04:38:19.053] iteration 6500 : loss : 0.292199, loss_ce: 0.048933
[04:38:23.127] iteration 6510 : loss : 0.276154, loss_ce: 0.053568
[04:38:27.194] iteration 6520 : loss : 0.419600, loss_ce: 0.070608
[04:38:31.272] iteration 6530 : loss : 0.403534, loss_ce: 0.061486
[04:38:35.341] iteration 6540 : loss : 0.446865, loss_ce: 0.053878
[04:38:39.421] iteration 6550 : loss : 0.396268, loss_ce: 0.055221
[04:38:43.493] iteration 6560 : loss : 0.343016, loss_ce: 0.025651
[04:38:47.576] iteration 6570 : loss : 0.381020, loss_ce: 0.019115
[04:38:51.651] iteration 6580 : loss : 0.190109, loss_ce: 0.026128
[04:38:55.738] iteration 6590 : loss : 0.145711, loss_ce: 0.016188
[04:38:59.816] iteration 6600 : loss : 0.317309, loss_ce: 0.014341
[04:39:03.900] iteration 6610 : loss : 0.195362, loss_ce: 0.024755
[04:39:07.980] iteration 6620 : loss : 0.366541, loss_ce: 0.077589
[04:39:12.071] iteration 6630 : loss : 0.343768, loss_ce: 0.016259
[04:39:16.155] iteration 6640 : loss : 0.306220, loss_ce: 0.016165
[04:39:20.258] iteration 6650 : loss : 0.148258, loss_ce: 0.032138
[04:39:24.342] iteration 6660 : loss : 0.337126, loss_ce: 0.019325
[04:39:28.451] iteration 6670 : loss : 0.318341, loss_ce: 0.023978
[04:39:32.545] iteration 6680 : loss : 0.264719, loss_ce: 0.033635
[04:39:36.645] iteration 6690 : loss : 0.268911, loss_ce: 0.044652
[04:39:40.641] iteration 6700 : loss : 0.347691, loss_ce: 0.051706
[04:40:41.021] iteration 6710 : loss : 0.438386, loss_ce: 0.090975
[04:40:45.077] iteration 6720 : loss : 0.458099, loss_ce: 0.074334
[04:40:49.138] iteration 6730 : loss : 0.447533, loss_ce: 0.072219
[04:40:53.195] iteration 6740 : loss : 0.433576, loss_ce: 0.040830
[04:40:57.266] iteration 6750 : loss : 0.429721, loss_ce: 0.058883
[04:41:01.329] iteration 6760 : loss : 0.431092, loss_ce: 0.036300
[04:41:05.404] iteration 6770 : loss : 0.415618, loss_ce: 0.043038
[04:41:09.470] iteration 6780 : loss : 0.286053, loss_ce: 0.009369
[04:41:13.546] iteration 6790 : loss : 0.419541, loss_ce: 0.069929
[04:41:17.611] iteration 6800 : loss : 0.414884, loss_ce: 0.030509
[04:41:21.685] iteration 6810 : loss : 0.372008, loss_ce: 0.033037
[04:41:25.750] iteration 6820 : loss : 0.210250, loss_ce: 0.037037
[04:41:29.830] iteration 6830 : loss : 0.184278, loss_ce: 0.016636
[04:41:33.900] iteration 6840 : loss : 0.309855, loss_ce: 0.019742
[04:41:37.983] iteration 6850 : loss : 0.217263, loss_ce: 0.018185
[04:41:42.053] iteration 6860 : loss : 0.290418, loss_ce: 0.026161
[04:41:46.134] iteration 6870 : loss : 0.150556, loss_ce: 0.014293
[04:41:50.208] iteration 6880 : loss : 0.261680, loss_ce: 0.022706
[04:41:54.291] iteration 6890 : loss : 0.288688, loss_ce: 0.028108
[04:41:58.366] iteration 6900 : loss : 0.358829, loss_ce: 0.034825
[04:42:02.454] iteration 6910 : loss : 0.192524, loss_ce: 0.016565
[04:42:06.535] iteration 6920 : loss : 0.349438, loss_ce: 0.044550
[04:42:10.620] iteration 6930 : loss : 0.129919, loss_ce: 0.017677
[04:42:14.699] iteration 6940 : loss : 0.268171, loss_ce: 0.009534
[04:42:18.792] iteration 6950 : loss : 0.179472, loss_ce: 0.019643
[04:42:22.873] iteration 6960 : loss : 0.330864, loss_ce: 0.021154
[04:43:23.123] iteration 6970 : loss : 0.437742, loss_ce: 0.054418
[04:43:27.175] iteration 6980 : loss : 0.501894, loss_ce: 0.127922
[04:43:31.235] iteration 6990 : loss : 0.345249, loss_ce: 0.116075
[04:43:35.290] iteration 7000 : loss : 0.480153, loss_ce: 0.075477
[04:43:39.355] iteration 7010 : loss : 0.462714, loss_ce: 0.033819
[04:43:43.412] iteration 7020 : loss : 0.460913, loss_ce: 0.094326
[04:43:47.487] iteration 7030 : loss : 0.436241, loss_ce: 0.049614
[04:43:51.552] iteration 7040 : loss : 0.433345, loss_ce: 0.047730
[04:43:55.628] iteration 7050 : loss : 0.286297, loss_ce: 0.062205
[04:43:59.695] iteration 7060 : loss : 0.433474, loss_ce: 0.049545
[04:44:03.777] iteration 7070 : loss : 0.430750, loss_ce: 0.042067
[04:44:07.847] iteration 7080 : loss : 0.422837, loss_ce: 0.039834
[04:44:11.925] iteration 7090 : loss : 0.442373, loss_ce: 0.067280
[04:44:15.994] iteration 7100 : loss : 0.409149, loss_ce: 0.058346
[04:44:20.073] iteration 7110 : loss : 0.406256, loss_ce: 0.034788
[04:44:24.146] iteration 7120 : loss : 0.379275, loss_ce: 0.039270
[04:44:28.228] iteration 7130 : loss : 0.361660, loss_ce: 0.053144
[04:44:32.302] iteration 7140 : loss : 0.263663, loss_ce: 0.033051
[04:44:36.383] iteration 7150 : loss : 0.325824, loss_ce: 0.020155
[04:44:40.458] iteration 7160 : loss : 0.379444, loss_ce: 0.035153
[04:44:44.542] iteration 7170 : loss : 0.419542, loss_ce: 0.130705
[04:44:48.619] iteration 7180 : loss : 0.261684, loss_ce: 0.090390
[04:44:52.710] iteration 7190 : loss : 0.339734, loss_ce: 0.016385
[04:44:56.800] iteration 7200 : loss : 0.175612, loss_ce: 0.010110
[04:45:00.892] iteration 7210 : loss : 0.201419, loss_ce: 0.022465
[04:45:04.969] iteration 7220 : loss : 0.339103, loss_ce: 0.020111
[04:45:09.057] iteration 7230 : loss : 0.333895, loss_ce: 0.012415
[04:45:58.503] iteration 7240 : loss : 0.347743, loss_ce: 0.095474
[04:46:02.565] iteration 7250 : loss : 0.443944, loss_ce: 0.054935
[04:46:06.622] iteration 7260 : loss : 0.457249, loss_ce: 0.085959
[04:46:10.692] iteration 7270 : loss : 0.433424, loss_ce: 0.035836
[04:46:14.757] iteration 7280 : loss : 0.244164, loss_ce: 0.037980
[04:46:18.830] iteration 7290 : loss : 0.560629, loss_ce: 0.479388
[04:46:22.897] iteration 7300 : loss : 0.355760, loss_ce: 0.023442
[04:46:26.971] iteration 7310 : loss : 0.166061, loss_ce: 0.034016
[04:46:31.037] iteration 7320 : loss : 0.188048, loss_ce: 0.032719
[04:46:35.121] iteration 7330 : loss : 0.349325, loss_ce: 0.018218
[04:46:39.191] iteration 7340 : loss : 0.224909, loss_ce: 0.011075
[04:46:43.273] iteration 7350 : loss : 0.272060, loss_ce: 0.013362
[04:46:47.344] iteration 7360 : loss : 0.222679, loss_ce: 0.020595
[04:46:51.424] iteration 7370 : loss : 0.233848, loss_ce: 0.072447
[04:46:55.498] iteration 7380 : loss : 0.368718, loss_ce: 0.031884
[04:46:59.579] iteration 7390 : loss : 0.332079, loss_ce: 0.028608
[04:47:03.654] iteration 7400 : loss : 0.233184, loss_ce: 0.016829
[04:47:07.742] iteration 7410 : loss : 0.311353, loss_ce: 0.021523
[04:47:11.820] iteration 7420 : loss : 0.223424, loss_ce: 0.011448
[04:47:15.909] iteration 7430 : loss : 0.357083, loss_ce: 0.045745
[04:47:19.985] iteration 7440 : loss : 0.167333, loss_ce: 0.018252
[04:47:24.075] iteration 7450 : loss : 0.294909, loss_ce: 0.022004
[04:47:28.161] iteration 7460 : loss : 0.097885, loss_ce: 0.013168
[04:47:32.257] iteration 7470 : loss : 0.116396, loss_ce: 0.010251
[04:47:36.352] iteration 7480 : loss : 0.325523, loss_ce: 0.011778
[04:47:40.450] iteration 7490 : loss : 0.178306, loss_ce: 0.007948
[04:47:44.545] iteration 7500 : loss : 0.312854, loss_ce: 0.011486
[04:48:44.870] iteration 7510 : loss : 0.468718, loss_ce: 0.046065
[04:48:48.921] iteration 7520 : loss : 0.461343, loss_ce: 0.047119
[04:48:52.981] iteration 7530 : loss : 0.461640, loss_ce: 0.031806
[04:48:57.034] iteration 7540 : loss : 0.478250, loss_ce: 0.076440
[04:49:01.097] iteration 7550 : loss : 0.470024, loss_ce: 0.072596
[04:49:05.157] iteration 7560 : loss : 0.459273, loss_ce: 0.063010
[04:49:09.228] iteration 7570 : loss : 0.450773, loss_ce: 0.069686
[04:49:13.290] iteration 7580 : loss : 0.454172, loss_ce: 0.075272
[04:49:17.362] iteration 7590 : loss : 0.298280, loss_ce: 0.082279
[04:49:21.428] iteration 7600 : loss : 0.449962, loss_ce: 0.040191
[04:49:25.501] iteration 7610 : loss : 0.308749, loss_ce: 0.075053
[04:49:29.570] iteration 7620 : loss : 0.433843, loss_ce: 0.064157
[04:49:33.643] iteration 7630 : loss : 0.445056, loss_ce: 0.094761
[04:49:37.710] iteration 7640 : loss : 0.431674, loss_ce: 0.071437
[04:49:41.789] iteration 7650 : loss : 0.433294, loss_ce: 0.075585
[04:49:45.858] iteration 7660 : loss : 0.427391, loss_ce: 0.070210
[04:49:49.940] iteration 7670 : loss : 0.426271, loss_ce: 0.054103
[04:49:54.010] iteration 7680 : loss : 0.428803, loss_ce: 0.050779
[04:49:58.090] iteration 7690 : loss : 0.271506, loss_ce: 0.064913
[04:50:02.162] iteration 7700 : loss : 0.270071, loss_ce: 0.053783
[04:50:06.257] iteration 7710 : loss : 0.435052, loss_ce: 0.069123
[04:50:10.331] iteration 7720 : loss : 0.424323, loss_ce: 0.076362
[04:50:14.414] iteration 7730 : loss : 0.410334, loss_ce: 0.045426
[04:50:18.496] iteration 7740 : loss : 0.399768, loss_ce: 0.042842
[04:50:22.592] iteration 7750 : loss : 0.399847, loss_ce: 0.045713
[04:50:26.675] iteration 7760 : loss : 0.372402, loss_ce: 0.066813
[04:50:30.774] iteration 7770 : loss : 0.371330, loss_ce: 0.041768
[04:51:31.001] iteration 7780 : loss : 0.467432, loss_ce: 0.058173
[04:51:35.062] iteration 7790 : loss : 0.467328, loss_ce: 0.049150
[04:51:39.115] iteration 7800 : loss : 0.457090, loss_ce: 0.098807
[04:51:43.179] iteration 7810 : loss : 0.439502, loss_ce: 0.041679
[04:51:47.232] iteration 7820 : loss : 0.435006, loss_ce: 0.055571
[04:51:51.301] iteration 7830 : loss : 0.425393, loss_ce: 0.047898
[04:51:55.362] iteration 7840 : loss : 0.412582, loss_ce: 0.056199
[04:51:59.433] iteration 7850 : loss : 0.414837, loss_ce: 0.055716
[04:52:03.497] iteration 7860 : loss : 0.422648, loss_ce: 0.047429
[04:52:07.575] iteration 7870 : loss : 0.258310, loss_ce: 0.062428
[04:52:11.643] iteration 7880 : loss : 0.236345, loss_ce: 0.044112
[04:52:15.720] iteration 7890 : loss : 0.426271, loss_ce: 0.043849
[04:52:19.789] iteration 7900 : loss : 0.248479, loss_ce: 0.039360
[04:52:23.879] iteration 7910 : loss : 0.358998, loss_ce: 0.024371
[04:52:27.951] iteration 7920 : loss : 0.291750, loss_ce: 0.012650
[04:52:32.032] iteration 7930 : loss : 0.307922, loss_ce: 0.013972
[04:52:36.104] iteration 7940 : loss : 0.275465, loss_ce: 0.017118
[04:52:40.187] iteration 7950 : loss : 0.261868, loss_ce: 0.022965
[04:52:44.259] iteration 7960 : loss : 0.336889, loss_ce: 0.019033
[04:52:48.341] iteration 7970 : loss : 0.305881, loss_ce: 0.024006
[04:52:52.421] iteration 7980 : loss : 0.265300, loss_ce: 0.020835
[04:52:56.506] iteration 7990 : loss : 0.353909, loss_ce: 0.043856
[04:53:00.582] iteration 8000 : loss : 0.089614, loss_ce: 0.016234
[04:53:04.668] iteration 8010 : loss : 0.278981, loss_ce: 0.016510
[04:53:08.750] iteration 8020 : loss : 0.322517, loss_ce: 0.015258
[04:53:12.840] iteration 8030 : loss : 0.262686, loss_ce: 0.015301
[04:53:16.835] iteration 8040 : loss : 0.204552, loss_ce: 0.015204
[04:53:52.523] save model to ./finetune_tpgm_kits23_continual_50iter\finetuned_epoch_29.pth
[04:54:06.772] iteration 8050 : loss : 0.467631, loss_ce: 0.063934
[04:54:10.831] iteration 8060 : loss : 0.450001, loss_ce: 0.059017
[04:54:14.897] iteration 8070 : loss : 0.450545, loss_ce: 0.038239
[04:54:18.957] iteration 8080 : loss : 0.286227, loss_ce: 0.045501
[04:54:23.027] iteration 8090 : loss : 0.427554, loss_ce: 0.079605
[04:54:27.091] iteration 8100 : loss : 0.443037, loss_ce: 0.044290
[04:54:31.164] iteration 8110 : loss : 0.299407, loss_ce: 0.038694
[04:54:35.230] iteration 8120 : loss : 0.286042, loss_ce: 0.023178
[04:54:39.311] iteration 8130 : loss : 0.405055, loss_ce: 0.051792
[04:54:43.381] iteration 8140 : loss : 0.252243, loss_ce: 0.043661
[04:54:47.458] iteration 8150 : loss : 0.356262, loss_ce: 0.036222
[04:54:51.528] iteration 8160 : loss : 0.347355, loss_ce: 0.029568
[04:54:55.608] iteration 8170 : loss : 0.312012, loss_ce: 0.008560
[04:54:59.681] iteration 8180 : loss : 0.323575, loss_ce: 0.021942
[04:55:03.765] iteration 8190 : loss : 0.113829, loss_ce: 0.022160
[04:55:07.837] iteration 8200 : loss : 0.339384, loss_ce: 0.026785
[04:55:11.918] iteration 8210 : loss : 0.342378, loss_ce: 0.020405
[04:55:15.996] iteration 8220 : loss : 0.165978, loss_ce: 0.008562
[04:55:20.084] iteration 8230 : loss : 0.337587, loss_ce: 0.026615
[04:55:24.162] iteration 8240 : loss : 0.322747, loss_ce: 0.016207
[04:55:28.252] iteration 8250 : loss : 0.315616, loss_ce: 0.014607
[04:55:32.333] iteration 8260 : loss : 0.166416, loss_ce: 0.006106
[04:55:36.420] iteration 8270 : loss : 0.313476, loss_ce: 0.015961
[04:55:40.508] iteration 8280 : loss : 0.302595, loss_ce: 0.019437
[04:55:44.606] iteration 8290 : loss : 0.321133, loss_ce: 0.011914
[04:55:48.700] iteration 8300 : loss : 0.232464, loss_ce: 0.014142
[04:56:49.083] iteration 8310 : loss : 0.431030, loss_ce: 0.071825
[04:56:53.133] iteration 8320 : loss : 0.315097, loss_ce: 0.036164
[04:56:57.194] iteration 8330 : loss : 0.455215, loss_ce: 0.049201
[04:57:01.245] iteration 8340 : loss : 0.449904, loss_ce: 0.058494
[04:57:05.311] iteration 8350 : loss : 0.436915, loss_ce: 0.058458
[04:57:09.372] iteration 8360 : loss : 0.429880, loss_ce: 0.053238
[04:57:13.443] iteration 8370 : loss : 0.418028, loss_ce: 0.049024
[04:57:17.507] iteration 8380 : loss : 0.429064, loss_ce: 0.051154
[04:57:21.582] iteration 8390 : loss : 0.410340, loss_ce: 0.040615
[04:57:25.650] iteration 8400 : loss : 0.408511, loss_ce: 0.040176
[04:57:29.728] iteration 8410 : loss : 0.332512, loss_ce: 0.018389
[04:57:33.796] iteration 8420 : loss : 0.331565, loss_ce: 0.028449
[04:57:37.875] iteration 8430 : loss : 0.379185, loss_ce: 0.025448
[04:57:41.948] iteration 8440 : loss : 0.293347, loss_ce: 0.024441
[04:57:46.031] iteration 8450 : loss : 0.248655, loss_ce: 0.025703
[04:57:50.102] iteration 8460 : loss : 0.286833, loss_ce: 0.020858
[04:57:54.185] iteration 8470 : loss : 0.344729, loss_ce: 0.043826
[04:57:58.258] iteration 8480 : loss : 0.225548, loss_ce: 0.019313
[04:58:02.343] iteration 8490 : loss : 0.219611, loss_ce: 0.020086
[04:58:06.421] iteration 8500 : loss : 0.248277, loss_ce: 0.032439
[04:58:10.508] iteration 8510 : loss : 0.181322, loss_ce: 0.013209
[04:58:14.584] iteration 8520 : loss : 0.243802, loss_ce: 0.017523
[04:58:18.678] iteration 8530 : loss : 0.241715, loss_ce: 0.011673
[04:58:22.759] iteration 8540 : loss : 0.276308, loss_ce: 0.027543
[04:58:26.850] iteration 8550 : loss : 0.313171, loss_ce: 0.025562
[04:58:30.928] iteration 8560 : loss : 0.278561, loss_ce: 0.015947
[04:58:35.020] iteration 8570 : loss : 0.310449, loss_ce: 0.021981
[04:59:35.441] iteration 8580 : loss : 0.473442, loss_ce: 0.057713
[04:59:39.502] iteration 8590 : loss : 0.456258, loss_ce: 0.050838
[04:59:43.557] iteration 8600 : loss : 0.444306, loss_ce: 0.068569
[04:59:47.621] iteration 8610 : loss : 0.291234, loss_ce: 0.068145
[04:59:51.680] iteration 8620 : loss : 0.429702, loss_ce: 0.065968
[04:59:55.748] iteration 8630 : loss : 0.416660, loss_ce: 0.050951
[04:59:59.812] iteration 8640 : loss : 0.390082, loss_ce: 0.045244
[05:00:03.886] iteration 8650 : loss : 0.392749, loss_ce: 0.052412
[05:00:07.953] iteration 8660 : loss : 0.342217, loss_ce: 0.041465
[05:00:12.031] iteration 8670 : loss : 0.324267, loss_ce: 0.039716
[05:00:16.097] iteration 8680 : loss : 0.205699, loss_ce: 0.012740
[05:00:20.173] iteration 8690 : loss : 0.287680, loss_ce: 0.017577
[05:00:24.241] iteration 8700 : loss : 0.336869, loss_ce: 0.070626
[05:00:28.322] iteration 8710 : loss : 0.311264, loss_ce: 0.025251
[05:00:32.393] iteration 8720 : loss : 0.347674, loss_ce: 0.027024
[05:00:36.474] iteration 8730 : loss : 0.200774, loss_ce: 0.012718
[05:00:40.547] iteration 8740 : loss : 0.301390, loss_ce: 0.024454
[05:00:44.628] iteration 8750 : loss : 0.340342, loss_ce: 0.011335
[05:00:48.702] iteration 8760 : loss : 0.249000, loss_ce: 0.013006
[05:00:52.786] iteration 8770 : loss : 0.213845, loss_ce: 0.014817
[05:00:56.861] iteration 8780 : loss : 0.263327, loss_ce: 0.015770
[05:01:00.963] iteration 8790 : loss : 0.195858, loss_ce: 0.007177
[05:01:05.043] iteration 8800 : loss : 0.316279, loss_ce: 0.019923
[05:01:09.133] iteration 8810 : loss : 0.327856, loss_ce: 0.060104
[05:01:13.216] iteration 8820 : loss : 0.143351, loss_ce: 0.007257
[05:01:17.315] iteration 8830 : loss : 0.113833, loss_ce: 0.015780
[05:01:21.396] iteration 8840 : loss : 0.323023, loss_ce: 0.010263
[05:02:11.009] iteration 8850 : loss : 0.436503, loss_ce: 0.065396
[05:02:15.063] iteration 8860 : loss : 0.385215, loss_ce: 0.083772
[05:02:19.127] iteration 8870 : loss : 0.360595, loss_ce: 0.029284
[05:02:23.188] iteration 8880 : loss : 0.372457, loss_ce: 0.037673
[05:02:27.263] iteration 8890 : loss : 0.336082, loss_ce: 0.013767
[05:02:31.326] iteration 8900 : loss : 0.345646, loss_ce: 0.050607
[05:02:35.401] iteration 8910 : loss : 0.340611, loss_ce: 0.018539
[05:02:39.467] iteration 8920 : loss : 0.350734, loss_ce: 0.038258
[05:02:43.547] iteration 8930 : loss : 0.241637, loss_ce: 0.032719
[05:02:47.616] iteration 8940 : loss : 0.327060, loss_ce: 0.019002
[05:02:51.698] iteration 8950 : loss : 0.268534, loss_ce: 0.033707
[05:02:55.768] iteration 8960 : loss : 0.282790, loss_ce: 0.015759
[05:02:59.851] iteration 8970 : loss : 0.324736, loss_ce: 0.023271
[05:03:03.922] iteration 8980 : loss : 0.249041, loss_ce: 0.015525
[05:03:08.004] iteration 8990 : loss : 0.282977, loss_ce: 0.012460
[05:03:12.087] iteration 9000 : loss : 0.374433, loss_ce: 0.023138
[05:03:16.176] iteration 9010 : loss : 0.310040, loss_ce: 0.010799
[05:03:20.251] iteration 9020 : loss : 0.337412, loss_ce: 0.020616
[05:03:24.335] iteration 9030 : loss : 0.278423, loss_ce: 0.007802
[05:03:28.414] iteration 9040 : loss : 0.282630, loss_ce: 0.023063
[05:03:32.510] iteration 9050 : loss : 0.201745, loss_ce: 0.009984
[05:03:36.597] iteration 9060 : loss : 0.225331, loss_ce: 0.009916
[05:03:40.691] iteration 9070 : loss : 0.180971, loss_ce: 0.010030
[05:03:44.778] iteration 9080 : loss : 0.184626, loss_ce: 0.013188
[05:03:48.876] iteration 9090 : loss : 0.196339, loss_ce: 0.010218
[05:03:52.968] iteration 9100 : loss : 0.195809, loss_ce: 0.005227
[05:03:57.065] iteration 9110 : loss : 0.265159, loss_ce: 0.011874
[05:04:57.573] iteration 9120 : loss : 0.323321, loss_ce: 0.055426
[05:05:01.638] iteration 9130 : loss : 0.475497, loss_ce: 0.066297
[05:05:05.689] iteration 9140 : loss : 0.458632, loss_ce: 0.053704
[05:05:09.755] iteration 9150 : loss : 0.304769, loss_ce: 0.059768
[05:05:13.808] iteration 9160 : loss : 0.445177, loss_ce: 0.052013
[05:05:17.874] iteration 9170 : loss : 0.279547, loss_ce: 0.058836
[05:05:21.932] iteration 9180 : loss : 0.451408, loss_ce: 0.063473
[05:05:25.998] iteration 9190 : loss : 0.442957, loss_ce: 0.049656
[05:05:30.061] iteration 9200 : loss : 0.432538, loss_ce: 0.032983
[05:05:34.134] iteration 9210 : loss : 0.431313, loss_ce: 0.057022
[05:05:38.197] iteration 9220 : loss : 0.411399, loss_ce: 0.064128
[05:05:42.273] iteration 9230 : loss : 0.255474, loss_ce: 0.043293
[05:05:46.342] iteration 9240 : loss : 0.404244, loss_ce: 0.048018
[05:05:50.421] iteration 9250 : loss : 0.407046, loss_ce: 0.033447
[05:05:54.492] iteration 9260 : loss : 0.389573, loss_ce: 0.041159
[05:05:58.569] iteration 9270 : loss : 0.378599, loss_ce: 0.014582
[05:06:02.641] iteration 9280 : loss : 0.406827, loss_ce: 0.086210
[05:06:06.722] iteration 9290 : loss : 0.365051, loss_ce: 0.026713
[05:06:10.799] iteration 9300 : loss : 0.247946, loss_ce: 0.028164
[05:06:14.883] iteration 9310 : loss : 0.351369, loss_ce: 0.068391
[05:06:18.960] iteration 9320 : loss : 0.170854, loss_ce: 0.042106
[05:06:23.047] iteration 9330 : loss : 0.278310, loss_ce: 0.057273
[05:06:27.128] iteration 9340 : loss : 0.315736, loss_ce: 0.020852
[05:06:31.217] iteration 9350 : loss : 0.402751, loss_ce: 0.055633
[05:06:35.297] iteration 9360 : loss : 0.289566, loss_ce: 0.020689
[05:06:39.386] iteration 9370 : loss : 0.298202, loss_ce: 0.029760
[05:06:43.368] iteration 9380 : loss : 0.346988, loss_ce: 0.019360
[05:07:44.117] iteration 9390 : loss : 0.337572, loss_ce: 0.090238
[05:07:48.170] iteration 9400 : loss : 0.293350, loss_ce: 0.056824
[05:07:52.236] iteration 9410 : loss : 0.355243, loss_ce: 0.023973
[05:07:56.291] iteration 9420 : loss : 0.364712, loss_ce: 0.043890
[05:08:00.363] iteration 9430 : loss : 0.330644, loss_ce: 0.019136
[05:08:04.424] iteration 9440 : loss : 0.185855, loss_ce: 0.035663
[05:08:08.499] iteration 9450 : loss : 0.329813, loss_ce: 0.020206
[05:08:12.564] iteration 9460 : loss : 0.335455, loss_ce: 0.045329
[05:08:16.640] iteration 9470 : loss : 0.333887, loss_ce: 0.011273
[05:08:20.709] iteration 9480 : loss : 0.339021, loss_ce: 0.044246
[05:08:24.785] iteration 9490 : loss : 0.336950, loss_ce: 0.029051
[05:08:28.855] iteration 9500 : loss : 0.209054, loss_ce: 0.017734
[05:08:32.934] iteration 9510 : loss : 0.198023, loss_ce: 0.021104
[05:08:37.003] iteration 9520 : loss : 0.168716, loss_ce: 0.028217
[05:08:41.082] iteration 9530 : loss : 0.250203, loss_ce: 0.011082
[05:08:45.152] iteration 9540 : loss : 0.328527, loss_ce: 0.034957
[05:08:49.235] iteration 9550 : loss : 0.242346, loss_ce: 0.015735
[05:08:53.310] iteration 9560 : loss : 0.208659, loss_ce: 0.007893
[05:08:57.392] iteration 9570 : loss : 0.108685, loss_ce: 0.014532
[05:09:01.470] iteration 9580 : loss : 0.304731, loss_ce: 0.010215
[05:09:05.557] iteration 9590 : loss : 0.329694, loss_ce: 0.037321
[05:09:09.633] iteration 9600 : loss : 0.322782, loss_ce: 0.007951
[05:09:13.721] iteration 9610 : loss : 0.308460, loss_ce: 0.005377
[05:09:17.802] iteration 9620 : loss : 0.222586, loss_ce: 0.047828
[05:09:21.894] iteration 9630 : loss : 0.312940, loss_ce: 0.012600
[05:09:25.974] iteration 9640 : loss : 0.176582, loss_ce: 0.003958
[05:10:15.640] iteration 9650 : loss : 0.446280, loss_ce: 0.066480
[05:10:19.697] iteration 9660 : loss : 0.354387, loss_ce: 0.040728
[05:10:23.767] iteration 9670 : loss : 0.361241, loss_ce: 0.047763
[05:10:27.828] iteration 9680 : loss : 0.224889, loss_ce: 0.022022
[05:10:31.903] iteration 9690 : loss : 0.340180, loss_ce: 0.016904
[05:10:35.968] iteration 9700 : loss : 0.339605, loss_ce: 0.029120
[05:10:40.043] iteration 9710 : loss : 0.315222, loss_ce: 0.011327
[05:10:44.114] iteration 9720 : loss : 0.264625, loss_ce: 0.046447
[05:10:48.190] iteration 9730 : loss : 0.333854, loss_ce: 0.024701
[05:10:52.259] iteration 9740 : loss : 0.330203, loss_ce: 0.019268
[05:10:56.333] iteration 9750 : loss : 0.341078, loss_ce: 0.037312
[05:11:00.406] iteration 9760 : loss : 0.186193, loss_ce: 0.028354
[05:11:04.486] iteration 9770 : loss : 0.320659, loss_ce: 0.009846
[05:11:08.558] iteration 9780 : loss : 0.127266, loss_ce: 0.011216
[05:11:12.642] iteration 9790 : loss : 0.238491, loss_ce: 0.013424
[05:11:16.715] iteration 9800 : loss : 0.272720, loss_ce: 0.030835
[05:11:20.802] iteration 9810 : loss : 0.290628, loss_ce: 0.010462
[05:11:24.875] iteration 9820 : loss : 0.179057, loss_ce: 0.005877
[05:11:28.958] iteration 9830 : loss : 0.310513, loss_ce: 0.056134
[05:11:33.039] iteration 9840 : loss : 0.231254, loss_ce: 0.017550
[05:11:37.138] iteration 9850 : loss : 0.370220, loss_ce: 0.017059
[05:11:41.218] iteration 9860 : loss : 0.294071, loss_ce: 0.021205
[05:11:45.308] iteration 9870 : loss : 0.238951, loss_ce: 0.016899
[05:11:49.386] iteration 9880 : loss : 0.049342, loss_ce: 0.013693
[05:11:53.474] iteration 9890 : loss : 0.151927, loss_ce: 0.011836
[05:11:57.566] iteration 9900 : loss : 0.286677, loss_ce: 0.014043
[05:12:01.665] iteration 9910 : loss : 0.304721, loss_ce: 0.020379
[05:13:02.001] iteration 9920 : loss : 0.399223, loss_ce: 0.058651
[05:13:06.063] iteration 9930 : loss : 0.359582, loss_ce: 0.041222
[05:13:10.119] iteration 9940 : loss : 0.363126, loss_ce: 0.086532
[05:13:14.189] iteration 9950 : loss : 0.336606, loss_ce: 0.029685
[05:13:18.249] iteration 9960 : loss : 0.326842, loss_ce: 0.014105
[05:13:22.319] iteration 9970 : loss : 0.340445, loss_ce: 0.032609
[05:13:26.381] iteration 9980 : loss : 0.200116, loss_ce: 0.055979
[05:13:30.453] iteration 9990 : loss : 0.319923, loss_ce: 0.013827
[05:13:34.518] iteration 10000 : loss : 0.189176, loss_ce: 0.020380
[05:13:38.597] iteration 10010 : loss : 0.322899, loss_ce: 0.032156
[05:13:42.666] iteration 10020 : loss : 0.327118, loss_ce: 0.034304
[05:13:46.745] iteration 10030 : loss : 0.191705, loss_ce: 0.029738
[05:13:50.814] iteration 10040 : loss : 0.295347, loss_ce: 0.015705
[05:13:54.894] iteration 10050 : loss : 0.254006, loss_ce: 0.017910
[05:13:58.964] iteration 10060 : loss : 0.257091, loss_ce: 0.016356
[05:14:03.045] iteration 10070 : loss : 0.287513, loss_ce: 0.026886
[05:14:07.118] iteration 10080 : loss : 0.089518, loss_ce: 0.025189
[05:14:11.203] iteration 10090 : loss : 0.164837, loss_ce: 0.029397
[05:14:15.277] iteration 10100 : loss : 0.303950, loss_ce: 0.039461
[05:14:19.358] iteration 10110 : loss : 0.205792, loss_ce: 0.010070
[05:14:23.438] iteration 10120 : loss : 0.273770, loss_ce: 0.020522
[05:14:27.526] iteration 10130 : loss : 0.320143, loss_ce: 0.017326
[05:14:31.604] iteration 10140 : loss : 0.229894, loss_ce: 0.010503
[05:14:35.694] iteration 10150 : loss : 0.122025, loss_ce: 0.006433
[05:14:39.775] iteration 10160 : loss : 0.281420, loss_ce: 0.042222
[05:14:43.869] iteration 10170 : loss : 0.214325, loss_ce: 0.033631
[05:14:47.960] iteration 10180 : loss : 0.232125, loss_ce: 0.007404
[05:15:48.365] iteration 10190 : loss : 0.366744, loss_ce: 0.029731
[05:15:52.418] iteration 10200 : loss : 0.350341, loss_ce: 0.015076
[05:15:56.483] iteration 10210 : loss : 0.334517, loss_ce: 0.023120
[05:16:00.538] iteration 10220 : loss : 0.330901, loss_ce: 0.019038
[05:16:04.606] iteration 10230 : loss : 0.335081, loss_ce: 0.041116
[05:16:08.665] iteration 10240 : loss : 0.343485, loss_ce: 0.034787
[05:16:12.737] iteration 10250 : loss : 0.336351, loss_ce: 0.011505
[05:16:16.798] iteration 10260 : loss : 0.289671, loss_ce: 0.014207
[05:16:20.872] iteration 10270 : loss : 0.327325, loss_ce: 0.027943
[05:16:24.936] iteration 10280 : loss : 0.318889, loss_ce: 0.017465
[05:16:29.010] iteration 10290 : loss : 0.184900, loss_ce: 0.025259
[05:16:33.079] iteration 10300 : loss : 0.319576, loss_ce: 0.033792
[05:16:37.157] iteration 10310 : loss : 0.184456, loss_ce: 0.028420
[05:16:41.228] iteration 10320 : loss : 0.081540, loss_ce: 0.012248
[05:16:45.309] iteration 10330 : loss : 0.196088, loss_ce: 0.038800
[05:16:49.382] iteration 10340 : loss : 0.325512, loss_ce: 0.009743
[05:16:53.465] iteration 10350 : loss : 0.305375, loss_ce: 0.070296
[05:16:57.537] iteration 10360 : loss : 0.243149, loss_ce: 0.011382
[05:17:01.619] iteration 10370 : loss : 0.131724, loss_ce: 0.012388
[05:17:05.698] iteration 10380 : loss : 0.050880, loss_ce: 0.009867
[05:17:09.783] iteration 10390 : loss : 0.316924, loss_ce: 0.013746
[05:17:13.858] iteration 10400 : loss : 0.296068, loss_ce: 0.017782
[05:17:17.943] iteration 10410 : loss : 0.206210, loss_ce: 0.011503
[05:17:22.024] iteration 10420 : loss : 0.319955, loss_ce: 0.007418
[05:17:26.120] iteration 10430 : loss : 0.221467, loss_ce: 0.016803
[05:17:30.204] iteration 10440 : loss : 0.184494, loss_ce: 0.004668
[05:17:34.292] iteration 10450 : loss : 0.188576, loss_ce: 0.006933
[05:18:23.726] iteration 10460 : loss : 0.383084, loss_ce: 0.073488
[05:18:27.792] iteration 10470 : loss : 0.339732, loss_ce: 0.024696
[05:18:31.851] iteration 10480 : loss : 0.166075, loss_ce: 0.005499
[05:18:35.919] iteration 10490 : loss : 0.176432, loss_ce: 0.015300
[05:18:39.984] iteration 10500 : loss : 0.338665, loss_ce: 0.030138
[05:18:44.059] iteration 10510 : loss : 0.191846, loss_ce: 0.034948
[05:18:48.124] iteration 10520 : loss : 0.317563, loss_ce: 0.009641
[05:18:52.202] iteration 10530 : loss : 0.325234, loss_ce: 0.010327
[05:18:56.270] iteration 10540 : loss : 0.328982, loss_ce: 0.019837
[05:19:00.347] iteration 10550 : loss : 0.333974, loss_ce: 0.021623
[05:19:04.418] iteration 10560 : loss : 0.351337, loss_ce: 0.048984
[05:19:08.500] iteration 10570 : loss : 0.172108, loss_ce: 0.015164
[05:19:12.573] iteration 10580 : loss : 0.337531, loss_ce: 0.006810
[05:19:16.657] iteration 10590 : loss : 0.105580, loss_ce: 0.014164
[05:19:20.730] iteration 10600 : loss : 0.242204, loss_ce: 0.012919
[05:19:24.814] iteration 10610 : loss : 0.294002, loss_ce: 0.037073
[05:19:28.886] iteration 10620 : loss : 0.188943, loss_ce: 0.010047
[05:19:32.974] iteration 10630 : loss : 0.234845, loss_ce: 0.019953
[05:19:37.055] iteration 10640 : loss : 0.348410, loss_ce: 0.057331
[05:19:41.147] iteration 10650 : loss : 0.226148, loss_ce: 0.010243
[05:19:45.226] iteration 10660 : loss : 0.254246, loss_ce: 0.019334
[05:19:49.316] iteration 10670 : loss : 0.234038, loss_ce: 0.019853
[05:19:53.397] iteration 10680 : loss : 0.312645, loss_ce: 0.002202
[05:19:57.489] iteration 10690 : loss : 0.316694, loss_ce: 0.016599
[05:20:01.577] iteration 10700 : loss : 0.215189, loss_ce: 0.015272
[05:20:05.674] iteration 10710 : loss : 0.275564, loss_ce: 0.004115
[05:20:09.671] iteration 10720 : loss : 0.067857, loss_ce: 0.010346
[05:20:56.019] save model to ./finetune_tpgm_kits23_continual_50iter\finetuned_epoch_39.pth
[05:21:10.279] iteration 10730 : loss : 0.436996, loss_ce: 0.139635
[05:21:14.333] iteration 10740 : loss : 0.345081, loss_ce: 0.039922
[05:21:18.400] iteration 10750 : loss : 0.341213, loss_ce: 0.023382
[05:21:22.462] iteration 10760 : loss : 0.361037, loss_ce: 0.034769
[05:21:26.533] iteration 10770 : loss : 0.318851, loss_ce: 0.011037
[05:21:30.595] iteration 10780 : loss : 0.334564, loss_ce: 0.028719
[05:21:34.667] iteration 10790 : loss : 0.317068, loss_ce: 0.007145
[05:21:38.731] iteration 10800 : loss : 0.325320, loss_ce: 0.017985
[05:21:42.810] iteration 10810 : loss : 0.339014, loss_ce: 0.018904
[05:21:46.880] iteration 10820 : loss : 0.323295, loss_ce: 0.028808
[05:21:50.959] iteration 10830 : loss : 0.169717, loss_ce: 0.018691
[05:21:55.028] iteration 10840 : loss : 0.299988, loss_ce: 0.023602
[05:21:59.107] iteration 10850 : loss : 0.161164, loss_ce: 0.006773
[05:22:03.175] iteration 10860 : loss : 0.291498, loss_ce: 0.025669
[05:22:07.257] iteration 10870 : loss : 0.156167, loss_ce: 0.014594
[05:22:11.329] iteration 10880 : loss : 0.090292, loss_ce: 0.004343
[05:22:15.412] iteration 10890 : loss : 0.234749, loss_ce: 0.009248
[05:22:19.482] iteration 10900 : loss : 0.221361, loss_ce: 0.012736
[05:22:23.568] iteration 10910 : loss : 0.338570, loss_ce: 0.009556
[05:22:27.643] iteration 10920 : loss : 0.331132, loss_ce: 0.033011
[05:22:31.730] iteration 10930 : loss : 0.094674, loss_ce: 0.005983
[05:22:35.810] iteration 10940 : loss : 0.082999, loss_ce: 0.016808
[05:22:39.898] iteration 10950 : loss : 0.172099, loss_ce: 0.009808
[05:22:43.981] iteration 10960 : loss : 0.257749, loss_ce: 0.008806
[05:22:48.076] iteration 10970 : loss : 0.312282, loss_ce: 0.008601
[05:22:52.166] iteration 10980 : loss : 0.125913, loss_ce: 0.018478
[05:23:52.387] iteration 10990 : loss : 0.515655, loss_ce: 0.176424
[05:23:56.438] iteration 11000 : loss : 0.375507, loss_ce: 0.080216
[05:24:00.499] iteration 11010 : loss : 0.315210, loss_ce: 0.007487
[05:24:04.556] iteration 11020 : loss : 0.362357, loss_ce: 0.092551
[05:24:08.623] iteration 11030 : loss : 0.194746, loss_ce: 0.012555
[05:24:12.683] iteration 11040 : loss : 0.325593, loss_ce: 0.013306
[05:24:16.753] iteration 11050 : loss : 0.182180, loss_ce: 0.020853
[05:24:20.814] iteration 11060 : loss : 0.344952, loss_ce: 0.051779
[05:24:24.884] iteration 11070 : loss : 0.315927, loss_ce: 0.004964
[05:24:28.947] iteration 11080 : loss : 0.180837, loss_ce: 0.022731
[05:24:33.019] iteration 11090 : loss : 0.336277, loss_ce: 0.023868
[05:24:37.086] iteration 11100 : loss : 0.341747, loss_ce: 0.069810
[05:24:41.163] iteration 11110 : loss : 0.320986, loss_ce: 0.011291
[05:24:45.232] iteration 11120 : loss : 0.322433, loss_ce: 0.024508
[05:24:49.314] iteration 11130 : loss : 0.319733, loss_ce: 0.008425
[05:24:53.385] iteration 11140 : loss : 0.228870, loss_ce: 0.028696
[05:24:57.469] iteration 11150 : loss : 0.345870, loss_ce: 0.026509
[05:25:01.541] iteration 11160 : loss : 0.308726, loss_ce: 0.015999
[05:25:05.624] iteration 11170 : loss : 0.239406, loss_ce: 0.015091
[05:25:09.699] iteration 11180 : loss : 0.276876, loss_ce: 0.022807
[05:25:13.780] iteration 11190 : loss : 0.209361, loss_ce: 0.047205
[05:25:17.855] iteration 11200 : loss : 0.234270, loss_ce: 0.013104
[05:25:21.944] iteration 11210 : loss : 0.317602, loss_ce: 0.014733
[05:25:26.023] iteration 11220 : loss : 0.210259, loss_ce: 0.011492
[05:25:30.108] iteration 11230 : loss : 0.210663, loss_ce: 0.018379
[05:25:34.187] iteration 11240 : loss : 0.204608, loss_ce: 0.011011
[05:25:38.278] iteration 11250 : loss : 0.072607, loss_ce: 0.009634
[05:26:27.772] iteration 11260 : loss : 0.383497, loss_ce: 0.034673
[05:26:31.837] iteration 11270 : loss : 0.357909, loss_ce: 0.024323
[05:26:35.894] iteration 11280 : loss : 0.213265, loss_ce: 0.055089
[05:26:39.965] iteration 11290 : loss : 0.349082, loss_ce: 0.047080
[05:26:44.029] iteration 11300 : loss : 0.353916, loss_ce: 0.047237
[05:26:48.109] iteration 11310 : loss : 0.337768, loss_ce: 0.025035
[05:26:52.178] iteration 11320 : loss : 0.335618, loss_ce: 0.049223
[05:26:56.260] iteration 11330 : loss : 0.195948, loss_ce: 0.075848
[05:27:00.330] iteration 11340 : loss : 0.343660, loss_ce: 0.034237
[05:27:04.410] iteration 11350 : loss : 0.337514, loss_ce: 0.033367
[05:27:08.479] iteration 11360 : loss : 0.334209, loss_ce: 0.017444
[05:27:12.562] iteration 11370 : loss : 0.346092, loss_ce: 0.048114
[05:27:16.632] iteration 11380 : loss : 0.341750, loss_ce: 0.019988
[05:27:20.718] iteration 11390 : loss : 0.315570, loss_ce: 0.011908
[05:27:24.791] iteration 11400 : loss : 0.313845, loss_ce: 0.009638
[05:27:28.875] iteration 11410 : loss : 0.261591, loss_ce: 0.016735
[05:27:32.952] iteration 11420 : loss : 0.219487, loss_ce: 0.008945
[05:27:37.043] iteration 11430 : loss : 0.237761, loss_ce: 0.022681
[05:27:41.120] iteration 11440 : loss : 0.212877, loss_ce: 0.015783
[05:27:45.209] iteration 11450 : loss : 0.326913, loss_ce: 0.020280
[05:27:49.286] iteration 11460 : loss : 0.265442, loss_ce: 0.037212
[05:27:53.379] iteration 11470 : loss : 0.139300, loss_ce: 0.017649
[05:27:57.462] iteration 11480 : loss : 0.324475, loss_ce: 0.015638
[05:28:01.563] iteration 11490 : loss : 0.322667, loss_ce: 0.017510
[05:28:05.657] iteration 11500 : loss : 0.225069, loss_ce: 0.016018
[05:28:09.766] iteration 11510 : loss : 0.321371, loss_ce: 0.011568
[05:28:13.865] iteration 11520 : loss : 0.218083, loss_ce: 0.011356
[05:29:14.307] iteration 11530 : loss : 0.396610, loss_ce: 0.051709
[05:29:18.361] iteration 11540 : loss : 0.360211, loss_ce: 0.060528
[05:29:22.423] iteration 11550 : loss : 0.347647, loss_ce: 0.032032
[05:29:26.479] iteration 11560 : loss : 0.395277, loss_ce: 0.050102
[05:29:30.550] iteration 11570 : loss : 0.186381, loss_ce: 0.036185
[05:29:34.614] iteration 11580 : loss : 0.339693, loss_ce: 0.032859
[05:29:38.687] iteration 11590 : loss : 0.216623, loss_ce: 0.088068
[05:29:42.751] iteration 11600 : loss : 0.332881, loss_ce: 0.030331
[05:29:46.827] iteration 11610 : loss : 0.173285, loss_ce: 0.014867
[05:29:50.893] iteration 11620 : loss : 0.331901, loss_ce: 0.016323
[05:29:54.970] iteration 11630 : loss : 0.168541, loss_ce: 0.012024
[05:29:59.040] iteration 11640 : loss : 0.333039, loss_ce: 0.041402
[05:30:03.120] iteration 11650 : loss : 0.327876, loss_ce: 0.022494
[05:30:07.189] iteration 11660 : loss : 0.173573, loss_ce: 0.006753
[05:30:11.272] iteration 11670 : loss : 0.324521, loss_ce: 0.013100
[05:30:15.344] iteration 11680 : loss : 0.330535, loss_ce: 0.021300
[05:30:19.428] iteration 11690 : loss : 0.320238, loss_ce: 0.012369
[05:30:23.501] iteration 11700 : loss : 0.318782, loss_ce: 0.021015
[05:30:27.583] iteration 11710 : loss : 0.318007, loss_ce: 0.014218
[05:30:31.657] iteration 11720 : loss : 0.166791, loss_ce: 0.017329
[05:30:35.740] iteration 11730 : loss : 0.331478, loss_ce: 0.042879
[05:30:39.817] iteration 11740 : loss : 0.300978, loss_ce: 0.031226
[05:30:43.904] iteration 11750 : loss : 0.326409, loss_ce: 0.015569
[05:30:47.984] iteration 11760 : loss : 0.253362, loss_ce: 0.014943
[05:30:52.073] iteration 11770 : loss : 0.274277, loss_ce: 0.029714
[05:30:56.151] iteration 11780 : loss : 0.316869, loss_ce: 0.016229
[05:31:00.243] iteration 11790 : loss : 0.305688, loss_ce: 0.016643
[05:32:00.469] iteration 11800 : loss : 0.381057, loss_ce: 0.045563
[05:32:04.531] iteration 11810 : loss : 0.384830, loss_ce: 0.114914
[05:32:08.585] iteration 11820 : loss : 0.344764, loss_ce: 0.034626
[05:32:12.653] iteration 11830 : loss : 0.332945, loss_ce: 0.031647
[05:32:16.713] iteration 11840 : loss : 0.324951, loss_ce: 0.018794
[05:32:20.787] iteration 11850 : loss : 0.333480, loss_ce: 0.025566
[05:32:24.851] iteration 11860 : loss : 0.209200, loss_ce: 0.043785
[05:32:28.928] iteration 11870 : loss : 0.332522, loss_ce: 0.031444
[05:32:32.993] iteration 11880 : loss : 0.340678, loss_ce: 0.042589
[05:32:37.071] iteration 11890 : loss : 0.190061, loss_ce: 0.038754
[05:32:41.137] iteration 11900 : loss : 0.317908, loss_ce: 0.011370
[05:32:45.213] iteration 11910 : loss : 0.316712, loss_ce: 0.008555
[05:32:49.284] iteration 11920 : loss : 0.336532, loss_ce: 0.047271
[05:32:53.365] iteration 11930 : loss : 0.172083, loss_ce: 0.025137
[05:32:57.437] iteration 11940 : loss : 0.333173, loss_ce: 0.053637
[05:33:01.517] iteration 11950 : loss : 0.345045, loss_ce: 0.051790
[05:33:05.590] iteration 11960 : loss : 0.180491, loss_ce: 0.010322
[05:33:09.677] iteration 11970 : loss : 0.313300, loss_ce: 0.004558
[05:33:13.751] iteration 11980 : loss : 0.330051, loss_ce: 0.031060
[05:33:17.843] iteration 11990 : loss : 0.215979, loss_ce: 0.095155
[05:33:21.918] iteration 12000 : loss : 0.322860, loss_ce: 0.017877
[05:33:26.004] iteration 12010 : loss : 0.337694, loss_ce: 0.045375
[05:33:30.081] iteration 12020 : loss : 0.330178, loss_ce: 0.015275
[05:33:34.173] iteration 12030 : loss : 0.181975, loss_ce: 0.022146
[05:33:38.260] iteration 12040 : loss : 0.367234, loss_ce: 0.102790
[05:33:42.351] iteration 12050 : loss : 0.177117, loss_ce: 0.047853
[05:33:46.336] iteration 12060 : loss : 0.324097, loss_ce: 0.021433
[05:34:36.046] iteration 12070 : loss : 0.362585, loss_ce: 0.040089
[05:34:40.110] iteration 12080 : loss : 0.338035, loss_ce: 0.055471
[05:34:44.178] iteration 12090 : loss : 0.232726, loss_ce: 0.060161
[05:34:48.238] iteration 12100 : loss : 0.366920, loss_ce: 0.073982
[05:34:52.308] iteration 12110 : loss : 0.183773, loss_ce: 0.013480
[05:34:56.371] iteration 12120 : loss : 0.354569, loss_ce: 0.023915
[05:35:00.447] iteration 12130 : loss : 0.181589, loss_ce: 0.009424
[05:35:04.513] iteration 12140 : loss : 0.321231, loss_ce: 0.012900
[05:35:08.592] iteration 12150 : loss : 0.324341, loss_ce: 0.021236
[05:35:12.664] iteration 12160 : loss : 0.325963, loss_ce: 0.019532
[05:35:16.744] iteration 12170 : loss : 0.355305, loss_ce: 0.079136
[05:35:20.815] iteration 12180 : loss : 0.169536, loss_ce: 0.008246
[05:35:24.898] iteration 12190 : loss : 0.333489, loss_ce: 0.044891
[05:35:28.972] iteration 12200 : loss : 0.325514, loss_ce: 0.034576
[05:35:33.054] iteration 12210 : loss : 0.329149, loss_ce: 0.035535
[05:35:37.128] iteration 12220 : loss : 0.325471, loss_ce: 0.032442
[05:35:41.212] iteration 12230 : loss : 0.315122, loss_ce: 0.005248
[05:35:45.286] iteration 12240 : loss : 0.325590, loss_ce: 0.011410
[05:35:49.371] iteration 12250 : loss : 0.354249, loss_ce: 0.032751
[05:35:53.447] iteration 12260 : loss : 0.314759, loss_ce: 0.008725
[05:35:57.540] iteration 12270 : loss : 0.321755, loss_ce: 0.017553
[05:36:01.630] iteration 12280 : loss : 0.324900, loss_ce: 0.016518
[05:36:05.729] iteration 12290 : loss : 0.314050, loss_ce: 0.003959
[05:36:09.816] iteration 12300 : loss : 0.190550, loss_ce: 0.026134
[05:36:13.916] iteration 12310 : loss : 0.334182, loss_ce: 0.045404
[05:36:18.007] iteration 12320 : loss : 0.325966, loss_ce: 0.021828
[05:37:18.335] iteration 12330 : loss : 0.462856, loss_ce: 0.061825
[05:37:22.389] iteration 12340 : loss : 0.337860, loss_ce: 0.014161
[05:37:26.453] iteration 12350 : loss : 0.347180, loss_ce: 0.023649
[05:37:30.511] iteration 12360 : loss : 0.323976, loss_ce: 0.032263
[05:37:34.577] iteration 12370 : loss : 0.323575, loss_ce: 0.016802
[05:37:38.638] iteration 12380 : loss : 0.198632, loss_ce: 0.007889
[05:37:42.711] iteration 12390 : loss : 0.338362, loss_ce: 0.028614
[05:37:46.776] iteration 12400 : loss : 0.327253, loss_ce: 0.017071
[05:37:50.853] iteration 12410 : loss : 0.356910, loss_ce: 0.061579
[05:37:54.922] iteration 12420 : loss : 0.222756, loss_ce: 0.069813
[05:37:58.999] iteration 12430 : loss : 0.349131, loss_ce: 0.053092
[05:38:03.069] iteration 12440 : loss : 0.352822, loss_ce: 0.046580
[05:38:07.146] iteration 12450 : loss : 0.334705, loss_ce: 0.036588
[05:38:11.216] iteration 12460 : loss : 0.320875, loss_ce: 0.007422
[05:38:15.297] iteration 12470 : loss : 0.320518, loss_ce: 0.007497
[05:38:19.373] iteration 12480 : loss : 0.341911, loss_ce: 0.019662
[05:38:23.452] iteration 12490 : loss : 0.320119, loss_ce: 0.014107
[05:38:27.525] iteration 12500 : loss : 0.329110, loss_ce: 0.040268
[05:38:31.613] iteration 12510 : loss : 0.313028, loss_ce: 0.005526
[05:38:35.694] iteration 12520 : loss : 0.158410, loss_ce: 0.003172
[05:38:39.788] iteration 12530 : loss : 0.320852, loss_ce: 0.017827
[05:38:43.865] iteration 12540 : loss : 0.325959, loss_ce: 0.026637
[05:38:47.950] iteration 12550 : loss : 0.350792, loss_ce: 0.075599
[05:38:52.033] iteration 12560 : loss : 0.332519, loss_ce: 0.019011
[05:38:56.126] iteration 12570 : loss : 0.333099, loss_ce: 0.023570
[05:39:00.211] iteration 12580 : loss : 0.324335, loss_ce: 0.016927
[05:39:04.308] iteration 12590 : loss : 0.316927, loss_ce: 0.004124
[05:40:04.715] iteration 12600 : loss : 0.471924, loss_ce: 0.087306
[05:40:08.778] iteration 12610 : loss : 0.367318, loss_ce: 0.025048
[05:40:12.835] iteration 12620 : loss : 0.344180, loss_ce: 0.034088
[05:40:16.903] iteration 12630 : loss : 0.326405, loss_ce: 0.011194
[05:40:20.961] iteration 12640 : loss : 0.353119, loss_ce: 0.029884
[05:40:25.030] iteration 12650 : loss : 0.336013, loss_ce: 0.015849
[05:40:29.091] iteration 12660 : loss : 0.335010, loss_ce: 0.044629
[05:40:33.165] iteration 12670 : loss : 0.352983, loss_ce: 0.039818
[05:40:37.231] iteration 12680 : loss : 0.304158, loss_ce: 0.027614
[05:40:41.306] iteration 12690 : loss : 0.332298, loss_ce: 0.021109
[05:40:45.375] iteration 12700 : loss : 0.333602, loss_ce: 0.034476
[05:40:49.456] iteration 12710 : loss : 0.322316, loss_ce: 0.023750
[05:40:53.524] iteration 12720 : loss : 0.326320, loss_ce: 0.013155
[05:40:57.605] iteration 12730 : loss : 0.341801, loss_ce: 0.058580
[05:41:01.678] iteration 12740 : loss : 0.338936, loss_ce: 0.040636
[05:41:05.760] iteration 12750 : loss : 0.338851, loss_ce: 0.020738
[05:41:09.833] iteration 12760 : loss : 0.179356, loss_ce: 0.012220
[05:41:13.916] iteration 12770 : loss : 0.334895, loss_ce: 0.047078
[05:41:17.990] iteration 12780 : loss : 0.197552, loss_ce: 0.039959
[05:41:22.073] iteration 12790 : loss : 0.181481, loss_ce: 0.024420
[05:41:26.151] iteration 12800 : loss : 0.342845, loss_ce: 0.062226
[05:41:30.240] iteration 12810 : loss : 0.207912, loss_ce: 0.086483
[05:41:34.321] iteration 12820 : loss : 0.325993, loss_ce: 0.022744
[05:41:38.408] iteration 12830 : loss : 0.319086, loss_ce: 0.018630
[05:41:42.492] iteration 12840 : loss : 0.329336, loss_ce: 0.030083
[05:41:46.584] iteration 12850 : loss : 0.336347, loss_ce: 0.026566
[05:41:50.673] iteration 12860 : loss : 0.316483, loss_ce: 0.011364
[05:42:40.279] iteration 12870 : loss : 0.441369, loss_ce: 0.036099
[05:42:44.339] iteration 12880 : loss : 0.401608, loss_ce: 0.044701
[05:42:48.412] iteration 12890 : loss : 0.379933, loss_ce: 0.063480
[05:42:52.476] iteration 12900 : loss : 0.352150, loss_ce: 0.026529
[05:42:56.551] iteration 12910 : loss : 0.352322, loss_ce: 0.017495
[05:43:00.617] iteration 12920 : loss : 0.351141, loss_ce: 0.042048
[05:43:04.692] iteration 12930 : loss : 0.338235, loss_ce: 0.023960
[05:43:08.759] iteration 12940 : loss : 0.342701, loss_ce: 0.061380
[05:43:12.835] iteration 12950 : loss : 0.329260, loss_ce: 0.024416
[05:43:16.904] iteration 12960 : loss : 0.315827, loss_ce: 0.003843
[05:43:20.984] iteration 12970 : loss : 0.342331, loss_ce: 0.059016
[05:43:25.055] iteration 12980 : loss : 0.321020, loss_ce: 0.017564
[05:43:29.137] iteration 12990 : loss : 0.352129, loss_ce: 0.026849
[05:43:33.210] iteration 13000 : loss : 0.336288, loss_ce: 0.028781
[05:43:37.295] iteration 13010 : loss : 0.313943, loss_ce: 0.019235
[05:43:41.369] iteration 13020 : loss : 0.344143, loss_ce: 0.070126
[05:43:45.452] iteration 13030 : loss : 0.325005, loss_ce: 0.013171
[05:43:49.529] iteration 13040 : loss : 0.336179, loss_ce: 0.031479
[05:43:53.615] iteration 13050 : loss : 0.335941, loss_ce: 0.027605
[05:43:57.691] iteration 13060 : loss : 0.324387, loss_ce: 0.005668
[05:44:01.777] iteration 13070 : loss : 0.345972, loss_ce: 0.048111
[05:44:05.856] iteration 13080 : loss : 0.319328, loss_ce: 0.013893
[05:44:09.945] iteration 13090 : loss : 0.343348, loss_ce: 0.045337
[05:44:14.035] iteration 13100 : loss : 0.329932, loss_ce: 0.021126
[05:44:18.139] iteration 13110 : loss : 0.338487, loss_ce: 0.033894
[05:44:22.231] iteration 13120 : loss : 0.327261, loss_ce: 0.039340
[05:44:26.329] iteration 13130 : loss : 0.341002, loss_ce: 0.054355
[05:45:26.820] iteration 13140 : loss : 0.454195, loss_ce: 0.048111
[05:45:30.880] iteration 13150 : loss : 0.475000, loss_ce: 0.134503
[05:45:34.936] iteration 13160 : loss : 0.432910, loss_ce: 0.077752
[05:45:39.002] iteration 13170 : loss : 0.407728, loss_ce: 0.038831
[05:45:43.063] iteration 13180 : loss : 0.371986, loss_ce: 0.016465
[05:45:47.133] iteration 13190 : loss : 0.367790, loss_ce: 0.036286
[05:45:51.195] iteration 13200 : loss : 0.368022, loss_ce: 0.023463
[05:45:55.269] iteration 13210 : loss : 0.375796, loss_ce: 0.059081
[05:45:59.331] iteration 13220 : loss : 0.344197, loss_ce: 0.031165
[05:46:03.407] iteration 13230 : loss : 0.344739, loss_ce: 0.041979
[05:46:07.472] iteration 13240 : loss : 0.331067, loss_ce: 0.014569
[05:46:11.554] iteration 13250 : loss : 0.339774, loss_ce: 0.021705
[05:46:15.624] iteration 13260 : loss : 0.358783, loss_ce: 0.036517
[05:46:19.704] iteration 13270 : loss : 0.364157, loss_ce: 0.056728
[05:46:23.777] iteration 13280 : loss : 0.344348, loss_ce: 0.010213
[05:46:27.864] iteration 13290 : loss : 0.345595, loss_ce: 0.018744
[05:46:31.936] iteration 13300 : loss : 0.336548, loss_ce: 0.014839
[05:46:36.018] iteration 13310 : loss : 0.368256, loss_ce: 0.101581
[05:46:40.092] iteration 13320 : loss : 0.335572, loss_ce: 0.022013
[05:46:44.178] iteration 13330 : loss : 0.334305, loss_ce: 0.034117
[05:46:48.253] iteration 13340 : loss : 0.334336, loss_ce: 0.033152
[05:46:52.342] iteration 13350 : loss : 0.350858, loss_ce: 0.039073
[05:46:56.422] iteration 13360 : loss : 0.343612, loss_ce: 0.024840
[05:47:00.507] iteration 13370 : loss : 0.331158, loss_ce: 0.016116
[05:47:04.587] iteration 13380 : loss : 0.333298, loss_ce: 0.041364
[05:47:08.682] iteration 13390 : loss : 0.336621, loss_ce: 0.015389
[05:47:12.673] iteration 13400 : loss : 0.402645, loss_ce: 0.181974
[05:47:58.883] save model to ./finetune_tpgm_kits23_continual_50iter\finetuned_epoch_49.pth
[05:47:59.035] save final model to ./finetune_tpgm_kits23_continual_50iter\finetuned_final.pth
