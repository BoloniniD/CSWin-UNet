[10:32:39.885] Namespace(root_path='../data/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=100, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=True, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[10:32:39.885] Continual Learning: 9 -> 12 classes
[10:32:39.885] Loading pre-trained model from ./pretrain/epoch_149.pth
[10:32:39.973] Pre-trained model loaded successfully
[10:32:39.974] Expanding model from 9 to 12 classes
[10:55:32.104] Namespace(root_path='../data/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=100, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=True, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[10:55:32.104] Continual Learning: 9 -> 12 classes
[10:55:32.104] Loading pre-trained model from ./pretrain/epoch_149.pth
[10:55:32.211] Pre-trained model loaded successfully
[10:55:32.211] Expanding CSWin-UNet from 9 to 12 classes
[11:02:05.747] Namespace(root_path='../data/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=100, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=True, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[11:02:05.747] Continual Learning: 9 -> 12 classes
[11:02:05.747] Loading pretrained model from ./pretrain/epoch_149.pth
[11:02:05.791] Loaded from standard checkpoint
[11:02:05.809] Loaded 462 backbone parameters
[11:02:05.809] Expanding CSWin-UNet model from 9 to 12 classes
[11:11:50.825] Namespace(root_path='../data/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=100, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=True, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[11:11:50.825] Continual Learning: 9 -> 12 classes
[11:11:50.825] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[11:11:50.825] Old task classes: 9, New task classes: 4
[11:11:50.871] Identified output layer keys: ['cswin_unet.stage1.0.mlp.fc1.weight', 'cswin_unet.stage1.0.mlp.fc1.bias', 'cswin_unet.stage1.0.mlp.fc2.weight', 'cswin_unet.stage1.0.mlp.fc2.bias', 'cswin_unet.stage2.0.mlp.fc1.weight', 'cswin_unet.stage2.0.mlp.fc1.bias', 'cswin_unet.stage2.0.mlp.fc2.weight', 'cswin_unet.stage2.0.mlp.fc2.bias', 'cswin_unet.stage2.1.mlp.fc1.weight', 'cswin_unet.stage2.1.mlp.fc1.bias', 'cswin_unet.stage2.1.mlp.fc2.weight', 'cswin_unet.stage2.1.mlp.fc2.bias', 'cswin_unet.stage3.0.mlp.fc1.weight', 'cswin_unet.stage3.0.mlp.fc1.bias', 'cswin_unet.stage3.0.mlp.fc2.weight', 'cswin_unet.stage3.0.mlp.fc2.bias', 'cswin_unet.stage3.1.mlp.fc1.weight', 'cswin_unet.stage3.1.mlp.fc1.bias', 'cswin_unet.stage3.1.mlp.fc2.weight', 'cswin_unet.stage3.1.mlp.fc2.bias', 'cswin_unet.stage3.2.mlp.fc1.weight', 'cswin_unet.stage3.2.mlp.fc1.bias', 'cswin_unet.stage3.2.mlp.fc2.weight', 'cswin_unet.stage3.2.mlp.fc2.bias', 'cswin_unet.stage3.3.mlp.fc1.weight', 'cswin_unet.stage3.3.mlp.fc1.bias', 'cswin_unet.stage3.3.mlp.fc2.weight', 'cswin_unet.stage3.3.mlp.fc2.bias', 'cswin_unet.stage3.4.mlp.fc1.weight', 'cswin_unet.stage3.4.mlp.fc1.bias', 'cswin_unet.stage3.4.mlp.fc2.weight', 'cswin_unet.stage3.4.mlp.fc2.bias', 'cswin_unet.stage3.5.mlp.fc1.weight', 'cswin_unet.stage3.5.mlp.fc1.bias', 'cswin_unet.stage3.5.mlp.fc2.weight', 'cswin_unet.stage3.5.mlp.fc2.bias', 'cswin_unet.stage3.6.mlp.fc1.weight', 'cswin_unet.stage3.6.mlp.fc1.bias', 'cswin_unet.stage3.6.mlp.fc2.weight', 'cswin_unet.stage3.6.mlp.fc2.bias', 'cswin_unet.stage3.7.mlp.fc1.weight', 'cswin_unet.stage3.7.mlp.fc1.bias', 'cswin_unet.stage3.7.mlp.fc2.weight', 'cswin_unet.stage3.7.mlp.fc2.bias', 'cswin_unet.stage3.8.mlp.fc1.weight', 'cswin_unet.stage3.8.mlp.fc1.bias', 'cswin_unet.stage3.8.mlp.fc2.weight', 'cswin_unet.stage3.8.mlp.fc2.bias', 'cswin_unet.stage4.0.mlp.fc1.weight', 'cswin_unet.stage4.0.mlp.fc1.bias', 'cswin_unet.stage4.0.mlp.fc2.weight', 'cswin_unet.stage4.0.mlp.fc2.bias', 'cswin_unet.stage_up4.0.mlp.fc1.weight', 'cswin_unet.stage_up4.0.mlp.fc1.bias', 'cswin_unet.stage_up4.0.mlp.fc2.weight', 'cswin_unet.stage_up4.0.mlp.fc2.bias', 'cswin_unet.concat_linear4.weight', 'cswin_unet.concat_linear4.bias', 'cswin_unet.stage_up3.0.mlp.fc1.weight', 'cswin_unet.stage_up3.0.mlp.fc1.bias', 'cswin_unet.stage_up3.0.mlp.fc2.weight', 'cswin_unet.stage_up3.0.mlp.fc2.bias', 'cswin_unet.stage_up3.1.mlp.fc1.weight', 'cswin_unet.stage_up3.1.mlp.fc1.bias', 'cswin_unet.stage_up3.1.mlp.fc2.weight', 'cswin_unet.stage_up3.1.mlp.fc2.bias', 'cswin_unet.stage_up3.2.mlp.fc1.weight', 'cswin_unet.stage_up3.2.mlp.fc1.bias', 'cswin_unet.stage_up3.2.mlp.fc2.weight', 'cswin_unet.stage_up3.2.mlp.fc2.bias', 'cswin_unet.stage_up3.3.mlp.fc1.weight', 'cswin_unet.stage_up3.3.mlp.fc1.bias', 'cswin_unet.stage_up3.3.mlp.fc2.weight', 'cswin_unet.stage_up3.3.mlp.fc2.bias', 'cswin_unet.stage_up3.4.mlp.fc1.weight', 'cswin_unet.stage_up3.4.mlp.fc1.bias', 'cswin_unet.stage_up3.4.mlp.fc2.weight', 'cswin_unet.stage_up3.4.mlp.fc2.bias', 'cswin_unet.stage_up3.5.mlp.fc1.weight', 'cswin_unet.stage_up3.5.mlp.fc1.bias', 'cswin_unet.stage_up3.5.mlp.fc2.weight', 'cswin_unet.stage_up3.5.mlp.fc2.bias', 'cswin_unet.stage_up3.6.mlp.fc1.weight', 'cswin_unet.stage_up3.6.mlp.fc1.bias', 'cswin_unet.stage_up3.6.mlp.fc2.weight', 'cswin_unet.stage_up3.6.mlp.fc2.bias', 'cswin_unet.stage_up3.7.mlp.fc1.weight', 'cswin_unet.stage_up3.7.mlp.fc1.bias', 'cswin_unet.stage_up3.7.mlp.fc2.weight', 'cswin_unet.stage_up3.7.mlp.fc2.bias', 'cswin_unet.stage_up3.8.mlp.fc1.weight', 'cswin_unet.stage_up3.8.mlp.fc1.bias', 'cswin_unet.stage_up3.8.mlp.fc2.weight', 'cswin_unet.stage_up3.8.mlp.fc2.bias', 'cswin_unet.concat_linear3.weight', 'cswin_unet.concat_linear3.bias', 'cswin_unet.stage_up2.0.mlp.fc1.weight', 'cswin_unet.stage_up2.0.mlp.fc1.bias', 'cswin_unet.stage_up2.0.mlp.fc2.weight', 'cswin_unet.stage_up2.0.mlp.fc2.bias', 'cswin_unet.stage_up2.1.mlp.fc1.weight', 'cswin_unet.stage_up2.1.mlp.fc1.bias', 'cswin_unet.stage_up2.1.mlp.fc2.weight', 'cswin_unet.stage_up2.1.mlp.fc2.bias', 'cswin_unet.concat_linear2.weight', 'cswin_unet.concat_linear2.bias', 'cswin_unet.stage_up1.0.mlp.fc1.weight', 'cswin_unet.stage_up1.0.mlp.fc1.bias', 'cswin_unet.stage_up1.0.mlp.fc2.weight', 'cswin_unet.stage_up1.0.mlp.fc2.bias', 'cswin_unet.output.weight']
[11:11:50.883] Loaded 352 backbone layers from pretrained model.
[11:11:50.883] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc1.weight' for the first 256 classes.
[11:11:50.883] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc1.bias' for the first 256 classes.
[11:11:50.883] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc2.weight' for the first 64 classes.
[11:11:50.883] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc2.bias' for the first 64 classes.
[11:11:50.884] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc1.weight' for the first 512 classes.
[11:11:50.884] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc1.bias' for the first 512 classes.
[11:11:50.884] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc2.weight' for the first 128 classes.
[11:11:50.884] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc2.bias' for the first 128 classes.
[11:11:50.884] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc1.weight' for the first 512 classes.
[11:11:50.884] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc1.bias' for the first 512 classes.
[11:11:50.884] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc2.weight' for the first 128 classes.
[11:11:50.884] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc2.bias' for the first 128 classes.
[11:11:50.885] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.885] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.885] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc2.weight' for the first 256 classes.
[11:11:50.885] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc2.bias' for the first 256 classes.
[11:11:50.885] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.885] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.885] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc2.weight' for the first 256 classes.
[11:11:50.885] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc2.bias' for the first 256 classes.
[11:11:50.885] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.885] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.886] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc2.weight' for the first 256 classes.
[11:11:50.886] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc2.bias' for the first 256 classes.
[11:11:50.886] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.886] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.886] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc2.weight' for the first 256 classes.
[11:11:50.886] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc2.bias' for the first 256 classes.
[11:11:50.886] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.886] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.886] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc2.weight' for the first 256 classes.
[11:11:50.886] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc2.bias' for the first 256 classes.
[11:11:50.888] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.888] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.888] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc2.weight' for the first 256 classes.
[11:11:50.888] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc2.bias' for the first 256 classes.
[11:11:50.888] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.888] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.888] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc2.weight' for the first 256 classes.
[11:11:50.889] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc2.bias' for the first 256 classes.
[11:11:50.889] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.889] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.889] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc2.weight' for the first 256 classes.
[11:11:50.889] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc2.bias' for the first 256 classes.
[11:11:50.889] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.889] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.890] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc2.weight' for the first 256 classes.
[11:11:50.890] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc2.bias' for the first 256 classes.
[11:11:50.890] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc1.weight' for the first 2048 classes.
[11:11:50.890] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc1.bias' for the first 2048 classes.
[11:11:50.891] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc2.weight' for the first 512 classes.
[11:11:50.891] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc2.bias' for the first 512 classes.
[11:11:50.891] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc1.weight' for the first 2048 classes.
[11:11:50.891] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc1.bias' for the first 2048 classes.
[11:11:50.892] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc2.weight' for the first 512 classes.
[11:11:50.892] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc2.bias' for the first 512 classes.
[11:11:50.892] Copied pretrained weights for 'cswin_unet.concat_linear4.weight' for the first 256 classes.
[11:11:50.892] Copied pretrained weights for 'cswin_unet.concat_linear4.bias' for the first 256 classes.
[11:11:50.892] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.892] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.893] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc2.weight' for the first 256 classes.
[11:11:50.893] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc2.bias' for the first 256 classes.
[11:11:50.893] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.893] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.893] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc2.weight' for the first 256 classes.
[11:11:50.893] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc2.bias' for the first 256 classes.
[11:11:50.894] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.894] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.894] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc2.weight' for the first 256 classes.
[11:11:50.894] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc2.bias' for the first 256 classes.
[11:11:50.894] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.894] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.894] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc2.weight' for the first 256 classes.
[11:11:50.894] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc2.bias' for the first 256 classes.
[11:11:50.895] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.895] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.895] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc2.weight' for the first 256 classes.
[11:11:50.895] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc2.bias' for the first 256 classes.
[11:11:50.895] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.895] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.895] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc2.weight' for the first 256 classes.
[11:11:50.896] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc2.bias' for the first 256 classes.
[11:11:50.896] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.896] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.896] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc2.weight' for the first 256 classes.
[11:11:50.896] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc2.bias' for the first 256 classes.
[11:11:50.896] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.896] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.897] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc2.weight' for the first 256 classes.
[11:11:50.897] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc2.bias' for the first 256 classes.
[11:11:50.897] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc1.weight' for the first 1024 classes.
[11:11:50.897] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc1.bias' for the first 1024 classes.
[11:11:50.897] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc2.weight' for the first 256 classes.
[11:11:50.897] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc2.bias' for the first 256 classes.
[11:11:50.897] Copied pretrained weights for 'cswin_unet.concat_linear3.weight' for the first 128 classes.
[11:11:50.897] Copied pretrained weights for 'cswin_unet.concat_linear3.bias' for the first 128 classes.
[11:11:50.897] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc1.weight' for the first 512 classes.
[11:11:50.898] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc1.bias' for the first 512 classes.
[11:11:50.898] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc2.weight' for the first 128 classes.
[11:11:50.898] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc2.bias' for the first 128 classes.
[11:11:50.898] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc1.weight' for the first 512 classes.
[11:11:50.898] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc1.bias' for the first 512 classes.
[11:11:50.898] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc2.weight' for the first 128 classes.
[11:11:50.898] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc2.bias' for the first 128 classes.
[11:11:50.898] Copied pretrained weights for 'cswin_unet.concat_linear2.weight' for the first 64 classes.
[11:11:50.898] Copied pretrained weights for 'cswin_unet.concat_linear2.bias' for the first 64 classes.
[11:11:50.898] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc1.weight' for the first 256 classes.
[11:11:50.899] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc1.bias' for the first 256 classes.
[11:11:50.899] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc2.weight' for the first 64 classes.
[11:11:50.899] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc2.bias' for the first 64 classes.
[11:11:50.899] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[11:11:50.903] Successfully loaded pretrained weights for continual learning.
[11:11:50.904] Expanding model from 9 to 12 classes
[11:11:50.906] Identified output layer: cswin_unet.stage1.0.attns.0.get_v
[11:18:09.995] Namespace(root_path='../data/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=100, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=True, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[11:18:09.995] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[11:18:09.995] Old task classes: 9, New task classes: 4
[11:18:10.043] Identified output layer keys: ['cswin_unet.stage1.0.mlp.fc1.weight', 'cswin_unet.stage1.0.mlp.fc1.bias', 'cswin_unet.stage1.0.mlp.fc2.weight', 'cswin_unet.stage1.0.mlp.fc2.bias', 'cswin_unet.stage2.0.mlp.fc1.weight', 'cswin_unet.stage2.0.mlp.fc1.bias', 'cswin_unet.stage2.0.mlp.fc2.weight', 'cswin_unet.stage2.0.mlp.fc2.bias', 'cswin_unet.stage2.1.mlp.fc1.weight', 'cswin_unet.stage2.1.mlp.fc1.bias', 'cswin_unet.stage2.1.mlp.fc2.weight', 'cswin_unet.stage2.1.mlp.fc2.bias', 'cswin_unet.stage3.0.mlp.fc1.weight', 'cswin_unet.stage3.0.mlp.fc1.bias', 'cswin_unet.stage3.0.mlp.fc2.weight', 'cswin_unet.stage3.0.mlp.fc2.bias', 'cswin_unet.stage3.1.mlp.fc1.weight', 'cswin_unet.stage3.1.mlp.fc1.bias', 'cswin_unet.stage3.1.mlp.fc2.weight', 'cswin_unet.stage3.1.mlp.fc2.bias', 'cswin_unet.stage3.2.mlp.fc1.weight', 'cswin_unet.stage3.2.mlp.fc1.bias', 'cswin_unet.stage3.2.mlp.fc2.weight', 'cswin_unet.stage3.2.mlp.fc2.bias', 'cswin_unet.stage3.3.mlp.fc1.weight', 'cswin_unet.stage3.3.mlp.fc1.bias', 'cswin_unet.stage3.3.mlp.fc2.weight', 'cswin_unet.stage3.3.mlp.fc2.bias', 'cswin_unet.stage3.4.mlp.fc1.weight', 'cswin_unet.stage3.4.mlp.fc1.bias', 'cswin_unet.stage3.4.mlp.fc2.weight', 'cswin_unet.stage3.4.mlp.fc2.bias', 'cswin_unet.stage3.5.mlp.fc1.weight', 'cswin_unet.stage3.5.mlp.fc1.bias', 'cswin_unet.stage3.5.mlp.fc2.weight', 'cswin_unet.stage3.5.mlp.fc2.bias', 'cswin_unet.stage3.6.mlp.fc1.weight', 'cswin_unet.stage3.6.mlp.fc1.bias', 'cswin_unet.stage3.6.mlp.fc2.weight', 'cswin_unet.stage3.6.mlp.fc2.bias', 'cswin_unet.stage3.7.mlp.fc1.weight', 'cswin_unet.stage3.7.mlp.fc1.bias', 'cswin_unet.stage3.7.mlp.fc2.weight', 'cswin_unet.stage3.7.mlp.fc2.bias', 'cswin_unet.stage3.8.mlp.fc1.weight', 'cswin_unet.stage3.8.mlp.fc1.bias', 'cswin_unet.stage3.8.mlp.fc2.weight', 'cswin_unet.stage3.8.mlp.fc2.bias', 'cswin_unet.stage4.0.mlp.fc1.weight', 'cswin_unet.stage4.0.mlp.fc1.bias', 'cswin_unet.stage4.0.mlp.fc2.weight', 'cswin_unet.stage4.0.mlp.fc2.bias', 'cswin_unet.stage_up4.0.mlp.fc1.weight', 'cswin_unet.stage_up4.0.mlp.fc1.bias', 'cswin_unet.stage_up4.0.mlp.fc2.weight', 'cswin_unet.stage_up4.0.mlp.fc2.bias', 'cswin_unet.concat_linear4.weight', 'cswin_unet.concat_linear4.bias', 'cswin_unet.stage_up3.0.mlp.fc1.weight', 'cswin_unet.stage_up3.0.mlp.fc1.bias', 'cswin_unet.stage_up3.0.mlp.fc2.weight', 'cswin_unet.stage_up3.0.mlp.fc2.bias', 'cswin_unet.stage_up3.1.mlp.fc1.weight', 'cswin_unet.stage_up3.1.mlp.fc1.bias', 'cswin_unet.stage_up3.1.mlp.fc2.weight', 'cswin_unet.stage_up3.1.mlp.fc2.bias', 'cswin_unet.stage_up3.2.mlp.fc1.weight', 'cswin_unet.stage_up3.2.mlp.fc1.bias', 'cswin_unet.stage_up3.2.mlp.fc2.weight', 'cswin_unet.stage_up3.2.mlp.fc2.bias', 'cswin_unet.stage_up3.3.mlp.fc1.weight', 'cswin_unet.stage_up3.3.mlp.fc1.bias', 'cswin_unet.stage_up3.3.mlp.fc2.weight', 'cswin_unet.stage_up3.3.mlp.fc2.bias', 'cswin_unet.stage_up3.4.mlp.fc1.weight', 'cswin_unet.stage_up3.4.mlp.fc1.bias', 'cswin_unet.stage_up3.4.mlp.fc2.weight', 'cswin_unet.stage_up3.4.mlp.fc2.bias', 'cswin_unet.stage_up3.5.mlp.fc1.weight', 'cswin_unet.stage_up3.5.mlp.fc1.bias', 'cswin_unet.stage_up3.5.mlp.fc2.weight', 'cswin_unet.stage_up3.5.mlp.fc2.bias', 'cswin_unet.stage_up3.6.mlp.fc1.weight', 'cswin_unet.stage_up3.6.mlp.fc1.bias', 'cswin_unet.stage_up3.6.mlp.fc2.weight', 'cswin_unet.stage_up3.6.mlp.fc2.bias', 'cswin_unet.stage_up3.7.mlp.fc1.weight', 'cswin_unet.stage_up3.7.mlp.fc1.bias', 'cswin_unet.stage_up3.7.mlp.fc2.weight', 'cswin_unet.stage_up3.7.mlp.fc2.bias', 'cswin_unet.stage_up3.8.mlp.fc1.weight', 'cswin_unet.stage_up3.8.mlp.fc1.bias', 'cswin_unet.stage_up3.8.mlp.fc2.weight', 'cswin_unet.stage_up3.8.mlp.fc2.bias', 'cswin_unet.concat_linear3.weight', 'cswin_unet.concat_linear3.bias', 'cswin_unet.stage_up2.0.mlp.fc1.weight', 'cswin_unet.stage_up2.0.mlp.fc1.bias', 'cswin_unet.stage_up2.0.mlp.fc2.weight', 'cswin_unet.stage_up2.0.mlp.fc2.bias', 'cswin_unet.stage_up2.1.mlp.fc1.weight', 'cswin_unet.stage_up2.1.mlp.fc1.bias', 'cswin_unet.stage_up2.1.mlp.fc2.weight', 'cswin_unet.stage_up2.1.mlp.fc2.bias', 'cswin_unet.concat_linear2.weight', 'cswin_unet.concat_linear2.bias', 'cswin_unet.stage_up1.0.mlp.fc1.weight', 'cswin_unet.stage_up1.0.mlp.fc1.bias', 'cswin_unet.stage_up1.0.mlp.fc2.weight', 'cswin_unet.stage_up1.0.mlp.fc2.bias', 'cswin_unet.output.weight']
[11:18:10.055] Loaded 352 backbone layers from pretrained model.
[11:18:10.055] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc1.weight' for the first 256 classes.
[11:18:10.055] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc1.bias' for the first 256 classes.
[11:18:10.055] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc2.weight' for the first 64 classes.
[11:18:10.055] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc2.bias' for the first 64 classes.
[11:18:10.055] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc1.weight' for the first 512 classes.
[11:18:10.055] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc1.bias' for the first 512 classes.
[11:18:10.055] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc2.weight' for the first 128 classes.
[11:18:10.055] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc2.bias' for the first 128 classes.
[11:18:10.055] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc1.weight' for the first 512 classes.
[11:18:10.055] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc1.bias' for the first 512 classes.
[11:18:10.055] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc2.weight' for the first 128 classes.
[11:18:10.055] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc2.bias' for the first 128 classes.
[11:18:10.057] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.057] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.057] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc2.weight' for the first 256 classes.
[11:18:10.057] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc2.bias' for the first 256 classes.
[11:18:10.057] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.057] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.057] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc2.weight' for the first 256 classes.
[11:18:10.058] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc2.bias' for the first 256 classes.
[11:18:10.058] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.058] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.058] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc2.weight' for the first 256 classes.
[11:18:10.058] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc2.bias' for the first 256 classes.
[11:18:10.058] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.059] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.059] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc2.weight' for the first 256 classes.
[11:18:10.059] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc2.bias' for the first 256 classes.
[11:18:10.059] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.059] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.059] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc2.weight' for the first 256 classes.
[11:18:10.059] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc2.bias' for the first 256 classes.
[11:18:10.059] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.059] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.059] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc2.weight' for the first 256 classes.
[11:18:10.059] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc2.bias' for the first 256 classes.
[11:18:10.060] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.060] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.060] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc2.weight' for the first 256 classes.
[11:18:10.060] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc2.bias' for the first 256 classes.
[11:18:10.060] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.060] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.061] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc2.weight' for the first 256 classes.
[11:18:10.061] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc2.bias' for the first 256 classes.
[11:18:10.061] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.061] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.061] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc2.weight' for the first 256 classes.
[11:18:10.061] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc2.bias' for the first 256 classes.
[11:18:10.062] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc1.weight' for the first 2048 classes.
[11:18:10.062] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc1.bias' for the first 2048 classes.
[11:18:10.062] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc2.weight' for the first 512 classes.
[11:18:10.062] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc2.bias' for the first 512 classes.
[11:18:10.063] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc1.weight' for the first 2048 classes.
[11:18:10.063] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc1.bias' for the first 2048 classes.
[11:18:10.063] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc2.weight' for the first 512 classes.
[11:18:10.063] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc2.bias' for the first 512 classes.
[11:18:10.063] Copied pretrained weights for 'cswin_unet.concat_linear4.weight' for the first 256 classes.
[11:18:10.063] Copied pretrained weights for 'cswin_unet.concat_linear4.bias' for the first 256 classes.
[11:18:10.064] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.064] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.064] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc2.weight' for the first 256 classes.
[11:18:10.064] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc2.bias' for the first 256 classes.
[11:18:10.064] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.064] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.064] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc2.weight' for the first 256 classes.
[11:18:10.064] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc2.bias' for the first 256 classes.
[11:18:10.065] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.065] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.065] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc2.weight' for the first 256 classes.
[11:18:10.065] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc2.bias' for the first 256 classes.
[11:18:10.065] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.065] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.065] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc2.weight' for the first 256 classes.
[11:18:10.066] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc2.bias' for the first 256 classes.
[11:18:10.066] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.066] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.066] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc2.weight' for the first 256 classes.
[11:18:10.066] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc2.bias' for the first 256 classes.
[11:18:10.066] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.067] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.067] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc2.weight' for the first 256 classes.
[11:18:10.067] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc2.bias' for the first 256 classes.
[11:18:10.067] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.067] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.067] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc2.weight' for the first 256 classes.
[11:18:10.067] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc2.bias' for the first 256 classes.
[11:18:10.068] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.068] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.068] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc2.weight' for the first 256 classes.
[11:18:10.068] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc2.bias' for the first 256 classes.
[11:18:10.068] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc1.weight' for the first 1024 classes.
[11:18:10.068] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc1.bias' for the first 1024 classes.
[11:18:10.069] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc2.weight' for the first 256 classes.
[11:18:10.069] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc2.bias' for the first 256 classes.
[11:18:10.069] Copied pretrained weights for 'cswin_unet.concat_linear3.weight' for the first 128 classes.
[11:18:10.069] Copied pretrained weights for 'cswin_unet.concat_linear3.bias' for the first 128 classes.
[11:18:10.069] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc1.weight' for the first 512 classes.
[11:18:10.069] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc1.bias' for the first 512 classes.
[11:18:10.069] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc2.weight' for the first 128 classes.
[11:18:10.069] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc2.bias' for the first 128 classes.
[11:18:10.069] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc1.weight' for the first 512 classes.
[11:18:10.069] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc1.bias' for the first 512 classes.
[11:18:10.070] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc2.weight' for the first 128 classes.
[11:18:10.070] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc2.bias' for the first 128 classes.
[11:18:10.070] Copied pretrained weights for 'cswin_unet.concat_linear2.weight' for the first 64 classes.
[11:18:10.070] Copied pretrained weights for 'cswin_unet.concat_linear2.bias' for the first 64 classes.
[11:18:10.070] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc1.weight' for the first 256 classes.
[11:18:10.070] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc1.bias' for the first 256 classes.
[11:18:10.071] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc2.weight' for the first 64 classes.
[11:18:10.071] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc2.bias' for the first 64 classes.
[11:18:10.071] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[11:18:10.075] Successfully loaded pretrained weights for continual learning.
[11:18:10.075] Detected 9 classes in pretrained model
[11:18:10.078] Specified old_num_classes (9) doesn't match current model output size (32). Using current model size.
[11:18:10.078] Expanding model from 32 to 35 classes
[11:18:10.080] Identified output layer: cswin_unet.stage1.0.attns.0.get_v
[11:18:10.108] Successfully expanded output layer to 35 classes
[11:18:10.108] Continual Learning: 32 -> 35 classes
[11:18:10.108] Task offset: 32
[11:18:10.140] TPGM enabled - saved expanded model state
[11:18:20.402] TPGM initialized for continual learning with excluded layers: ['cswin_unet.stage1.0.attns.0.get_v.weight', 'cswin_unet.stage1.0.attns.0.get_v.bias']
[11:18:20.403] Surgical fine-tuning enabled for continual learning
[11:18:20.405] 2976 iterations per epoch. 297600 max iterations
[11:18:20.405] Updating surgical weights at epoch 0
[11:19:47.712] Namespace(root_path='../datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=100, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=True, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[11:19:47.712] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[11:19:47.712] Old task classes: 9, New task classes: 4
[11:19:47.758] Identified output layer keys: ['cswin_unet.stage1.0.mlp.fc1.weight', 'cswin_unet.stage1.0.mlp.fc1.bias', 'cswin_unet.stage1.0.mlp.fc2.weight', 'cswin_unet.stage1.0.mlp.fc2.bias', 'cswin_unet.stage2.0.mlp.fc1.weight', 'cswin_unet.stage2.0.mlp.fc1.bias', 'cswin_unet.stage2.0.mlp.fc2.weight', 'cswin_unet.stage2.0.mlp.fc2.bias', 'cswin_unet.stage2.1.mlp.fc1.weight', 'cswin_unet.stage2.1.mlp.fc1.bias', 'cswin_unet.stage2.1.mlp.fc2.weight', 'cswin_unet.stage2.1.mlp.fc2.bias', 'cswin_unet.stage3.0.mlp.fc1.weight', 'cswin_unet.stage3.0.mlp.fc1.bias', 'cswin_unet.stage3.0.mlp.fc2.weight', 'cswin_unet.stage3.0.mlp.fc2.bias', 'cswin_unet.stage3.1.mlp.fc1.weight', 'cswin_unet.stage3.1.mlp.fc1.bias', 'cswin_unet.stage3.1.mlp.fc2.weight', 'cswin_unet.stage3.1.mlp.fc2.bias', 'cswin_unet.stage3.2.mlp.fc1.weight', 'cswin_unet.stage3.2.mlp.fc1.bias', 'cswin_unet.stage3.2.mlp.fc2.weight', 'cswin_unet.stage3.2.mlp.fc2.bias', 'cswin_unet.stage3.3.mlp.fc1.weight', 'cswin_unet.stage3.3.mlp.fc1.bias', 'cswin_unet.stage3.3.mlp.fc2.weight', 'cswin_unet.stage3.3.mlp.fc2.bias', 'cswin_unet.stage3.4.mlp.fc1.weight', 'cswin_unet.stage3.4.mlp.fc1.bias', 'cswin_unet.stage3.4.mlp.fc2.weight', 'cswin_unet.stage3.4.mlp.fc2.bias', 'cswin_unet.stage3.5.mlp.fc1.weight', 'cswin_unet.stage3.5.mlp.fc1.bias', 'cswin_unet.stage3.5.mlp.fc2.weight', 'cswin_unet.stage3.5.mlp.fc2.bias', 'cswin_unet.stage3.6.mlp.fc1.weight', 'cswin_unet.stage3.6.mlp.fc1.bias', 'cswin_unet.stage3.6.mlp.fc2.weight', 'cswin_unet.stage3.6.mlp.fc2.bias', 'cswin_unet.stage3.7.mlp.fc1.weight', 'cswin_unet.stage3.7.mlp.fc1.bias', 'cswin_unet.stage3.7.mlp.fc2.weight', 'cswin_unet.stage3.7.mlp.fc2.bias', 'cswin_unet.stage3.8.mlp.fc1.weight', 'cswin_unet.stage3.8.mlp.fc1.bias', 'cswin_unet.stage3.8.mlp.fc2.weight', 'cswin_unet.stage3.8.mlp.fc2.bias', 'cswin_unet.stage4.0.mlp.fc1.weight', 'cswin_unet.stage4.0.mlp.fc1.bias', 'cswin_unet.stage4.0.mlp.fc2.weight', 'cswin_unet.stage4.0.mlp.fc2.bias', 'cswin_unet.stage_up4.0.mlp.fc1.weight', 'cswin_unet.stage_up4.0.mlp.fc1.bias', 'cswin_unet.stage_up4.0.mlp.fc2.weight', 'cswin_unet.stage_up4.0.mlp.fc2.bias', 'cswin_unet.concat_linear4.weight', 'cswin_unet.concat_linear4.bias', 'cswin_unet.stage_up3.0.mlp.fc1.weight', 'cswin_unet.stage_up3.0.mlp.fc1.bias', 'cswin_unet.stage_up3.0.mlp.fc2.weight', 'cswin_unet.stage_up3.0.mlp.fc2.bias', 'cswin_unet.stage_up3.1.mlp.fc1.weight', 'cswin_unet.stage_up3.1.mlp.fc1.bias', 'cswin_unet.stage_up3.1.mlp.fc2.weight', 'cswin_unet.stage_up3.1.mlp.fc2.bias', 'cswin_unet.stage_up3.2.mlp.fc1.weight', 'cswin_unet.stage_up3.2.mlp.fc1.bias', 'cswin_unet.stage_up3.2.mlp.fc2.weight', 'cswin_unet.stage_up3.2.mlp.fc2.bias', 'cswin_unet.stage_up3.3.mlp.fc1.weight', 'cswin_unet.stage_up3.3.mlp.fc1.bias', 'cswin_unet.stage_up3.3.mlp.fc2.weight', 'cswin_unet.stage_up3.3.mlp.fc2.bias', 'cswin_unet.stage_up3.4.mlp.fc1.weight', 'cswin_unet.stage_up3.4.mlp.fc1.bias', 'cswin_unet.stage_up3.4.mlp.fc2.weight', 'cswin_unet.stage_up3.4.mlp.fc2.bias', 'cswin_unet.stage_up3.5.mlp.fc1.weight', 'cswin_unet.stage_up3.5.mlp.fc1.bias', 'cswin_unet.stage_up3.5.mlp.fc2.weight', 'cswin_unet.stage_up3.5.mlp.fc2.bias', 'cswin_unet.stage_up3.6.mlp.fc1.weight', 'cswin_unet.stage_up3.6.mlp.fc1.bias', 'cswin_unet.stage_up3.6.mlp.fc2.weight', 'cswin_unet.stage_up3.6.mlp.fc2.bias', 'cswin_unet.stage_up3.7.mlp.fc1.weight', 'cswin_unet.stage_up3.7.mlp.fc1.bias', 'cswin_unet.stage_up3.7.mlp.fc2.weight', 'cswin_unet.stage_up3.7.mlp.fc2.bias', 'cswin_unet.stage_up3.8.mlp.fc1.weight', 'cswin_unet.stage_up3.8.mlp.fc1.bias', 'cswin_unet.stage_up3.8.mlp.fc2.weight', 'cswin_unet.stage_up3.8.mlp.fc2.bias', 'cswin_unet.concat_linear3.weight', 'cswin_unet.concat_linear3.bias', 'cswin_unet.stage_up2.0.mlp.fc1.weight', 'cswin_unet.stage_up2.0.mlp.fc1.bias', 'cswin_unet.stage_up2.0.mlp.fc2.weight', 'cswin_unet.stage_up2.0.mlp.fc2.bias', 'cswin_unet.stage_up2.1.mlp.fc1.weight', 'cswin_unet.stage_up2.1.mlp.fc1.bias', 'cswin_unet.stage_up2.1.mlp.fc2.weight', 'cswin_unet.stage_up2.1.mlp.fc2.bias', 'cswin_unet.concat_linear2.weight', 'cswin_unet.concat_linear2.bias', 'cswin_unet.stage_up1.0.mlp.fc1.weight', 'cswin_unet.stage_up1.0.mlp.fc1.bias', 'cswin_unet.stage_up1.0.mlp.fc2.weight', 'cswin_unet.stage_up1.0.mlp.fc2.bias', 'cswin_unet.output.weight']
[11:19:47.769] Loaded 352 backbone layers from pretrained model.
[11:19:47.769] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc1.weight' for the first 256 classes.
[11:19:47.770] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc1.bias' for the first 256 classes.
[11:19:47.770] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc2.weight' for the first 64 classes.
[11:19:47.770] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc2.bias' for the first 64 classes.
[11:19:47.770] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc1.weight' for the first 512 classes.
[11:19:47.771] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc1.bias' for the first 512 classes.
[11:19:47.771] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc2.weight' for the first 128 classes.
[11:19:47.771] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc2.bias' for the first 128 classes.
[11:19:47.771] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc1.weight' for the first 512 classes.
[11:19:47.771] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc1.bias' for the first 512 classes.
[11:19:47.771] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc2.weight' for the first 128 classes.
[11:19:47.771] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc2.bias' for the first 128 classes.
[11:19:47.771] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.772] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.772] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc2.weight' for the first 256 classes.
[11:19:47.772] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc2.bias' for the first 256 classes.
[11:19:47.772] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.772] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.772] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc2.weight' for the first 256 classes.
[11:19:47.772] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc2.bias' for the first 256 classes.
[11:19:47.772] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.773] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.773] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc2.weight' for the first 256 classes.
[11:19:47.773] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc2.bias' for the first 256 classes.
[11:19:47.773] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.773] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.773] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc2.weight' for the first 256 classes.
[11:19:47.773] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc2.bias' for the first 256 classes.
[11:19:47.774] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.774] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.774] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc2.weight' for the first 256 classes.
[11:19:47.774] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc2.bias' for the first 256 classes.
[11:19:47.774] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.774] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.774] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc2.weight' for the first 256 classes.
[11:19:47.774] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc2.bias' for the first 256 classes.
[11:19:47.775] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.775] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.775] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc2.weight' for the first 256 classes.
[11:19:47.775] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc2.bias' for the first 256 classes.
[11:19:47.775] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.775] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.775] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc2.weight' for the first 256 classes.
[11:19:47.776] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc2.bias' for the first 256 classes.
[11:19:47.776] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.776] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.776] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc2.weight' for the first 256 classes.
[11:19:47.776] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc2.bias' for the first 256 classes.
[11:19:47.776] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc1.weight' for the first 2048 classes.
[11:19:47.776] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc1.bias' for the first 2048 classes.
[11:19:47.776] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc2.weight' for the first 512 classes.
[11:19:47.777] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc2.bias' for the first 512 classes.
[11:19:47.777] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc1.weight' for the first 2048 classes.
[11:19:47.777] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc1.bias' for the first 2048 classes.
[11:19:47.778] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc2.weight' for the first 512 classes.
[11:19:47.778] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc2.bias' for the first 512 classes.
[11:19:47.778] Copied pretrained weights for 'cswin_unet.concat_linear4.weight' for the first 256 classes.
[11:19:47.778] Copied pretrained weights for 'cswin_unet.concat_linear4.bias' for the first 256 classes.
[11:19:47.778] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.778] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.779] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc2.weight' for the first 256 classes.
[11:19:47.779] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc2.bias' for the first 256 classes.
[11:19:47.779] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.779] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.779] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc2.weight' for the first 256 classes.
[11:19:47.779] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc2.bias' for the first 256 classes.
[11:19:47.780] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.780] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.780] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc2.weight' for the first 256 classes.
[11:19:47.780] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc2.bias' for the first 256 classes.
[11:19:47.780] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.780] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.781] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc2.weight' for the first 256 classes.
[11:19:47.781] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc2.bias' for the first 256 classes.
[11:19:47.781] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.781] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.781] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc2.weight' for the first 256 classes.
[11:19:47.781] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc2.bias' for the first 256 classes.
[11:19:47.782] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.782] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.782] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc2.weight' for the first 256 classes.
[11:19:47.782] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc2.bias' for the first 256 classes.
[11:19:47.782] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.782] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.782] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc2.weight' for the first 256 classes.
[11:19:47.782] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc2.bias' for the first 256 classes.
[11:19:47.782] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.782] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.784] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc2.weight' for the first 256 classes.
[11:19:47.784] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc2.bias' for the first 256 classes.
[11:19:47.784] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc1.weight' for the first 1024 classes.
[11:19:47.784] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc1.bias' for the first 1024 classes.
[11:19:47.784] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc2.weight' for the first 256 classes.
[11:19:47.784] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc2.bias' for the first 256 classes.
[11:19:47.784] Copied pretrained weights for 'cswin_unet.concat_linear3.weight' for the first 128 classes.
[11:19:47.785] Copied pretrained weights for 'cswin_unet.concat_linear3.bias' for the first 128 classes.
[11:19:47.785] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc1.weight' for the first 512 classes.
[11:19:47.785] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc1.bias' for the first 512 classes.
[11:19:47.785] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc2.weight' for the first 128 classes.
[11:19:47.785] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc2.bias' for the first 128 classes.
[11:19:47.785] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc1.weight' for the first 512 classes.
[11:19:47.785] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc1.bias' for the first 512 classes.
[11:19:47.785] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc2.weight' for the first 128 classes.
[11:19:47.785] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc2.bias' for the first 128 classes.
[11:19:47.785] Copied pretrained weights for 'cswin_unet.concat_linear2.weight' for the first 64 classes.
[11:19:47.785] Copied pretrained weights for 'cswin_unet.concat_linear2.bias' for the first 64 classes.
[11:19:47.785] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc1.weight' for the first 256 classes.
[11:19:47.786] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc1.bias' for the first 256 classes.
[11:19:47.786] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc2.weight' for the first 64 classes.
[11:19:47.786] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc2.bias' for the first 64 classes.
[11:19:47.786] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[11:19:47.791] Successfully loaded pretrained weights for continual learning.
[11:19:47.791] Detected 9 classes in pretrained model
[11:19:47.794] Specified old_num_classes (9) doesn't match current model output size (32). Using current model size.
[11:19:47.794] Expanding model from 32 to 35 classes
[11:19:47.796] Identified output layer: cswin_unet.stage1.0.attns.0.get_v
[11:19:47.819] Successfully expanded output layer to 35 classes
[11:19:47.819] Continual Learning: 32 -> 35 classes
[11:19:47.819] Task offset: 32
[11:19:47.845] TPGM enabled - saved expanded model state
[11:19:57.932] TPGM initialized for continual learning with excluded layers: ['cswin_unet.stage1.0.attns.0.get_v.weight', 'cswin_unet.stage1.0.attns.0.get_v.bias']
[11:19:57.933] Surgical fine-tuning enabled for continual learning
[11:19:57.935] 2976 iterations per epoch. 297600 max iterations
[11:19:57.935] Updating surgical weights at epoch 0
[11:20:40.174] Namespace(root_path='../datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=30, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=True, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[11:20:40.174] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[11:20:40.174] Old task classes: 9, New task classes: 4
[11:20:40.220] Identified output layer keys: ['cswin_unet.stage1.0.mlp.fc1.weight', 'cswin_unet.stage1.0.mlp.fc1.bias', 'cswin_unet.stage1.0.mlp.fc2.weight', 'cswin_unet.stage1.0.mlp.fc2.bias', 'cswin_unet.stage2.0.mlp.fc1.weight', 'cswin_unet.stage2.0.mlp.fc1.bias', 'cswin_unet.stage2.0.mlp.fc2.weight', 'cswin_unet.stage2.0.mlp.fc2.bias', 'cswin_unet.stage2.1.mlp.fc1.weight', 'cswin_unet.stage2.1.mlp.fc1.bias', 'cswin_unet.stage2.1.mlp.fc2.weight', 'cswin_unet.stage2.1.mlp.fc2.bias', 'cswin_unet.stage3.0.mlp.fc1.weight', 'cswin_unet.stage3.0.mlp.fc1.bias', 'cswin_unet.stage3.0.mlp.fc2.weight', 'cswin_unet.stage3.0.mlp.fc2.bias', 'cswin_unet.stage3.1.mlp.fc1.weight', 'cswin_unet.stage3.1.mlp.fc1.bias', 'cswin_unet.stage3.1.mlp.fc2.weight', 'cswin_unet.stage3.1.mlp.fc2.bias', 'cswin_unet.stage3.2.mlp.fc1.weight', 'cswin_unet.stage3.2.mlp.fc1.bias', 'cswin_unet.stage3.2.mlp.fc2.weight', 'cswin_unet.stage3.2.mlp.fc2.bias', 'cswin_unet.stage3.3.mlp.fc1.weight', 'cswin_unet.stage3.3.mlp.fc1.bias', 'cswin_unet.stage3.3.mlp.fc2.weight', 'cswin_unet.stage3.3.mlp.fc2.bias', 'cswin_unet.stage3.4.mlp.fc1.weight', 'cswin_unet.stage3.4.mlp.fc1.bias', 'cswin_unet.stage3.4.mlp.fc2.weight', 'cswin_unet.stage3.4.mlp.fc2.bias', 'cswin_unet.stage3.5.mlp.fc1.weight', 'cswin_unet.stage3.5.mlp.fc1.bias', 'cswin_unet.stage3.5.mlp.fc2.weight', 'cswin_unet.stage3.5.mlp.fc2.bias', 'cswin_unet.stage3.6.mlp.fc1.weight', 'cswin_unet.stage3.6.mlp.fc1.bias', 'cswin_unet.stage3.6.mlp.fc2.weight', 'cswin_unet.stage3.6.mlp.fc2.bias', 'cswin_unet.stage3.7.mlp.fc1.weight', 'cswin_unet.stage3.7.mlp.fc1.bias', 'cswin_unet.stage3.7.mlp.fc2.weight', 'cswin_unet.stage3.7.mlp.fc2.bias', 'cswin_unet.stage3.8.mlp.fc1.weight', 'cswin_unet.stage3.8.mlp.fc1.bias', 'cswin_unet.stage3.8.mlp.fc2.weight', 'cswin_unet.stage3.8.mlp.fc2.bias', 'cswin_unet.stage4.0.mlp.fc1.weight', 'cswin_unet.stage4.0.mlp.fc1.bias', 'cswin_unet.stage4.0.mlp.fc2.weight', 'cswin_unet.stage4.0.mlp.fc2.bias', 'cswin_unet.stage_up4.0.mlp.fc1.weight', 'cswin_unet.stage_up4.0.mlp.fc1.bias', 'cswin_unet.stage_up4.0.mlp.fc2.weight', 'cswin_unet.stage_up4.0.mlp.fc2.bias', 'cswin_unet.concat_linear4.weight', 'cswin_unet.concat_linear4.bias', 'cswin_unet.stage_up3.0.mlp.fc1.weight', 'cswin_unet.stage_up3.0.mlp.fc1.bias', 'cswin_unet.stage_up3.0.mlp.fc2.weight', 'cswin_unet.stage_up3.0.mlp.fc2.bias', 'cswin_unet.stage_up3.1.mlp.fc1.weight', 'cswin_unet.stage_up3.1.mlp.fc1.bias', 'cswin_unet.stage_up3.1.mlp.fc2.weight', 'cswin_unet.stage_up3.1.mlp.fc2.bias', 'cswin_unet.stage_up3.2.mlp.fc1.weight', 'cswin_unet.stage_up3.2.mlp.fc1.bias', 'cswin_unet.stage_up3.2.mlp.fc2.weight', 'cswin_unet.stage_up3.2.mlp.fc2.bias', 'cswin_unet.stage_up3.3.mlp.fc1.weight', 'cswin_unet.stage_up3.3.mlp.fc1.bias', 'cswin_unet.stage_up3.3.mlp.fc2.weight', 'cswin_unet.stage_up3.3.mlp.fc2.bias', 'cswin_unet.stage_up3.4.mlp.fc1.weight', 'cswin_unet.stage_up3.4.mlp.fc1.bias', 'cswin_unet.stage_up3.4.mlp.fc2.weight', 'cswin_unet.stage_up3.4.mlp.fc2.bias', 'cswin_unet.stage_up3.5.mlp.fc1.weight', 'cswin_unet.stage_up3.5.mlp.fc1.bias', 'cswin_unet.stage_up3.5.mlp.fc2.weight', 'cswin_unet.stage_up3.5.mlp.fc2.bias', 'cswin_unet.stage_up3.6.mlp.fc1.weight', 'cswin_unet.stage_up3.6.mlp.fc1.bias', 'cswin_unet.stage_up3.6.mlp.fc2.weight', 'cswin_unet.stage_up3.6.mlp.fc2.bias', 'cswin_unet.stage_up3.7.mlp.fc1.weight', 'cswin_unet.stage_up3.7.mlp.fc1.bias', 'cswin_unet.stage_up3.7.mlp.fc2.weight', 'cswin_unet.stage_up3.7.mlp.fc2.bias', 'cswin_unet.stage_up3.8.mlp.fc1.weight', 'cswin_unet.stage_up3.8.mlp.fc1.bias', 'cswin_unet.stage_up3.8.mlp.fc2.weight', 'cswin_unet.stage_up3.8.mlp.fc2.bias', 'cswin_unet.concat_linear3.weight', 'cswin_unet.concat_linear3.bias', 'cswin_unet.stage_up2.0.mlp.fc1.weight', 'cswin_unet.stage_up2.0.mlp.fc1.bias', 'cswin_unet.stage_up2.0.mlp.fc2.weight', 'cswin_unet.stage_up2.0.mlp.fc2.bias', 'cswin_unet.stage_up2.1.mlp.fc1.weight', 'cswin_unet.stage_up2.1.mlp.fc1.bias', 'cswin_unet.stage_up2.1.mlp.fc2.weight', 'cswin_unet.stage_up2.1.mlp.fc2.bias', 'cswin_unet.concat_linear2.weight', 'cswin_unet.concat_linear2.bias', 'cswin_unet.stage_up1.0.mlp.fc1.weight', 'cswin_unet.stage_up1.0.mlp.fc1.bias', 'cswin_unet.stage_up1.0.mlp.fc2.weight', 'cswin_unet.stage_up1.0.mlp.fc2.bias', 'cswin_unet.output.weight']
[11:20:40.232] Loaded 352 backbone layers from pretrained model.
[11:20:40.232] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc1.weight' for the first 256 classes.
[11:20:40.232] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc1.bias' for the first 256 classes.
[11:20:40.233] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc2.weight' for the first 64 classes.
[11:20:40.233] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc2.bias' for the first 64 classes.
[11:20:40.233] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc1.weight' for the first 512 classes.
[11:20:40.233] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc1.bias' for the first 512 classes.
[11:20:40.233] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc2.weight' for the first 128 classes.
[11:20:40.233] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc2.bias' for the first 128 classes.
[11:20:40.233] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc1.weight' for the first 512 classes.
[11:20:40.233] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc1.bias' for the first 512 classes.
[11:20:40.234] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc2.weight' for the first 128 classes.
[11:20:40.234] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc2.bias' for the first 128 classes.
[11:20:40.234] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.234] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.234] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc2.weight' for the first 256 classes.
[11:20:40.234] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc2.bias' for the first 256 classes.
[11:20:40.235] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.235] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.235] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc2.weight' for the first 256 classes.
[11:20:40.235] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc2.bias' for the first 256 classes.
[11:20:40.235] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.235] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.236] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc2.weight' for the first 256 classes.
[11:20:40.236] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc2.bias' for the first 256 classes.
[11:20:40.236] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.236] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.236] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc2.weight' for the first 256 classes.
[11:20:40.236] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc2.bias' for the first 256 classes.
[11:20:40.237] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.237] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.237] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc2.weight' for the first 256 classes.
[11:20:40.237] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc2.bias' for the first 256 classes.
[11:20:40.237] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.237] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.237] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc2.weight' for the first 256 classes.
[11:20:40.237] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc2.bias' for the first 256 classes.
[11:20:40.237] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.237] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.238] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc2.weight' for the first 256 classes.
[11:20:40.238] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc2.bias' for the first 256 classes.
[11:20:40.238] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.238] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.238] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc2.weight' for the first 256 classes.
[11:20:40.238] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc2.bias' for the first 256 classes.
[11:20:40.239] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.239] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.239] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc2.weight' for the first 256 classes.
[11:20:40.239] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc2.bias' for the first 256 classes.
[11:20:40.239] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc1.weight' for the first 2048 classes.
[11:20:40.239] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc1.bias' for the first 2048 classes.
[11:20:40.241] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc2.weight' for the first 512 classes.
[11:20:40.241] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc2.bias' for the first 512 classes.
[11:20:40.241] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc1.weight' for the first 2048 classes.
[11:20:40.241] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc1.bias' for the first 2048 classes.
[11:20:40.242] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc2.weight' for the first 512 classes.
[11:20:40.242] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc2.bias' for the first 512 classes.
[11:20:40.242] Copied pretrained weights for 'cswin_unet.concat_linear4.weight' for the first 256 classes.
[11:20:40.242] Copied pretrained weights for 'cswin_unet.concat_linear4.bias' for the first 256 classes.
[11:20:40.242] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.243] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.243] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc2.weight' for the first 256 classes.
[11:20:40.243] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc2.bias' for the first 256 classes.
[11:20:40.243] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.243] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.243] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc2.weight' for the first 256 classes.
[11:20:40.244] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc2.bias' for the first 256 classes.
[11:20:40.244] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.244] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.244] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc2.weight' for the first 256 classes.
[11:20:40.244] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc2.bias' for the first 256 classes.
[11:20:40.244] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.245] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.245] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc2.weight' for the first 256 classes.
[11:20:40.245] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc2.bias' for the first 256 classes.
[11:20:40.245] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.245] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.245] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc2.weight' for the first 256 classes.
[11:20:40.245] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc2.bias' for the first 256 classes.
[11:20:40.245] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.245] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.245] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc2.weight' for the first 256 classes.
[11:20:40.245] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc2.bias' for the first 256 classes.
[11:20:40.246] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.246] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.246] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc2.weight' for the first 256 classes.
[11:20:40.246] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc2.bias' for the first 256 classes.
[11:20:40.246] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.246] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.247] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc2.weight' for the first 256 classes.
[11:20:40.247] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc2.bias' for the first 256 classes.
[11:20:40.247] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc1.weight' for the first 1024 classes.
[11:20:40.247] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc1.bias' for the first 1024 classes.
[11:20:40.247] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc2.weight' for the first 256 classes.
[11:20:40.247] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc2.bias' for the first 256 classes.
[11:20:40.248] Copied pretrained weights for 'cswin_unet.concat_linear3.weight' for the first 128 classes.
[11:20:40.248] Copied pretrained weights for 'cswin_unet.concat_linear3.bias' for the first 128 classes.
[11:20:40.248] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc1.weight' for the first 512 classes.
[11:20:40.248] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc1.bias' for the first 512 classes.
[11:20:40.248] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc2.weight' for the first 128 classes.
[11:20:40.249] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc2.bias' for the first 128 classes.
[11:20:40.249] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc1.weight' for the first 512 classes.
[11:20:40.249] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc1.bias' for the first 512 classes.
[11:20:40.249] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc2.weight' for the first 128 classes.
[11:20:40.249] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc2.bias' for the first 128 classes.
[11:20:40.249] Copied pretrained weights for 'cswin_unet.concat_linear2.weight' for the first 64 classes.
[11:20:40.249] Copied pretrained weights for 'cswin_unet.concat_linear2.bias' for the first 64 classes.
[11:20:40.250] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc1.weight' for the first 256 classes.
[11:20:40.250] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc1.bias' for the first 256 classes.
[11:20:40.250] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc2.weight' for the first 64 classes.
[11:20:40.250] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc2.bias' for the first 64 classes.
[11:20:40.250] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[11:20:40.255] Successfully loaded pretrained weights for continual learning.
[11:20:40.255] Detected 9 classes in pretrained model
[11:20:40.258] Specified old_num_classes (9) doesn't match current model output size (32). Using current model size.
[11:20:40.258] Expanding model from 32 to 35 classes
[11:20:40.259] Identified output layer: cswin_unet.stage1.0.attns.0.get_v
[11:20:40.284] Successfully expanded output layer to 35 classes
[11:20:40.284] Continual Learning: 32 -> 35 classes
[11:20:40.284] Task offset: 32
[11:20:40.328] TPGM enabled - saved expanded model state
[11:20:50.445] TPGM initialized for continual learning with excluded layers: ['cswin_unet.stage1.0.attns.0.get_v.weight', 'cswin_unet.stage1.0.attns.0.get_v.bias']
[11:20:50.446] Surgical fine-tuning enabled for continual learning
[11:20:50.447] 2976 iterations per epoch. 89280 max iterations
[11:20:50.448] Updating surgical weights at epoch 0
[11:22:04.290] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=30, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=True, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[11:22:04.290] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[11:22:04.290] Old task classes: 9, New task classes: 4
[11:22:04.336] Identified output layer keys: ['cswin_unet.stage1.0.mlp.fc1.weight', 'cswin_unet.stage1.0.mlp.fc1.bias', 'cswin_unet.stage1.0.mlp.fc2.weight', 'cswin_unet.stage1.0.mlp.fc2.bias', 'cswin_unet.stage2.0.mlp.fc1.weight', 'cswin_unet.stage2.0.mlp.fc1.bias', 'cswin_unet.stage2.0.mlp.fc2.weight', 'cswin_unet.stage2.0.mlp.fc2.bias', 'cswin_unet.stage2.1.mlp.fc1.weight', 'cswin_unet.stage2.1.mlp.fc1.bias', 'cswin_unet.stage2.1.mlp.fc2.weight', 'cswin_unet.stage2.1.mlp.fc2.bias', 'cswin_unet.stage3.0.mlp.fc1.weight', 'cswin_unet.stage3.0.mlp.fc1.bias', 'cswin_unet.stage3.0.mlp.fc2.weight', 'cswin_unet.stage3.0.mlp.fc2.bias', 'cswin_unet.stage3.1.mlp.fc1.weight', 'cswin_unet.stage3.1.mlp.fc1.bias', 'cswin_unet.stage3.1.mlp.fc2.weight', 'cswin_unet.stage3.1.mlp.fc2.bias', 'cswin_unet.stage3.2.mlp.fc1.weight', 'cswin_unet.stage3.2.mlp.fc1.bias', 'cswin_unet.stage3.2.mlp.fc2.weight', 'cswin_unet.stage3.2.mlp.fc2.bias', 'cswin_unet.stage3.3.mlp.fc1.weight', 'cswin_unet.stage3.3.mlp.fc1.bias', 'cswin_unet.stage3.3.mlp.fc2.weight', 'cswin_unet.stage3.3.mlp.fc2.bias', 'cswin_unet.stage3.4.mlp.fc1.weight', 'cswin_unet.stage3.4.mlp.fc1.bias', 'cswin_unet.stage3.4.mlp.fc2.weight', 'cswin_unet.stage3.4.mlp.fc2.bias', 'cswin_unet.stage3.5.mlp.fc1.weight', 'cswin_unet.stage3.5.mlp.fc1.bias', 'cswin_unet.stage3.5.mlp.fc2.weight', 'cswin_unet.stage3.5.mlp.fc2.bias', 'cswin_unet.stage3.6.mlp.fc1.weight', 'cswin_unet.stage3.6.mlp.fc1.bias', 'cswin_unet.stage3.6.mlp.fc2.weight', 'cswin_unet.stage3.6.mlp.fc2.bias', 'cswin_unet.stage3.7.mlp.fc1.weight', 'cswin_unet.stage3.7.mlp.fc1.bias', 'cswin_unet.stage3.7.mlp.fc2.weight', 'cswin_unet.stage3.7.mlp.fc2.bias', 'cswin_unet.stage3.8.mlp.fc1.weight', 'cswin_unet.stage3.8.mlp.fc1.bias', 'cswin_unet.stage3.8.mlp.fc2.weight', 'cswin_unet.stage3.8.mlp.fc2.bias', 'cswin_unet.stage4.0.mlp.fc1.weight', 'cswin_unet.stage4.0.mlp.fc1.bias', 'cswin_unet.stage4.0.mlp.fc2.weight', 'cswin_unet.stage4.0.mlp.fc2.bias', 'cswin_unet.stage_up4.0.mlp.fc1.weight', 'cswin_unet.stage_up4.0.mlp.fc1.bias', 'cswin_unet.stage_up4.0.mlp.fc2.weight', 'cswin_unet.stage_up4.0.mlp.fc2.bias', 'cswin_unet.concat_linear4.weight', 'cswin_unet.concat_linear4.bias', 'cswin_unet.stage_up3.0.mlp.fc1.weight', 'cswin_unet.stage_up3.0.mlp.fc1.bias', 'cswin_unet.stage_up3.0.mlp.fc2.weight', 'cswin_unet.stage_up3.0.mlp.fc2.bias', 'cswin_unet.stage_up3.1.mlp.fc1.weight', 'cswin_unet.stage_up3.1.mlp.fc1.bias', 'cswin_unet.stage_up3.1.mlp.fc2.weight', 'cswin_unet.stage_up3.1.mlp.fc2.bias', 'cswin_unet.stage_up3.2.mlp.fc1.weight', 'cswin_unet.stage_up3.2.mlp.fc1.bias', 'cswin_unet.stage_up3.2.mlp.fc2.weight', 'cswin_unet.stage_up3.2.mlp.fc2.bias', 'cswin_unet.stage_up3.3.mlp.fc1.weight', 'cswin_unet.stage_up3.3.mlp.fc1.bias', 'cswin_unet.stage_up3.3.mlp.fc2.weight', 'cswin_unet.stage_up3.3.mlp.fc2.bias', 'cswin_unet.stage_up3.4.mlp.fc1.weight', 'cswin_unet.stage_up3.4.mlp.fc1.bias', 'cswin_unet.stage_up3.4.mlp.fc2.weight', 'cswin_unet.stage_up3.4.mlp.fc2.bias', 'cswin_unet.stage_up3.5.mlp.fc1.weight', 'cswin_unet.stage_up3.5.mlp.fc1.bias', 'cswin_unet.stage_up3.5.mlp.fc2.weight', 'cswin_unet.stage_up3.5.mlp.fc2.bias', 'cswin_unet.stage_up3.6.mlp.fc1.weight', 'cswin_unet.stage_up3.6.mlp.fc1.bias', 'cswin_unet.stage_up3.6.mlp.fc2.weight', 'cswin_unet.stage_up3.6.mlp.fc2.bias', 'cswin_unet.stage_up3.7.mlp.fc1.weight', 'cswin_unet.stage_up3.7.mlp.fc1.bias', 'cswin_unet.stage_up3.7.mlp.fc2.weight', 'cswin_unet.stage_up3.7.mlp.fc2.bias', 'cswin_unet.stage_up3.8.mlp.fc1.weight', 'cswin_unet.stage_up3.8.mlp.fc1.bias', 'cswin_unet.stage_up3.8.mlp.fc2.weight', 'cswin_unet.stage_up3.8.mlp.fc2.bias', 'cswin_unet.concat_linear3.weight', 'cswin_unet.concat_linear3.bias', 'cswin_unet.stage_up2.0.mlp.fc1.weight', 'cswin_unet.stage_up2.0.mlp.fc1.bias', 'cswin_unet.stage_up2.0.mlp.fc2.weight', 'cswin_unet.stage_up2.0.mlp.fc2.bias', 'cswin_unet.stage_up2.1.mlp.fc1.weight', 'cswin_unet.stage_up2.1.mlp.fc1.bias', 'cswin_unet.stage_up2.1.mlp.fc2.weight', 'cswin_unet.stage_up2.1.mlp.fc2.bias', 'cswin_unet.concat_linear2.weight', 'cswin_unet.concat_linear2.bias', 'cswin_unet.stage_up1.0.mlp.fc1.weight', 'cswin_unet.stage_up1.0.mlp.fc1.bias', 'cswin_unet.stage_up1.0.mlp.fc2.weight', 'cswin_unet.stage_up1.0.mlp.fc2.bias', 'cswin_unet.output.weight']
[11:22:04.348] Loaded 352 backbone layers from pretrained model.
[11:22:04.348] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc1.weight' for the first 256 classes.
[11:22:04.348] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc1.bias' for the first 256 classes.
[11:22:04.348] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc2.weight' for the first 64 classes.
[11:22:04.348] Copied pretrained weights for 'cswin_unet.stage1.0.mlp.fc2.bias' for the first 64 classes.
[11:22:04.349] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc1.weight' for the first 512 classes.
[11:22:04.349] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc1.bias' for the first 512 classes.
[11:22:04.349] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc2.weight' for the first 128 classes.
[11:22:04.349] Copied pretrained weights for 'cswin_unet.stage2.0.mlp.fc2.bias' for the first 128 classes.
[11:22:04.349] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc1.weight' for the first 512 classes.
[11:22:04.349] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc1.bias' for the first 512 classes.
[11:22:04.350] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc2.weight' for the first 128 classes.
[11:22:04.350] Copied pretrained weights for 'cswin_unet.stage2.1.mlp.fc2.bias' for the first 128 classes.
[11:22:04.350] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.350] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.350] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc2.weight' for the first 256 classes.
[11:22:04.350] Copied pretrained weights for 'cswin_unet.stage3.0.mlp.fc2.bias' for the first 256 classes.
[11:22:04.351] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.351] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.351] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc2.weight' for the first 256 classes.
[11:22:04.351] Copied pretrained weights for 'cswin_unet.stage3.1.mlp.fc2.bias' for the first 256 classes.
[11:22:04.351] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.351] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.351] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc2.weight' for the first 256 classes.
[11:22:04.351] Copied pretrained weights for 'cswin_unet.stage3.2.mlp.fc2.bias' for the first 256 classes.
[11:22:04.352] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.352] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.352] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc2.weight' for the first 256 classes.
[11:22:04.352] Copied pretrained weights for 'cswin_unet.stage3.3.mlp.fc2.bias' for the first 256 classes.
[11:22:04.352] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.352] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.352] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc2.weight' for the first 256 classes.
[11:22:04.353] Copied pretrained weights for 'cswin_unet.stage3.4.mlp.fc2.bias' for the first 256 classes.
[11:22:04.353] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.353] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.353] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc2.weight' for the first 256 classes.
[11:22:04.353] Copied pretrained weights for 'cswin_unet.stage3.5.mlp.fc2.bias' for the first 256 classes.
[11:22:04.353] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.354] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.354] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc2.weight' for the first 256 classes.
[11:22:04.354] Copied pretrained weights for 'cswin_unet.stage3.6.mlp.fc2.bias' for the first 256 classes.
[11:22:04.354] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.354] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.354] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc2.weight' for the first 256 classes.
[11:22:04.355] Copied pretrained weights for 'cswin_unet.stage3.7.mlp.fc2.bias' for the first 256 classes.
[11:22:04.355] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.355] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.355] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc2.weight' for the first 256 classes.
[11:22:04.355] Copied pretrained weights for 'cswin_unet.stage3.8.mlp.fc2.bias' for the first 256 classes.
[11:22:04.356] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc1.weight' for the first 2048 classes.
[11:22:04.356] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc1.bias' for the first 2048 classes.
[11:22:04.356] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc2.weight' for the first 512 classes.
[11:22:04.356] Copied pretrained weights for 'cswin_unet.stage4.0.mlp.fc2.bias' for the first 512 classes.
[11:22:04.357] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc1.weight' for the first 2048 classes.
[11:22:04.357] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc1.bias' for the first 2048 classes.
[11:22:04.357] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc2.weight' for the first 512 classes.
[11:22:04.357] Copied pretrained weights for 'cswin_unet.stage_up4.0.mlp.fc2.bias' for the first 512 classes.
[11:22:04.357] Copied pretrained weights for 'cswin_unet.concat_linear4.weight' for the first 256 classes.
[11:22:04.357] Copied pretrained weights for 'cswin_unet.concat_linear4.bias' for the first 256 classes.
[11:22:04.358] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.358] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.358] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc2.weight' for the first 256 classes.
[11:22:04.358] Copied pretrained weights for 'cswin_unet.stage_up3.0.mlp.fc2.bias' for the first 256 classes.
[11:22:04.358] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.358] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.358] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc2.weight' for the first 256 classes.
[11:22:04.359] Copied pretrained weights for 'cswin_unet.stage_up3.1.mlp.fc2.bias' for the first 256 classes.
[11:22:04.359] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.359] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.359] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc2.weight' for the first 256 classes.
[11:22:04.359] Copied pretrained weights for 'cswin_unet.stage_up3.2.mlp.fc2.bias' for the first 256 classes.
[11:22:04.359] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.359] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.359] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc2.weight' for the first 256 classes.
[11:22:04.360] Copied pretrained weights for 'cswin_unet.stage_up3.3.mlp.fc2.bias' for the first 256 classes.
[11:22:04.360] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.360] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.360] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc2.weight' for the first 256 classes.
[11:22:04.360] Copied pretrained weights for 'cswin_unet.stage_up3.4.mlp.fc2.bias' for the first 256 classes.
[11:22:04.360] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.360] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.361] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc2.weight' for the first 256 classes.
[11:22:04.361] Copied pretrained weights for 'cswin_unet.stage_up3.5.mlp.fc2.bias' for the first 256 classes.
[11:22:04.361] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.361] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.361] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc2.weight' for the first 256 classes.
[11:22:04.361] Copied pretrained weights for 'cswin_unet.stage_up3.6.mlp.fc2.bias' for the first 256 classes.
[11:22:04.362] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.362] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.362] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc2.weight' for the first 256 classes.
[11:22:04.362] Copied pretrained weights for 'cswin_unet.stage_up3.7.mlp.fc2.bias' for the first 256 classes.
[11:22:04.362] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc1.weight' for the first 1024 classes.
[11:22:04.362] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc1.bias' for the first 1024 classes.
[11:22:04.362] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc2.weight' for the first 256 classes.
[11:22:04.363] Copied pretrained weights for 'cswin_unet.stage_up3.8.mlp.fc2.bias' for the first 256 classes.
[11:22:04.363] Copied pretrained weights for 'cswin_unet.concat_linear3.weight' for the first 128 classes.
[11:22:04.363] Copied pretrained weights for 'cswin_unet.concat_linear3.bias' for the first 128 classes.
[11:22:04.363] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc1.weight' for the first 512 classes.
[11:22:04.363] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc1.bias' for the first 512 classes.
[11:22:04.363] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc2.weight' for the first 128 classes.
[11:22:04.363] Copied pretrained weights for 'cswin_unet.stage_up2.0.mlp.fc2.bias' for the first 128 classes.
[11:22:04.363] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc1.weight' for the first 512 classes.
[11:22:04.363] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc1.bias' for the first 512 classes.
[11:22:04.363] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc2.weight' for the first 128 classes.
[11:22:04.363] Copied pretrained weights for 'cswin_unet.stage_up2.1.mlp.fc2.bias' for the first 128 classes.
[11:22:04.364] Copied pretrained weights for 'cswin_unet.concat_linear2.weight' for the first 64 classes.
[11:22:04.364] Copied pretrained weights for 'cswin_unet.concat_linear2.bias' for the first 64 classes.
[11:22:04.364] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc1.weight' for the first 256 classes.
[11:22:04.364] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc1.bias' for the first 256 classes.
[11:22:04.364] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc2.weight' for the first 64 classes.
[11:22:04.364] Copied pretrained weights for 'cswin_unet.stage_up1.0.mlp.fc2.bias' for the first 64 classes.
[11:22:04.364] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[11:22:04.368] Successfully loaded pretrained weights for continual learning.
[11:22:04.370] Detected 9 classes in pretrained model
[11:22:04.372] Specified old_num_classes (9) doesn't match current model output size (32). Using current model size.
[11:22:04.372] Expanding model from 32 to 35 classes
[11:22:04.374] Identified output layer: cswin_unet.stage1.0.attns.0.get_v
[11:22:04.397] Successfully expanded output layer to 35 classes
[11:22:04.397] Continual Learning: 32 -> 35 classes
[11:22:04.397] Task offset: 32
[11:22:04.424] TPGM enabled - saved expanded model state
[11:22:14.186] TPGM initialized for continual learning with excluded layers: ['cswin_unet.stage1.0.attns.0.get_v.weight', 'cswin_unet.stage1.0.attns.0.get_v.bias']
[11:22:14.187] Surgical fine-tuning enabled for continual learning
[11:22:14.189] 2976 iterations per epoch. 89280 max iterations
[11:22:14.190] Updating surgical weights at epoch 0
[11:30:28.060] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=30, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=True, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[11:30:28.060] Continual Learning: 9 -> 12 classes
[11:30:28.060] Loading pretrained model from ./pretrain/epoch_149.pth
[11:30:28.166] Loaded 463/463 layers from pretrained model
[11:30:28.167] Expanding model from 9 to 12 classes
[11:30:28.169] Identified output layer keys: ['cswin_unet.output.weight']
[11:30:28.189] Expanded cswin_unet.output.weight from shape torch.Size([9, 64, 1, 1]) to torch.Size([12, 64, 1, 1])
[11:34:45.501] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=30, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=True, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[11:34:45.502] Continual Learning: 9 -> 12 classes
[11:34:45.502] Task offset: 9, New classes: 4
[11:34:45.502] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[11:34:45.502] Old task classes: 9, New task classes: 4
[11:34:45.548] Identified output layer keys: ['cswin_unet.output.weight']
[11:34:45.549] Loaded 462 backbone layers from pretrained model.
[11:34:45.549] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[11:34:45.567] Successfully adapted model for continual learning.
[11:34:45.568] Pretrained weights loaded and adapted for continual learning
[11:34:45.603] TPGM enabled - saved model state
[11:34:55.553] TPGM initialized for continual learning with exclude list: ['cswin_unet.output.weight']
[11:34:55.555] Surgical fine-tuning enabled for continual learning
[11:34:55.557] 2976 iterations per epoch. 89280 max iterations
[11:34:55.558] Updating surgical weights at epoch 0
[11:39:39.259] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=30, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=True, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[11:39:39.259] Continual Learning: 9 -> 12 classes
[11:39:39.259] Task offset: 9, New classes: 4
[11:39:39.259] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[11:39:39.259] Old task classes: 9, New task classes: 4
[11:39:39.309] Identified output layer keys: ['cswin_unet.output.weight']
[11:39:39.309] Loaded 462 backbone layers from pretrained model.
[11:39:39.309] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[11:39:39.342] Successfully adapted model for continual learning.
[11:39:39.342] Pretrained weights loaded and adapted for continual learning
[11:39:39.368] TPGM enabled - saved model state
[11:39:49.170] TPGM initialized for continual learning with exclude list: ['cswin_unet.output.weight']
[11:39:49.171] Surgical fine-tuning enabled for continual learning
[11:39:49.172] 2976 iterations per epoch. 89280 max iterations
[11:39:49.173] Updating surgical weights at epoch 0
[11:40:13.193] Surgical weights range: [0.0002, 1.0000]
[11:40:13.193] Applying TPGM projection update at epoch 0
[11:44:09.711] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=30, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=True, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[11:44:09.711] Continual Learning: 9 -> 12 classes
[11:44:09.711] Task offset: 9, New classes: 4
[11:44:09.711] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[11:44:09.711] Old task classes: 9, New task classes: 4
[11:44:09.759] Identified output layer keys: ['cswin_unet.output.weight']
[11:44:09.759] Loaded 462 backbone layers from pretrained model.
[11:44:09.759] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[11:44:09.796] Successfully adapted model for continual learning.
[11:44:09.796] Pretrained weights loaded and adapted for continual learning
[11:44:09.824] TPGM enabled - saved model state
[11:44:19.963] TPGM initialized for continual learning with exclude list: ['cswin_unet.output.weight']
[11:44:19.963] Surgical fine-tuning enabled for continual learning
[11:44:19.965] 2976 iterations per epoch. 89280 max iterations
[11:44:19.966] Updating surgical weights at epoch 0
[11:44:44.385] Surgical weights range: [0.0002, 1.0000]
[11:44:44.385] Applying TPGM projection update at epoch 0
[11:45:51.870] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=30, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=True, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[11:45:51.871] Continual Learning: 9 -> 12 classes
[11:45:51.871] Task offset: 9, New classes: 4
[11:45:51.871] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[11:45:51.871] Old task classes: 9, New task classes: 4
[11:45:51.917] Identified output layer keys: ['cswin_unet.output.weight']
[11:45:51.918] Loaded 462 backbone layers from pretrained model.
[11:45:51.918] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[11:45:51.953] Successfully adapted model for continual learning.
[11:45:51.953] Pretrained weights loaded and adapted for continual learning
[11:45:51.978] TPGM enabled - saved model state
[11:46:02.040] TPGM initialized for continual learning with exclude list: ['cswin_unet.output.weight']
[11:46:02.041] Surgical fine-tuning enabled for continual learning
[11:46:02.043] 2976 iterations per epoch. 89280 max iterations
[11:46:02.044] Updating surgical weights at epoch 0
[11:46:26.252] Surgical weights range: [0.0002, 1.0000]
[11:46:26.252] Applying TPGM projection update at epoch 0
[11:51:12.219] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=30, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=True, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[11:51:12.219] Continual Learning: 9 -> 12 classes
[11:51:12.219] Task offset: 9, New classes: 4
[11:51:12.219] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[11:51:12.219] Old task classes: 9, New task classes: 4
[11:51:12.266] Identified output layer keys: ['cswin_unet.output.weight']
[11:51:12.267] Loaded 462 backbone layers from pretrained model.
[11:51:12.267] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[11:51:12.301] Successfully adapted model for continual learning.
[11:51:12.301] Pretrained weights loaded and adapted for continual learning
[11:51:12.327] TPGM enabled - saved model state
[11:51:22.482] TPGM initialized for continual learning with exclude list: ['cswin_unet.output.weight']
[11:51:22.483] Surgical fine-tuning enabled for continual learning
[11:51:22.484] 2976 iterations per epoch. 89280 max iterations
[11:51:22.485] Updating surgical weights at epoch 0
[11:51:46.821] Surgical weights range: [0.0002, 1.0000]
[11:51:46.821] Applying TPGM projection update at epoch 0
[11:57:46.174] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=30, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=True, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[11:57:46.174] Continual Learning: 9 -> 12 classes
[11:57:46.174] Task offset: 9, New classes: 4
[11:57:46.174] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[11:57:46.174] Old task classes: 9, New task classes: 4
[11:57:46.222] Identified output layer keys: ['cswin_unet.output.weight']
[11:57:46.222] Loaded 462 backbone layers from pretrained model.
[11:57:46.223] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[11:57:46.256] Successfully adapted model for continual learning.
[11:57:46.257] Pretrained weights loaded and adapted for continual learning
[11:57:46.283] TPGM enabled - saved model state
[11:57:56.638] TPGM initialized for continual learning with exclude list: ['cswin_unet.output.weight']
[11:57:56.639] Surgical fine-tuning enabled for continual learning
[11:57:56.641] 2976 iterations per epoch. 89280 max iterations
[11:57:56.642] Updating surgical weights at epoch 0
[11:58:21.564] Surgical weights range: [0.0002, 1.0000]
[11:58:21.564] Applying TPGM projection update at epoch 0
[12:07:10.223] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=30, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=True, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[12:07:10.224] Continual Learning: 9 -> 12 classes
[12:07:10.224] Task offset: 9, New classes: 4
[12:07:10.224] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[12:07:10.224] Old task classes: 9, New task classes: 4
[12:07:10.277] Identified output layer keys: ['cswin_unet.output.weight']
[12:07:10.277] Loaded 462 backbone layers from pretrained model.
[12:07:10.278] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[12:07:10.314] Successfully adapted model for continual learning.
[12:07:10.314] Pretrained weights loaded and adapted for continual learning
[12:07:10.341] TPGM enabled - saved model state
[12:07:20.640] TPGM initialized for continual learning with exclude list: ['cswin_unet.output.weight']
[12:07:20.641] Surgical fine-tuning enabled for continual learning
[12:07:20.642] 2976 iterations per epoch. 89280 max iterations
[12:07:20.643] Updating surgical weights at epoch 0
[12:07:45.346] Surgical weights range: [0.0002, 1.0000]
[12:07:45.346] Applying TPGM projection update at epoch 0
[12:07:55.095] TPGM iteration 19/50 completed, loss: 0.0327
[12:08:05.305] TPGM iteration 39/50 completed, loss: 0.2043
[12:08:32.049] Epoch [0/30] Iteration [0/2976]: Loss: 0.6008, CE: 0.1281
[12:08:37.505] Epoch [0/30] Iteration [10/2976]: Loss: 0.6207, CE: 0.1752
[12:08:43.010] Epoch [0/30] Iteration [20/2976]: Loss: 0.6015, CE: 0.1284
[12:08:48.479] Epoch [0/30] Iteration [30/2976]: Loss: 0.5868, CE: 0.0919
[12:08:53.912] Epoch [0/30] Iteration [40/2976]: Loss: 0.6278, CE: 0.1932
[12:08:59.368] Epoch [0/30] Iteration [50/2976]: Loss: 0.5835, CE: 0.0842
[12:09:04.835] Epoch [0/30] Iteration [60/2976]: Loss: 0.5966, CE: 0.1158
[12:09:10.269] Epoch [0/30] Iteration [70/2976]: Loss: 0.5955, CE: 0.1133
[12:09:15.763] Epoch [0/30] Iteration [80/2976]: Loss: 0.6144, CE: 0.1669
[12:09:21.219] Epoch [0/30] Iteration [90/2976]: Loss: 0.5905, CE: 0.1009
[12:09:26.689] Epoch [0/30] Iteration [100/2976]: Loss: 0.6141, CE: 0.1641
[12:09:32.178] Epoch [0/30] Iteration [110/2976]: Loss: 0.5847, CE: 0.0864
[12:09:37.629] Epoch [0/30] Iteration [120/2976]: Loss: 0.5870, CE: 0.0919
[12:09:43.069] Epoch [0/30] Iteration [130/2976]: Loss: 0.5804, CE: 0.0756
[12:09:48.544] Epoch [0/30] Iteration [140/2976]: Loss: 0.5982, CE: 0.1234
[12:09:53.999] Epoch [0/30] Iteration [150/2976]: Loss: 0.6028, CE: 0.1314
[12:09:59.470] Epoch [0/30] Iteration [160/2976]: Loss: 0.5681, CE: 0.0459
[12:10:04.900] Epoch [0/30] Iteration [170/2976]: Loss: 0.5781, CE: 0.0780
[12:10:10.362] Epoch [0/30] Iteration [180/2976]: Loss: 0.5886, CE: 0.0962
[12:10:15.823] Epoch [0/30] Iteration [190/2976]: Loss: 0.5606, CE: 0.0297
[12:10:21.312] Epoch [0/30] Iteration [200/2976]: Loss: 0.5935, CE: 0.1100
[12:10:26.815] Epoch [0/30] Iteration [210/2976]: Loss: 0.5921, CE: 0.1056
[12:10:32.294] Epoch [0/30] Iteration [220/2976]: Loss: 0.6008, CE: 0.1263
[12:10:37.817] Epoch [0/30] Iteration [230/2976]: Loss: 0.5754, CE: 0.0639
[12:10:43.257] Epoch [0/30] Iteration [240/2976]: Loss: 0.5982, CE: 0.1250
[12:10:48.774] Epoch [0/30] Iteration [250/2976]: Loss: 0.5763, CE: 0.0934
[12:10:54.325] Epoch [0/30] Iteration [260/2976]: Loss: 0.5824, CE: 0.0823
[12:10:59.776] Epoch [0/30] Iteration [270/2976]: Loss: 0.5858, CE: 0.0892
[12:11:05.233] Epoch [0/30] Iteration [280/2976]: Loss: 0.5793, CE: 0.0731
[12:11:10.720] Epoch [0/30] Iteration [290/2976]: Loss: 0.5822, CE: 0.0800
[12:11:16.179] Epoch [0/30] Iteration [300/2976]: Loss: 0.5830, CE: 0.0887
[12:11:21.652] Epoch [0/30] Iteration [310/2976]: Loss: 0.5834, CE: 0.0855
[12:11:27.104] Epoch [0/30] Iteration [320/2976]: Loss: 0.5904, CE: 0.1019
[12:11:32.614] Epoch [0/30] Iteration [330/2976]: Loss: 0.5774, CE: 0.0844
[12:11:38.065] Epoch [0/30] Iteration [340/2976]: Loss: 0.5856, CE: 0.0886
[12:11:43.513] Epoch [0/30] Iteration [350/2976]: Loss: 0.5967, CE: 0.1164
[12:11:48.960] Epoch [0/30] Iteration [360/2976]: Loss: 0.5942, CE: 0.1357
[12:11:54.418] Epoch [0/30] Iteration [370/2976]: Loss: 0.5715, CE: 0.0798
[12:11:59.896] Epoch [0/30] Iteration [380/2976]: Loss: 0.5735, CE: 0.0588
[12:12:05.335] Epoch [0/30] Iteration [390/2976]: Loss: 0.5730, CE: 0.0580
[12:12:10.809] Epoch [0/30] Iteration [400/2976]: Loss: 0.6378, CE: 0.2188
[12:12:16.253] Epoch [0/30] Iteration [410/2976]: Loss: 0.5750, CE: 0.0627
[12:12:21.706] Epoch [0/30] Iteration [420/2976]: Loss: 0.5827, CE: 0.0849
[12:12:27.140] Epoch [0/30] Iteration [430/2976]: Loss: 0.5829, CE: 0.0821
[12:12:32.631] Epoch [0/30] Iteration [440/2976]: Loss: 0.5884, CE: 0.0958
[12:12:38.069] Epoch [0/30] Iteration [450/2976]: Loss: 0.5908, CE: 0.1162
[12:12:43.555] Epoch [0/30] Iteration [460/2976]: Loss: 0.5546, CE: 0.0257
[12:12:49.009] Epoch [0/30] Iteration [470/2976]: Loss: 0.5877, CE: 0.0951
[12:12:54.469] Epoch [0/30] Iteration [480/2976]: Loss: 0.5843, CE: 0.0855
[12:12:59.924] Epoch [0/30] Iteration [490/2976]: Loss: 0.5724, CE: 0.0675
[12:13:05.372] Epoch [0/30] Iteration [500/2976]: Loss: 0.5782, CE: 0.0805
[12:13:10.851] Epoch [0/30] Iteration [510/2976]: Loss: 0.6082, CE: 0.1455
[12:13:16.320] Epoch [0/30] Iteration [520/2976]: Loss: 0.5661, CE: 0.0572
[12:13:21.773] Epoch [0/30] Iteration [530/2976]: Loss: 0.5804, CE: 0.0801
[12:13:27.212] Epoch [0/30] Iteration [540/2976]: Loss: 0.5687, CE: 0.0821
[12:13:32.670] Epoch [0/30] Iteration [550/2976]: Loss: 0.5782, CE: 0.0764
[12:13:38.117] Epoch [0/30] Iteration [560/2976]: Loss: 0.5753, CE: 0.0644
[12:13:43.597] Epoch [0/30] Iteration [570/2976]: Loss: 0.5791, CE: 0.0902
[12:13:49.056] Epoch [0/30] Iteration [580/2976]: Loss: 0.5751, CE: 0.0851
[12:13:54.509] Epoch [0/30] Iteration [590/2976]: Loss: 0.5901, CE: 0.0999
[12:13:59.959] Epoch [0/30] Iteration [600/2976]: Loss: 0.5846, CE: 0.0873
[12:14:05.424] Epoch [0/30] Iteration [610/2976]: Loss: 0.6037, CE: 0.1336
[12:14:10.893] Epoch [0/30] Iteration [620/2976]: Loss: 0.5889, CE: 0.0974
[12:14:16.398] Epoch [0/30] Iteration [630/2976]: Loss: 0.5690, CE: 0.0742
[12:14:21.864] Epoch [0/30] Iteration [640/2976]: Loss: 0.5970, CE: 0.1171
[12:14:27.315] Epoch [0/30] Iteration [650/2976]: Loss: 0.5958, CE: 0.1142
[12:14:32.771] Epoch [0/30] Iteration [660/2976]: Loss: 0.5796, CE: 0.0751
[12:14:38.213] Epoch [0/30] Iteration [670/2976]: Loss: 0.5928, CE: 0.1066
[12:14:43.688] Epoch [0/30] Iteration [680/2976]: Loss: 0.5775, CE: 0.0688
[12:14:49.136] Epoch [0/30] Iteration [690/2976]: Loss: 0.5905, CE: 0.1008
[12:14:54.601] Epoch [0/30] Iteration [700/2976]: Loss: 0.6043, CE: 0.1353
[12:15:00.041] Epoch [0/30] Iteration [710/2976]: Loss: 0.5682, CE: 0.0459
[12:15:05.505] Epoch [0/30] Iteration [720/2976]: Loss: 0.5877, CE: 0.0941
[12:15:10.944] Epoch [0/30] Iteration [730/2976]: Loss: 0.5737, CE: 0.0596
[12:15:16.405] Epoch [0/30] Iteration [740/2976]: Loss: 0.6123, CE: 0.1587
[12:15:21.848] Epoch [0/30] Iteration [750/2976]: Loss: 0.5738, CE: 0.0597
[12:15:27.311] Epoch [0/30] Iteration [760/2976]: Loss: 0.5851, CE: 0.0876
[12:15:32.757] Epoch [0/30] Iteration [770/2976]: Loss: 0.5717, CE: 0.0852
[12:15:38.237] Epoch [0/30] Iteration [780/2976]: Loss: 0.6262, CE: 0.1898
[12:15:43.685] Epoch [0/30] Iteration [790/2976]: Loss: 0.5820, CE: 0.0799
[12:15:49.132] Epoch [0/30] Iteration [800/2976]: Loss: 0.5854, CE: 0.0883
[12:15:54.597] Epoch [0/30] Iteration [810/2976]: Loss: 0.6189, CE: 0.1757
[12:16:00.066] Epoch [0/30] Iteration [820/2976]: Loss: 0.5749, CE: 0.0624
[12:16:05.523] Epoch [0/30] Iteration [830/2976]: Loss: 0.5860, CE: 0.0977
[12:16:10.982] Epoch [0/30] Iteration [840/2976]: Loss: 0.5748, CE: 0.0646
[12:16:16.434] Epoch [0/30] Iteration [850/2976]: Loss: 0.6175, CE: 0.1682
[12:16:21.923] Epoch [0/30] Iteration [860/2976]: Loss: 0.5943, CE: 0.1105
[12:16:27.396] Epoch [0/30] Iteration [870/2976]: Loss: 0.5598, CE: 0.0420
[12:16:32.961] Epoch [0/30] Iteration [880/2976]: Loss: 0.5949, CE: 0.1139
[12:16:38.422] Epoch [0/30] Iteration [890/2976]: Loss: 0.5837, CE: 0.0854
[12:16:43.870] Epoch [0/30] Iteration [900/2976]: Loss: 0.5926, CE: 0.1064
[12:16:49.331] Epoch [0/30] Iteration [910/2976]: Loss: 0.5831, CE: 0.0832
[12:16:54.788] Epoch [0/30] Iteration [920/2976]: Loss: 0.5888, CE: 0.1255
[12:17:00.234] Epoch [0/30] Iteration [930/2976]: Loss: 0.5665, CE: 0.0452
[12:17:05.696] Epoch [0/30] Iteration [940/2976]: Loss: 0.5695, CE: 0.0514
[12:17:11.145] Epoch [0/30] Iteration [950/2976]: Loss: 0.5931, CE: 0.1077
[12:17:16.617] Epoch [0/30] Iteration [960/2976]: Loss: 0.5943, CE: 0.1328
[12:17:22.057] Epoch [0/30] Iteration [970/2976]: Loss: 0.5852, CE: 0.0881
[12:17:27.518] Epoch [0/30] Iteration [980/2976]: Loss: 0.5766, CE: 0.0667
[12:17:32.960] Epoch [0/30] Iteration [990/2976]: Loss: 0.5775, CE: 0.0716
[12:17:38.421] Epoch [0/30] Iteration [1000/2976]: Loss: 0.6180, CE: 0.1704
[12:17:43.867] Epoch [0/30] Iteration [1010/2976]: Loss: 0.5891, CE: 0.1332
[12:17:49.352] Epoch [0/30] Iteration [1020/2976]: Loss: 0.5764, CE: 0.0663
[12:17:54.804] Epoch [0/30] Iteration [1030/2976]: Loss: 0.5896, CE: 0.1072
[12:18:00.274] Epoch [0/30] Iteration [1040/2976]: Loss: 0.5983, CE: 0.1205
[12:18:05.713] Epoch [0/30] Iteration [1050/2976]: Loss: 0.5848, CE: 0.0897
[12:18:11.185] Epoch [0/30] Iteration [1060/2976]: Loss: 0.5870, CE: 0.0928
[12:18:16.651] Epoch [0/30] Iteration [1070/2976]: Loss: 0.5531, CE: 0.0730
[12:18:22.108] Epoch [0/30] Iteration [1080/2976]: Loss: 0.5964, CE: 0.1258
[12:18:27.567] Epoch [0/30] Iteration [1090/2976]: Loss: 0.5749, CE: 0.0625
[12:18:33.031] Epoch [0/30] Iteration [1100/2976]: Loss: 0.5961, CE: 0.1173
[12:18:38.491] Epoch [0/30] Iteration [1110/2976]: Loss: 0.5776, CE: 0.0778
[12:18:43.959] Epoch [0/30] Iteration [1120/2976]: Loss: 0.5937, CE: 0.1112
[12:18:49.437] Epoch [0/30] Iteration [1130/2976]: Loss: 0.5960, CE: 0.1269
[12:18:54.897] Epoch [0/30] Iteration [1140/2976]: Loss: 0.5786, CE: 0.0716
[12:19:00.377] Epoch [0/30] Iteration [1150/2976]: Loss: 0.5754, CE: 0.0875
[12:19:05.875] Epoch [0/30] Iteration [1160/2976]: Loss: 0.5757, CE: 0.0657
[12:19:11.318] Epoch [0/30] Iteration [1170/2976]: Loss: 0.5963, CE: 0.1157
[12:19:16.800] Epoch [0/30] Iteration [1180/2976]: Loss: 0.6066, CE: 0.1412
[12:19:22.314] Epoch [0/30] Iteration [1190/2976]: Loss: 0.5763, CE: 0.0906
[12:19:27.787] Epoch [0/30] Iteration [1200/2976]: Loss: 0.5975, CE: 0.1184
[12:19:33.236] Epoch [0/30] Iteration [1210/2976]: Loss: 0.5877, CE: 0.0943
[12:19:38.743] Epoch [0/30] Iteration [1220/2976]: Loss: 0.6007, CE: 0.1264
[12:19:44.215] Epoch [0/30] Iteration [1230/2976]: Loss: 0.5823, CE: 0.0807
[12:19:49.689] Epoch [0/30] Iteration [1240/2976]: Loss: 0.6014, CE: 0.1454
[12:19:55.148] Epoch [0/30] Iteration [1250/2976]: Loss: 0.5847, CE: 0.0863
[12:20:00.640] Epoch [0/30] Iteration [1260/2976]: Loss: 0.5954, CE: 0.1133
[12:20:06.123] Epoch [0/30] Iteration [1270/2976]: Loss: 0.5963, CE: 0.1158
[12:20:11.611] Epoch [0/30] Iteration [1280/2976]: Loss: 0.6001, CE: 0.1250
[12:20:17.067] Epoch [0/30] Iteration [1290/2976]: Loss: 0.5751, CE: 0.0630
[12:20:22.545] Epoch [0/30] Iteration [1300/2976]: Loss: 0.5926, CE: 0.1067
[12:20:28.007] Epoch [0/30] Iteration [1310/2976]: Loss: 0.5845, CE: 0.0863
[12:20:33.493] Epoch [0/30] Iteration [1320/2976]: Loss: 0.5670, CE: 0.0671
[12:20:38.949] Epoch [0/30] Iteration [1330/2976]: Loss: 0.5780, CE: 0.0823
[12:20:44.471] Epoch [0/30] Iteration [1340/2976]: Loss: 0.5467, CE: 0.0508
[12:20:49.941] Epoch [0/30] Iteration [1350/2976]: Loss: 0.5590, CE: 0.0551
[12:20:55.424] Epoch [0/30] Iteration [1360/2976]: Loss: 0.5933, CE: 0.1081
[12:21:00.880] Epoch [0/30] Iteration [1370/2976]: Loss: 0.5775, CE: 0.0846
[12:21:06.356] Epoch [0/30] Iteration [1380/2976]: Loss: 0.5545, CE: 0.0763
[12:21:11.868] Epoch [0/30] Iteration [1390/2976]: Loss: 0.5867, CE: 0.0916
[12:21:17.326] Epoch [0/30] Iteration [1400/2976]: Loss: 0.6109, CE: 0.1518
[12:21:22.803] Epoch [0/30] Iteration [1410/2976]: Loss: 0.5752, CE: 0.0650
[12:21:28.264] Epoch [0/30] Iteration [1420/2976]: Loss: 0.5793, CE: 0.0738
[12:21:33.707] Epoch [0/30] Iteration [1430/2976]: Loss: 0.5735, CE: 0.0590
[12:21:39.156] Epoch [0/30] Iteration [1440/2976]: Loss: 0.6029, CE: 0.1321
[12:21:44.634] Epoch [0/30] Iteration [1450/2976]: Loss: 0.5857, CE: 0.0891
[12:21:50.144] Epoch [0/30] Iteration [1460/2976]: Loss: 0.5843, CE: 0.0855
[12:21:55.601] Epoch [0/30] Iteration [1470/2976]: Loss: 0.5829, CE: 0.0821
[12:22:01.058] Epoch [0/30] Iteration [1480/2976]: Loss: 0.5808, CE: 0.1054
[12:22:06.518] Epoch [0/30] Iteration [1490/2976]: Loss: 0.6086, CE: 0.1569
[12:22:11.984] Epoch [0/30] Iteration [1500/2976]: Loss: 0.5830, CE: 0.0829
[12:22:17.437] Epoch [0/30] Iteration [1510/2976]: Loss: 0.5840, CE: 0.0850
[12:22:22.926] Epoch [0/30] Iteration [1520/2976]: Loss: 0.6049, CE: 0.1373
[12:22:28.355] Epoch [0/30] Iteration [1530/2976]: Loss: 0.5860, CE: 0.0925
[12:22:33.867] Epoch [0/30] Iteration [1540/2976]: Loss: 0.5700, CE: 0.0535
[12:22:39.320] Epoch [0/30] Iteration [1550/2976]: Loss: 0.5761, CE: 0.0654
[12:22:44.781] Epoch [0/30] Iteration [1560/2976]: Loss: 0.5816, CE: 0.0900
[12:22:50.211] Epoch [0/30] Iteration [1570/2976]: Loss: 0.5921, CE: 0.1048
[12:22:55.704] Epoch [0/30] Iteration [1580/2976]: Loss: 0.5819, CE: 0.0798
[12:23:01.188] Epoch [0/30] Iteration [1590/2976]: Loss: 0.5818, CE: 0.0795
[12:40:16.857] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=30, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', dataset_fraction=0.3, enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=True, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[12:40:16.857] Continual Learning: 9 -> 12 classes
[12:40:16.857] Task offset: 9, New classes: 4
[12:40:16.857] Dataset fraction: 0.3
[12:40:16.857] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[12:40:16.857] Old task classes: 9, New task classes: 4
[12:40:16.904] Identified output layer keys: ['cswin_unet.output.weight']
[12:40:16.905] Loaded 462 backbone layers from pretrained model.
[12:40:16.905] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[12:40:16.939] Successfully adapted model for continual learning.
[12:40:16.940] Pretrained weights loaded and adapted for continual learning
[12:40:16.965] TPGM enabled - saved model state
[12:40:16.976] Using 28566/95221 samples (30.00% of dataset)
[12:40:26.804] TPGM initialized for continual learning with exclude list: ['cswin_unet.output.weight']
[12:40:26.806] Surgical fine-tuning enabled for continual learning
[12:40:26.807] 893 iterations per epoch. 26790 max iterations
[12:40:26.808] Updating surgical weights at epoch 0
[12:40:50.338] Surgical weights range: [0.0002, 1.0000]
[12:40:50.338] Applying TPGM projection update at epoch 0
[12:40:59.860] TPGM iteration 19/50 completed, loss: 0.1438
[12:41:09.796] TPGM iteration 39/50 completed, loss: 0.1374
[12:41:35.857] Epoch [0/30] Iteration [0/893]: Loss: 0.5893, CE: 0.0977
[12:41:41.331] Epoch [0/30] Iteration [10/893]: Loss: 0.5947, CE: 0.1112
[12:41:46.787] Epoch [0/30] Iteration [20/893]: Loss: 0.5923, CE: 0.1050
[12:41:52.231] Epoch [0/30] Iteration [30/893]: Loss: 0.5855, CE: 0.1099
[12:41:57.647] Epoch [0/30] Iteration [40/893]: Loss: 0.5857, CE: 0.0886
[12:42:03.057] Epoch [0/30] Iteration [50/893]: Loss: 0.6032, CE: 0.1322
[12:42:08.503] Epoch [0/30] Iteration [60/893]: Loss: 0.6110, CE: 0.1516
[12:42:13.909] Epoch [0/30] Iteration [70/893]: Loss: 0.5897, CE: 0.0997
[12:42:19.373] Epoch [0/30] Iteration [80/893]: Loss: 0.5858, CE: 0.0895
[12:42:24.786] Epoch [0/30] Iteration [90/893]: Loss: 0.6164, CE: 0.1648
[12:42:30.234] Epoch [0/30] Iteration [100/893]: Loss: 0.6002, CE: 0.1270
[12:42:35.667] Epoch [0/30] Iteration [110/893]: Loss: 0.6165, CE: 0.1656
[12:42:41.086] Epoch [0/30] Iteration [120/893]: Loss: 0.6135, CE: 0.1580
[12:42:46.537] Epoch [0/30] Iteration [130/893]: Loss: 0.5836, CE: 0.0836
[12:42:52.010] Epoch [0/30] Iteration [140/893]: Loss: 0.5901, CE: 0.0996
[12:42:57.440] Epoch [0/30] Iteration [150/893]: Loss: 0.5855, CE: 0.1175
[12:43:02.886] Epoch [0/30] Iteration [160/893]: Loss: 0.5824, CE: 0.0808
[12:43:08.325] Epoch [0/30] Iteration [170/893]: Loss: 0.5830, CE: 0.0829
[12:43:13.771] Epoch [0/30] Iteration [180/893]: Loss: 0.6058, CE: 0.1390
[12:43:19.239] Epoch [0/30] Iteration [190/893]: Loss: 0.6287, CE: 0.1958
[12:43:24.693] Epoch [0/30] Iteration [200/893]: Loss: 0.6183, CE: 0.1708
[12:43:30.165] Epoch [0/30] Iteration [210/893]: Loss: 0.5970, CE: 0.1170
[12:43:35.603] Epoch [0/30] Iteration [220/893]: Loss: 0.5852, CE: 0.1163
[12:43:41.060] Epoch [0/30] Iteration [230/893]: Loss: 0.5824, CE: 0.0821
[12:43:46.549] Epoch [0/30] Iteration [240/893]: Loss: 0.6123, CE: 0.1551
[12:43:52.006] Epoch [0/30] Iteration [250/893]: Loss: 0.5819, CE: 0.0966
[12:43:57.473] Epoch [0/30] Iteration [260/893]: Loss: 0.5792, CE: 0.0981
[12:44:02.927] Epoch [0/30] Iteration [270/893]: Loss: 0.5853, CE: 0.0879
[12:44:08.387] Epoch [0/30] Iteration [280/893]: Loss: 0.6226, CE: 0.1807
[12:44:13.836] Epoch [0/30] Iteration [290/893]: Loss: 0.5734, CE: 0.0584
[12:44:19.313] Epoch [0/30] Iteration [300/893]: Loss: 0.6400, CE: 0.2243
[12:44:24.768] Epoch [0/30] Iteration [310/893]: Loss: 0.5975, CE: 0.1181
[12:44:30.252] Epoch [0/30] Iteration [320/893]: Loss: 0.5724, CE: 0.0573
[12:44:35.700] Epoch [0/30] Iteration [330/893]: Loss: 0.5981, CE: 0.1371
[12:44:41.172] Epoch [0/30] Iteration [340/893]: Loss: 0.6172, CE: 0.1670
[12:44:46.634] Epoch [0/30] Iteration [350/893]: Loss: 0.5700, CE: 0.0498
[12:44:52.088] Epoch [0/30] Iteration [360/893]: Loss: 0.6001, CE: 0.1254
[12:44:57.567] Epoch [0/30] Iteration [370/893]: Loss: 0.6239, CE: 0.1869
[12:45:03.019] Epoch [0/30] Iteration [380/893]: Loss: 0.5643, CE: 0.1280
[12:45:08.476] Epoch [0/30] Iteration [390/893]: Loss: 0.5954, CE: 0.1145
[12:45:13.928] Epoch [0/30] Iteration [400/893]: Loss: 0.5749, CE: 0.0725
[12:45:19.376] Epoch [0/30] Iteration [410/893]: Loss: 0.6040, CE: 0.1346
[12:45:24.830] Epoch [0/30] Iteration [420/893]: Loss: 0.5862, CE: 0.0902
[12:45:30.289] Epoch [0/30] Iteration [430/893]: Loss: 0.5758, CE: 0.0660
[12:45:35.764] Epoch [0/30] Iteration [440/893]: Loss: 0.5888, CE: 0.0966
[12:45:41.302] Epoch [0/30] Iteration [450/893]: Loss: 0.6048, CE: 0.1365
[12:45:46.814] Epoch [0/30] Iteration [460/893]: Loss: 0.5851, CE: 0.1354
[12:45:52.291] Epoch [0/30] Iteration [470/893]: Loss: 0.5742, CE: 0.0634
[12:45:57.748] Epoch [0/30] Iteration [480/893]: Loss: 0.5872, CE: 0.0926
[12:46:03.216] Epoch [0/30] Iteration [490/893]: Loss: 0.6093, CE: 0.1599
[12:46:08.718] Epoch [0/30] Iteration [500/893]: Loss: 0.5960, CE: 0.1144
[12:46:14.196] Epoch [0/30] Iteration [510/893]: Loss: 0.5939, CE: 0.1105
[12:46:19.673] Epoch [0/30] Iteration [520/893]: Loss: 0.5871, CE: 0.1363
[12:46:25.135] Epoch [0/30] Iteration [530/893]: Loss: 0.5903, CE: 0.1006
[12:46:30.616] Epoch [0/30] Iteration [540/893]: Loss: 0.5592, CE: 0.0391
[12:46:36.072] Epoch [0/30] Iteration [550/893]: Loss: 0.6082, CE: 0.1449
[12:46:41.551] Epoch [0/30] Iteration [560/893]: Loss: 0.5976, CE: 0.1198
[12:46:46.997] Epoch [0/30] Iteration [570/893]: Loss: 0.6162, CE: 0.1646
[12:46:52.463] Epoch [0/30] Iteration [580/893]: Loss: 0.5775, CE: 0.1053
[12:46:57.915] Epoch [0/30] Iteration [590/893]: Loss: 0.5945, CE: 0.1107
[12:47:03.389] Epoch [0/30] Iteration [600/893]: Loss: 0.5958, CE: 0.1155
[12:47:08.850] Epoch [0/30] Iteration [610/893]: Loss: 0.5944, CE: 0.1105
[12:47:14.330] Epoch [0/30] Iteration [620/893]: Loss: 0.5833, CE: 0.0893
[12:47:19.789] Epoch [0/30] Iteration [630/893]: Loss: 0.5936, CE: 0.1086
[12:47:25.260] Epoch [0/30] Iteration [640/893]: Loss: 0.5715, CE: 0.0604
[12:47:30.742] Epoch [0/30] Iteration [650/893]: Loss: 0.5872, CE: 0.0925
[12:47:36.210] Epoch [0/30] Iteration [660/893]: Loss: 0.5762, CE: 0.0706
[12:47:41.722] Epoch [0/30] Iteration [670/893]: Loss: 0.5723, CE: 0.0558
[12:47:47.194] Epoch [0/30] Iteration [680/893]: Loss: 0.6003, CE: 0.1253
[12:47:52.658] Epoch [0/30] Iteration [690/893]: Loss: 0.5960, CE: 0.1143
[12:47:58.135] Epoch [0/30] Iteration [700/893]: Loss: 0.5838, CE: 0.0839
[12:48:03.603] Epoch [0/30] Iteration [710/893]: Loss: 0.5896, CE: 0.0986
[12:48:09.076] Epoch [0/30] Iteration [720/893]: Loss: 0.5667, CE: 0.0882
[12:48:14.543] Epoch [0/30] Iteration [730/893]: Loss: 0.5740, CE: 0.0599
[12:48:19.997] Epoch [0/30] Iteration [740/893]: Loss: 0.5756, CE: 0.0900
[12:48:25.440] Epoch [0/30] Iteration [750/893]: Loss: 0.5841, CE: 0.0852
[12:48:30.906] Epoch [0/30] Iteration [760/893]: Loss: 0.6101, CE: 0.1500
[12:48:36.375] Epoch [0/30] Iteration [770/893]: Loss: 0.5692, CE: 0.0705
[12:48:41.891] Epoch [0/30] Iteration [780/893]: Loss: 0.5941, CE: 0.1097
[12:48:47.386] Epoch [0/30] Iteration [790/893]: Loss: 0.5848, CE: 0.0869
[12:48:52.998] Epoch [0/30] Iteration [800/893]: Loss: 0.5860, CE: 0.0915
[12:48:58.449] Epoch [0/30] Iteration [810/893]: Loss: 0.5850, CE: 0.0870
[12:49:03.928] Epoch [0/30] Iteration [820/893]: Loss: 0.5768, CE: 0.0687
[12:49:09.382] Epoch [0/30] Iteration [830/893]: Loss: 0.5875, CE: 0.0932
[12:49:14.848] Epoch [0/30] Iteration [840/893]: Loss: 0.5963, CE: 0.1155
[12:49:20.321] Epoch [0/30] Iteration [850/893]: Loss: 0.5951, CE: 0.1591
[12:49:25.812] Epoch [0/30] Iteration [860/893]: Loss: 0.5870, CE: 0.0923
[12:49:31.299] Epoch [0/30] Iteration [870/893]: Loss: 0.5848, CE: 0.0864
[12:49:36.880] Epoch [0/30] Iteration [880/893]: Loss: 0.5942, CE: 0.1100
[12:49:42.329] Epoch [0/30] Iteration [890/893]: Loss: 0.5655, CE: 0.0494
[12:49:44.025] Epoch [0/30] Average Loss: 0.5916, CE: 0.1079, Dice: 0.9141
[12:50:05.085] Epoch [1/30] Iteration [0/893]: Loss: 0.6061, CE: 0.1396
[12:50:10.533] Epoch [1/30] Iteration [10/893]: Loss: 0.5584, CE: 0.0527
[12:50:15.999] Epoch [1/30] Iteration [20/893]: Loss: 0.5869, CE: 0.1236
[12:50:21.453] Epoch [1/30] Iteration [30/893]: Loss: 0.5956, CE: 0.1152
[12:50:26.901] Epoch [1/30] Iteration [40/893]: Loss: 0.6041, CE: 0.1344
[12:50:32.357] Epoch [1/30] Iteration [50/893]: Loss: 0.5771, CE: 0.0681
[12:50:37.815] Epoch [1/30] Iteration [60/893]: Loss: 0.5927, CE: 0.1068
[12:50:43.277] Epoch [1/30] Iteration [70/893]: Loss: 0.5604, CE: 0.0287
[12:50:48.754] Epoch [1/30] Iteration [80/893]: Loss: 0.5603, CE: 0.0423
[12:50:54.220] Epoch [1/30] Iteration [90/893]: Loss: 0.5626, CE: 0.0886
[12:50:59.671] Epoch [1/30] Iteration [100/893]: Loss: 0.5975, CE: 0.1209
[12:51:05.150] Epoch [1/30] Iteration [110/893]: Loss: 0.5850, CE: 0.0872
[12:51:10.591] Epoch [1/30] Iteration [120/893]: Loss: 0.6072, CE: 0.1440
[12:51:16.054] Epoch [1/30] Iteration [130/893]: Loss: 0.6005, CE: 0.1256
[12:51:21.491] Epoch [1/30] Iteration [140/893]: Loss: 0.6023, CE: 0.1302
[12:51:26.963] Epoch [1/30] Iteration [150/893]: Loss: 0.5898, CE: 0.1018
[12:51:32.395] Epoch [1/30] Iteration [160/893]: Loss: 0.5742, CE: 0.0640
[12:51:37.866] Epoch [1/30] Iteration [170/893]: Loss: 0.6106, CE: 0.1504
[12:51:43.300] Epoch [1/30] Iteration [180/893]: Loss: 0.5944, CE: 0.1104
[12:51:48.787] Epoch [1/30] Iteration [190/893]: Loss: 0.5818, CE: 0.0791
[12:51:54.237] Epoch [1/30] Iteration [200/893]: Loss: 0.5714, CE: 0.0755
[12:51:59.744] Epoch [1/30] Iteration [210/893]: Loss: 0.5976, CE: 0.1190
[12:52:05.185] Epoch [1/30] Iteration [220/893]: Loss: 0.5979, CE: 0.1195
[12:52:10.631] Epoch [1/30] Iteration [230/893]: Loss: 0.5757, CE: 0.0652
[12:52:16.077] Epoch [1/30] Iteration [240/893]: Loss: 0.5859, CE: 0.0894
[12:52:21.541] Epoch [1/30] Iteration [250/893]: Loss: 0.6153, CE: 0.1626
[12:52:27.010] Epoch [1/30] Iteration [260/893]: Loss: 0.5870, CE: 0.0921
[12:52:32.467] Epoch [1/30] Iteration [270/893]: Loss: 0.5996, CE: 0.1237
[12:52:37.915] Epoch [1/30] Iteration [280/893]: Loss: 0.6264, CE: 0.1904
[12:52:43.374] Epoch [1/30] Iteration [290/893]: Loss: 0.5832, CE: 0.0854
[12:52:48.820] Epoch [1/30] Iteration [300/893]: Loss: 0.5996, CE: 0.1233
[12:52:54.289] Epoch [1/30] Iteration [310/893]: Loss: 0.5959, CE: 0.1142
[12:52:59.752] Epoch [1/30] Iteration [320/893]: Loss: 0.6063, CE: 0.1406
[12:53:05.215] Epoch [1/30] Iteration [330/893]: Loss: 0.6077, CE: 0.1451
[12:53:10.669] Epoch [1/30] Iteration [340/893]: Loss: 0.5921, CE: 0.1051
[12:53:16.145] Epoch [1/30] Iteration [350/893]: Loss: 0.5923, CE: 0.1062
[12:53:21.583] Epoch [1/30] Iteration [360/893]: Loss: 0.5625, CE: 0.0801
[12:53:27.044] Epoch [1/30] Iteration [370/893]: Loss: 0.6041, CE: 0.1345
[12:53:32.517] Epoch [1/30] Iteration [380/893]: Loss: 0.6114, CE: 0.1529
[12:53:38.010] Epoch [1/30] Iteration [390/893]: Loss: 0.6007, CE: 0.1260
[12:53:43.453] Epoch [1/30] Iteration [400/893]: Loss: 0.5644, CE: 0.1169
[12:53:48.905] Epoch [1/30] Iteration [410/893]: Loss: 0.5781, CE: 0.0703
[12:53:54.349] Epoch [1/30] Iteration [420/893]: Loss: 0.6006, CE: 0.1259
[12:53:59.869] Epoch [1/30] Iteration [430/893]: Loss: 0.5760, CE: 0.0649
[12:54:05.321] Epoch [1/30] Iteration [440/893]: Loss: 0.5879, CE: 0.0945
[12:54:10.800] Epoch [1/30] Iteration [450/893]: Loss: 0.5586, CE: 0.0293
[12:54:16.238] Epoch [1/30] Iteration [460/893]: Loss: 0.5906, CE: 0.1011
[12:54:21.699] Epoch [1/30] Iteration [470/893]: Loss: 0.5963, CE: 0.1154
[12:54:27.146] Epoch [1/30] Iteration [480/893]: Loss: 0.5969, CE: 0.1182
[12:54:32.636] Epoch [1/30] Iteration [490/893]: Loss: 0.5734, CE: 0.0677
[12:54:38.091] Epoch [1/30] Iteration [500/893]: Loss: 0.5884, CE: 0.0963
[12:54:43.552] Epoch [1/30] Iteration [510/893]: Loss: 0.5971, CE: 0.1171
[12:54:48.997] Epoch [1/30] Iteration [520/893]: Loss: 0.5955, CE: 0.1130
[12:54:54.467] Epoch [1/30] Iteration [530/893]: Loss: 0.5922, CE: 0.1083
[12:54:59.935] Epoch [1/30] Iteration [540/893]: Loss: 0.6039, CE: 0.1341
[12:55:05.424] Epoch [1/30] Iteration [550/893]: Loss: 0.5878, CE: 0.0941
[12:55:10.917] Epoch [1/30] Iteration [560/893]: Loss: 0.5832, CE: 0.0841
[12:55:16.369] Epoch [1/30] Iteration [570/893]: Loss: 0.5916, CE: 0.1049
[12:55:21.836] Epoch [1/30] Iteration [580/893]: Loss: 0.6005, CE: 0.1257
[12:55:27.350] Epoch [1/30] Iteration [590/893]: Loss: 0.5897, CE: 0.0987
[12:55:32.997] Epoch [1/30] Iteration [600/893]: Loss: 0.5826, CE: 0.0831
[12:55:38.712] Epoch [1/30] Iteration [610/893]: Loss: 0.6009, CE: 0.1269
[12:55:44.227] Epoch [1/30] Iteration [620/893]: Loss: 0.5874, CE: 0.0930
[12:55:49.761] Epoch [1/30] Iteration [630/893]: Loss: 0.5960, CE: 0.1145
[12:55:55.313] Epoch [1/30] Iteration [640/893]: Loss: 0.5674, CE: 0.0440
[12:56:00.875] Epoch [1/30] Iteration [650/893]: Loss: 0.6058, CE: 0.1390
[12:56:06.399] Epoch [1/30] Iteration [660/893]: Loss: 0.5948, CE: 0.1163
[12:56:11.935] Epoch [1/30] Iteration [670/893]: Loss: 0.5736, CE: 0.0603
[12:56:17.555] Epoch [1/30] Iteration [680/893]: Loss: 0.5745, CE: 0.0609
[12:56:23.194] Epoch [1/30] Iteration [690/893]: Loss: 0.5958, CE: 0.1143
[12:56:28.706] Epoch [1/30] Iteration [700/893]: Loss: 0.5894, CE: 0.1147
[12:56:34.221] Epoch [1/30] Iteration [710/893]: Loss: 0.5988, CE: 0.1215
[12:56:39.744] Epoch [1/30] Iteration [720/893]: Loss: 0.5997, CE: 0.1243
[12:56:45.281] Epoch [1/30] Iteration [730/893]: Loss: 0.6063, CE: 0.1403
[12:56:50.804] Epoch [1/30] Iteration [740/893]: Loss: 0.5780, CE: 0.0695
[12:56:56.340] Epoch [1/30] Iteration [750/893]: Loss: 0.5861, CE: 0.0898
[12:57:01.838] Epoch [1/30] Iteration [760/893]: Loss: 0.5919, CE: 0.1041
[12:57:07.388] Epoch [1/30] Iteration [770/893]: Loss: 0.5921, CE: 0.1120
[12:57:12.898] Epoch [1/30] Iteration [780/893]: Loss: 0.5959, CE: 0.1142
[12:57:18.426] Epoch [1/30] Iteration [790/893]: Loss: 0.6080, CE: 0.1443
[12:57:23.963] Epoch [1/30] Iteration [800/893]: Loss: 0.6089, CE: 0.1476
[12:57:29.498] Epoch [1/30] Iteration [810/893]: Loss: 0.5851, CE: 0.0875
[12:57:35.109] Epoch [1/30] Iteration [820/893]: Loss: 0.5871, CE: 0.0934
[12:57:40.653] Epoch [1/30] Iteration [830/893]: Loss: 0.5842, CE: 0.0852
[12:57:46.229] Epoch [1/30] Iteration [840/893]: Loss: 0.5726, CE: 0.0562
[12:57:51.724] Epoch [1/30] Iteration [850/893]: Loss: 0.5728, CE: 0.0710
[12:57:57.251] Epoch [1/30] Iteration [860/893]: Loss: 0.5900, CE: 0.0994
[12:58:02.781] Epoch [1/30] Iteration [870/893]: Loss: 0.5603, CE: 0.0576
[12:58:08.396] Epoch [1/30] Iteration [880/893]: Loss: 0.6000, CE: 0.1244
[12:58:13.963] Epoch [1/30] Iteration [890/893]: Loss: 0.6022, CE: 0.1300
[12:58:15.700] Epoch [1/30] Average Loss: 0.5905, CE: 0.1049, Dice: 0.9142
[12:58:37.337] Epoch [2/30] Iteration [0/893]: Loss: 0.5882, CE: 0.0952
[12:58:42.828] Epoch [2/30] Iteration [10/893]: Loss: 0.5772, CE: 0.0688
[12:58:48.358] Epoch [2/30] Iteration [20/893]: Loss: 0.5866, CE: 0.0914
[12:58:53.868] Epoch [2/30] Iteration [30/893]: Loss: 0.5737, CE: 0.0643
[12:58:59.402] Epoch [2/30] Iteration [40/893]: Loss: 0.5917, CE: 0.1043
[12:59:04.924] Epoch [2/30] Iteration [50/893]: Loss: 0.5822, CE: 0.0802
[12:59:10.416] Epoch [2/30] Iteration [60/893]: Loss: 0.5816, CE: 0.0789
[12:59:15.948] Epoch [2/30] Iteration [70/893]: Loss: 0.6077, CE: 0.1438
[12:59:21.455] Epoch [2/30] Iteration [80/893]: Loss: 0.6045, CE: 0.1367
[12:59:26.983] Epoch [2/30] Iteration [90/893]: Loss: 0.5811, CE: 0.0783
[12:59:32.521] Epoch [2/30] Iteration [100/893]: Loss: 0.5739, CE: 0.0609
[12:59:37.977] Epoch [2/30] Iteration [110/893]: Loss: 0.5657, CE: 0.0394
[12:59:43.440] Epoch [2/30] Iteration [120/893]: Loss: 0.5818, CE: 0.0793
[12:59:48.949] Epoch [2/30] Iteration [130/893]: Loss: 0.6193, CE: 0.1725
[12:59:54.438] Epoch [2/30] Iteration [140/893]: Loss: 0.5995, CE: 0.1230
[12:59:59.882] Epoch [2/30] Iteration [150/893]: Loss: 0.5990, CE: 0.1218
[13:00:05.342] Epoch [2/30] Iteration [160/893]: Loss: 0.5876, CE: 0.0935
[13:00:10.799] Epoch [2/30] Iteration [170/893]: Loss: 0.5714, CE: 0.0566
[13:00:16.250] Epoch [2/30] Iteration [180/893]: Loss: 0.5964, CE: 0.1154
[13:00:21.699] Epoch [2/30] Iteration [190/893]: Loss: 0.5853, CE: 0.1152
[13:00:27.165] Epoch [2/30] Iteration [200/893]: Loss: 0.5728, CE: 0.0568
[13:00:32.597] Epoch [2/30] Iteration [210/893]: Loss: 0.6034, CE: 0.1458
[13:00:38.044] Epoch [2/30] Iteration [220/893]: Loss: 0.5994, CE: 0.1231
[13:00:43.494] Epoch [2/30] Iteration [230/893]: Loss: 0.5832, CE: 0.0827
[13:00:48.969] Epoch [2/30] Iteration [240/893]: Loss: 0.5767, CE: 0.0667
[13:00:54.409] Epoch [2/30] Iteration [250/893]: Loss: 0.6094, CE: 0.1476
[13:00:59.892] Epoch [2/30] Iteration [260/893]: Loss: 0.6320, CE: 0.2039
[13:01:05.344] Epoch [2/30] Iteration [270/893]: Loss: 0.5943, CE: 0.1105
[13:01:10.799] Epoch [2/30] Iteration [280/893]: Loss: 0.5692, CE: 0.0481
[13:01:16.321] Epoch [2/30] Iteration [290/893]: Loss: 0.5811, CE: 0.0818
[13:01:21.796] Epoch [2/30] Iteration [300/893]: Loss: 0.5894, CE: 0.0980
[13:01:27.241] Epoch [2/30] Iteration [310/893]: Loss: 0.6043, CE: 0.1373
[13:01:32.730] Epoch [2/30] Iteration [320/893]: Loss: 0.6050, CE: 0.1376
[13:01:38.196] Epoch [2/30] Iteration [330/893]: Loss: 0.5932, CE: 0.1100
[13:01:43.649] Epoch [2/30] Iteration [340/893]: Loss: 0.5838, CE: 0.0847
[13:01:49.107] Epoch [2/30] Iteration [350/893]: Loss: 0.5924, CE: 0.1053
[13:01:54.566] Epoch [2/30] Iteration [360/893]: Loss: 0.5950, CE: 0.1122
[13:02:00.037] Epoch [2/30] Iteration [370/893]: Loss: 0.5812, CE: 0.0923
[13:02:05.502] Epoch [2/30] Iteration [380/893]: Loss: 0.5771, CE: 0.0678
[13:02:10.946] Epoch [2/30] Iteration [390/893]: Loss: 0.6013, CE: 0.1276
[13:02:16.398] Epoch [2/30] Iteration [400/893]: Loss: 0.5902, CE: 0.0999
[13:02:21.855] Epoch [2/30] Iteration [410/893]: Loss: 0.5953, CE: 0.1129
[13:02:27.323] Epoch [2/30] Iteration [420/893]: Loss: 0.5893, CE: 0.0976
[13:02:32.771] Epoch [2/30] Iteration [430/893]: Loss: 0.5742, CE: 0.0624
[13:02:38.221] Epoch [2/30] Iteration [440/893]: Loss: 0.5708, CE: 0.0813
[13:02:43.674] Epoch [2/30] Iteration [450/893]: Loss: 0.5746, CE: 0.0824
[13:02:49.131] Epoch [2/30] Iteration [460/893]: Loss: 0.6071, CE: 0.1441
[13:02:54.584] Epoch [2/30] Iteration [470/893]: Loss: 0.6041, CE: 0.1389
[13:03:00.060] Epoch [2/30] Iteration [480/893]: Loss: 0.6444, CE: 0.2355
[13:03:05.521] Epoch [2/30] Iteration [490/893]: Loss: 0.5810, CE: 0.0772
[13:03:10.993] Epoch [2/30] Iteration [500/893]: Loss: 0.5729, CE: 0.0705
[13:03:16.441] Epoch [2/30] Iteration [510/893]: Loss: 0.6062, CE: 0.1401
[13:03:21.906] Epoch [2/30] Iteration [520/893]: Loss: 0.5855, CE: 0.0882
[13:03:27.332] Epoch [2/30] Iteration [530/893]: Loss: 0.5765, CE: 0.0660
[13:03:32.799] Epoch [2/30] Iteration [540/893]: Loss: 0.5902, CE: 0.0999
[13:03:38.245] Epoch [2/30] Iteration [550/893]: Loss: 0.5783, CE: 0.0710
[13:03:43.739] Epoch [2/30] Iteration [560/893]: Loss: 0.5882, CE: 0.0950
[13:03:49.188] Epoch [2/30] Iteration [570/893]: Loss: 0.5856, CE: 0.0886
[13:03:54.643] Epoch [2/30] Iteration [580/893]: Loss: 0.5770, CE: 0.0680
[13:04:00.106] Epoch [2/30] Iteration [590/893]: Loss: 0.5882, CE: 0.1003
[13:04:05.571] Epoch [2/30] Iteration [600/893]: Loss: 0.6011, CE: 0.1349
[13:04:11.033] Epoch [2/30] Iteration [610/893]: Loss: 0.5886, CE: 0.0961
[13:04:16.506] Epoch [2/30] Iteration [620/893]: Loss: 0.5799, CE: 0.0746
[13:04:21.965] Epoch [2/30] Iteration [630/893]: Loss: 0.6092, CE: 0.1474
[13:04:27.454] Epoch [2/30] Iteration [640/893]: Loss: 0.5822, CE: 0.0803
[13:04:32.898] Epoch [2/30] Iteration [650/893]: Loss: 0.6008, CE: 0.1514
[13:04:38.371] Epoch [2/30] Iteration [660/893]: Loss: 0.5882, CE: 0.0952
[13:04:43.857] Epoch [2/30] Iteration [670/893]: Loss: 0.6055, CE: 0.1386
[13:04:49.328] Epoch [2/30] Iteration [680/893]: Loss: 0.5860, CE: 0.0936
[13:04:54.774] Epoch [2/30] Iteration [690/893]: Loss: 0.5691, CE: 0.0504
[13:05:00.255] Epoch [2/30] Iteration [700/893]: Loss: 0.5831, CE: 0.0829
[13:05:05.711] Epoch [2/30] Iteration [710/893]: Loss: 0.6009, CE: 0.1268
[13:05:11.173] Epoch [2/30] Iteration [720/893]: Loss: 0.5683, CE: 0.0459
[13:05:16.673] Epoch [2/30] Iteration [730/893]: Loss: 0.6031, CE: 0.1362
[13:05:22.132] Epoch [2/30] Iteration [740/893]: Loss: 0.6029, CE: 0.1463
[13:05:27.585] Epoch [2/30] Iteration [750/893]: Loss: 0.5823, CE: 0.0805
[13:05:33.059] Epoch [2/30] Iteration [760/893]: Loss: 0.6031, CE: 0.1346
[13:05:38.508] Epoch [2/30] Iteration [770/893]: Loss: 0.6079, CE: 0.1444
[13:05:44.004] Epoch [2/30] Iteration [780/893]: Loss: 0.6068, CE: 0.1414
[13:05:49.466] Epoch [2/30] Iteration [790/893]: Loss: 0.5981, CE: 0.1199
[13:05:54.933] Epoch [2/30] Iteration [800/893]: Loss: 0.5734, CE: 0.0645
[13:06:00.384] Epoch [2/30] Iteration [810/893]: Loss: 0.6046, CE: 0.1360
[13:06:05.860] Epoch [2/30] Iteration [820/893]: Loss: 0.5950, CE: 0.1119
[13:06:11.325] Epoch [2/30] Iteration [830/893]: Loss: 0.5926, CE: 0.1059
[13:06:16.805] Epoch [2/30] Iteration [840/893]: Loss: 0.5820, CE: 0.0798
[13:06:22.260] Epoch [2/30] Iteration [850/893]: Loss: 0.5823, CE: 0.0808
[13:06:27.736] Epoch [2/30] Iteration [860/893]: Loss: 0.5854, CE: 0.0881
[13:06:33.194] Epoch [2/30] Iteration [870/893]: Loss: 0.5808, CE: 0.0779
[13:06:38.680] Epoch [2/30] Iteration [880/893]: Loss: 0.5822, CE: 0.0803
[13:06:44.133] Epoch [2/30] Iteration [890/893]: Loss: 0.5947, CE: 0.1114
[13:06:45.838] Epoch [2/30] Average Loss: 0.5907, CE: 0.1053, Dice: 0.9143
[13:07:07.172] Epoch [3/30] Iteration [0/893]: Loss: 0.5908, CE: 0.1017
[13:07:12.638] Epoch [3/30] Iteration [10/893]: Loss: 0.5765, CE: 0.0756
[13:07:18.097] Epoch [3/30] Iteration [20/893]: Loss: 0.5935, CE: 0.1082
[13:07:23.543] Epoch [3/30] Iteration [30/893]: Loss: 0.6082, CE: 0.1448
[13:07:28.990] Epoch [3/30] Iteration [40/893]: Loss: 0.5852, CE: 0.0879
[13:07:34.444] Epoch [3/30] Iteration [50/893]: Loss: 0.5816, CE: 0.0786
[13:07:39.891] Epoch [3/30] Iteration [60/893]: Loss: 0.5765, CE: 0.0660
[13:07:45.346] Epoch [3/30] Iteration [70/893]: Loss: 0.5758, CE: 0.0659
[13:07:50.821] Epoch [3/30] Iteration [80/893]: Loss: 0.5829, CE: 0.0822
[13:07:56.297] Epoch [3/30] Iteration [90/893]: Loss: 0.5850, CE: 0.0870
[13:08:01.747] Epoch [3/30] Iteration [100/893]: Loss: 0.5456, CE: 0.0635
[13:08:07.210] Epoch [3/30] Iteration [110/893]: Loss: 0.5997, CE: 0.1236
[13:08:12.646] Epoch [3/30] Iteration [120/893]: Loss: 0.6068, CE: 0.1412
[13:08:18.135] Epoch [3/30] Iteration [130/893]: Loss: 0.5958, CE: 0.1140
[13:08:23.581] Epoch [3/30] Iteration [140/893]: Loss: 0.5942, CE: 0.1486
[13:08:29.053] Epoch [3/30] Iteration [150/893]: Loss: 0.5850, CE: 0.0872
[13:08:34.509] Epoch [3/30] Iteration [160/893]: Loss: 0.5912, CE: 0.1029
[13:08:39.960] Epoch [3/30] Iteration [170/893]: Loss: 0.5863, CE: 0.0902
[13:08:45.416] Epoch [3/30] Iteration [180/893]: Loss: 0.6146, CE: 0.1612
[13:08:50.888] Epoch [3/30] Iteration [190/893]: Loss: 0.5892, CE: 0.0983
[13:08:56.332] Epoch [3/30] Iteration [200/893]: Loss: 0.5934, CE: 0.1085
[13:09:01.870] Epoch [3/30] Iteration [210/893]: Loss: 0.5805, CE: 0.0834
[13:09:07.352] Epoch [3/30] Iteration [220/893]: Loss: 0.6048, CE: 0.1363
[13:09:12.841] Epoch [3/30] Iteration [230/893]: Loss: 0.5688, CE: 0.0495
[13:09:18.299] Epoch [3/30] Iteration [240/893]: Loss: 0.5871, CE: 0.1072
[13:09:23.746] Epoch [3/30] Iteration [250/893]: Loss: 0.5847, CE: 0.0863
[13:09:29.233] Epoch [3/30] Iteration [260/893]: Loss: 0.5880, CE: 0.0957
[13:09:34.691] Epoch [3/30] Iteration [270/893]: Loss: 0.5851, CE: 0.0873
[13:09:40.174] Epoch [3/30] Iteration [280/893]: Loss: 0.5931, CE: 0.1072
[13:09:45.684] Epoch [3/30] Iteration [290/893]: Loss: 0.5353, CE: 0.0489
[13:09:51.151] Epoch [3/30] Iteration [300/893]: Loss: 0.6018, CE: 0.1288
[13:09:56.637] Epoch [3/30] Iteration [310/893]: Loss: 0.5803, CE: 0.0783
[13:10:02.107] Epoch [3/30] Iteration [320/893]: Loss: 0.5887, CE: 0.1085
[13:10:07.561] Epoch [3/30] Iteration [330/893]: Loss: 0.6004, CE: 0.1318
[13:10:13.018] Epoch [3/30] Iteration [340/893]: Loss: 0.5824, CE: 0.0809
[13:10:18.472] Epoch [3/30] Iteration [350/893]: Loss: 0.6080, CE: 0.1444
[13:10:23.945] Epoch [3/30] Iteration [360/893]: Loss: 0.6143, CE: 0.1603
[13:10:29.417] Epoch [3/30] Iteration [370/893]: Loss: 0.5754, CE: 0.0649
[13:10:34.881] Epoch [3/30] Iteration [380/893]: Loss: 0.5932, CE: 0.1075
[13:10:40.364] Epoch [3/30] Iteration [390/893]: Loss: 0.5913, CE: 0.1027
[13:10:45.822] Epoch [3/30] Iteration [400/893]: Loss: 0.6062, CE: 0.1400
[13:10:51.304] Epoch [3/30] Iteration [410/893]: Loss: 0.6032, CE: 0.1328
[13:10:56.757] Epoch [3/30] Iteration [420/893]: Loss: 0.5983, CE: 0.1202
[13:11:02.238] Epoch [3/30] Iteration [430/893]: Loss: 0.6051, CE: 0.1370
[13:11:07.687] Epoch [3/30] Iteration [440/893]: Loss: 0.6009, CE: 0.1305
[13:11:13.158] Epoch [3/30] Iteration [450/893]: Loss: 0.5897, CE: 0.0988
[13:11:18.610] Epoch [3/30] Iteration [460/893]: Loss: 0.5964, CE: 0.1165
[13:11:24.086] Epoch [3/30] Iteration [470/893]: Loss: 0.5900, CE: 0.0995
[13:11:29.538] Epoch [3/30] Iteration [480/893]: Loss: 0.5787, CE: 0.0715
[13:11:35.003] Epoch [3/30] Iteration [490/893]: Loss: 0.5835, CE: 0.0837
[13:11:40.484] Epoch [3/30] Iteration [500/893]: Loss: 0.5831, CE: 0.0828
[13:11:45.963] Epoch [3/30] Iteration [510/893]: Loss: 0.5696, CE: 0.0557
[13:11:51.417] Epoch [3/30] Iteration [520/893]: Loss: 0.5953, CE: 0.1128
[13:11:56.879] Epoch [3/30] Iteration [530/893]: Loss: 0.5789, CE: 0.0718
[13:12:02.348] Epoch [3/30] Iteration [540/893]: Loss: 0.5776, CE: 0.0686
[13:12:07.823] Epoch [3/30] Iteration [550/893]: Loss: 0.5771, CE: 0.0679
[13:12:13.280] Epoch [3/30] Iteration [560/893]: Loss: 0.5779, CE: 0.0698
[13:12:18.729] Epoch [3/30] Iteration [570/893]: Loss: 0.5986, CE: 0.1228
[13:12:24.183] Epoch [3/30] Iteration [580/893]: Loss: 0.5820, CE: 0.0832
[13:12:29.740] Epoch [3/30] Iteration [590/893]: Loss: 0.5980, CE: 0.1194
[13:12:35.193] Epoch [3/30] Iteration [600/893]: Loss: 0.5925, CE: 0.1060
[13:12:40.659] Epoch [3/30] Iteration [610/893]: Loss: 0.6053, CE: 0.1379
[13:12:46.122] Epoch [3/30] Iteration [620/893]: Loss: 0.5858, CE: 0.0889
[13:12:51.575] Epoch [3/30] Iteration [630/893]: Loss: 0.5860, CE: 0.1078
[13:12:57.081] Epoch [3/30] Iteration [640/893]: Loss: 0.5949, CE: 0.1120
[13:13:02.537] Epoch [3/30] Iteration [650/893]: Loss: 0.6294, CE: 0.1974
[13:13:08.001] Epoch [3/30] Iteration [660/893]: Loss: 0.5911, CE: 0.1024
[13:13:13.468] Epoch [3/30] Iteration [670/893]: Loss: 0.6085, CE: 0.1462
[13:13:18.958] Epoch [3/30] Iteration [680/893]: Loss: 0.5816, CE: 0.0788
[13:13:24.425] Epoch [3/30] Iteration [690/893]: Loss: 0.5752, CE: 0.0633
[13:13:29.886] Epoch [3/30] Iteration [700/893]: Loss: 0.6019, CE: 0.1364
[13:13:35.338] Epoch [3/30] Iteration [710/893]: Loss: 0.5673, CE: 0.0456
[13:13:40.915] Epoch [3/30] Iteration [720/893]: Loss: 0.5869, CE: 0.0920
[13:13:46.378] Epoch [3/30] Iteration [730/893]: Loss: 0.5944, CE: 0.1102
[13:13:51.825] Epoch [3/30] Iteration [740/893]: Loss: 0.5867, CE: 0.0914
[13:13:57.291] Epoch [3/30] Iteration [750/893]: Loss: 0.5795, CE: 0.0743
[13:14:02.747] Epoch [3/30] Iteration [760/893]: Loss: 0.5872, CE: 0.0981
[13:14:08.215] Epoch [3/30] Iteration [770/893]: Loss: 0.5730, CE: 0.0573
[13:14:13.681] Epoch [3/30] Iteration [780/893]: Loss: 0.6098, CE: 0.1489
[13:14:19.141] Epoch [3/30] Iteration [790/893]: Loss: 0.5763, CE: 0.0658
[13:14:24.617] Epoch [3/30] Iteration [800/893]: Loss: 0.5533, CE: 0.0656
[13:14:30.065] Epoch [3/30] Iteration [810/893]: Loss: 0.5885, CE: 0.0988
[13:14:35.538] Epoch [3/30] Iteration [820/893]: Loss: 0.5997, CE: 0.1235
[13:14:41.016] Epoch [3/30] Iteration [830/893]: Loss: 0.5493, CE: 0.0251
[13:14:46.458] Epoch [3/30] Iteration [840/893]: Loss: 0.5943, CE: 0.1157
[13:14:51.926] Epoch [3/30] Iteration [850/893]: Loss: 0.6090, CE: 0.1469
[13:14:57.377] Epoch [3/30] Iteration [860/893]: Loss: 0.5911, CE: 0.1021
[13:15:02.858] Epoch [3/30] Iteration [870/893]: Loss: 0.6695, CE: 0.2977
[13:15:08.363] Epoch [3/30] Iteration [880/893]: Loss: 0.5774, CE: 0.0689
[13:15:13.826] Epoch [3/30] Iteration [890/893]: Loss: 0.5932, CE: 0.1075
[13:15:15.527] Epoch [3/30] Average Loss: 0.5903, CE: 0.1049, Dice: 0.9140
[13:15:36.625] Epoch [4/30] Iteration [0/893]: Loss: 0.5808, CE: 0.0770
[13:15:42.061] Epoch [4/30] Iteration [10/893]: Loss: 0.5735, CE: 0.0588
[13:15:47.504] Epoch [4/30] Iteration [20/893]: Loss: 0.5677, CE: 0.0929
[13:15:52.959] Epoch [4/30] Iteration [30/893]: Loss: 0.5793, CE: 0.1060
[13:15:58.422] Epoch [4/30] Iteration [40/893]: Loss: 0.6056, CE: 0.1380
[13:16:03.871] Epoch [4/30] Iteration [50/893]: Loss: 0.5814, CE: 0.0784
[13:16:09.334] Epoch [4/30] Iteration [60/893]: Loss: 0.5930, CE: 0.1070
[13:16:14.794] Epoch [4/30] Iteration [70/893]: Loss: 0.5955, CE: 0.1132
[13:16:20.235] Epoch [4/30] Iteration [80/893]: Loss: 0.5834, CE: 0.0831
[13:16:25.695] Epoch [4/30] Iteration [90/893]: Loss: 0.5781, CE: 0.0706
[13:16:31.145] Epoch [4/30] Iteration [100/893]: Loss: 0.6102, CE: 0.1500
[13:16:36.588] Epoch [4/30] Iteration [110/893]: Loss: 0.5970, CE: 0.1234
[13:16:42.056] Epoch [4/30] Iteration [120/893]: Loss: 0.5779, CE: 0.0697
[13:16:47.532] Epoch [4/30] Iteration [130/893]: Loss: 0.5653, CE: 0.0387
[13:16:52.976] Epoch [4/30] Iteration [140/893]: Loss: 0.5918, CE: 0.1040
[13:16:58.444] Epoch [4/30] Iteration [150/893]: Loss: 0.5901, CE: 0.0997
[13:17:03.893] Epoch [4/30] Iteration [160/893]: Loss: 0.6043, CE: 0.1354
[13:17:09.340] Epoch [4/30] Iteration [170/893]: Loss: 0.5844, CE: 0.0859
[13:17:14.784] Epoch [4/30] Iteration [180/893]: Loss: 0.5951, CE: 0.1337
[13:17:20.260] Epoch [4/30] Iteration [190/893]: Loss: 0.5683, CE: 0.0459
[13:17:25.742] Epoch [4/30] Iteration [200/893]: Loss: 0.5817, CE: 0.1281
[13:17:31.214] Epoch [4/30] Iteration [210/893]: Loss: 0.5767, CE: 0.0677
[13:17:36.652] Epoch [4/30] Iteration [220/893]: Loss: 0.5857, CE: 0.0886
[13:17:42.150] Epoch [4/30] Iteration [230/893]: Loss: 0.5804, CE: 0.0759
[13:17:47.611] Epoch [4/30] Iteration [240/893]: Loss: 0.6211, CE: 0.1774
[13:17:53.080] Epoch [4/30] Iteration [250/893]: Loss: 0.5707, CE: 0.0526
[13:17:58.527] Epoch [4/30] Iteration [260/893]: Loss: 0.5912, CE: 0.1024
[13:18:03.999] Epoch [4/30] Iteration [270/893]: Loss: 0.5893, CE: 0.0980
[13:18:09.452] Epoch [4/30] Iteration [280/893]: Loss: 0.5836, CE: 0.0838
[13:18:14.920] Epoch [4/30] Iteration [290/893]: Loss: 0.5880, CE: 0.0945
[13:18:20.357] Epoch [4/30] Iteration [300/893]: Loss: 0.5899, CE: 0.0993
[13:18:25.814] Epoch [4/30] Iteration [310/893]: Loss: 0.5947, CE: 0.1815
[13:18:31.267] Epoch [4/30] Iteration [320/893]: Loss: 0.5768, CE: 0.0691
[13:18:36.718] Epoch [4/30] Iteration [330/893]: Loss: 0.5867, CE: 0.0947
[13:18:42.184] Epoch [4/30] Iteration [340/893]: Loss: 0.5959, CE: 0.1142
[13:18:47.631] Epoch [4/30] Iteration [350/893]: Loss: 0.6147, CE: 0.1618
[13:18:53.067] Epoch [4/30] Iteration [360/893]: Loss: 0.5960, CE: 0.1149
[13:18:58.526] Epoch [4/30] Iteration [370/893]: Loss: 0.5808, CE: 0.0767
[13:19:03.977] Epoch [4/30] Iteration [380/893]: Loss: 0.6101, CE: 0.1497
[13:19:09.476] Epoch [4/30] Iteration [390/893]: Loss: 0.5913, CE: 0.1024
[13:19:14.933] Epoch [4/30] Iteration [400/893]: Loss: 0.5756, CE: 0.0636
[13:19:20.388] Epoch [4/30] Iteration [410/893]: Loss: 0.5881, CE: 0.0948
[13:19:25.856] Epoch [4/30] Iteration [420/893]: Loss: 0.5982, CE: 0.1199
[13:19:31.322] Epoch [4/30] Iteration [430/893]: Loss: 0.6028, CE: 0.1313
[13:19:36.792] Epoch [4/30] Iteration [440/893]: Loss: 0.5638, CE: 0.0562
[13:19:42.277] Epoch [4/30] Iteration [450/893]: Loss: 0.5819, CE: 0.0794
[13:19:47.719] Epoch [4/30] Iteration [460/893]: Loss: 0.5858, CE: 0.0895
[13:19:53.196] Epoch [4/30] Iteration [470/893]: Loss: 0.6149, CE: 0.1616
[13:19:58.647] Epoch [4/30] Iteration [480/893]: Loss: 0.5834, CE: 0.0832
[13:20:04.148] Epoch [4/30] Iteration [490/893]: Loss: 0.5887, CE: 0.1074
[13:20:09.599] Epoch [4/30] Iteration [500/893]: Loss: 0.5919, CE: 0.1051
[13:20:15.081] Epoch [4/30] Iteration [510/893]: Loss: 0.5945, CE: 0.1109
[13:20:20.525] Epoch [4/30] Iteration [520/893]: Loss: 0.5863, CE: 0.0904
[13:20:26.005] Epoch [4/30] Iteration [530/893]: Loss: 0.5885, CE: 0.1016
[13:20:31.445] Epoch [4/30] Iteration [540/893]: Loss: 0.5822, CE: 0.0801
[13:20:36.909] Epoch [4/30] Iteration [550/893]: Loss: 0.6101, CE: 0.1498
[13:20:42.371] Epoch [4/30] Iteration [560/893]: Loss: 0.5783, CE: 0.0703
[13:20:47.833] Epoch [4/30] Iteration [570/893]: Loss: 0.5830, CE: 0.0825
[13:20:53.319] Epoch [4/30] Iteration [580/893]: Loss: 0.5940, CE: 0.1101
[13:20:58.767] Epoch [4/30] Iteration [590/893]: Loss: 0.6040, CE: 0.1346
[13:21:04.214] Epoch [4/30] Iteration [600/893]: Loss: 0.5828, CE: 0.0818
[13:21:09.694] Epoch [4/30] Iteration [610/893]: Loss: 0.5849, CE: 0.0901
[13:21:15.159] Epoch [4/30] Iteration [620/893]: Loss: 0.6029, CE: 0.1438
[13:21:20.629] Epoch [4/30] Iteration [630/893]: Loss: 0.5840, CE: 0.0940
[13:21:26.150] Epoch [4/30] Iteration [640/893]: Loss: 0.5776, CE: 0.0685
[13:21:31.653] Epoch [4/30] Iteration [650/893]: Loss: 0.5839, CE: 0.0844
[13:21:37.104] Epoch [4/30] Iteration [660/893]: Loss: 0.6062, CE: 0.1398
[13:21:42.573] Epoch [4/30] Iteration [670/893]: Loss: 0.5800, CE: 0.0746
[13:21:48.051] Epoch [4/30] Iteration [680/893]: Loss: 0.5892, CE: 0.0972
[13:21:53.509] Epoch [4/30] Iteration [690/893]: Loss: 0.6023, CE: 0.1301
[13:21:58.977] Epoch [4/30] Iteration [700/893]: Loss: 0.5773, CE: 0.0694
[13:22:04.436] Epoch [4/30] Iteration [710/893]: Loss: 0.5896, CE: 0.1247
[13:22:09.897] Epoch [4/30] Iteration [720/893]: Loss: 0.5814, CE: 0.0782
[13:22:15.367] Epoch [4/30] Iteration [730/893]: Loss: 0.5992, CE: 0.1239
[13:22:20.814] Epoch [4/30] Iteration [740/893]: Loss: 0.5981, CE: 0.1197
[13:22:26.307] Epoch [4/30] Iteration [750/893]: Loss: 0.6079, CE: 0.1477
[13:22:31.750] Epoch [4/30] Iteration [760/893]: Loss: 0.5901, CE: 0.0997
[13:22:37.206] Epoch [4/30] Iteration [770/893]: Loss: 0.6290, CE: 0.2004
[13:22:42.643] Epoch [4/30] Iteration [780/893]: Loss: 0.5836, CE: 0.0836
[13:22:48.120] Epoch [4/30] Iteration [790/893]: Loss: 0.5866, CE: 0.0912
[13:22:53.560] Epoch [4/30] Iteration [800/893]: Loss: 0.5945, CE: 0.1211
[13:22:59.040] Epoch [4/30] Iteration [810/893]: Loss: 0.6010, CE: 0.1270
[13:23:04.497] Epoch [4/30] Iteration [820/893]: Loss: 0.5819, CE: 0.0793
[13:23:09.959] Epoch [4/30] Iteration [830/893]: Loss: 0.5996, CE: 0.1237
[13:23:15.427] Epoch [4/30] Iteration [840/893]: Loss: 0.5944, CE: 0.1180
[13:23:20.891] Epoch [4/30] Iteration [850/893]: Loss: 0.5926, CE: 0.1063
[13:23:26.360] Epoch [4/30] Iteration [860/893]: Loss: 0.6034, CE: 0.1434
[13:23:31.837] Epoch [4/30] Iteration [870/893]: Loss: 0.5748, CE: 0.0621
[13:23:37.264] Epoch [4/30] Iteration [880/893]: Loss: 0.5953, CE: 0.1128
[13:23:42.709] Epoch [4/30] Iteration [890/893]: Loss: 0.5727, CE: 0.0651
[13:23:44.425] Epoch [4/30] Average Loss: 0.5904, CE: 0.1050, Dice: 0.9140
[13:23:44.425] Applying TPGM projection update at epoch 5
[13:23:53.651] TPGM iteration 19/50 completed, loss: 0.2057
[13:24:03.239] TPGM iteration 39/50 completed, loss: 0.0982
[13:24:29.871] Epoch [5/30] Iteration [0/893]: Loss: 0.6119, CE: 0.1640
[13:24:35.335] Epoch [5/30] Iteration [10/893]: Loss: 0.5959, CE: 0.1151
[13:24:40.829] Epoch [5/30] Iteration [20/893]: Loss: 0.6032, CE: 0.1321
[13:24:46.265] Epoch [5/30] Iteration [30/893]: Loss: 0.5893, CE: 0.0977
[13:24:51.714] Epoch [5/30] Iteration [40/893]: Loss: 0.6177, CE: 0.1714
[13:24:57.160] Epoch [5/30] Iteration [50/893]: Loss: 0.5872, CE: 0.0927
[13:25:02.598] Epoch [5/30] Iteration [60/893]: Loss: 0.5807, CE: 0.0764
[13:25:08.034] Epoch [5/30] Iteration [70/893]: Loss: 0.5714, CE: 0.0539
[13:25:13.477] Epoch [5/30] Iteration [80/893]: Loss: 0.5847, CE: 0.0865
[13:25:18.923] Epoch [5/30] Iteration [90/893]: Loss: 0.5858, CE: 0.0894
[13:25:24.363] Epoch [5/30] Iteration [100/893]: Loss: 0.5914, CE: 0.1030
[13:25:29.813] Epoch [5/30] Iteration [110/893]: Loss: 0.5831, CE: 0.0824
[13:25:35.272] Epoch [5/30] Iteration [120/893]: Loss: 0.5716, CE: 0.0550
[13:25:40.729] Epoch [5/30] Iteration [130/893]: Loss: 0.5755, CE: 0.0637
[13:25:46.234] Epoch [5/30] Iteration [140/893]: Loss: 0.5879, CE: 0.1342
[13:25:51.661] Epoch [5/30] Iteration [150/893]: Loss: 0.5773, CE: 0.0682
[13:25:57.150] Epoch [5/30] Iteration [160/893]: Loss: 0.5948, CE: 0.1115
[13:26:02.591] Epoch [5/30] Iteration [170/893]: Loss: 0.5997, CE: 0.1236
[13:26:08.038] Epoch [5/30] Iteration [180/893]: Loss: 0.6006, CE: 0.1258
[13:26:13.511] Epoch [5/30] Iteration [190/893]: Loss: 0.6040, CE: 0.1350
[13:26:18.960] Epoch [5/30] Iteration [200/893]: Loss: 0.5904, CE: 0.1005
[13:26:24.375] Epoch [5/30] Iteration [210/893]: Loss: 0.5826, CE: 0.0811
[13:26:29.834] Epoch [5/30] Iteration [220/893]: Loss: 0.6027, CE: 0.1314
[13:26:35.258] Epoch [5/30] Iteration [230/893]: Loss: 0.6065, CE: 0.1403
[13:26:40.733] Epoch [5/30] Iteration [240/893]: Loss: 0.5879, CE: 0.0942
[13:26:46.178] Epoch [5/30] Iteration [250/893]: Loss: 0.5891, CE: 0.0986
[13:26:51.616] Epoch [5/30] Iteration [260/893]: Loss: 0.6006, CE: 0.1261
[13:26:57.062] Epoch [5/30] Iteration [270/893]: Loss: 0.5869, CE: 0.0923
[13:27:02.499] Epoch [5/30] Iteration [280/893]: Loss: 0.5696, CE: 0.0759
[13:27:07.959] Epoch [5/30] Iteration [290/893]: Loss: 0.5872, CE: 0.0925
[13:27:13.398] Epoch [5/30] Iteration [300/893]: Loss: 0.5920, CE: 0.1045
[13:27:18.892] Epoch [5/30] Iteration [310/893]: Loss: 0.6020, CE: 0.1303
[13:27:24.342] Epoch [5/30] Iteration [320/893]: Loss: 0.6025, CE: 0.1309
[13:27:29.832] Epoch [5/30] Iteration [330/893]: Loss: 0.6164, CE: 0.1650
[13:27:35.289] Epoch [5/30] Iteration [340/893]: Loss: 0.5778, CE: 0.0691
[13:27:40.750] Epoch [5/30] Iteration [350/893]: Loss: 0.6020, CE: 0.1292
[13:27:46.237] Epoch [5/30] Iteration [360/893]: Loss: 0.5661, CE: 0.0426
[13:27:51.678] Epoch [5/30] Iteration [370/893]: Loss: 0.5803, CE: 0.0756
[13:27:57.146] Epoch [5/30] Iteration [380/893]: Loss: 0.5841, CE: 0.0847
[13:28:02.614] Epoch [5/30] Iteration [390/893]: Loss: 0.5513, CE: 0.0882
[13:28:08.095] Epoch [5/30] Iteration [400/893]: Loss: 0.5773, CE: 0.1283
[13:28:13.530] Epoch [5/30] Iteration [410/893]: Loss: 0.5983, CE: 0.1209
[13:28:18.989] Epoch [5/30] Iteration [420/893]: Loss: 0.5721, CE: 0.0552
[13:28:24.439] Epoch [5/30] Iteration [430/893]: Loss: 0.5788, CE: 0.0869
[13:28:29.912] Epoch [5/30] Iteration [440/893]: Loss: 0.5830, CE: 0.0823
[13:28:35.375] Epoch [5/30] Iteration [450/893]: Loss: 0.6026, CE: 0.1306
[13:28:40.917] Epoch [5/30] Iteration [460/893]: Loss: 0.6032, CE: 0.1327
[13:28:46.386] Epoch [5/30] Iteration [470/893]: Loss: 0.5811, CE: 0.0774
[13:28:51.844] Epoch [5/30] Iteration [480/893]: Loss: 0.5938, CE: 0.1117
[13:28:57.365] Epoch [5/30] Iteration [490/893]: Loss: 0.5952, CE: 0.1126
[13:29:02.843] Epoch [5/30] Iteration [500/893]: Loss: 0.6057, CE: 0.1388
[13:29:08.317] Epoch [5/30] Iteration [510/893]: Loss: 0.5672, CE: 0.0523
[13:29:13.816] Epoch [5/30] Iteration [520/893]: Loss: 0.5835, CE: 0.0839
[13:29:19.257] Epoch [5/30] Iteration [530/893]: Loss: 0.5807, CE: 0.0765
[13:29:24.711] Epoch [5/30] Iteration [540/893]: Loss: 0.5951, CE: 0.1283
[13:29:30.148] Epoch [5/30] Iteration [550/893]: Loss: 0.6146, CE: 0.1607
[13:29:35.603] Epoch [5/30] Iteration [560/893]: Loss: 0.5975, CE: 0.1198
[13:29:41.093] Epoch [5/30] Iteration [570/893]: Loss: 0.5917, CE: 0.1038
[13:29:46.549] Epoch [5/30] Iteration [580/893]: Loss: 0.5818, CE: 0.0791
[13:29:51.995] Epoch [5/30] Iteration [590/893]: Loss: 0.5970, CE: 0.1168
[13:29:57.468] Epoch [5/30] Iteration [600/893]: Loss: 0.6017, CE: 0.1413
[13:30:02.912] Epoch [5/30] Iteration [610/893]: Loss: 0.5761, CE: 0.1132
[13:30:08.357] Epoch [5/30] Iteration [620/893]: Loss: 0.5775, CE: 0.0883
[13:30:13.820] Epoch [5/30] Iteration [630/893]: Loss: 0.6072, CE: 0.1434
[13:30:19.295] Epoch [5/30] Iteration [640/893]: Loss: 0.5855, CE: 0.0893
[13:30:24.744] Epoch [5/30] Iteration [650/893]: Loss: 0.5812, CE: 0.0798
[13:30:30.192] Epoch [5/30] Iteration [660/893]: Loss: 0.6006, CE: 0.1261
[13:30:35.678] Epoch [5/30] Iteration [670/893]: Loss: 0.5817, CE: 0.0789
[13:30:41.159] Epoch [5/30] Iteration [680/893]: Loss: 0.6168, CE: 0.1663
[13:30:46.636] Epoch [5/30] Iteration [690/893]: Loss: 0.5917, CE: 0.1039
[13:30:52.113] Epoch [5/30] Iteration [700/893]: Loss: 0.5825, CE: 0.0808
[13:30:57.556] Epoch [5/30] Iteration [710/893]: Loss: 0.6730, CE: 0.3069
[13:31:03.017] Epoch [5/30] Iteration [720/893]: Loss: 0.5926, CE: 0.1060
[13:31:08.479] Epoch [5/30] Iteration [730/893]: Loss: 0.5276, CE: 0.0294
[13:31:13.941] Epoch [5/30] Iteration [740/893]: Loss: 0.5928, CE: 0.1098
[13:31:19.375] Epoch [5/30] Iteration [750/893]: Loss: 0.5697, CE: 0.0509
[13:31:24.846] Epoch [5/30] Iteration [760/893]: Loss: 0.5787, CE: 0.0718
[13:31:30.300] Epoch [5/30] Iteration [770/893]: Loss: 0.6028, CE: 0.1316
[13:31:35.755] Epoch [5/30] Iteration [780/893]: Loss: 0.6404, CE: 0.2389
[13:31:41.186] Epoch [5/30] Iteration [790/893]: Loss: 0.5789, CE: 0.0727
[13:31:46.632] Epoch [5/30] Iteration [800/893]: Loss: 0.5997, CE: 0.1236
[13:31:52.078] Epoch [5/30] Iteration [810/893]: Loss: 0.5734, CE: 0.0604
[13:31:57.547] Epoch [5/30] Iteration [820/893]: Loss: 0.5816, CE: 0.0933
[13:32:02.991] Epoch [5/30] Iteration [830/893]: Loss: 0.6052, CE: 0.1372
[13:32:08.455] Epoch [5/30] Iteration [840/893]: Loss: 0.5920, CE: 0.1045
[13:32:13.914] Epoch [5/30] Iteration [850/893]: Loss: 0.5895, CE: 0.0985
[13:32:19.387] Epoch [5/30] Iteration [860/893]: Loss: 0.5860, CE: 0.0899
[13:32:24.862] Epoch [5/30] Iteration [870/893]: Loss: 0.5858, CE: 0.0889
[13:32:30.341] Epoch [5/30] Iteration [880/893]: Loss: 0.5938, CE: 0.1092
[13:32:35.775] Epoch [5/30] Iteration [890/893]: Loss: 0.5685, CE: 0.0561
[13:32:37.458] Epoch [5/30] Average Loss: 0.5906, CE: 0.1053, Dice: 0.9141
[13:32:58.720] Epoch [6/30] Iteration [0/893]: Loss: 0.5753, CE: 0.0631
[13:33:04.167] Epoch [6/30] Iteration [10/893]: Loss: 0.5809, CE: 0.0770
[13:33:09.624] Epoch [6/30] Iteration [20/893]: Loss: 0.5725, CE: 0.0753
[13:33:15.072] Epoch [6/30] Iteration [30/893]: Loss: 0.5772, CE: 0.0727
[13:33:20.527] Epoch [6/30] Iteration [40/893]: Loss: 0.5878, CE: 0.0939
[13:33:25.978] Epoch [6/30] Iteration [50/893]: Loss: 0.6041, CE: 0.1687
[13:33:31.419] Epoch [6/30] Iteration [60/893]: Loss: 0.5790, CE: 0.0727
[13:33:36.880] Epoch [6/30] Iteration [70/893]: Loss: 0.6030, CE: 0.1320
[13:33:42.325] Epoch [6/30] Iteration [80/893]: Loss: 0.5803, CE: 0.0754
[13:33:47.751] Epoch [6/30] Iteration [90/893]: Loss: 0.5965, CE: 0.1157
[13:33:53.197] Epoch [6/30] Iteration [100/893]: Loss: 0.5905, CE: 0.1007
[13:33:58.671] Epoch [6/30] Iteration [110/893]: Loss: 0.5846, CE: 0.0863
[13:34:04.119] Epoch [6/30] Iteration [120/893]: Loss: 0.5977, CE: 0.1187
[13:34:09.589] Epoch [6/30] Iteration [130/893]: Loss: 0.6043, CE: 0.1350
[13:34:15.063] Epoch [6/30] Iteration [140/893]: Loss: 0.5782, CE: 0.0707
[13:34:20.506] Epoch [6/30] Iteration [150/893]: Loss: 0.6005, CE: 0.1257
[13:34:25.943] Epoch [6/30] Iteration [160/893]: Loss: 0.5756, CE: 0.0638
[13:34:31.409] Epoch [6/30] Iteration [170/893]: Loss: 0.5902, CE: 0.0999
[13:34:36.833] Epoch [6/30] Iteration [180/893]: Loss: 0.5863, CE: 0.0904
[13:34:42.305] Epoch [6/30] Iteration [190/893]: Loss: 0.6161, CE: 0.1672
[13:34:47.766] Epoch [6/30] Iteration [200/893]: Loss: 0.5714, CE: 0.0570
[13:34:53.215] Epoch [6/30] Iteration [210/893]: Loss: 0.6033, CE: 0.1326
[13:34:58.669] Epoch [6/30] Iteration [220/893]: Loss: 0.5816, CE: 0.0788
[13:35:04.110] Epoch [6/30] Iteration [230/893]: Loss: 0.5989, CE: 0.1218
[13:35:09.539] Epoch [6/30] Iteration [240/893]: Loss: 0.5741, CE: 0.0599
[13:35:14.975] Epoch [6/30] Iteration [250/893]: Loss: 0.5614, CE: 0.0818
[13:35:20.419] Epoch [6/30] Iteration [260/893]: Loss: 0.5942, CE: 0.1103
[13:35:25.864] Epoch [6/30] Iteration [270/893]: Loss: 0.5807, CE: 0.0767
[13:35:31.301] Epoch [6/30] Iteration [280/893]: Loss: 0.6041, CE: 0.1374
[13:35:36.760] Epoch [6/30] Iteration [290/893]: Loss: 0.5980, CE: 0.1194
[13:35:42.255] Epoch [6/30] Iteration [300/893]: Loss: 0.6090, CE: 0.1470
[13:35:47.825] Epoch [6/30] Iteration [310/893]: Loss: 0.5965, CE: 0.1161
[13:35:53.301] Epoch [6/30] Iteration [320/893]: Loss: 0.5692, CE: 0.0746
[13:35:58.738] Epoch [6/30] Iteration [330/893]: Loss: 0.5859, CE: 0.0896
[13:36:04.215] Epoch [6/30] Iteration [340/893]: Loss: 0.5862, CE: 0.1172
[13:36:09.661] Epoch [6/30] Iteration [350/893]: Loss: 0.6141, CE: 0.1625
[13:36:15.123] Epoch [6/30] Iteration [360/893]: Loss: 0.5743, CE: 0.0621
[13:36:20.593] Epoch [6/30] Iteration [370/893]: Loss: 0.5979, CE: 0.1192
[13:36:26.044] Epoch [6/30] Iteration [380/893]: Loss: 0.5792, CE: 0.0727
[13:36:31.525] Epoch [6/30] Iteration [390/893]: Loss: 0.5929, CE: 0.1068
[13:36:36.961] Epoch [6/30] Iteration [400/893]: Loss: 0.6082, CE: 0.1468
[13:36:42.394] Epoch [6/30] Iteration [410/893]: Loss: 0.6225, CE: 0.1802
[13:36:47.863] Epoch [6/30] Iteration [420/893]: Loss: 0.5809, CE: 0.0787
[13:36:53.336] Epoch [6/30] Iteration [430/893]: Loss: 0.6079, CE: 0.1441
[13:36:58.806] Epoch [6/30] Iteration [440/893]: Loss: 0.5879, CE: 0.0943
[13:37:04.259] Epoch [6/30] Iteration [450/893]: Loss: 0.5920, CE: 0.1044
[13:37:09.721] Epoch [6/30] Iteration [460/893]: Loss: 0.5948, CE: 0.1118
[13:37:15.194] Epoch [6/30] Iteration [470/893]: Loss: 0.6035, CE: 0.1331
[13:37:20.697] Epoch [6/30] Iteration [480/893]: Loss: 0.6053, CE: 0.1455
[13:37:26.207] Epoch [6/30] Iteration [490/893]: Loss: 0.6071, CE: 0.1420
[13:37:31.696] Epoch [6/30] Iteration [500/893]: Loss: 0.6108, CE: 0.1512
[13:37:37.153] Epoch [6/30] Iteration [510/893]: Loss: 0.6010, CE: 0.1268
[13:37:42.584] Epoch [6/30] Iteration [520/893]: Loss: 0.5775, CE: 0.0687
[13:37:48.034] Epoch [6/30] Iteration [530/893]: Loss: 0.5950, CE: 0.1120
[13:37:53.473] Epoch [6/30] Iteration [540/893]: Loss: 0.5920, CE: 0.1047
[13:37:58.959] Epoch [6/30] Iteration [550/893]: Loss: 0.5771, CE: 0.0776
[13:38:04.400] Epoch [6/30] Iteration [560/893]: Loss: 0.5654, CE: 0.0396
[13:38:09.872] Epoch [6/30] Iteration [570/893]: Loss: 0.5885, CE: 0.0960
[13:38:15.328] Epoch [6/30] Iteration [580/893]: Loss: 0.5995, CE: 0.1258
[13:38:20.790] Epoch [6/30] Iteration [590/893]: Loss: 0.5827, CE: 0.0819
[13:38:26.267] Epoch [6/30] Iteration [600/893]: Loss: 0.5856, CE: 0.0967
[13:38:31.810] Epoch [6/30] Iteration [610/893]: Loss: 0.5901, CE: 0.1014
[13:38:37.300] Epoch [6/30] Iteration [620/893]: Loss: 0.6072, CE: 0.1440
[13:38:42.823] Epoch [6/30] Iteration [630/893]: Loss: 0.5613, CE: 0.0635
[13:38:48.343] Epoch [6/30] Iteration [640/893]: Loss: 0.5837, CE: 0.0838
[13:38:53.879] Epoch [6/30] Iteration [650/893]: Loss: 0.6015, CE: 0.1281
[13:38:59.456] Epoch [6/30] Iteration [660/893]: Loss: 0.5828, CE: 0.0816
[13:39:04.919] Epoch [6/30] Iteration [670/893]: Loss: 0.6210, CE: 0.1767
[13:39:10.454] Epoch [6/30] Iteration [680/893]: Loss: 0.6201, CE: 0.1748
[13:39:15.952] Epoch [6/30] Iteration [690/893]: Loss: 0.5780, CE: 0.1350
[13:39:21.390] Epoch [6/30] Iteration [700/893]: Loss: 0.5901, CE: 0.0996
[13:39:26.855] Epoch [6/30] Iteration [710/893]: Loss: 0.6271, CE: 0.1944
[13:39:32.314] Epoch [6/30] Iteration [720/893]: Loss: 0.6009, CE: 0.1266
[13:39:37.773] Epoch [6/30] Iteration [730/893]: Loss: 0.5951, CE: 0.1148
[13:39:43.198] Epoch [6/30] Iteration [740/893]: Loss: 0.5712, CE: 0.0569
[13:39:48.639] Epoch [6/30] Iteration [750/893]: Loss: 0.5890, CE: 0.0970
[13:39:54.134] Epoch [6/30] Iteration [760/893]: Loss: 0.5859, CE: 0.0911
[13:39:59.622] Epoch [6/30] Iteration [770/893]: Loss: 0.5971, CE: 0.1174
[13:40:05.087] Epoch [6/30] Iteration [780/893]: Loss: 0.5926, CE: 0.1058
[13:40:10.604] Epoch [6/30] Iteration [790/893]: Loss: 0.5982, CE: 0.1199
[13:40:16.043] Epoch [6/30] Iteration [800/893]: Loss: 0.6015, CE: 0.1280
[13:40:21.499] Epoch [6/30] Iteration [810/893]: Loss: 0.6107, CE: 0.1555
[13:40:26.962] Epoch [6/30] Iteration [820/893]: Loss: 0.5696, CE: 0.0858
[13:40:32.417] Epoch [6/30] Iteration [830/893]: Loss: 0.5769, CE: 0.0675
[13:40:37.889] Epoch [6/30] Iteration [840/893]: Loss: 0.5903, CE: 0.1005
[13:40:43.322] Epoch [6/30] Iteration [850/893]: Loss: 0.5824, CE: 0.0809
[13:40:48.792] Epoch [6/30] Iteration [860/893]: Loss: 0.6365, CE: 0.2173
[13:40:54.264] Epoch [6/30] Iteration [870/893]: Loss: 0.6047, CE: 0.1425
[13:40:59.730] Epoch [6/30] Iteration [880/893]: Loss: 0.6162, CE: 0.1651
[13:41:05.156] Epoch [6/30] Iteration [890/893]: Loss: 0.6000, CE: 0.1243
[13:41:06.839] Epoch [6/30] Average Loss: 0.5903, CE: 0.1050, Dice: 0.9139
[13:41:27.962] Epoch [7/30] Iteration [0/893]: Loss: 0.5756, CE: 0.0670
[13:41:33.446] Epoch [7/30] Iteration [10/893]: Loss: 0.5787, CE: 0.0848
[13:41:38.881] Epoch [7/30] Iteration [20/893]: Loss: 0.5817, CE: 0.0790
[13:41:44.318] Epoch [7/30] Iteration [30/893]: Loss: 0.5964, CE: 0.1154
[13:41:49.795] Epoch [7/30] Iteration [40/893]: Loss: 0.5837, CE: 0.0840
[13:41:55.251] Epoch [7/30] Iteration [50/893]: Loss: 0.5927, CE: 0.1135
[13:42:00.694] Epoch [7/30] Iteration [60/893]: Loss: 0.5839, CE: 0.0842
[13:42:06.126] Epoch [7/30] Iteration [70/893]: Loss: 0.5867, CE: 0.0917
[13:42:11.559] Epoch [7/30] Iteration [80/893]: Loss: 0.5580, CE: 0.0588
[13:42:16.994] Epoch [7/30] Iteration [90/893]: Loss: 0.5668, CE: 0.0686
[13:42:22.410] Epoch [7/30] Iteration [100/893]: Loss: 0.5951, CE: 0.1123
[13:42:27.923] Epoch [7/30] Iteration [110/893]: Loss: 0.6405, CE: 0.2257
[13:42:33.365] Epoch [7/30] Iteration [120/893]: Loss: 0.5740, CE: 0.0670
[13:42:38.793] Epoch [7/30] Iteration [130/893]: Loss: 0.5888, CE: 0.1006
[13:42:44.289] Epoch [7/30] Iteration [140/893]: Loss: 0.5845, CE: 0.0862
[13:42:49.743] Epoch [7/30] Iteration [150/893]: Loss: 0.5907, CE: 0.1210
[13:42:55.176] Epoch [7/30] Iteration [160/893]: Loss: 0.5655, CE: 0.0409
[13:43:00.637] Epoch [7/30] Iteration [170/893]: Loss: 0.5782, CE: 0.0706
[13:43:06.088] Epoch [7/30] Iteration [180/893]: Loss: 0.5858, CE: 0.0889
[13:43:11.529] Epoch [7/30] Iteration [190/893]: Loss: 0.5778, CE: 0.0693
[13:43:17.016] Epoch [7/30] Iteration [200/893]: Loss: 0.5721, CE: 0.0559
[13:43:22.476] Epoch [7/30] Iteration [210/893]: Loss: 0.6099, CE: 0.1490
[13:43:27.939] Epoch [7/30] Iteration [220/893]: Loss: 0.6178, CE: 0.1688
[13:43:33.389] Epoch [7/30] Iteration [230/893]: Loss: 0.5994, CE: 0.1232
[13:43:38.869] Epoch [7/30] Iteration [240/893]: Loss: 0.5764, CE: 0.0844
[13:43:44.323] Epoch [7/30] Iteration [250/893]: Loss: 0.5964, CE: 0.1160
[13:43:49.778] Epoch [7/30] Iteration [260/893]: Loss: 0.5888, CE: 0.0966
[13:43:55.212] Epoch [7/30] Iteration [270/893]: Loss: 0.5863, CE: 0.1227
[13:44:00.669] Epoch [7/30] Iteration [280/893]: Loss: 0.5990, CE: 0.1228
[13:44:06.123] Epoch [7/30] Iteration [290/893]: Loss: 0.5862, CE: 0.1085
[13:44:11.543] Epoch [7/30] Iteration [300/893]: Loss: 0.5872, CE: 0.0923
[13:44:16.970] Epoch [7/30] Iteration [310/893]: Loss: 0.5858, CE: 0.0893
[13:44:22.387] Epoch [7/30] Iteration [320/893]: Loss: 0.5955, CE: 0.1135
[13:44:27.828] Epoch [7/30] Iteration [330/893]: Loss: 0.5831, CE: 0.0827
[13:44:33.263] Epoch [7/30] Iteration [340/893]: Loss: 0.5880, CE: 0.0944
[13:44:38.717] Epoch [7/30] Iteration [350/893]: Loss: 0.5741, CE: 0.0612
[13:44:44.188] Epoch [7/30] Iteration [360/893]: Loss: 0.5693, CE: 0.0607
[13:44:49.640] Epoch [7/30] Iteration [370/893]: Loss: 0.5754, CE: 0.0934
[13:44:55.089] Epoch [7/30] Iteration [380/893]: Loss: 0.5810, CE: 0.0773
[13:45:00.530] Epoch [7/30] Iteration [390/893]: Loss: 0.5503, CE: 0.0534
[13:45:05.961] Epoch [7/30] Iteration [400/893]: Loss: 0.6492, CE: 0.2477
[13:45:11.460] Epoch [7/30] Iteration [410/893]: Loss: 0.5826, CE: 0.0812
[13:45:16.922] Epoch [7/30] Iteration [420/893]: Loss: 0.5733, CE: 0.0638
[13:45:22.374] Epoch [7/30] Iteration [430/893]: Loss: 0.6017, CE: 0.1285
[13:45:27.817] Epoch [7/30] Iteration [440/893]: Loss: 0.5829, CE: 0.0825
[13:45:33.258] Epoch [7/30] Iteration [450/893]: Loss: 0.6030, CE: 0.1320
[13:45:38.738] Epoch [7/30] Iteration [460/893]: Loss: 0.5510, CE: 0.0756
[13:45:44.187] Epoch [7/30] Iteration [470/893]: Loss: 0.5631, CE: 0.0969
[13:45:49.632] Epoch [7/30] Iteration [480/893]: Loss: 0.5851, CE: 0.0877
[13:45:55.075] Epoch [7/30] Iteration [490/893]: Loss: 0.6291, CE: 0.1972
[13:46:00.545] Epoch [7/30] Iteration [500/893]: Loss: 0.5838, CE: 0.0856
[13:46:05.990] Epoch [7/30] Iteration [510/893]: Loss: 0.5791, CE: 0.0760
[13:46:11.446] Epoch [7/30] Iteration [520/893]: Loss: 0.5765, CE: 0.0719
[13:46:16.907] Epoch [7/30] Iteration [530/893]: Loss: 0.6166, CE: 0.1659
[13:46:22.364] Epoch [7/30] Iteration [540/893]: Loss: 0.5935, CE: 0.1082
[13:46:27.846] Epoch [7/30] Iteration [550/893]: Loss: 0.5709, CE: 0.0567
[13:46:33.279] Epoch [7/30] Iteration [560/893]: Loss: 0.5872, CE: 0.0926
[13:46:38.736] Epoch [7/30] Iteration [570/893]: Loss: 0.5878, CE: 0.1145
[13:46:44.220] Epoch [7/30] Iteration [580/893]: Loss: 0.5945, CE: 0.1139
[13:46:49.661] Epoch [7/30] Iteration [590/893]: Loss: 0.6076, CE: 0.1434
[13:46:55.106] Epoch [7/30] Iteration [600/893]: Loss: 0.5924, CE: 0.1056
[13:47:00.555] Epoch [7/30] Iteration [610/893]: Loss: 0.5760, CE: 0.0645
[13:47:06.014] Epoch [7/30] Iteration [620/893]: Loss: 0.5972, CE: 0.1176
[13:47:11.484] Epoch [7/30] Iteration [630/893]: Loss: 0.6508, CE: 0.2510
[13:47:16.928] Epoch [7/30] Iteration [640/893]: Loss: 0.5871, CE: 0.0924
[13:47:22.380] Epoch [7/30] Iteration [650/893]: Loss: 0.5865, CE: 0.0965
[13:47:27.819] Epoch [7/30] Iteration [660/893]: Loss: 0.6130, CE: 0.1593
[13:47:33.262] Epoch [7/30] Iteration [670/893]: Loss: 0.5836, CE: 0.0841
[13:47:38.694] Epoch [7/30] Iteration [680/893]: Loss: 0.5980, CE: 0.1194
[13:47:44.147] Epoch [7/30] Iteration [690/893]: Loss: 0.5777, CE: 0.0689
[13:47:49.598] Epoch [7/30] Iteration [700/893]: Loss: 0.6139, CE: 0.1590
[13:47:55.061] Epoch [7/30] Iteration [710/893]: Loss: 0.5975, CE: 0.1211
[13:48:00.528] Epoch [7/30] Iteration [720/893]: Loss: 0.6132, CE: 0.1574
[13:48:05.987] Epoch [7/30] Iteration [730/893]: Loss: 0.5982, CE: 0.1198
[13:48:11.478] Epoch [7/30] Iteration [740/893]: Loss: 0.6015, CE: 0.1432
[13:48:16.905] Epoch [7/30] Iteration [750/893]: Loss: 0.5891, CE: 0.1039
[13:48:22.332] Epoch [7/30] Iteration [760/893]: Loss: 0.5960, CE: 0.1145
[13:48:27.770] Epoch [7/30] Iteration [770/893]: Loss: 0.5969, CE: 0.1167
[13:48:33.235] Epoch [7/30] Iteration [780/893]: Loss: 0.6013, CE: 0.1283
[13:48:38.682] Epoch [7/30] Iteration [790/893]: Loss: 0.6022, CE: 0.1301
[13:48:44.117] Epoch [7/30] Iteration [800/893]: Loss: 0.5978, CE: 0.1189
[13:48:49.607] Epoch [7/30] Iteration [810/893]: Loss: 0.5732, CE: 0.0722
[13:48:55.042] Epoch [7/30] Iteration [820/893]: Loss: 0.5746, CE: 0.0613
[13:49:00.494] Epoch [7/30] Iteration [830/893]: Loss: 0.5781, CE: 0.0704
[13:49:05.928] Epoch [7/30] Iteration [840/893]: Loss: 0.5932, CE: 0.1075
[13:49:11.357] Epoch [7/30] Iteration [850/893]: Loss: 0.5660, CE: 0.0400
[13:49:16.798] Epoch [7/30] Iteration [860/893]: Loss: 0.5781, CE: 0.0751
[13:49:22.243] Epoch [7/30] Iteration [870/893]: Loss: 0.5849, CE: 0.0895
[13:49:27.686] Epoch [7/30] Iteration [880/893]: Loss: 0.5882, CE: 0.0964
[13:49:33.138] Epoch [7/30] Iteration [890/893]: Loss: 0.5883, CE: 0.0953
[13:49:34.822] Epoch [7/30] Average Loss: 0.5904, CE: 0.1051, Dice: 0.9139
[13:49:56.001] Epoch [8/30] Iteration [0/893]: Loss: 0.5940, CE: 0.1129
[13:50:01.460] Epoch [8/30] Iteration [10/893]: Loss: 0.5891, CE: 0.0971
[13:50:06.879] Epoch [8/30] Iteration [20/893]: Loss: 0.5856, CE: 0.1057
[13:50:12.366] Epoch [8/30] Iteration [30/893]: Loss: 0.5756, CE: 0.0703
[13:50:17.821] Epoch [8/30] Iteration [40/893]: Loss: 0.5878, CE: 0.0945
[13:50:23.269] Epoch [8/30] Iteration [50/893]: Loss: 0.5883, CE: 0.0958
[13:50:28.800] Epoch [8/30] Iteration [60/893]: Loss: 0.5838, CE: 0.0842
[13:50:34.223] Epoch [8/30] Iteration [70/893]: Loss: 0.5960, CE: 0.1154
[13:50:39.676] Epoch [8/30] Iteration [80/893]: Loss: 0.5889, CE: 0.0968
[13:50:45.121] Epoch [8/30] Iteration [90/893]: Loss: 0.5752, CE: 0.0627
[13:50:50.613] Epoch [8/30] Iteration [100/893]: Loss: 0.5858, CE: 0.0890
[13:50:56.055] Epoch [8/30] Iteration [110/893]: Loss: 0.5902, CE: 0.1002
[13:51:01.498] Epoch [8/30] Iteration [120/893]: Loss: 0.5829, CE: 0.0819
[13:51:06.956] Epoch [8/30] Iteration [130/893]: Loss: 0.5982, CE: 0.1209
[13:51:12.430] Epoch [8/30] Iteration [140/893]: Loss: 0.5850, CE: 0.0869
[13:51:17.881] Epoch [8/30] Iteration [150/893]: Loss: 0.5692, CE: 0.0482
[13:51:23.336] Epoch [8/30] Iteration [160/893]: Loss: 0.6007, CE: 0.1348
[13:51:28.877] Epoch [8/30] Iteration [170/893]: Loss: 0.5955, CE: 0.1138
[13:51:34.396] Epoch [8/30] Iteration [180/893]: Loss: 0.5949, CE: 0.1118
[13:51:39.872] Epoch [8/30] Iteration [190/893]: Loss: 0.5736, CE: 0.0662
[13:51:45.345] Epoch [8/30] Iteration [200/893]: Loss: 0.6276, CE: 0.1963
[13:51:50.803] Epoch [8/30] Iteration [210/893]: Loss: 0.5829, CE: 0.0829
[13:51:56.289] Epoch [8/30] Iteration [220/893]: Loss: 0.5851, CE: 0.0880
[13:52:01.748] Epoch [8/30] Iteration [230/893]: Loss: 0.5961, CE: 0.1146
[13:52:07.191] Epoch [8/30] Iteration [240/893]: Loss: 0.5798, CE: 0.0743
[13:52:12.629] Epoch [8/30] Iteration [250/893]: Loss: 0.5932, CE: 0.1251
[13:52:18.073] Epoch [8/30] Iteration [260/893]: Loss: 0.5989, CE: 0.1221
[13:52:23.518] Epoch [8/30] Iteration [270/893]: Loss: 0.5904, CE: 0.1006
[13:52:28.984] Epoch [8/30] Iteration [280/893]: Loss: 0.5795, CE: 0.0736
[13:52:34.422] Epoch [8/30] Iteration [290/893]: Loss: 0.5911, CE: 0.1120
[13:52:39.863] Epoch [8/30] Iteration [300/893]: Loss: 0.5949, CE: 0.1118
[13:52:45.308] Epoch [8/30] Iteration [310/893]: Loss: 0.6052, CE: 0.1387
[13:52:50.756] Epoch [8/30] Iteration [320/893]: Loss: 0.6034, CE: 0.1337
[13:52:56.238] Epoch [8/30] Iteration [330/893]: Loss: 0.5721, CE: 0.0551
[13:53:01.692] Epoch [8/30] Iteration [340/893]: Loss: 0.5775, CE: 0.0683
[13:53:07.171] Epoch [8/30] Iteration [350/893]: Loss: 0.5937, CE: 0.1086
[13:53:12.641] Epoch [8/30] Iteration [360/893]: Loss: 0.5838, CE: 0.0850
[13:53:18.071] Epoch [8/30] Iteration [370/893]: Loss: 0.5850, CE: 0.0871
[13:53:23.535] Epoch [8/30] Iteration [380/893]: Loss: 0.5764, CE: 0.0738
[13:53:29.009] Epoch [8/30] Iteration [390/893]: Loss: 0.5915, CE: 0.1034
[13:53:34.465] Epoch [8/30] Iteration [400/893]: Loss: 0.5905, CE: 0.1101
[13:53:39.918] Epoch [8/30] Iteration [410/893]: Loss: 0.5696, CE: 0.0991
[13:53:45.369] Epoch [8/30] Iteration [420/893]: Loss: 0.6025, CE: 0.1306
[13:53:50.800] Epoch [8/30] Iteration [430/893]: Loss: 0.5710, CE: 0.0822
[13:53:56.270] Epoch [8/30] Iteration [440/893]: Loss: 0.5907, CE: 0.1017
[13:54:01.722] Epoch [8/30] Iteration [450/893]: Loss: 0.5865, CE: 0.0962
[13:54:07.176] Epoch [8/30] Iteration [460/893]: Loss: 0.6041, CE: 0.1346
[13:54:12.631] Epoch [8/30] Iteration [470/893]: Loss: 0.6010, CE: 0.1270
[13:54:18.106] Epoch [8/30] Iteration [480/893]: Loss: 0.6018, CE: 0.1323
[13:54:23.580] Epoch [8/30] Iteration [490/893]: Loss: 0.6034, CE: 0.1358
[13:54:29.077] Epoch [8/30] Iteration [500/893]: Loss: 0.5881, CE: 0.0949
[13:54:34.562] Epoch [8/30] Iteration [510/893]: Loss: 0.5759, CE: 0.0818
[13:54:40.031] Epoch [8/30] Iteration [520/893]: Loss: 0.5896, CE: 0.0987
[13:54:45.477] Epoch [8/30] Iteration [530/893]: Loss: 0.5694, CE: 0.0597
[13:54:50.968] Epoch [8/30] Iteration [540/893]: Loss: 0.5952, CE: 0.1133
[13:54:56.414] Epoch [8/30] Iteration [550/893]: Loss: 0.5880, CE: 0.1121
[13:55:01.895] Epoch [8/30] Iteration [560/893]: Loss: 0.6026, CE: 0.1308
[13:55:07.369] Epoch [8/30] Iteration [570/893]: Loss: 0.5876, CE: 0.1311
[13:55:12.881] Epoch [8/30] Iteration [580/893]: Loss: 0.6056, CE: 0.1386
[13:55:18.338] Epoch [8/30] Iteration [590/893]: Loss: 0.5845, CE: 0.0862
[13:55:23.819] Epoch [8/30] Iteration [600/893]: Loss: 0.5802, CE: 0.0828
[13:55:29.319] Epoch [8/30] Iteration [610/893]: Loss: 0.5946, CE: 0.1113
[13:55:34.788] Epoch [8/30] Iteration [620/893]: Loss: 0.5793, CE: 0.0730
[13:55:40.267] Epoch [8/30] Iteration [630/893]: Loss: 0.5830, CE: 0.0822
[13:55:45.731] Epoch [8/30] Iteration [640/893]: Loss: 0.5855, CE: 0.0884
[13:55:51.181] Epoch [8/30] Iteration [650/893]: Loss: 0.5848, CE: 0.0869
[13:55:56.680] Epoch [8/30] Iteration [660/893]: Loss: 0.5925, CE: 0.1057
[13:56:02.146] Epoch [8/30] Iteration [670/893]: Loss: 0.5878, CE: 0.0944
[13:56:07.731] Epoch [8/30] Iteration [680/893]: Loss: 0.5797, CE: 0.0741
[13:56:13.293] Epoch [8/30] Iteration [690/893]: Loss: 0.5892, CE: 0.0973
[13:56:18.768] Epoch [8/30] Iteration [700/893]: Loss: 0.6321, CE: 0.2043
[13:56:24.194] Epoch [8/30] Iteration [710/893]: Loss: 0.5908, CE: 0.1016
[13:56:29.649] Epoch [8/30] Iteration [720/893]: Loss: 0.5963, CE: 0.1151
[13:56:35.096] Epoch [8/30] Iteration [730/893]: Loss: 0.5945, CE: 0.1113
[13:56:40.574] Epoch [8/30] Iteration [740/893]: Loss: 0.5980, CE: 0.1202
[13:56:46.080] Epoch [8/30] Iteration [750/893]: Loss: 0.5919, CE: 0.1056
[13:56:51.572] Epoch [8/30] Iteration [760/893]: Loss: 0.5815, CE: 0.0806
[13:56:57.010] Epoch [8/30] Iteration [770/893]: Loss: 0.5998, CE: 0.1240
[13:57:02.465] Epoch [8/30] Iteration [780/893]: Loss: 0.5904, CE: 0.1008
[13:57:07.939] Epoch [8/30] Iteration [790/893]: Loss: 0.5700, CE: 0.0517
[13:57:13.429] Epoch [8/30] Iteration [800/893]: Loss: 0.5866, CE: 0.0917
[13:57:18.855] Epoch [8/30] Iteration [810/893]: Loss: 0.5799, CE: 0.0959
[13:57:24.330] Epoch [8/30] Iteration [820/893]: Loss: 0.5730, CE: 0.0784
[13:57:29.776] Epoch [8/30] Iteration [830/893]: Loss: 0.6276, CE: 0.1930
[13:57:35.249] Epoch [8/30] Iteration [840/893]: Loss: 0.5699, CE: 0.0500
[13:57:40.751] Epoch [8/30] Iteration [850/893]: Loss: 0.6191, CE: 0.1720
[13:57:46.289] Epoch [8/30] Iteration [860/893]: Loss: 0.5921, CE: 0.1044
[13:57:51.739] Epoch [8/30] Iteration [870/893]: Loss: 0.5693, CE: 0.0506
[13:57:57.332] Epoch [8/30] Iteration [880/893]: Loss: 0.5864, CE: 0.0905
[13:58:02.781] Epoch [8/30] Iteration [890/893]: Loss: 0.6053, CE: 0.1389
[13:58:04.477] Epoch [8/30] Average Loss: 0.5908, CE: 0.1053, Dice: 0.9145
[13:58:26.051] Epoch [9/30] Iteration [0/893]: Loss: 0.6109, CE: 0.1530
[13:58:31.519] Epoch [9/30] Iteration [10/893]: Loss: 0.5699, CE: 0.0540
[13:58:36.965] Epoch [9/30] Iteration [20/893]: Loss: 0.6209, CE: 0.1769
[13:58:42.451] Epoch [9/30] Iteration [30/893]: Loss: 0.5986, CE: 0.1210
[13:58:47.919] Epoch [9/30] Iteration [40/893]: Loss: 0.6044, CE: 0.1355
[13:58:53.411] Epoch [9/30] Iteration [50/893]: Loss: 0.5763, CE: 0.0682
[13:58:58.867] Epoch [9/30] Iteration [60/893]: Loss: 0.5962, CE: 0.1147
[13:59:04.309] Epoch [9/30] Iteration [70/893]: Loss: 0.5876, CE: 0.0938
[13:59:09.737] Epoch [9/30] Iteration [80/893]: Loss: 0.5736, CE: 0.0600
[13:59:15.188] Epoch [9/30] Iteration [90/893]: Loss: 0.5902, CE: 0.1000
[13:59:20.624] Epoch [9/30] Iteration [100/893]: Loss: 0.5995, CE: 0.1233
[13:59:26.071] Epoch [9/30] Iteration [110/893]: Loss: 0.5941, CE: 0.1095
[13:59:31.511] Epoch [9/30] Iteration [120/893]: Loss: 0.5896, CE: 0.0988
[13:59:36.969] Epoch [9/30] Iteration [130/893]: Loss: 0.5922, CE: 0.1052
[13:59:42.441] Epoch [9/30] Iteration [140/893]: Loss: 0.5995, CE: 0.1238
[13:59:47.900] Epoch [9/30] Iteration [150/893]: Loss: 0.5788, CE: 0.0715
[13:59:53.368] Epoch [9/30] Iteration [160/893]: Loss: 0.5858, CE: 0.0891
[13:59:58.912] Epoch [9/30] Iteration [170/893]: Loss: 0.5765, CE: 0.0658
[14:00:04.375] Epoch [9/30] Iteration [180/893]: Loss: 0.5838, CE: 0.0841
[14:00:09.850] Epoch [9/30] Iteration [190/893]: Loss: 0.5877, CE: 0.0942
[14:00:15.332] Epoch [9/30] Iteration [200/893]: Loss: 0.5943, CE: 0.1103
[14:00:20.824] Epoch [9/30] Iteration [210/893]: Loss: 0.5762, CE: 0.0665
[14:00:26.284] Epoch [9/30] Iteration [220/893]: Loss: 0.5737, CE: 0.0596
[14:00:31.764] Epoch [9/30] Iteration [230/893]: Loss: 0.5932, CE: 0.1078
[14:00:37.192] Epoch [9/30] Iteration [240/893]: Loss: 0.6087, CE: 0.1464
[14:00:42.684] Epoch [9/30] Iteration [250/893]: Loss: 0.5792, CE: 0.0729
[14:00:48.119] Epoch [9/30] Iteration [260/893]: Loss: 0.5968, CE: 0.1165
[14:00:53.585] Epoch [9/30] Iteration [270/893]: Loss: 0.5746, CE: 0.0621
[14:00:59.036] Epoch [9/30] Iteration [280/893]: Loss: 0.5834, CE: 0.0833
[14:01:04.479] Epoch [9/30] Iteration [290/893]: Loss: 0.5942, CE: 0.1117
[14:01:09.939] Epoch [9/30] Iteration [300/893]: Loss: 0.6055, CE: 0.1380
[14:01:15.420] Epoch [9/30] Iteration [310/893]: Loss: 0.6076, CE: 0.1435
[14:01:20.886] Epoch [9/30] Iteration [320/893]: Loss: 0.6178, CE: 0.1689
[14:01:26.328] Epoch [9/30] Iteration [330/893]: Loss: 0.5876, CE: 0.0939
[14:01:31.818] Epoch [9/30] Iteration [340/893]: Loss: 0.5962, CE: 0.1147
[14:01:37.276] Epoch [9/30] Iteration [350/893]: Loss: 0.5588, CE: 0.0779
[14:01:42.751] Epoch [9/30] Iteration [360/893]: Loss: 0.5849, CE: 0.0870
[14:01:48.210] Epoch [9/30] Iteration [370/893]: Loss: 0.5919, CE: 0.1046
[14:01:53.691] Epoch [9/30] Iteration [380/893]: Loss: 0.6088, CE: 0.1521
[14:01:59.143] Epoch [9/30] Iteration [390/893]: Loss: 0.6100, CE: 0.1493
[14:02:04.582] Epoch [9/30] Iteration [400/893]: Loss: 0.5734, CE: 0.0582
[14:02:10.041] Epoch [9/30] Iteration [410/893]: Loss: 0.5812, CE: 0.0777
[14:02:15.514] Epoch [9/30] Iteration [420/893]: Loss: 0.5813, CE: 0.0854
[14:02:21.110] Epoch [9/30] Iteration [430/893]: Loss: 0.5803, CE: 0.0757
[14:02:26.566] Epoch [9/30] Iteration [440/893]: Loss: 0.6087, CE: 0.1481
[14:02:32.037] Epoch [9/30] Iteration [450/893]: Loss: 0.5816, CE: 0.0786
[14:02:37.462] Epoch [9/30] Iteration [460/893]: Loss: 0.6033, CE: 0.1325
[14:02:42.948] Epoch [9/30] Iteration [470/893]: Loss: 0.5722, CE: 0.0661
[14:02:48.466] Epoch [9/30] Iteration [480/893]: Loss: 0.5866, CE: 0.1241
[14:02:53.970] Epoch [9/30] Iteration [490/893]: Loss: 0.5946, CE: 0.1114
[14:02:59.430] Epoch [9/30] Iteration [500/893]: Loss: 0.5829, CE: 0.0820
[14:03:04.889] Epoch [9/30] Iteration [510/893]: Loss: 0.5865, CE: 0.0943
[14:03:10.327] Epoch [9/30] Iteration [520/893]: Loss: 0.5971, CE: 0.1181
[14:03:15.851] Epoch [9/30] Iteration [530/893]: Loss: 0.5865, CE: 0.0909
[14:03:21.353] Epoch [9/30] Iteration [540/893]: Loss: 0.5970, CE: 0.1245
[14:03:26.832] Epoch [9/30] Iteration [550/893]: Loss: 0.5996, CE: 0.1238
[14:03:32.302] Epoch [9/30] Iteration [560/893]: Loss: 0.5724, CE: 0.0556
[14:03:37.797] Epoch [9/30] Iteration [570/893]: Loss: 0.6057, CE: 0.1392
[14:03:43.265] Epoch [9/30] Iteration [580/893]: Loss: 0.5806, CE: 0.0766
[14:03:48.738] Epoch [9/30] Iteration [590/893]: Loss: 0.5870, CE: 0.0923
[14:03:54.190] Epoch [9/30] Iteration [600/893]: Loss: 0.6024, CE: 0.1307
[14:03:59.700] Epoch [9/30] Iteration [610/893]: Loss: 0.5739, CE: 0.0604
[14:04:05.157] Epoch [9/30] Iteration [620/893]: Loss: 0.5739, CE: 0.0680
[14:04:10.625] Epoch [9/30] Iteration [630/893]: Loss: 0.5772, CE: 0.0685
[14:04:16.071] Epoch [9/30] Iteration [640/893]: Loss: 0.5959, CE: 0.1202
[14:04:21.555] Epoch [9/30] Iteration [650/893]: Loss: 0.5774, CE: 0.0748
[14:04:27.004] Epoch [9/30] Iteration [660/893]: Loss: 0.5861, CE: 0.1031
[14:04:32.462] Epoch [9/30] Iteration [670/893]: Loss: 0.6047, CE: 0.1362
[14:04:37.912] Epoch [9/30] Iteration [680/893]: Loss: 0.5950, CE: 0.1120
[14:04:43.375] Epoch [9/30] Iteration [690/893]: Loss: 0.5747, CE: 0.0777
[14:04:48.825] Epoch [9/30] Iteration [700/893]: Loss: 0.5699, CE: 0.0655
[14:04:54.301] Epoch [9/30] Iteration [710/893]: Loss: 0.5793, CE: 0.0731
[14:04:59.811] Epoch [9/30] Iteration [720/893]: Loss: 0.6002, CE: 0.1248
[14:05:05.273] Epoch [9/30] Iteration [730/893]: Loss: 0.5662, CE: 0.0491
[14:05:10.754] Epoch [9/30] Iteration [740/893]: Loss: 0.5867, CE: 0.0914
[14:05:16.219] Epoch [9/30] Iteration [750/893]: Loss: 0.5946, CE: 0.1108
[14:05:21.685] Epoch [9/30] Iteration [760/893]: Loss: 0.6599, CE: 0.2736
[14:05:27.179] Epoch [9/30] Iteration [770/893]: Loss: 0.5736, CE: 0.0597
[14:05:32.645] Epoch [9/30] Iteration [780/893]: Loss: 0.5834, CE: 0.0877
[14:05:38.129] Epoch [9/30] Iteration [790/893]: Loss: 0.5753, CE: 0.0661
[14:05:43.623] Epoch [9/30] Iteration [800/893]: Loss: 0.5971, CE: 0.1174
[14:05:49.117] Epoch [9/30] Iteration [810/893]: Loss: 0.5826, CE: 0.0817
[14:05:54.581] Epoch [9/30] Iteration [820/893]: Loss: 0.6055, CE: 0.1380
[14:06:00.016] Epoch [9/30] Iteration [830/893]: Loss: 0.5896, CE: 0.0984
[14:06:05.456] Epoch [9/30] Iteration [840/893]: Loss: 0.5788, CE: 0.0730
[14:06:10.917] Epoch [9/30] Iteration [850/893]: Loss: 0.6136, CE: 0.1582
[14:06:16.375] Epoch [9/30] Iteration [860/893]: Loss: 0.5863, CE: 0.0905
[14:06:21.856] Epoch [9/30] Iteration [870/893]: Loss: 0.5668, CE: 0.0486
[14:06:27.294] Epoch [9/30] Iteration [880/893]: Loss: 0.5871, CE: 0.0931
[14:06:32.746] Epoch [9/30] Iteration [890/893]: Loss: 0.5872, CE: 0.0927
[14:06:34.431] Epoch [9/30] Average Loss: 0.5903, CE: 0.1051, Dice: 0.9138
[14:06:34.431] Updating surgical weights at epoch 10
[14:06:57.769] Surgical weights range: [0.0003, 1.0000]
[14:06:57.769] Applying TPGM projection update at epoch 10
[14:07:07.030] TPGM iteration 19/50 completed, loss: 0.1594
[14:07:16.713] TPGM iteration 39/50 completed, loss: 0.1638
[14:07:43.271] Epoch [10/30] Iteration [0/893]: Loss: 0.5857, CE: 0.1090
[14:07:48.700] Epoch [10/30] Iteration [10/893]: Loss: 0.5889, CE: 0.0988
[14:07:54.118] Epoch [10/30] Iteration [20/893]: Loss: 0.5847, CE: 0.0931
[14:07:59.538] Epoch [10/30] Iteration [30/893]: Loss: 0.5826, CE: 0.0811
[14:08:04.987] Epoch [10/30] Iteration [40/893]: Loss: 0.6043, CE: 0.1352
[14:08:10.443] Epoch [10/30] Iteration [50/893]: Loss: 0.5953, CE: 0.1126
[14:08:15.899] Epoch [10/30] Iteration [60/893]: Loss: 0.5832, CE: 0.0828
[14:08:21.374] Epoch [10/30] Iteration [70/893]: Loss: 0.5874, CE: 0.1128
[14:08:26.828] Epoch [10/30] Iteration [80/893]: Loss: 0.6056, CE: 0.1407
[14:08:32.274] Epoch [10/30] Iteration [90/893]: Loss: 0.6085, CE: 0.1459
[14:08:37.714] Epoch [10/30] Iteration [100/893]: Loss: 0.5902, CE: 0.1023
[14:08:43.266] Epoch [10/30] Iteration [110/893]: Loss: 0.5744, CE: 0.0628
[14:08:48.705] Epoch [10/30] Iteration [120/893]: Loss: 0.5786, CE: 0.0724
[14:08:54.165] Epoch [10/30] Iteration [130/893]: Loss: 0.5903, CE: 0.1003
[14:08:59.637] Epoch [10/30] Iteration [140/893]: Loss: 0.6144, CE: 0.1604
[14:09:05.085] Epoch [10/30] Iteration [150/893]: Loss: 0.6098, CE: 0.1488
[14:09:10.536] Epoch [10/30] Iteration [160/893]: Loss: 0.5875, CE: 0.0933
[14:09:15.983] Epoch [10/30] Iteration [170/893]: Loss: 0.5783, CE: 0.0709
[14:09:21.424] Epoch [10/30] Iteration [180/893]: Loss: 0.5977, CE: 0.1193
[14:09:26.889] Epoch [10/30] Iteration [190/893]: Loss: 0.5877, CE: 0.0936
[14:09:32.345] Epoch [10/30] Iteration [200/893]: Loss: 0.5893, CE: 0.1027
[14:09:37.805] Epoch [10/30] Iteration [210/893]: Loss: 0.6112, CE: 0.1524
[14:09:43.259] Epoch [10/30] Iteration [220/893]: Loss: 0.5711, CE: 0.0530
[14:09:48.731] Epoch [10/30] Iteration [230/893]: Loss: 0.5806, CE: 0.0763
[14:09:54.175] Epoch [10/30] Iteration [240/893]: Loss: 0.6029, CE: 0.1314
[14:09:59.636] Epoch [10/30] Iteration [250/893]: Loss: 0.6133, CE: 0.1575
[14:10:05.103] Epoch [10/30] Iteration [260/893]: Loss: 0.6148, CE: 0.1613
[14:10:10.542] Epoch [10/30] Iteration [270/893]: Loss: 0.5948, CE: 0.1116
[14:10:15.995] Epoch [10/30] Iteration [280/893]: Loss: 0.5741, CE: 0.0605
[14:10:21.443] Epoch [10/30] Iteration [290/893]: Loss: 0.6042, CE: 0.1351
[14:10:26.889] Epoch [10/30] Iteration [300/893]: Loss: 0.5805, CE: 0.0794
[14:10:32.327] Epoch [10/30] Iteration [310/893]: Loss: 0.5886, CE: 0.0959
[14:10:37.800] Epoch [10/30] Iteration [320/893]: Loss: 0.6034, CE: 0.1330
[14:10:43.233] Epoch [10/30] Iteration [330/893]: Loss: 0.6165, CE: 0.1658
[14:10:48.726] Epoch [10/30] Iteration [340/893]: Loss: 0.5755, CE: 0.0664
[14:10:54.200] Epoch [10/30] Iteration [350/893]: Loss: 0.5672, CE: 0.0591
[14:10:59.670] Epoch [10/30] Iteration [360/893]: Loss: 0.6084, CE: 0.1455
[14:11:05.144] Epoch [10/30] Iteration [370/893]: Loss: 0.5895, CE: 0.0990
[14:11:10.627] Epoch [10/30] Iteration [380/893]: Loss: 0.5909, CE: 0.1043
[14:11:16.091] Epoch [10/30] Iteration [390/893]: Loss: 0.6622, CE: 0.2797
[14:11:21.551] Epoch [10/30] Iteration [400/893]: Loss: 0.5916, CE: 0.1036
[14:11:27.029] Epoch [10/30] Iteration [410/893]: Loss: 0.6046, CE: 0.1358
[14:11:32.465] Epoch [10/30] Iteration [420/893]: Loss: 0.5844, CE: 0.0854
[14:11:37.938] Epoch [10/30] Iteration [430/893]: Loss: 0.6005, CE: 0.1461
[14:11:43.378] Epoch [10/30] Iteration [440/893]: Loss: 0.6247, CE: 0.1860
[14:11:48.847] Epoch [10/30] Iteration [450/893]: Loss: 0.5855, CE: 0.0893
[14:11:54.304] Epoch [10/30] Iteration [460/893]: Loss: 0.5875, CE: 0.0935
[14:11:59.781] Epoch [10/30] Iteration [470/893]: Loss: 0.6134, CE: 0.1579
[14:12:05.211] Epoch [10/30] Iteration [480/893]: Loss: 0.5733, CE: 0.0581
[14:12:10.659] Epoch [10/30] Iteration [490/893]: Loss: 0.5744, CE: 0.0609
[14:12:16.110] Epoch [10/30] Iteration [500/893]: Loss: 0.5932, CE: 0.1074
[14:12:21.555] Epoch [10/30] Iteration [510/893]: Loss: 0.6063, CE: 0.1403
[14:12:27.009] Epoch [10/30] Iteration [520/893]: Loss: 0.5760, CE: 0.0747
[14:12:32.519] Epoch [10/30] Iteration [530/893]: Loss: 0.5847, CE: 0.0890
[14:12:37.960] Epoch [10/30] Iteration [540/893]: Loss: 0.5774, CE: 0.0683
[14:12:43.422] Epoch [10/30] Iteration [550/893]: Loss: 0.6099, CE: 0.1489
[14:12:48.896] Epoch [10/30] Iteration [560/893]: Loss: 0.5795, CE: 0.0737
[14:12:54.332] Epoch [10/30] Iteration [570/893]: Loss: 0.6205, CE: 0.1754
[14:12:59.792] Epoch [10/30] Iteration [580/893]: Loss: 0.5909, CE: 0.1209
[14:13:05.240] Epoch [10/30] Iteration [590/893]: Loss: 0.5899, CE: 0.1058
[14:13:10.719] Epoch [10/30] Iteration [600/893]: Loss: 0.6048, CE: 0.1363
[14:13:16.194] Epoch [10/30] Iteration [610/893]: Loss: 0.6049, CE: 0.1424
[14:13:21.650] Epoch [10/30] Iteration [620/893]: Loss: 0.5753, CE: 0.0643
[14:13:27.136] Epoch [10/30] Iteration [630/893]: Loss: 0.5925, CE: 0.1294
[14:13:32.598] Epoch [10/30] Iteration [640/893]: Loss: 0.6369, CE: 0.2163
[14:13:38.050] Epoch [10/30] Iteration [650/893]: Loss: 0.6079, CE: 0.1442
[14:13:43.494] Epoch [10/30] Iteration [660/893]: Loss: 0.5772, CE: 0.0679
[14:13:48.971] Epoch [10/30] Iteration [670/893]: Loss: 0.5834, CE: 0.0833
[14:13:54.413] Epoch [10/30] Iteration [680/893]: Loss: 0.5869, CE: 0.0921
[14:13:59.901] Epoch [10/30] Iteration [690/893]: Loss: 0.5756, CE: 0.1166
[14:14:05.423] Epoch [10/30] Iteration [700/893]: Loss: 0.5837, CE: 0.0861
[14:14:10.893] Epoch [10/30] Iteration [710/893]: Loss: 0.5890, CE: 0.0967
[14:14:16.333] Epoch [10/30] Iteration [720/893]: Loss: 0.6134, CE: 0.1580
[14:14:21.788] Epoch [10/30] Iteration [730/893]: Loss: 0.5988, CE: 0.1212
[14:14:27.261] Epoch [10/30] Iteration [740/893]: Loss: 0.5779, CE: 0.0694
[14:14:32.712] Epoch [10/30] Iteration [750/893]: Loss: 0.5852, CE: 0.0879
[14:14:38.235] Epoch [10/30] Iteration [760/893]: Loss: 0.5914, CE: 0.1032
[14:14:43.726] Epoch [10/30] Iteration [770/893]: Loss: 0.5909, CE: 0.1044
[14:14:49.184] Epoch [10/30] Iteration [780/893]: Loss: 0.5983, CE: 0.1202
[14:14:54.655] Epoch [10/30] Iteration [790/893]: Loss: 0.5834, CE: 0.0837
[14:15:00.115] Epoch [10/30] Iteration [800/893]: Loss: 0.6138, CE: 0.1587
[14:15:05.611] Epoch [10/30] Iteration [810/893]: Loss: 0.6070, CE: 0.1421
[14:15:11.059] Epoch [10/30] Iteration [820/893]: Loss: 0.5806, CE: 0.0774
[14:15:16.547] Epoch [10/30] Iteration [830/893]: Loss: 0.5608, CE: 0.0949
[14:15:22.005] Epoch [10/30] Iteration [840/893]: Loss: 0.6331, CE: 0.2078
[14:15:27.502] Epoch [10/30] Iteration [850/893]: Loss: 0.5743, CE: 0.0812
[14:15:32.993] Epoch [10/30] Iteration [860/893]: Loss: 0.5766, CE: 0.0681
[14:15:38.461] Epoch [10/30] Iteration [870/893]: Loss: 0.5946, CE: 0.1117
[14:15:43.909] Epoch [10/30] Iteration [880/893]: Loss: 0.5883, CE: 0.1110
[14:15:49.367] Epoch [10/30] Iteration [890/893]: Loss: 0.5927, CE: 0.1061
[14:15:51.041] Epoch [10/30] Average Loss: 0.5904, CE: 0.1049, Dice: 0.9140
[14:16:11.734] Epoch [11/30] Iteration [0/893]: Loss: 0.5876, CE: 0.0948
[14:16:17.188] Epoch [11/30] Iteration [10/893]: Loss: 0.5967, CE: 0.1174
[14:16:22.658] Epoch [11/30] Iteration [20/893]: Loss: 0.5913, CE: 0.1029
[14:16:28.125] Epoch [11/30] Iteration [30/893]: Loss: 0.5869, CE: 0.0920
[14:16:33.577] Epoch [11/30] Iteration [40/893]: Loss: 0.6076, CE: 0.1601
[14:16:39.003] Epoch [11/30] Iteration [50/893]: Loss: 0.5714, CE: 0.0533
[14:16:44.450] Epoch [11/30] Iteration [60/893]: Loss: 0.5938, CE: 0.1090
[14:16:49.919] Epoch [11/30] Iteration [70/893]: Loss: 0.5807, CE: 0.1159
[14:16:55.355] Epoch [11/30] Iteration [80/893]: Loss: 0.6211, CE: 0.1772
[14:17:00.818] Epoch [11/30] Iteration [90/893]: Loss: 0.5887, CE: 0.0983
[14:17:06.259] Epoch [11/30] Iteration [100/893]: Loss: 0.5831, CE: 0.0829
[14:17:11.713] Epoch [11/30] Iteration [110/893]: Loss: 0.5897, CE: 0.0987
[14:17:17.184] Epoch [11/30] Iteration [120/893]: Loss: 0.5952, CE: 0.1132
[14:17:22.616] Epoch [11/30] Iteration [130/893]: Loss: 0.6068, CE: 0.1418
[14:17:28.051] Epoch [11/30] Iteration [140/893]: Loss: 0.5882, CE: 0.0953
[14:17:33.557] Epoch [11/30] Iteration [150/893]: Loss: 0.5825, CE: 0.0812
[14:17:39.018] Epoch [11/30] Iteration [160/893]: Loss: 0.5802, CE: 0.0757
[14:17:44.465] Epoch [11/30] Iteration [170/893]: Loss: 0.5806, CE: 0.0824
[14:17:49.922] Epoch [11/30] Iteration [180/893]: Loss: 0.6101, CE: 0.1495
[14:17:55.400] Epoch [11/30] Iteration [190/893]: Loss: 0.5954, CE: 0.1127
[14:18:00.844] Epoch [11/30] Iteration [200/893]: Loss: 0.5891, CE: 0.0974
[14:18:06.307] Epoch [11/30] Iteration [210/893]: Loss: 0.5557, CE: 0.0893
[14:18:11.795] Epoch [11/30] Iteration [220/893]: Loss: 0.6086, CE: 0.1675
[14:18:17.240] Epoch [11/30] Iteration [230/893]: Loss: 0.5945, CE: 0.1114
[14:18:22.691] Epoch [11/30] Iteration [240/893]: Loss: 0.6115, CE: 0.1529
[14:18:28.160] Epoch [11/30] Iteration [250/893]: Loss: 0.5920, CE: 0.1063
[14:18:33.676] Epoch [11/30] Iteration [260/893]: Loss: 0.5834, CE: 0.0835
[14:18:39.121] Epoch [11/30] Iteration [270/893]: Loss: 0.5885, CE: 0.0955
[14:18:44.624] Epoch [11/30] Iteration [280/893]: Loss: 0.5919, CE: 0.1048
[14:18:50.121] Epoch [11/30] Iteration [290/893]: Loss: 0.5764, CE: 0.0659
[14:18:55.639] Epoch [11/30] Iteration [300/893]: Loss: 0.5581, CE: 0.0680
[14:19:01.232] Epoch [11/30] Iteration [310/893]: Loss: 0.6012, CE: 0.1298
[14:19:06.760] Epoch [11/30] Iteration [320/893]: Loss: 0.5736, CE: 0.0863
[14:19:12.236] Epoch [11/30] Iteration [330/893]: Loss: 0.5883, CE: 0.0956
[14:19:17.730] Epoch [11/30] Iteration [340/893]: Loss: 0.5966, CE: 0.1195
[14:19:23.175] Epoch [11/30] Iteration [350/893]: Loss: 0.5766, CE: 0.0667
[14:19:28.634] Epoch [11/30] Iteration [360/893]: Loss: 0.5751, CE: 0.0649
[14:19:34.074] Epoch [11/30] Iteration [370/893]: Loss: 0.5819, CE: 0.0834
[14:19:39.529] Epoch [11/30] Iteration [380/893]: Loss: 0.5912, CE: 0.1127
[14:19:45.000] Epoch [11/30] Iteration [390/893]: Loss: 0.5963, CE: 0.1265
[14:19:50.458] Epoch [11/30] Iteration [400/893]: Loss: 0.6087, CE: 0.1458
[14:19:55.956] Epoch [11/30] Iteration [410/893]: Loss: 0.5833, CE: 0.0850
[14:20:01.392] Epoch [11/30] Iteration [420/893]: Loss: 0.6139, CE: 0.1586
[14:20:06.851] Epoch [11/30] Iteration [430/893]: Loss: 0.5777, CE: 0.0697
[14:20:12.294] Epoch [11/30] Iteration [440/893]: Loss: 0.5827, CE: 0.0846
[14:20:17.780] Epoch [11/30] Iteration [450/893]: Loss: 0.5622, CE: 0.0353
[14:20:23.225] Epoch [11/30] Iteration [460/893]: Loss: 0.6152, CE: 0.1625
[14:20:28.672] Epoch [11/30] Iteration [470/893]: Loss: 0.5954, CE: 0.1191
[14:20:34.133] Epoch [11/30] Iteration [480/893]: Loss: 0.5949, CE: 0.1125
[14:20:39.578] Epoch [11/30] Iteration [490/893]: Loss: 0.5978, CE: 0.1435
[14:20:45.064] Epoch [11/30] Iteration [500/893]: Loss: 0.5947, CE: 0.1135
[14:20:50.554] Epoch [11/30] Iteration [510/893]: Loss: 0.5481, CE: 0.0754
[14:20:56.015] Epoch [11/30] Iteration [520/893]: Loss: 0.5820, CE: 0.0798
[14:21:01.446] Epoch [11/30] Iteration [530/893]: Loss: 0.5730, CE: 0.0574
[14:21:06.945] Epoch [11/30] Iteration [540/893]: Loss: 0.5711, CE: 0.0530
[14:21:12.418] Epoch [11/30] Iteration [550/893]: Loss: 0.5828, CE: 0.0814
[14:21:17.869] Epoch [11/30] Iteration [560/893]: Loss: 0.5974, CE: 0.1181
[14:21:23.325] Epoch [11/30] Iteration [570/893]: Loss: 0.5931, CE: 0.1283
[14:21:28.828] Epoch [11/30] Iteration [580/893]: Loss: 0.6055, CE: 0.1382
[14:21:34.310] Epoch [11/30] Iteration [590/893]: Loss: 0.5896, CE: 0.0984
[14:21:39.787] Epoch [11/30] Iteration [600/893]: Loss: 0.5851, CE: 0.0872
[14:21:45.253] Epoch [11/30] Iteration [610/893]: Loss: 0.5806, CE: 0.0761
[14:21:50.741] Epoch [11/30] Iteration [620/893]: Loss: 0.5841, CE: 0.0852
[14:21:56.194] Epoch [11/30] Iteration [630/893]: Loss: 0.5830, CE: 0.0821
[14:22:01.655] Epoch [11/30] Iteration [640/893]: Loss: 0.5884, CE: 0.0963
[14:22:07.123] Epoch [11/30] Iteration [650/893]: Loss: 0.5720, CE: 0.0620
[14:22:12.617] Epoch [11/30] Iteration [660/893]: Loss: 0.5666, CE: 0.0705
[14:22:18.093] Epoch [11/30] Iteration [670/893]: Loss: 0.5607, CE: 0.0326
[14:22:23.621] Epoch [11/30] Iteration [680/893]: Loss: 0.5920, CE: 0.1045
[14:22:29.074] Epoch [11/30] Iteration [690/893]: Loss: 0.5885, CE: 0.1013
[14:22:34.562] Epoch [11/30] Iteration [700/893]: Loss: 0.6093, CE: 0.1477
[14:22:40.030] Epoch [11/30] Iteration [710/893]: Loss: 0.5757, CE: 0.0640
[14:22:45.498] Epoch [11/30] Iteration [720/893]: Loss: 0.5913, CE: 0.1026
[14:22:51.005] Epoch [11/30] Iteration [730/893]: Loss: 0.5810, CE: 0.0775
[14:22:56.494] Epoch [11/30] Iteration [740/893]: Loss: 0.5958, CE: 0.1140
[14:23:01.966] Epoch [11/30] Iteration [750/893]: Loss: 0.6037, CE: 0.1336
[14:23:07.441] Epoch [11/30] Iteration [760/893]: Loss: 0.5909, CE: 0.1017
[14:23:12.908] Epoch [11/30] Iteration [770/893]: Loss: 0.5842, CE: 0.0851
[14:23:18.380] Epoch [11/30] Iteration [780/893]: Loss: 0.5875, CE: 0.0934
[14:23:23.879] Epoch [11/30] Iteration [790/893]: Loss: 0.5883, CE: 0.0960
[14:23:29.338] Epoch [11/30] Iteration [800/893]: Loss: 0.5967, CE: 0.1165
[14:23:34.820] Epoch [11/30] Iteration [810/893]: Loss: 0.5913, CE: 0.1061
[14:23:40.324] Epoch [11/30] Iteration [820/893]: Loss: 0.5866, CE: 0.0914
[14:23:45.777] Epoch [11/30] Iteration [830/893]: Loss: 0.5795, CE: 0.0735
[14:23:51.228] Epoch [11/30] Iteration [840/893]: Loss: 0.5879, CE: 0.1315
[14:23:56.695] Epoch [11/30] Iteration [850/893]: Loss: 0.5941, CE: 0.1132
[14:24:02.146] Epoch [11/30] Iteration [860/893]: Loss: 0.6193, CE: 0.1724
[14:24:07.614] Epoch [11/30] Iteration [870/893]: Loss: 0.5653, CE: 0.0654
[14:24:13.116] Epoch [11/30] Iteration [880/893]: Loss: 0.5895, CE: 0.0994
[14:24:18.544] Epoch [11/30] Iteration [890/893]: Loss: 0.5862, CE: 0.0903
[14:24:20.211] Epoch [11/30] Average Loss: 0.5907, CE: 0.1050, Dice: 0.9145
[14:24:41.378] Epoch [12/30] Iteration [0/893]: Loss: 0.6000, CE: 0.1243
[14:24:46.824] Epoch [12/30] Iteration [10/893]: Loss: 0.5931, CE: 0.1071
[14:24:52.260] Epoch [12/30] Iteration [20/893]: Loss: 0.5971, CE: 0.1172
[14:24:57.722] Epoch [12/30] Iteration [30/893]: Loss: 0.6250, CE: 0.1886
[14:25:03.177] Epoch [12/30] Iteration [40/893]: Loss: 0.6098, CE: 0.1495
[14:25:08.616] Epoch [12/30] Iteration [50/893]: Loss: 0.5963, CE: 0.1239
[14:25:14.064] Epoch [12/30] Iteration [60/893]: Loss: 0.5781, CE: 0.0878
[14:25:19.514] Epoch [12/30] Iteration [70/893]: Loss: 0.5839, CE: 0.1009
[14:25:24.971] Epoch [12/30] Iteration [80/893]: Loss: 0.5900, CE: 0.0996
[14:25:30.430] Epoch [12/30] Iteration [90/893]: Loss: 0.5729, CE: 0.0926
[14:25:35.879] Epoch [12/30] Iteration [100/893]: Loss: 0.6000, CE: 0.1247
[14:25:41.358] Epoch [12/30] Iteration [110/893]: Loss: 0.5881, CE: 0.0948
[14:25:46.837] Epoch [12/30] Iteration [120/893]: Loss: 0.5895, CE: 0.1024
[14:25:52.341] Epoch [12/30] Iteration [130/893]: Loss: 0.5897, CE: 0.1016
[14:25:57.809] Epoch [12/30] Iteration [140/893]: Loss: 0.6003, CE: 0.1446
[14:26:03.270] Epoch [12/30] Iteration [150/893]: Loss: 0.5731, CE: 0.0579
[14:26:08.715] Epoch [12/30] Iteration [160/893]: Loss: 0.5949, CE: 0.1118
[14:26:14.175] Epoch [12/30] Iteration [170/893]: Loss: 0.5746, CE: 0.0616
[14:26:19.658] Epoch [12/30] Iteration [180/893]: Loss: 0.5760, CE: 0.0647
[14:26:25.114] Epoch [12/30] Iteration [190/893]: Loss: 0.5889, CE: 0.1003
[14:26:30.570] Epoch [12/30] Iteration [200/893]: Loss: 0.5908, CE: 0.1152
[14:26:36.039] Epoch [12/30] Iteration [210/893]: Loss: 0.5879, CE: 0.0943
[14:26:41.531] Epoch [12/30] Iteration [220/893]: Loss: 0.5980, CE: 0.1204
[14:26:46.999] Epoch [12/30] Iteration [230/893]: Loss: 0.5842, CE: 0.0852
[14:26:52.438] Epoch [12/30] Iteration [240/893]: Loss: 0.6108, CE: 0.1513
[14:26:57.893] Epoch [12/30] Iteration [250/893]: Loss: 0.5914, CE: 0.1030
[14:27:03.341] Epoch [12/30] Iteration [260/893]: Loss: 0.5810, CE: 0.0773
[14:27:08.833] Epoch [12/30] Iteration [270/893]: Loss: 0.5962, CE: 0.1150
[14:27:14.282] Epoch [12/30] Iteration [280/893]: Loss: 0.5873, CE: 0.0928
[14:27:19.756] Epoch [12/30] Iteration [290/893]: Loss: 0.5848, CE: 0.0869
[14:27:25.224] Epoch [12/30] Iteration [300/893]: Loss: 0.5859, CE: 0.0896
[14:27:30.703] Epoch [12/30] Iteration [310/893]: Loss: 0.5919, CE: 0.1148
[14:27:36.176] Epoch [12/30] Iteration [320/893]: Loss: 0.5615, CE: 0.0287
[14:27:41.686] Epoch [12/30] Iteration [330/893]: Loss: 0.6041, CE: 0.1345
[14:27:47.131] Epoch [12/30] Iteration [340/893]: Loss: 0.5999, CE: 0.1270
[14:27:52.596] Epoch [12/30] Iteration [350/893]: Loss: 0.6028, CE: 0.1314
[14:27:58.085] Epoch [12/30] Iteration [360/893]: Loss: 0.5978, CE: 0.1194
[14:28:03.587] Epoch [12/30] Iteration [370/893]: Loss: 0.5855, CE: 0.0886
[14:28:09.036] Epoch [12/30] Iteration [380/893]: Loss: 0.5904, CE: 0.1005
[14:28:14.554] Epoch [12/30] Iteration [390/893]: Loss: 0.6020, CE: 0.1367
[14:28:20.028] Epoch [12/30] Iteration [400/893]: Loss: 0.6013, CE: 0.1275
[14:28:25.500] Epoch [12/30] Iteration [410/893]: Loss: 0.5762, CE: 0.0702
[14:28:30.942] Epoch [12/30] Iteration [420/893]: Loss: 0.5980, CE: 0.1198
[14:28:36.460] Epoch [12/30] Iteration [430/893]: Loss: 0.5579, CE: 0.0818
[14:28:41.938] Epoch [12/30] Iteration [440/893]: Loss: 0.6172, CE: 0.1675
[14:28:47.496] Epoch [12/30] Iteration [450/893]: Loss: 0.5992, CE: 0.1224
[14:28:52.956] Epoch [12/30] Iteration [460/893]: Loss: 0.5761, CE: 0.0653
[14:28:58.445] Epoch [12/30] Iteration [470/893]: Loss: 0.6053, CE: 0.1375
[14:29:03.903] Epoch [12/30] Iteration [480/893]: Loss: 0.5781, CE: 0.0990
[14:29:09.349] Epoch [12/30] Iteration [490/893]: Loss: 0.5990, CE: 0.1297
[14:29:14.807] Epoch [12/30] Iteration [500/893]: Loss: 0.5692, CE: 0.0510
[14:29:20.255] Epoch [12/30] Iteration [510/893]: Loss: 0.5862, CE: 0.0937
[14:29:25.716] Epoch [12/30] Iteration [520/893]: Loss: 0.6344, CE: 0.2098
[14:29:31.150] Epoch [12/30] Iteration [530/893]: Loss: 0.5887, CE: 0.0962
[14:29:36.593] Epoch [12/30] Iteration [540/893]: Loss: 0.5902, CE: 0.1043
[14:29:42.069] Epoch [12/30] Iteration [550/893]: Loss: 0.5909, CE: 0.1018
[14:29:47.515] Epoch [12/30] Iteration [560/893]: Loss: 0.5811, CE: 0.0776
[14:29:52.963] Epoch [12/30] Iteration [570/893]: Loss: 0.6115, CE: 0.1532
[14:29:58.403] Epoch [12/30] Iteration [580/893]: Loss: 0.5839, CE: 0.0927
[14:30:03.847] Epoch [12/30] Iteration [590/893]: Loss: 0.5663, CE: 0.0682
[14:30:09.316] Epoch [12/30] Iteration [600/893]: Loss: 0.5660, CE: 0.0467
[14:30:14.769] Epoch [12/30] Iteration [610/893]: Loss: 0.6051, CE: 0.1369
[14:30:20.307] Epoch [12/30] Iteration [620/893]: Loss: 0.5827, CE: 0.0813
[14:30:25.786] Epoch [12/30] Iteration [630/893]: Loss: 0.5852, CE: 0.0887
[14:30:31.248] Epoch [12/30] Iteration [640/893]: Loss: 0.5975, CE: 0.1183
[14:30:36.706] Epoch [12/30] Iteration [650/893]: Loss: 0.5829, CE: 0.0844
[14:30:42.160] Epoch [12/30] Iteration [660/893]: Loss: 0.5766, CE: 0.0666
[14:30:47.627] Epoch [12/30] Iteration [670/893]: Loss: 0.5868, CE: 0.0939
[14:30:53.065] Epoch [12/30] Iteration [680/893]: Loss: 0.5779, CE: 0.0706
[14:30:58.520] Epoch [12/30] Iteration [690/893]: Loss: 0.6082, CE: 0.1655
[14:31:03.998] Epoch [12/30] Iteration [700/893]: Loss: 0.5699, CE: 0.0588
[14:31:09.466] Epoch [12/30] Iteration [710/893]: Loss: 0.5825, CE: 0.0808
[14:31:14.945] Epoch [12/30] Iteration [720/893]: Loss: 0.5697, CE: 0.0528
[14:31:20.414] Epoch [12/30] Iteration [730/893]: Loss: 0.5714, CE: 0.0537
[14:31:25.910] Epoch [12/30] Iteration [740/893]: Loss: 0.5953, CE: 0.1130
[14:31:31.375] Epoch [12/30] Iteration [750/893]: Loss: 0.5821, CE: 0.0798
[14:31:36.851] Epoch [12/30] Iteration [760/893]: Loss: 0.5942, CE: 0.1099
[14:31:42.337] Epoch [12/30] Iteration [770/893]: Loss: 0.5906, CE: 0.1027
[14:31:47.860] Epoch [12/30] Iteration [780/893]: Loss: 0.5791, CE: 0.0752
[14:31:53.333] Epoch [12/30] Iteration [790/893]: Loss: 0.5977, CE: 0.1186
[14:31:58.788] Epoch [12/30] Iteration [800/893]: Loss: 0.6123, CE: 0.1552
[14:32:04.306] Epoch [12/30] Iteration [810/893]: Loss: 0.5975, CE: 0.1182
[14:32:09.756] Epoch [12/30] Iteration [820/893]: Loss: 0.5958, CE: 0.1139
[14:32:15.243] Epoch [12/30] Iteration [830/893]: Loss: 0.6155, CE: 0.1628
[14:32:20.703] Epoch [12/30] Iteration [840/893]: Loss: 0.5924, CE: 0.1060
[14:32:26.173] Epoch [12/30] Iteration [850/893]: Loss: 0.5807, CE: 0.0764
[14:32:31.667] Epoch [12/30] Iteration [860/893]: Loss: 0.5939, CE: 0.1114
[14:32:37.109] Epoch [12/30] Iteration [870/893]: Loss: 0.5868, CE: 0.0917
[14:32:42.619] Epoch [12/30] Iteration [880/893]: Loss: 0.6017, CE: 0.1288
[14:32:48.118] Epoch [12/30] Iteration [890/893]: Loss: 0.5944, CE: 0.1109
[14:32:49.786] Epoch [12/30] Average Loss: 0.5905, CE: 0.1050, Dice: 0.9142
[14:33:12.048] Epoch [13/30] Iteration [0/893]: Loss: 0.5764, CE: 0.0658
[14:33:17.464] Epoch [13/30] Iteration [10/893]: Loss: 0.5719, CE: 0.0548
[14:33:22.936] Epoch [13/30] Iteration [20/893]: Loss: 0.5971, CE: 0.1172
[14:33:28.406] Epoch [13/30] Iteration [30/893]: Loss: 0.5706, CE: 0.1084
[14:33:33.871] Epoch [13/30] Iteration [40/893]: Loss: 0.6046, CE: 0.1362
[14:33:39.317] Epoch [13/30] Iteration [50/893]: Loss: 0.5871, CE: 0.0933
[14:33:44.928] Epoch [13/30] Iteration [60/893]: Loss: 0.5758, CE: 0.0642
[14:33:50.530] Epoch [13/30] Iteration [70/893]: Loss: 0.5664, CE: 0.0476
[14:33:56.088] Epoch [13/30] Iteration [80/893]: Loss: 0.6131, CE: 0.1569
[14:34:01.567] Epoch [13/30] Iteration [90/893]: Loss: 0.5746, CE: 0.0671
[14:34:07.075] Epoch [13/30] Iteration [100/893]: Loss: 0.5685, CE: 0.0465
[14:34:12.574] Epoch [13/30] Iteration [110/893]: Loss: 0.6055, CE: 0.1381
[14:34:18.071] Epoch [13/30] Iteration [120/893]: Loss: 0.5819, CE: 0.0807
[14:34:23.526] Epoch [13/30] Iteration [130/893]: Loss: 0.5611, CE: 0.0289
[14:34:29.012] Epoch [13/30] Iteration [140/893]: Loss: 0.5800, CE: 0.0775
[14:34:34.514] Epoch [13/30] Iteration [150/893]: Loss: 0.5974, CE: 0.1181
[14:34:40.016] Epoch [13/30] Iteration [160/893]: Loss: 0.5898, CE: 0.0998
[14:34:45.474] Epoch [13/30] Iteration [170/893]: Loss: 0.6017, CE: 0.1286
[14:34:50.925] Epoch [13/30] Iteration [180/893]: Loss: 0.5929, CE: 0.1093
[14:34:56.376] Epoch [13/30] Iteration [190/893]: Loss: 0.5905, CE: 0.1054
[14:35:01.869] Epoch [13/30] Iteration [200/893]: Loss: 0.5849, CE: 0.0932
[14:35:07.345] Epoch [13/30] Iteration [210/893]: Loss: 0.5834, CE: 0.0837
[14:35:12.867] Epoch [13/30] Iteration [220/893]: Loss: 0.5943, CE: 0.1104
[14:35:18.344] Epoch [13/30] Iteration [230/893]: Loss: 0.6129, CE: 0.1563
[14:35:23.786] Epoch [13/30] Iteration [240/893]: Loss: 0.6150, CE: 0.1618
[14:35:29.231] Epoch [13/30] Iteration [250/893]: Loss: 0.6340, CE: 0.2090
[14:35:34.671] Epoch [13/30] Iteration [260/893]: Loss: 0.5858, CE: 0.0893
[14:35:40.128] Epoch [13/30] Iteration [270/893]: Loss: 0.5925, CE: 0.1056
[14:35:45.582] Epoch [13/30] Iteration [280/893]: Loss: 0.5986, CE: 0.1217
[14:35:51.032] Epoch [13/30] Iteration [290/893]: Loss: 0.5919, CE: 0.1043
[14:35:56.485] Epoch [13/30] Iteration [300/893]: Loss: 0.5822, CE: 0.0852
[14:36:01.957] Epoch [13/30] Iteration [310/893]: Loss: 0.6076, CE: 0.1431
[14:36:07.425] Epoch [13/30] Iteration [320/893]: Loss: 0.5943, CE: 0.1111
[14:36:12.870] Epoch [13/30] Iteration [330/893]: Loss: 0.5974, CE: 0.1180
[14:36:18.340] Epoch [13/30] Iteration [340/893]: Loss: 0.6049, CE: 0.1405
[14:36:23.777] Epoch [13/30] Iteration [350/893]: Loss: 0.5937, CE: 0.1089
[14:36:29.270] Epoch [13/30] Iteration [360/893]: Loss: 0.5945, CE: 0.1130
[14:36:34.717] Epoch [13/30] Iteration [370/893]: Loss: 0.5763, CE: 0.0658
[14:36:40.185] Epoch [13/30] Iteration [380/893]: Loss: 0.5924, CE: 0.1063
[14:36:45.653] Epoch [13/30] Iteration [390/893]: Loss: 0.5831, CE: 0.0827
[14:36:51.106] Epoch [13/30] Iteration [400/893]: Loss: 0.5538, CE: 0.0402
[14:36:56.547] Epoch [13/30] Iteration [410/893]: Loss: 0.5982, CE: 0.1305
[14:37:02.026] Epoch [13/30] Iteration [420/893]: Loss: 0.5806, CE: 0.0772
[14:37:07.475] Epoch [13/30] Iteration [430/893]: Loss: 0.5890, CE: 0.0970
[14:37:12.926] Epoch [13/30] Iteration [440/893]: Loss: 0.5699, CE: 0.0597
[14:37:18.401] Epoch [13/30] Iteration [450/893]: Loss: 0.5791, CE: 0.0727
[14:37:23.857] Epoch [13/30] Iteration [460/893]: Loss: 0.5819, CE: 0.0798
[14:37:29.292] Epoch [13/30] Iteration [470/893]: Loss: 0.6121, CE: 0.1571
[14:37:34.789] Epoch [13/30] Iteration [480/893]: Loss: 0.6006, CE: 0.1261
[14:37:40.271] Epoch [13/30] Iteration [490/893]: Loss: 0.5856, CE: 0.0917
[14:37:45.724] Epoch [13/30] Iteration [500/893]: Loss: 0.5693, CE: 0.0600
[14:37:51.192] Epoch [13/30] Iteration [510/893]: Loss: 0.5778, CE: 0.0743
[14:37:56.667] Epoch [13/30] Iteration [520/893]: Loss: 0.5949, CE: 0.1122
[14:38:02.115] Epoch [13/30] Iteration [530/893]: Loss: 0.5822, CE: 0.0808
[14:38:07.557] Epoch [13/30] Iteration [540/893]: Loss: 0.5752, CE: 0.0629
[14:38:12.991] Epoch [13/30] Iteration [550/893]: Loss: 0.5835, CE: 0.0836
[14:38:18.483] Epoch [13/30] Iteration [560/893]: Loss: 0.5933, CE: 0.1090
[14:38:23.912] Epoch [13/30] Iteration [570/893]: Loss: 0.5948, CE: 0.1114
[14:38:29.418] Epoch [13/30] Iteration [580/893]: Loss: 0.5824, CE: 0.0805
[14:38:34.856] Epoch [13/30] Iteration [590/893]: Loss: 0.5802, CE: 0.1074
[14:38:40.355] Epoch [13/30] Iteration [600/893]: Loss: 0.5937, CE: 0.1087
[14:38:45.813] Epoch [13/30] Iteration [610/893]: Loss: 0.5841, CE: 0.0846
[14:38:51.262] Epoch [13/30] Iteration [620/893]: Loss: 0.5851, CE: 0.0875
[14:38:56.689] Epoch [13/30] Iteration [630/893]: Loss: 0.5663, CE: 0.0588
[14:39:02.137] Epoch [13/30] Iteration [640/893]: Loss: 0.6085, CE: 0.1458
[14:39:07.575] Epoch [13/30] Iteration [650/893]: Loss: 0.5806, CE: 0.0802
[14:39:13.001] Epoch [13/30] Iteration [660/893]: Loss: 0.6569, CE: 0.2659
[14:39:18.433] Epoch [13/30] Iteration [670/893]: Loss: 0.5864, CE: 0.1107
[14:39:23.888] Epoch [13/30] Iteration [680/893]: Loss: 0.5752, CE: 0.0633
[14:39:29.317] Epoch [13/30] Iteration [690/893]: Loss: 0.5811, CE: 0.0779
[14:39:34.761] Epoch [13/30] Iteration [700/893]: Loss: 0.5878, CE: 0.1096
[14:39:40.259] Epoch [13/30] Iteration [710/893]: Loss: 0.5924, CE: 0.1054
[14:39:45.727] Epoch [13/30] Iteration [720/893]: Loss: 0.6051, CE: 0.1406
[14:39:51.168] Epoch [13/30] Iteration [730/893]: Loss: 0.5874, CE: 0.0929
[14:39:56.618] Epoch [13/30] Iteration [740/893]: Loss: 0.5836, CE: 0.0836
[14:40:02.044] Epoch [13/30] Iteration [750/893]: Loss: 0.6374, CE: 0.2178
[14:40:07.485] Epoch [13/30] Iteration [760/893]: Loss: 0.5797, CE: 0.0936
[14:40:12.953] Epoch [13/30] Iteration [770/893]: Loss: 0.6021, CE: 0.1296
[14:40:18.399] Epoch [13/30] Iteration [780/893]: Loss: 0.5905, CE: 0.1010
[14:40:23.865] Epoch [13/30] Iteration [790/893]: Loss: 0.5964, CE: 0.1161
[14:40:29.312] Epoch [13/30] Iteration [800/893]: Loss: 0.5800, CE: 0.0747
[14:40:34.749] Epoch [13/30] Iteration [810/893]: Loss: 0.5650, CE: 0.0653
[14:40:40.223] Epoch [13/30] Iteration [820/893]: Loss: 0.5799, CE: 0.0981
[14:40:45.714] Epoch [13/30] Iteration [830/893]: Loss: 0.6298, CE: 0.1985
[14:40:51.169] Epoch [13/30] Iteration [840/893]: Loss: 0.5790, CE: 0.0779
[14:40:56.641] Epoch [13/30] Iteration [850/893]: Loss: 0.5925, CE: 0.1056
[14:41:02.099] Epoch [13/30] Iteration [860/893]: Loss: 0.5792, CE: 0.0728
[14:41:07.570] Epoch [13/30] Iteration [870/893]: Loss: 0.5852, CE: 0.0875
[14:41:13.034] Epoch [13/30] Iteration [880/893]: Loss: 0.5845, CE: 0.0860
[14:41:18.522] Epoch [13/30] Iteration [890/893]: Loss: 0.6096, CE: 0.1485
[14:41:20.186] Epoch [13/30] Average Loss: 0.5904, CE: 0.1051, Dice: 0.9140
[14:41:41.479] Epoch [14/30] Iteration [0/893]: Loss: 0.5957, CE: 0.1136
[14:41:46.914] Epoch [14/30] Iteration [10/893]: Loss: 0.6047, CE: 0.1362
[14:41:52.437] Epoch [14/30] Iteration [20/893]: Loss: 0.5851, CE: 0.0874
[14:41:57.885] Epoch [14/30] Iteration [30/893]: Loss: 0.5849, CE: 0.0878
[14:42:03.456] Epoch [14/30] Iteration [40/893]: Loss: 0.5936, CE: 0.1087
[14:42:09.012] Epoch [14/30] Iteration [50/893]: Loss: 0.5756, CE: 0.0637
[14:42:14.530] Epoch [14/30] Iteration [60/893]: Loss: 0.6149, CE: 0.1621
[14:42:20.094] Epoch [14/30] Iteration [70/893]: Loss: 0.5805, CE: 0.0761
[14:42:25.614] Epoch [14/30] Iteration [80/893]: Loss: 0.5735, CE: 0.0584
[14:42:31.092] Epoch [14/30] Iteration [90/893]: Loss: 0.5974, CE: 0.1182
[14:42:36.653] Epoch [14/30] Iteration [100/893]: Loss: 0.5992, CE: 0.1285
[14:42:42.146] Epoch [14/30] Iteration [110/893]: Loss: 0.5894, CE: 0.0982
[14:42:47.656] Epoch [14/30] Iteration [120/893]: Loss: 0.5877, CE: 0.0938
[14:42:53.127] Epoch [14/30] Iteration [130/893]: Loss: 0.5775, CE: 0.0744
[14:42:58.587] Epoch [14/30] Iteration [140/893]: Loss: 0.5838, CE: 0.0841
[14:43:04.080] Epoch [14/30] Iteration [150/893]: Loss: 0.5865, CE: 0.0910
[14:43:09.582] Epoch [14/30] Iteration [160/893]: Loss: 0.5782, CE: 0.0724
[14:43:15.174] Epoch [14/30] Iteration [170/893]: Loss: 0.5941, CE: 0.1103
[14:43:20.640] Epoch [14/30] Iteration [180/893]: Loss: 0.5694, CE: 0.0641
[14:43:26.075] Epoch [14/30] Iteration [190/893]: Loss: 0.6000, CE: 0.1266
[14:43:31.576] Epoch [14/30] Iteration [200/893]: Loss: 0.5810, CE: 0.0861
[14:43:37.161] Epoch [14/30] Iteration [210/893]: Loss: 0.5712, CE: 0.0642
[14:43:42.660] Epoch [14/30] Iteration [220/893]: Loss: 0.5889, CE: 0.1000
[14:43:48.115] Epoch [14/30] Iteration [230/893]: Loss: 0.5942, CE: 0.1103
[14:43:53.565] Epoch [14/30] Iteration [240/893]: Loss: 0.5859, CE: 0.0894
[14:43:59.019] Epoch [14/30] Iteration [250/893]: Loss: 0.5818, CE: 0.0791
[14:44:04.497] Epoch [14/30] Iteration [260/893]: Loss: 0.5866, CE: 0.0911
[14:44:10.004] Epoch [14/30] Iteration [270/893]: Loss: 0.5924, CE: 0.1056
[14:44:15.470] Epoch [14/30] Iteration [280/893]: Loss: 0.6078, CE: 0.1439
[14:44:20.915] Epoch [14/30] Iteration [290/893]: Loss: 0.5850, CE: 0.0886
[14:44:26.412] Epoch [14/30] Iteration [300/893]: Loss: 0.6058, CE: 0.1414
[14:44:31.888] Epoch [14/30] Iteration [310/893]: Loss: 0.5858, CE: 0.0895
[14:44:37.349] Epoch [14/30] Iteration [320/893]: Loss: 0.6091, CE: 0.1470
[14:44:42.815] Epoch [14/30] Iteration [330/893]: Loss: 0.5904, CE: 0.1006
[14:44:48.289] Epoch [14/30] Iteration [340/893]: Loss: 0.5522, CE: 0.0756
[14:44:53.772] Epoch [14/30] Iteration [350/893]: Loss: 0.5988, CE: 0.1213
[14:44:59.265] Epoch [14/30] Iteration [360/893]: Loss: 0.5927, CE: 0.1158
[14:45:04.742] Epoch [14/30] Iteration [370/893]: Loss: 0.5890, CE: 0.0971
[14:45:10.211] Epoch [14/30] Iteration [380/893]: Loss: 0.5682, CE: 0.0769
[14:45:15.683] Epoch [14/30] Iteration [390/893]: Loss: 0.5819, CE: 0.0797
[14:45:21.142] Epoch [14/30] Iteration [400/893]: Loss: 0.6055, CE: 0.1396
[14:45:26.589] Epoch [14/30] Iteration [410/893]: Loss: 0.5780, CE: 0.1042
[14:45:32.056] Epoch [14/30] Iteration [420/893]: Loss: 0.5972, CE: 0.1176
[14:45:37.552] Epoch [14/30] Iteration [430/893]: Loss: 0.6062, CE: 0.1397
[14:45:43.012] Epoch [14/30] Iteration [440/893]: Loss: 0.5838, CE: 0.0861
[14:45:48.453] Epoch [14/30] Iteration [450/893]: Loss: 0.5757, CE: 0.0643
[14:45:53.921] Epoch [14/30] Iteration [460/893]: Loss: 0.5709, CE: 0.0520
[14:45:59.355] Epoch [14/30] Iteration [470/893]: Loss: 0.5925, CE: 0.1056
[14:46:04.789] Epoch [14/30] Iteration [480/893]: Loss: 0.5972, CE: 0.1175
[14:46:10.254] Epoch [14/30] Iteration [490/893]: Loss: 0.5722, CE: 0.0957
[14:46:15.711] Epoch [14/30] Iteration [500/893]: Loss: 0.5789, CE: 0.0732
[14:46:21.172] Epoch [14/30] Iteration [510/893]: Loss: 0.5766, CE: 0.0664
[14:46:26.617] Epoch [14/30] Iteration [520/893]: Loss: 0.6116, CE: 0.1540
[14:46:32.077] Epoch [14/30] Iteration [530/893]: Loss: 0.5894, CE: 0.0981
[14:46:37.537] Epoch [14/30] Iteration [540/893]: Loss: 0.5969, CE: 0.1169
[14:46:42.981] Epoch [14/30] Iteration [550/893]: Loss: 0.5832, CE: 0.0826
[14:46:48.439] Epoch [14/30] Iteration [560/893]: Loss: 0.5787, CE: 0.0713
[14:46:53.869] Epoch [14/30] Iteration [570/893]: Loss: 0.5905, CE: 0.1106
[14:46:59.322] Epoch [14/30] Iteration [580/893]: Loss: 0.6100, CE: 0.1494
[14:47:04.753] Epoch [14/30] Iteration [590/893]: Loss: 0.5731, CE: 0.0689
[14:47:10.184] Epoch [14/30] Iteration [600/893]: Loss: 0.5916, CE: 0.1080
[14:47:15.642] Epoch [14/30] Iteration [610/893]: Loss: 0.5660, CE: 0.0431
[14:47:21.094] Epoch [14/30] Iteration [620/893]: Loss: 0.6188, CE: 0.1712
[14:47:26.512] Epoch [14/30] Iteration [630/893]: Loss: 0.5838, CE: 0.0901
[14:47:31.972] Epoch [14/30] Iteration [640/893]: Loss: 0.5773, CE: 0.0685
[14:47:37.394] Epoch [14/30] Iteration [650/893]: Loss: 0.5790, CE: 0.0725
[14:47:42.863] Epoch [14/30] Iteration [660/893]: Loss: 0.6021, CE: 0.1296
[14:47:48.304] Epoch [14/30] Iteration [670/893]: Loss: 0.5698, CE: 0.0917
[14:47:53.756] Epoch [14/30] Iteration [680/893]: Loss: 0.5641, CE: 0.0745
[14:47:59.208] Epoch [14/30] Iteration [690/893]: Loss: 0.5827, CE: 0.0821
[14:48:04.657] Epoch [14/30] Iteration [700/893]: Loss: 0.5818, CE: 0.0809
[14:48:10.094] Epoch [14/30] Iteration [710/893]: Loss: 0.5975, CE: 0.1183
[14:48:15.549] Epoch [14/30] Iteration [720/893]: Loss: 0.6243, CE: 0.1847
[14:48:21.002] Epoch [14/30] Iteration [730/893]: Loss: 0.5710, CE: 0.0528
[14:48:26.437] Epoch [14/30] Iteration [740/893]: Loss: 0.5809, CE: 0.0773
[14:48:31.909] Epoch [14/30] Iteration [750/893]: Loss: 0.5807, CE: 0.0763
[14:48:37.392] Epoch [14/30] Iteration [760/893]: Loss: 0.5869, CE: 0.0921
[14:48:42.848] Epoch [14/30] Iteration [770/893]: Loss: 0.5746, CE: 0.0617
[14:48:48.299] Epoch [14/30] Iteration [780/893]: Loss: 0.6024, CE: 0.1312
[14:48:53.788] Epoch [14/30] Iteration [790/893]: Loss: 0.5835, CE: 0.0836
[14:48:59.239] Epoch [14/30] Iteration [800/893]: Loss: 0.5726, CE: 0.0572
[14:49:04.710] Epoch [14/30] Iteration [810/893]: Loss: 0.5883, CE: 0.0957
[14:49:10.162] Epoch [14/30] Iteration [820/893]: Loss: 0.5967, CE: 0.1167
[14:49:15.619] Epoch [14/30] Iteration [830/893]: Loss: 0.5932, CE: 0.1086
[14:49:21.110] Epoch [14/30] Iteration [840/893]: Loss: 0.5858, CE: 0.0895
[14:49:26.586] Epoch [14/30] Iteration [850/893]: Loss: 0.6132, CE: 0.1574
[14:49:32.024] Epoch [14/30] Iteration [860/893]: Loss: 0.5656, CE: 0.0655
[14:49:37.471] Epoch [14/30] Iteration [870/893]: Loss: 0.5885, CE: 0.0977
[14:49:42.969] Epoch [14/30] Iteration [880/893]: Loss: 0.5775, CE: 0.0688
[14:49:48.419] Epoch [14/30] Iteration [890/893]: Loss: 0.6102, CE: 0.1502
[14:49:50.090] Epoch [14/30] Average Loss: 0.5906, CE: 0.1051, Dice: 0.9143
[14:49:50.091] Applying TPGM projection update at epoch 15
[14:49:59.187] TPGM iteration 19/50 completed, loss: 0.0240
[14:50:08.974] TPGM iteration 39/50 completed, loss: 0.0786
[14:50:35.307] Epoch [15/30] Iteration [0/893]: Loss: 0.5745, CE: 0.0719
[14:50:40.743] Epoch [15/30] Iteration [10/893]: Loss: 0.5899, CE: 0.0996
[14:50:46.158] Epoch [15/30] Iteration [20/893]: Loss: 0.5908, CE: 0.1015
[14:50:51.608] Epoch [15/30] Iteration [30/893]: Loss: 0.5822, CE: 0.0802
[14:50:57.047] Epoch [15/30] Iteration [40/893]: Loss: 0.6464, CE: 0.2402
[14:51:02.510] Epoch [15/30] Iteration [50/893]: Loss: 0.5936, CE: 0.1085
[14:51:07.950] Epoch [15/30] Iteration [60/893]: Loss: 0.5730, CE: 0.0576
[14:51:13.364] Epoch [15/30] Iteration [70/893]: Loss: 0.6051, CE: 0.1386
[14:51:18.804] Epoch [15/30] Iteration [80/893]: Loss: 0.5772, CE: 0.0730
[14:51:24.250] Epoch [15/30] Iteration [90/893]: Loss: 0.5814, CE: 0.0929
[14:51:29.717] Epoch [15/30] Iteration [100/893]: Loss: 0.5926, CE: 0.1271
[14:51:35.188] Epoch [15/30] Iteration [110/893]: Loss: 0.5900, CE: 0.1038
[14:51:40.627] Epoch [15/30] Iteration [120/893]: Loss: 0.5758, CE: 0.0646
[14:51:46.084] Epoch [15/30] Iteration [130/893]: Loss: 0.5963, CE: 0.1153
[14:51:51.530] Epoch [15/30] Iteration [140/893]: Loss: 0.5851, CE: 0.0873
[14:51:57.002] Epoch [15/30] Iteration [150/893]: Loss: 0.5957, CE: 0.1353
[14:52:02.434] Epoch [15/30] Iteration [160/893]: Loss: 0.5821, CE: 0.0798
[14:52:07.861] Epoch [15/30] Iteration [170/893]: Loss: 0.6031, CE: 0.1321
[14:52:13.342] Epoch [15/30] Iteration [180/893]: Loss: 0.5751, CE: 0.0629
[14:52:18.796] Epoch [15/30] Iteration [190/893]: Loss: 0.5809, CE: 0.0771
[14:52:24.223] Epoch [15/30] Iteration [200/893]: Loss: 0.5619, CE: 0.0498
[14:52:29.653] Epoch [15/30] Iteration [210/893]: Loss: 0.6173, CE: 0.1675
[14:52:35.098] Epoch [15/30] Iteration [220/893]: Loss: 0.6300, CE: 0.1988
[14:52:40.586] Epoch [15/30] Iteration [230/893]: Loss: 0.6030, CE: 0.1318
[14:52:46.021] Epoch [15/30] Iteration [240/893]: Loss: 0.6161, CE: 0.1651
[14:52:51.477] Epoch [15/30] Iteration [250/893]: Loss: 0.5726, CE: 0.0560
[14:52:56.938] Epoch [15/30] Iteration [260/893]: Loss: 0.5687, CE: 0.0580
[14:53:02.397] Epoch [15/30] Iteration [270/893]: Loss: 0.5679, CE: 0.0451
[14:53:07.888] Epoch [15/30] Iteration [280/893]: Loss: 0.5993, CE: 0.1225
[14:53:13.392] Epoch [15/30] Iteration [290/893]: Loss: 0.6019, CE: 0.1292
[14:53:18.832] Epoch [15/30] Iteration [300/893]: Loss: 0.5916, CE: 0.1045
[14:53:24.305] Epoch [15/30] Iteration [310/893]: Loss: 0.6057, CE: 0.1386
[14:53:29.758] Epoch [15/30] Iteration [320/893]: Loss: 0.5880, CE: 0.0945
[14:53:35.218] Epoch [15/30] Iteration [330/893]: Loss: 0.6022, CE: 0.1309
[14:53:40.668] Epoch [15/30] Iteration [340/893]: Loss: 0.5846, CE: 0.0867
[14:53:46.131] Epoch [15/30] Iteration [350/893]: Loss: 0.6106, CE: 0.1511
[14:53:51.578] Epoch [15/30] Iteration [360/893]: Loss: 0.6084, CE: 0.1452
[14:53:57.068] Epoch [15/30] Iteration [370/893]: Loss: 0.5834, CE: 0.0833
[14:54:02.499] Epoch [15/30] Iteration [380/893]: Loss: 0.5925, CE: 0.1086
[14:54:07.942] Epoch [15/30] Iteration [390/893]: Loss: 0.6057, CE: 0.1398
[14:54:13.381] Epoch [15/30] Iteration [400/893]: Loss: 0.6232, CE: 0.1824
[14:54:18.850] Epoch [15/30] Iteration [410/893]: Loss: 0.5759, CE: 0.0643
[14:54:24.285] Epoch [15/30] Iteration [420/893]: Loss: 0.6158, CE: 0.1637
[14:54:29.824] Epoch [15/30] Iteration [430/893]: Loss: 0.6016, CE: 0.1313
[14:54:35.337] Epoch [15/30] Iteration [440/893]: Loss: 0.5899, CE: 0.0994
[14:54:40.875] Epoch [15/30] Iteration [450/893]: Loss: 0.5841, CE: 0.0851
[14:54:46.396] Epoch [15/30] Iteration [460/893]: Loss: 0.5975, CE: 0.1181
[14:54:51.931] Epoch [15/30] Iteration [470/893]: Loss: 0.5834, CE: 0.0833
[14:54:57.418] Epoch [15/30] Iteration [480/893]: Loss: 0.5753, CE: 0.0656
[14:55:02.957] Epoch [15/30] Iteration [490/893]: Loss: 0.5706, CE: 0.0515
[14:55:08.481] Epoch [15/30] Iteration [500/893]: Loss: 0.5852, CE: 0.0874
[14:55:13.978] Epoch [15/30] Iteration [510/893]: Loss: 0.5866, CE: 0.0980
[14:55:19.474] Epoch [15/30] Iteration [520/893]: Loss: 0.6019, CE: 0.1295
[14:55:25.020] Epoch [15/30] Iteration [530/893]: Loss: 0.5831, CE: 0.0826
[14:55:30.479] Epoch [15/30] Iteration [540/893]: Loss: 0.6190, CE: 0.1718
[14:55:36.021] Epoch [15/30] Iteration [550/893]: Loss: 0.5872, CE: 0.0930
[14:55:41.595] Epoch [15/30] Iteration [560/893]: Loss: 0.5801, CE: 0.1053
[14:55:47.089] Epoch [15/30] Iteration [570/893]: Loss: 0.5945, CE: 0.1129
[14:55:52.617] Epoch [15/30] Iteration [580/893]: Loss: 0.5895, CE: 0.0999
[14:55:58.130] Epoch [15/30] Iteration [590/893]: Loss: 0.5916, CE: 0.1079
[14:56:03.644] Epoch [15/30] Iteration [600/893]: Loss: 0.5661, CE: 0.0400
[14:56:09.146] Epoch [15/30] Iteration [610/893]: Loss: 0.6079, CE: 0.1456
[14:56:14.697] Epoch [15/30] Iteration [620/893]: Loss: 0.6360, CE: 0.2140
[14:56:20.179] Epoch [15/30] Iteration [630/893]: Loss: 0.5854, CE: 0.0917
[14:56:25.683] Epoch [15/30] Iteration [640/893]: Loss: 0.5713, CE: 0.0537
[14:56:31.182] Epoch [15/30] Iteration [650/893]: Loss: 0.6002, CE: 0.1249
[14:56:36.648] Epoch [15/30] Iteration [660/893]: Loss: 0.5937, CE: 0.1092
[14:56:42.153] Epoch [15/30] Iteration [670/893]: Loss: 0.5969, CE: 0.1168
[14:56:47.660] Epoch [15/30] Iteration [680/893]: Loss: 0.5835, CE: 0.0835
[14:56:53.138] Epoch [15/30] Iteration [690/893]: Loss: 0.5831, CE: 0.0824
[14:56:58.653] Epoch [15/30] Iteration [700/893]: Loss: 0.6218, CE: 0.1789
[14:57:04.091] Epoch [15/30] Iteration [710/893]: Loss: 0.5762, CE: 0.0693
[14:57:09.533] Epoch [15/30] Iteration [720/893]: Loss: 0.5790, CE: 0.1015
[14:57:14.992] Epoch [15/30] Iteration [730/893]: Loss: 0.6064, CE: 0.1410
[14:57:20.454] Epoch [15/30] Iteration [740/893]: Loss: 0.5872, CE: 0.0944
[14:57:26.010] Epoch [15/30] Iteration [750/893]: Loss: 0.6032, CE: 0.1331
[14:57:31.460] Epoch [15/30] Iteration [760/893]: Loss: 0.5876, CE: 0.0941
[14:57:36.931] Epoch [15/30] Iteration [770/893]: Loss: 0.5932, CE: 0.1078
[14:57:42.375] Epoch [15/30] Iteration [780/893]: Loss: 0.5974, CE: 0.1179
[14:57:47.808] Epoch [15/30] Iteration [790/893]: Loss: 0.6397, CE: 0.2233
[14:57:53.272] Epoch [15/30] Iteration [800/893]: Loss: 0.5947, CE: 0.1114
[14:57:58.738] Epoch [15/30] Iteration [810/893]: Loss: 0.5908, CE: 0.1051
[14:58:04.200] Epoch [15/30] Iteration [820/893]: Loss: 0.5798, CE: 0.0779
[14:58:09.646] Epoch [15/30] Iteration [830/893]: Loss: 0.5942, CE: 0.1099
[14:58:15.087] Epoch [15/30] Iteration [840/893]: Loss: 0.5845, CE: 0.0862
[14:58:20.550] Epoch [15/30] Iteration [850/893]: Loss: 0.5911, CE: 0.1023
[14:58:26.020] Epoch [15/30] Iteration [860/893]: Loss: 0.6084, CE: 0.1453
[14:58:31.523] Epoch [15/30] Iteration [870/893]: Loss: 0.6159, CE: 0.1655
[14:58:36.974] Epoch [15/30] Iteration [880/893]: Loss: 0.5647, CE: 0.0373
[14:58:42.465] Epoch [15/30] Iteration [890/893]: Loss: 0.6198, CE: 0.1736
[14:58:44.130] Epoch [15/30] Average Loss: 0.5906, CE: 0.1055, Dice: 0.9141
[14:59:05.566] Epoch [16/30] Iteration [0/893]: Loss: 0.5841, CE: 0.0850
[14:59:11.019] Epoch [16/30] Iteration [10/893]: Loss: 0.6069, CE: 0.1415
[14:59:16.510] Epoch [16/30] Iteration [20/893]: Loss: 0.5816, CE: 0.0927
[14:59:22.041] Epoch [16/30] Iteration [30/893]: Loss: 0.5719, CE: 0.0936
[14:59:27.530] Epoch [16/30] Iteration [40/893]: Loss: 0.5972, CE: 0.1174
[14:59:32.991] Epoch [16/30] Iteration [50/893]: Loss: 0.5712, CE: 0.0651
[14:59:38.501] Epoch [16/30] Iteration [60/893]: Loss: 0.5765, CE: 0.0663
[14:59:43.972] Epoch [16/30] Iteration [70/893]: Loss: 0.5842, CE: 0.0894
[14:59:49.438] Epoch [16/30] Iteration [80/893]: Loss: 0.5814, CE: 0.0820
[14:59:54.873] Epoch [16/30] Iteration [90/893]: Loss: 0.6034, CE: 0.1331
[15:00:00.331] Epoch [16/30] Iteration [100/893]: Loss: 0.6094, CE: 0.1494
[15:00:05.776] Epoch [16/30] Iteration [110/893]: Loss: 0.5884, CE: 0.0954
[15:00:11.226] Epoch [16/30] Iteration [120/893]: Loss: 0.5982, CE: 0.1208
[15:00:16.681] Epoch [16/30] Iteration [130/893]: Loss: 0.6148, CE: 0.1630
[15:00:22.117] Epoch [16/30] Iteration [140/893]: Loss: 0.5834, CE: 0.0830
[15:00:27.567] Epoch [16/30] Iteration [150/893]: Loss: 0.6011, CE: 0.1276
[15:00:33.063] Epoch [16/30] Iteration [160/893]: Loss: 0.5855, CE: 0.1388
[15:00:38.543] Epoch [16/30] Iteration [170/893]: Loss: 0.5912, CE: 0.1028
[15:00:43.990] Epoch [16/30] Iteration [180/893]: Loss: 0.5982, CE: 0.1206
[15:00:49.456] Epoch [16/30] Iteration [190/893]: Loss: 0.5812, CE: 0.0778
[15:00:54.889] Epoch [16/30] Iteration [200/893]: Loss: 0.6049, CE: 0.1529
[15:01:00.338] Epoch [16/30] Iteration [210/893]: Loss: 0.6061, CE: 0.1399
[15:01:05.836] Epoch [16/30] Iteration [220/893]: Loss: 0.5893, CE: 0.0982
[15:01:11.267] Epoch [16/30] Iteration [230/893]: Loss: 0.5972, CE: 0.1174
[15:01:16.718] Epoch [16/30] Iteration [240/893]: Loss: 0.5742, CE: 0.0605
[15:01:22.166] Epoch [16/30] Iteration [250/893]: Loss: 0.5689, CE: 0.0547
[15:01:27.622] Epoch [16/30] Iteration [260/893]: Loss: 0.5807, CE: 0.0802
[15:01:33.088] Epoch [16/30] Iteration [270/893]: Loss: 0.5775, CE: 0.0684
[15:01:38.541] Epoch [16/30] Iteration [280/893]: Loss: 0.6069, CE: 0.1486
[15:01:43.992] Epoch [16/30] Iteration [290/893]: Loss: 0.5989, CE: 0.1218
[15:01:49.446] Epoch [16/30] Iteration [300/893]: Loss: 0.5737, CE: 0.0591
[15:01:54.904] Epoch [16/30] Iteration [310/893]: Loss: 0.5775, CE: 0.0684
[15:02:00.372] Epoch [16/30] Iteration [320/893]: Loss: 0.5998, CE: 0.1269
[15:02:05.834] Epoch [16/30] Iteration [330/893]: Loss: 0.5801, CE: 0.0792
[15:02:11.324] Epoch [16/30] Iteration [340/893]: Loss: 0.5851, CE: 0.0872
[15:02:16.768] Epoch [16/30] Iteration [350/893]: Loss: 0.5855, CE: 0.0882
[15:02:22.208] Epoch [16/30] Iteration [360/893]: Loss: 0.5881, CE: 0.0951
[15:02:27.681] Epoch [16/30] Iteration [370/893]: Loss: 0.5795, CE: 0.0738
[15:02:33.173] Epoch [16/30] Iteration [380/893]: Loss: 0.5838, CE: 0.0842
[15:02:38.639] Epoch [16/30] Iteration [390/893]: Loss: 0.6000, CE: 0.1244
[15:02:44.126] Epoch [16/30] Iteration [400/893]: Loss: 0.5810, CE: 0.0772
[15:02:49.617] Epoch [16/30] Iteration [410/893]: Loss: 0.5916, CE: 0.1037
[15:02:55.072] Epoch [16/30] Iteration [420/893]: Loss: 0.5761, CE: 0.0650
[15:03:00.534] Epoch [16/30] Iteration [430/893]: Loss: 0.5761, CE: 0.0651
[15:03:06.022] Epoch [16/30] Iteration [440/893]: Loss: 0.5927, CE: 0.1163
[15:03:11.456] Epoch [16/30] Iteration [450/893]: Loss: 0.5891, CE: 0.0984
[15:03:16.917] Epoch [16/30] Iteration [460/893]: Loss: 0.6151, CE: 0.1761
[15:03:22.374] Epoch [16/30] Iteration [470/893]: Loss: 0.5768, CE: 0.0672
[15:03:27.902] Epoch [16/30] Iteration [480/893]: Loss: 0.5518, CE: 0.0772
[15:03:33.362] Epoch [16/30] Iteration [490/893]: Loss: 0.5944, CE: 0.1103
[15:03:38.835] Epoch [16/30] Iteration [500/893]: Loss: 0.5947, CE: 0.1116
[15:03:44.333] Epoch [16/30] Iteration [510/893]: Loss: 0.6048, CE: 0.1369
[15:03:49.801] Epoch [16/30] Iteration [520/893]: Loss: 0.5816, CE: 0.0811
[15:03:55.238] Epoch [16/30] Iteration [530/893]: Loss: 0.6040, CE: 0.1402
[15:04:00.836] Epoch [16/30] Iteration [540/893]: Loss: 0.5834, CE: 0.0832
[15:04:06.322] Epoch [16/30] Iteration [550/893]: Loss: 0.5813, CE: 0.0780
[15:04:11.800] Epoch [16/30] Iteration [560/893]: Loss: 0.6034, CE: 0.1391
[15:04:17.336] Epoch [16/30] Iteration [570/893]: Loss: 0.5884, CE: 0.0958
[15:04:22.870] Epoch [16/30] Iteration [580/893]: Loss: 0.5816, CE: 0.0789
[15:04:28.403] Epoch [16/30] Iteration [590/893]: Loss: 0.6019, CE: 0.1289
[15:04:33.924] Epoch [16/30] Iteration [600/893]: Loss: 0.6000, CE: 0.1246
[15:04:39.434] Epoch [16/30] Iteration [610/893]: Loss: 0.6340, CE: 0.2093
[15:04:44.960] Epoch [16/30] Iteration [620/893]: Loss: 0.5808, CE: 0.0808
[15:04:50.421] Epoch [16/30] Iteration [630/893]: Loss: 0.5836, CE: 0.0844
[15:04:55.924] Epoch [16/30] Iteration [640/893]: Loss: 0.6087, CE: 0.1464
[15:05:01.391] Epoch [16/30] Iteration [650/893]: Loss: 0.5791, CE: 0.0727
[15:05:06.923] Epoch [16/30] Iteration [660/893]: Loss: 0.5876, CE: 0.0937
[15:05:12.404] Epoch [16/30] Iteration [670/893]: Loss: 0.6201, CE: 0.1769
[15:05:17.915] Epoch [16/30] Iteration [680/893]: Loss: 0.5663, CE: 0.0581
[15:05:23.422] Epoch [16/30] Iteration [690/893]: Loss: 0.5861, CE: 0.0901
[15:05:28.928] Epoch [16/30] Iteration [700/893]: Loss: 0.5728, CE: 0.0570
[15:05:34.447] Epoch [16/30] Iteration [710/893]: Loss: 0.5859, CE: 0.0935
[15:05:39.911] Epoch [16/30] Iteration [720/893]: Loss: 0.5900, CE: 0.1002
[15:05:45.359] Epoch [16/30] Iteration [730/893]: Loss: 0.6068, CE: 0.1412
[15:05:50.820] Epoch [16/30] Iteration [740/893]: Loss: 0.5736, CE: 0.0679
[15:05:56.278] Epoch [16/30] Iteration [750/893]: Loss: 0.5805, CE: 0.0759
[15:06:01.739] Epoch [16/30] Iteration [760/893]: Loss: 0.5889, CE: 0.1249
[15:06:07.201] Epoch [16/30] Iteration [770/893]: Loss: 0.6004, CE: 0.1251
[15:06:12.652] Epoch [16/30] Iteration [780/893]: Loss: 0.5811, CE: 0.0774
[15:06:18.109] Epoch [16/30] Iteration [790/893]: Loss: 0.5948, CE: 0.1202
[15:06:23.594] Epoch [16/30] Iteration [800/893]: Loss: 0.6022, CE: 0.1298
[15:06:29.055] Epoch [16/30] Iteration [810/893]: Loss: 0.5894, CE: 0.0982
[15:06:34.549] Epoch [16/30] Iteration [820/893]: Loss: 0.6112, CE: 0.1541
[15:06:40.021] Epoch [16/30] Iteration [830/893]: Loss: 0.5758, CE: 0.0705
[15:06:45.511] Epoch [16/30] Iteration [840/893]: Loss: 0.5871, CE: 0.0925
[15:06:50.957] Epoch [16/30] Iteration [850/893]: Loss: 0.5838, CE: 0.0917
[15:06:56.432] Epoch [16/30] Iteration [860/893]: Loss: 0.5810, CE: 0.0773
[15:07:01.906] Epoch [16/30] Iteration [870/893]: Loss: 0.6082, CE: 0.1486
[15:07:07.384] Epoch [16/30] Iteration [880/893]: Loss: 0.5974, CE: 0.1205
[15:07:12.851] Epoch [16/30] Iteration [890/893]: Loss: 0.5835, CE: 0.0847
[15:07:14.515] Epoch [16/30] Average Loss: 0.5904, CE: 0.1049, Dice: 0.9141
[15:07:35.613] Epoch [17/30] Iteration [0/893]: Loss: 0.5912, CE: 0.1066
[15:07:41.054] Epoch [17/30] Iteration [10/893]: Loss: 0.5928, CE: 0.1066
[15:07:46.499] Epoch [17/30] Iteration [20/893]: Loss: 0.5932, CE: 0.1074
[15:07:51.938] Epoch [17/30] Iteration [30/893]: Loss: 0.5805, CE: 0.0760
[15:07:57.388] Epoch [17/30] Iteration [40/893]: Loss: 0.6004, CE: 0.1292
[15:08:02.859] Epoch [17/30] Iteration [50/893]: Loss: 0.5735, CE: 0.0586
[15:08:08.309] Epoch [17/30] Iteration [60/893]: Loss: 0.6071, CE: 0.1438
[15:08:13.767] Epoch [17/30] Iteration [70/893]: Loss: 0.5945, CE: 0.1585
[15:08:19.225] Epoch [17/30] Iteration [80/893]: Loss: 0.6000, CE: 0.1245
[15:08:24.665] Epoch [17/30] Iteration [90/893]: Loss: 0.5817, CE: 0.1163
[15:08:30.159] Epoch [17/30] Iteration [100/893]: Loss: 0.5890, CE: 0.0969
[15:08:35.672] Epoch [17/30] Iteration [110/893]: Loss: 0.5749, CE: 0.0624
[15:08:41.136] Epoch [17/30] Iteration [120/893]: Loss: 0.6095, CE: 0.1505
[15:08:46.561] Epoch [17/30] Iteration [130/893]: Loss: 0.5914, CE: 0.1034
[15:08:52.001] Epoch [17/30] Iteration [140/893]: Loss: 0.5802, CE: 0.0827
[15:08:57.570] Epoch [17/30] Iteration [150/893]: Loss: 0.5885, CE: 0.0974
[15:09:03.044] Epoch [17/30] Iteration [160/893]: Loss: 0.5937, CE: 0.1091
[15:09:08.504] Epoch [17/30] Iteration [170/893]: Loss: 0.5735, CE: 0.0694
[15:09:13.979] Epoch [17/30] Iteration [180/893]: Loss: 0.5859, CE: 0.0892
[15:09:19.446] Epoch [17/30] Iteration [190/893]: Loss: 0.5788, CE: 0.0723
[15:09:24.924] Epoch [17/30] Iteration [200/893]: Loss: 0.5719, CE: 0.0548
[15:09:30.365] Epoch [17/30] Iteration [210/893]: Loss: 0.5861, CE: 0.0925
[15:09:35.849] Epoch [17/30] Iteration [220/893]: Loss: 0.6012, CE: 0.1277
[15:09:41.399] Epoch [17/30] Iteration [230/893]: Loss: 0.5761, CE: 0.0649
[15:09:46.948] Epoch [17/30] Iteration [240/893]: Loss: 0.5891, CE: 0.0975
[15:09:52.433] Epoch [17/30] Iteration [250/893]: Loss: 0.5975, CE: 0.1181
[15:09:57.902] Epoch [17/30] Iteration [260/893]: Loss: 0.5815, CE: 0.0782
[15:10:03.356] Epoch [17/30] Iteration [270/893]: Loss: 0.5683, CE: 0.0578
[15:10:08.813] Epoch [17/30] Iteration [280/893]: Loss: 0.6002, CE: 0.1250
[15:10:14.272] Epoch [17/30] Iteration [290/893]: Loss: 0.6099, CE: 0.1490
[15:10:19.747] Epoch [17/30] Iteration [300/893]: Loss: 0.5949, CE: 0.1260
[15:10:25.181] Epoch [17/30] Iteration [310/893]: Loss: 0.5801, CE: 0.1045
[15:10:30.677] Epoch [17/30] Iteration [320/893]: Loss: 0.5788, CE: 0.0715
[15:10:36.124] Epoch [17/30] Iteration [330/893]: Loss: 0.5869, CE: 0.0919
[15:10:41.573] Epoch [17/30] Iteration [340/893]: Loss: 0.6149, CE: 0.1617
[15:10:47.062] Epoch [17/30] Iteration [350/893]: Loss: 0.5953, CE: 0.1159
[15:10:52.531] Epoch [17/30] Iteration [360/893]: Loss: 0.5818, CE: 0.0798
[15:10:57.981] Epoch [17/30] Iteration [370/893]: Loss: 0.6041, CE: 0.1344
[15:11:03.445] Epoch [17/30] Iteration [380/893]: Loss: 0.5856, CE: 0.0887
[15:11:08.914] Epoch [17/30] Iteration [390/893]: Loss: 0.6350, CE: 0.2119
[15:11:14.374] Epoch [17/30] Iteration [400/893]: Loss: 0.5798, CE: 0.0743
[15:11:19.825] Epoch [17/30] Iteration [410/893]: Loss: 0.6110, CE: 0.1529
[15:11:25.276] Epoch [17/30] Iteration [420/893]: Loss: 0.5823, CE: 0.0893
[15:11:30.725] Epoch [17/30] Iteration [430/893]: Loss: 0.5742, CE: 0.0659
[15:11:36.207] Epoch [17/30] Iteration [440/893]: Loss: 0.5887, CE: 0.0981
[15:11:41.646] Epoch [17/30] Iteration [450/893]: Loss: 0.5571, CE: 0.0271
[15:11:47.121] Epoch [17/30] Iteration [460/893]: Loss: 0.6039, CE: 0.1346
[15:11:52.594] Epoch [17/30] Iteration [470/893]: Loss: 0.6092, CE: 0.1483
[15:11:58.057] Epoch [17/30] Iteration [480/893]: Loss: 0.5733, CE: 0.0611
[15:12:03.532] Epoch [17/30] Iteration [490/893]: Loss: 0.5621, CE: 0.0617
[15:12:09.003] Epoch [17/30] Iteration [500/893]: Loss: 0.6082, CE: 0.1450
[15:12:14.464] Epoch [17/30] Iteration [510/893]: Loss: 0.5869, CE: 0.0920
[15:12:19.918] Epoch [17/30] Iteration [520/893]: Loss: 0.5712, CE: 0.0529
[15:12:25.392] Epoch [17/30] Iteration [530/893]: Loss: 0.5711, CE: 0.0525
[15:12:30.844] Epoch [17/30] Iteration [540/893]: Loss: 0.5778, CE: 0.0691
[15:12:36.321] Epoch [17/30] Iteration [550/893]: Loss: 0.5983, CE: 0.1203
[15:12:41.819] Epoch [17/30] Iteration [560/893]: Loss: 0.6075, CE: 0.1586
[15:12:47.298] Epoch [17/30] Iteration [570/893]: Loss: 0.5886, CE: 0.0988
[15:12:52.784] Epoch [17/30] Iteration [580/893]: Loss: 0.5917, CE: 0.1039
[15:12:58.226] Epoch [17/30] Iteration [590/893]: Loss: 0.5911, CE: 0.1025
[15:13:03.724] Epoch [17/30] Iteration [600/893]: Loss: 0.5971, CE: 0.1170
[15:13:09.167] Epoch [17/30] Iteration [610/893]: Loss: 0.5935, CE: 0.1123
[15:13:14.631] Epoch [17/30] Iteration [620/893]: Loss: 0.5816, CE: 0.0785
[15:13:20.076] Epoch [17/30] Iteration [630/893]: Loss: 0.5777, CE: 0.0690
[15:13:25.592] Epoch [17/30] Iteration [640/893]: Loss: 0.6047, CE: 0.1367
[15:13:31.126] Epoch [17/30] Iteration [650/893]: Loss: 0.5765, CE: 0.0664
[15:13:36.623] Epoch [17/30] Iteration [660/893]: Loss: 0.5782, CE: 0.0808
[15:13:42.111] Epoch [17/30] Iteration [670/893]: Loss: 0.5924, CE: 0.1056
[15:13:47.653] Epoch [17/30] Iteration [680/893]: Loss: 0.6202, CE: 0.1746
[15:13:53.134] Epoch [17/30] Iteration [690/893]: Loss: 0.5922, CE: 0.1051
[15:13:58.649] Epoch [17/30] Iteration [700/893]: Loss: 0.6069, CE: 0.1415
[15:14:04.133] Epoch [17/30] Iteration [710/893]: Loss: 0.5900, CE: 0.1009
[15:14:09.606] Epoch [17/30] Iteration [720/893]: Loss: 0.5969, CE: 0.1166
[15:14:15.032] Epoch [17/30] Iteration [730/893]: Loss: 0.5762, CE: 0.0652
[15:14:20.511] Epoch [17/30] Iteration [740/893]: Loss: 0.6002, CE: 0.1249
[15:14:25.946] Epoch [17/30] Iteration [750/893]: Loss: 0.5930, CE: 0.1077
[15:14:31.444] Epoch [17/30] Iteration [760/893]: Loss: 0.5854, CE: 0.0881
[15:14:36.908] Epoch [17/30] Iteration [770/893]: Loss: 0.5928, CE: 0.1067
[15:14:42.362] Epoch [17/30] Iteration [780/893]: Loss: 0.5848, CE: 0.0866
[15:14:47.840] Epoch [17/30] Iteration [790/893]: Loss: 0.5871, CE: 0.0928
[15:14:53.317] Epoch [17/30] Iteration [800/893]: Loss: 0.5773, CE: 0.0680
[15:14:58.835] Epoch [17/30] Iteration [810/893]: Loss: 0.5881, CE: 0.0949
[15:15:04.323] Epoch [17/30] Iteration [820/893]: Loss: 0.5898, CE: 0.0990
[15:15:09.887] Epoch [17/30] Iteration [830/893]: Loss: 0.5977, CE: 0.1214
[15:15:15.439] Epoch [17/30] Iteration [840/893]: Loss: 0.5776, CE: 0.0697
[15:15:20.938] Epoch [17/30] Iteration [850/893]: Loss: 0.5640, CE: 0.0665
[15:15:26.391] Epoch [17/30] Iteration [860/893]: Loss: 0.5798, CE: 0.0742
[15:15:31.837] Epoch [17/30] Iteration [870/893]: Loss: 0.5925, CE: 0.1056
[15:15:37.291] Epoch [17/30] Iteration [880/893]: Loss: 0.6017, CE: 0.1289
[15:15:42.716] Epoch [17/30] Iteration [890/893]: Loss: 0.6120, CE: 0.1542
[15:15:44.353] Epoch [17/30] Average Loss: 0.5905, CE: 0.1051, Dice: 0.9141
[15:16:05.070] Epoch [18/30] Iteration [0/893]: Loss: 0.5658, CE: 0.0630
[15:16:10.502] Epoch [18/30] Iteration [10/893]: Loss: 0.6076, CE: 0.1432
[15:16:15.972] Epoch [18/30] Iteration [20/893]: Loss: 0.6042, CE: 0.1350
[15:16:21.486] Epoch [18/30] Iteration [30/893]: Loss: 0.5761, CE: 0.0650
[15:16:26.943] Epoch [18/30] Iteration [40/893]: Loss: 0.6061, CE: 0.1404
[15:16:32.381] Epoch [18/30] Iteration [50/893]: Loss: 0.5824, CE: 0.0813
[15:16:37.833] Epoch [18/30] Iteration [60/893]: Loss: 0.6069, CE: 0.1417
[15:16:43.289] Epoch [18/30] Iteration [70/893]: Loss: 0.5981, CE: 0.1199
[15:16:48.810] Epoch [18/30] Iteration [80/893]: Loss: 0.5851, CE: 0.0874
[15:16:54.249] Epoch [18/30] Iteration [90/893]: Loss: 0.5878, CE: 0.0942
[15:16:59.673] Epoch [18/30] Iteration [100/893]: Loss: 0.6028, CE: 0.1322
[15:17:05.144] Epoch [18/30] Iteration [110/893]: Loss: 0.6116, CE: 0.1536
[15:17:10.581] Epoch [18/30] Iteration [120/893]: Loss: 0.5738, CE: 0.0594
[15:17:16.047] Epoch [18/30] Iteration [130/893]: Loss: 0.5977, CE: 0.1188
[15:17:21.504] Epoch [18/30] Iteration [140/893]: Loss: 0.5815, CE: 0.0786
[15:17:26.985] Epoch [18/30] Iteration [150/893]: Loss: 0.6037, CE: 0.1361
[15:17:32.437] Epoch [18/30] Iteration [160/893]: Loss: 0.5769, CE: 0.0675
[15:17:37.900] Epoch [18/30] Iteration [170/893]: Loss: 0.5918, CE: 0.1047
[15:17:43.387] Epoch [18/30] Iteration [180/893]: Loss: 0.5813, CE: 0.0781
[15:17:48.861] Epoch [18/30] Iteration [190/893]: Loss: 0.5950, CE: 0.1141
[15:17:54.289] Epoch [18/30] Iteration [200/893]: Loss: 0.5935, CE: 0.1106
[15:17:59.740] Epoch [18/30] Iteration [210/893]: Loss: 0.5693, CE: 0.0480
[15:18:05.180] Epoch [18/30] Iteration [220/893]: Loss: 0.6009, CE: 0.1266
[15:18:10.629] Epoch [18/30] Iteration [230/893]: Loss: 0.5874, CE: 0.0936
[15:18:16.069] Epoch [18/30] Iteration [240/893]: Loss: 0.5869, CE: 0.0928
[15:18:21.496] Epoch [18/30] Iteration [250/893]: Loss: 0.5821, CE: 0.0806
[15:18:26.940] Epoch [18/30] Iteration [260/893]: Loss: 0.5472, CE: 0.0688
[15:18:32.402] Epoch [18/30] Iteration [270/893]: Loss: 0.5832, CE: 0.0958
[15:18:37.866] Epoch [18/30] Iteration [280/893]: Loss: 0.5922, CE: 0.1051
[15:18:43.324] Epoch [18/30] Iteration [290/893]: Loss: 0.5967, CE: 0.1181
[15:18:48.762] Epoch [18/30] Iteration [300/893]: Loss: 0.5924, CE: 0.1057
[15:18:54.229] Epoch [18/30] Iteration [310/893]: Loss: 0.5912, CE: 0.1073
[15:18:59.657] Epoch [18/30] Iteration [320/893]: Loss: 0.5919, CE: 0.1043
[15:19:05.137] Epoch [18/30] Iteration [330/893]: Loss: 0.5923, CE: 0.1053
[15:19:10.595] Epoch [18/30] Iteration [340/893]: Loss: 0.5800, CE: 0.0748
[15:19:16.029] Epoch [18/30] Iteration [350/893]: Loss: 0.6240, CE: 0.1842
[15:19:21.467] Epoch [18/30] Iteration [360/893]: Loss: 0.6014, CE: 0.1279
[15:19:26.921] Epoch [18/30] Iteration [370/893]: Loss: 0.5760, CE: 0.0807
[15:19:32.357] Epoch [18/30] Iteration [380/893]: Loss: 0.5952, CE: 0.1125
[15:19:37.831] Epoch [18/30] Iteration [390/893]: Loss: 0.5881, CE: 0.0963
[15:19:43.285] Epoch [18/30] Iteration [400/893]: Loss: 0.5821, CE: 0.0798
[15:19:48.738] Epoch [18/30] Iteration [410/893]: Loss: 0.5996, CE: 0.1237
[15:19:54.223] Epoch [18/30] Iteration [420/893]: Loss: 0.5697, CE: 0.0533
[15:19:59.665] Epoch [18/30] Iteration [430/893]: Loss: 0.5838, CE: 0.0841
[15:20:05.115] Epoch [18/30] Iteration [440/893]: Loss: 0.5696, CE: 0.0494
[15:20:10.626] Epoch [18/30] Iteration [450/893]: Loss: 0.5933, CE: 0.1076
[15:20:16.157] Epoch [18/30] Iteration [460/893]: Loss: 0.5786, CE: 0.0716
[15:20:21.595] Epoch [18/30] Iteration [470/893]: Loss: 0.5862, CE: 0.0973
[15:20:27.029] Epoch [18/30] Iteration [480/893]: Loss: 0.5697, CE: 0.0573
[15:20:32.481] Epoch [18/30] Iteration [490/893]: Loss: 0.5763, CE: 0.0657
[15:20:37.923] Epoch [18/30] Iteration [500/893]: Loss: 0.6072, CE: 0.1426
[15:20:43.370] Epoch [18/30] Iteration [510/893]: Loss: 0.5974, CE: 0.1199
[15:20:48.842] Epoch [18/30] Iteration [520/893]: Loss: 0.5907, CE: 0.1015
[15:20:54.318] Epoch [18/30] Iteration [530/893]: Loss: 0.5936, CE: 0.1085
[15:20:59.773] Epoch [18/30] Iteration [540/893]: Loss: 0.5937, CE: 0.1209
[15:21:05.205] Epoch [18/30] Iteration [550/893]: Loss: 0.5834, CE: 0.0841
[15:21:10.678] Epoch [18/30] Iteration [560/893]: Loss: 0.5940, CE: 0.1096
[15:21:16.142] Epoch [18/30] Iteration [570/893]: Loss: 0.5987, CE: 0.1214
[15:21:21.604] Epoch [18/30] Iteration [580/893]: Loss: 0.5894, CE: 0.0983
[15:21:27.083] Epoch [18/30] Iteration [590/893]: Loss: 0.5693, CE: 0.0481
[15:21:32.519] Epoch [18/30] Iteration [600/893]: Loss: 0.5789, CE: 0.0723
[15:21:37.972] Epoch [18/30] Iteration [610/893]: Loss: 0.5746, CE: 0.0613
[15:21:43.424] Epoch [18/30] Iteration [620/893]: Loss: 0.5804, CE: 0.0889
[15:21:48.857] Epoch [18/30] Iteration [630/893]: Loss: 0.5845, CE: 0.0888
[15:21:54.316] Epoch [18/30] Iteration [640/893]: Loss: 0.5979, CE: 0.1222
[15:21:59.749] Epoch [18/30] Iteration [650/893]: Loss: 0.5744, CE: 0.0609
[15:22:05.176] Epoch [18/30] Iteration [660/893]: Loss: 0.5755, CE: 0.0680
[15:22:10.629] Epoch [18/30] Iteration [670/893]: Loss: 0.5767, CE: 0.0663
[15:22:16.088] Epoch [18/30] Iteration [680/893]: Loss: 0.5775, CE: 0.0688
[15:22:21.545] Epoch [18/30] Iteration [690/893]: Loss: 0.5728, CE: 0.0568
[15:22:26.983] Epoch [18/30] Iteration [700/893]: Loss: 0.5917, CE: 0.1072
[15:22:32.438] Epoch [18/30] Iteration [710/893]: Loss: 0.5867, CE: 0.0914
[15:22:37.905] Epoch [18/30] Iteration [720/893]: Loss: 0.5519, CE: 0.0589
[15:22:43.366] Epoch [18/30] Iteration [730/893]: Loss: 0.5879, CE: 0.0943
[15:22:48.811] Epoch [18/30] Iteration [740/893]: Loss: 0.5972, CE: 0.1175
[15:22:54.256] Epoch [18/30] Iteration [750/893]: Loss: 0.6272, CE: 0.1920
[15:22:59.709] Epoch [18/30] Iteration [760/893]: Loss: 0.5744, CE: 0.0608
[15:23:05.156] Epoch [18/30] Iteration [770/893]: Loss: 0.6231, CE: 0.1822
[15:23:10.574] Epoch [18/30] Iteration [780/893]: Loss: 0.5707, CE: 0.0516
[15:23:16.020] Epoch [18/30] Iteration [790/893]: Loss: 0.5927, CE: 0.1060
[15:23:21.469] Epoch [18/30] Iteration [800/893]: Loss: 0.5886, CE: 0.0966
[15:23:26.936] Epoch [18/30] Iteration [810/893]: Loss: 0.5969, CE: 0.1168
[15:23:32.417] Epoch [18/30] Iteration [820/893]: Loss: 0.5992, CE: 0.1230
[15:23:37.861] Epoch [18/30] Iteration [830/893]: Loss: 0.5725, CE: 0.0568
[15:23:43.299] Epoch [18/30] Iteration [840/893]: Loss: 0.5854, CE: 0.0884
[15:23:48.737] Epoch [18/30] Iteration [850/893]: Loss: 0.5894, CE: 0.1004
[15:23:54.222] Epoch [18/30] Iteration [860/893]: Loss: 0.6088, CE: 0.1464
[15:23:59.702] Epoch [18/30] Iteration [870/893]: Loss: 0.5882, CE: 0.0954
[15:24:05.147] Epoch [18/30] Iteration [880/893]: Loss: 0.5892, CE: 0.0982
[15:24:10.616] Epoch [18/30] Iteration [890/893]: Loss: 0.5911, CE: 0.1023
[15:24:12.258] Epoch [18/30] Average Loss: 0.5904, CE: 0.1050, Dice: 0.9140
[15:24:33.345] Epoch [19/30] Iteration [0/893]: Loss: 0.5807, CE: 0.0767
[15:24:38.774] Epoch [19/30] Iteration [10/893]: Loss: 0.5688, CE: 0.0470
[15:24:44.179] Epoch [19/30] Iteration [20/893]: Loss: 0.6038, CE: 0.1413
[15:24:49.614] Epoch [19/30] Iteration [30/893]: Loss: 0.5868, CE: 0.0916
[15:24:55.051] Epoch [19/30] Iteration [40/893]: Loss: 0.6184, CE: 0.1704
[15:25:00.490] Epoch [19/30] Iteration [50/893]: Loss: 0.5895, CE: 0.0984
[15:25:05.921] Epoch [19/30] Iteration [60/893]: Loss: 0.5808, CE: 0.1003
[15:25:11.361] Epoch [19/30] Iteration [70/893]: Loss: 0.5910, CE: 0.1021
[15:25:16.791] Epoch [19/30] Iteration [80/893]: Loss: 0.5934, CE: 0.1082
[15:25:22.239] Epoch [19/30] Iteration [90/893]: Loss: 0.5932, CE: 0.1072
[15:25:27.698] Epoch [19/30] Iteration [100/893]: Loss: 0.5794, CE: 0.0732
[15:25:33.117] Epoch [19/30] Iteration [110/893]: Loss: 0.5667, CE: 0.0476
[15:25:38.591] Epoch [19/30] Iteration [120/893]: Loss: 0.5771, CE: 0.0716
[15:25:44.036] Epoch [19/30] Iteration [130/893]: Loss: 0.5717, CE: 0.0540
[15:25:49.498] Epoch [19/30] Iteration [140/893]: Loss: 0.5664, CE: 0.0417
[15:25:54.951] Epoch [19/30] Iteration [150/893]: Loss: 0.5690, CE: 0.0483
[15:26:00.409] Epoch [19/30] Iteration [160/893]: Loss: 0.5873, CE: 0.0930
[15:26:05.842] Epoch [19/30] Iteration [170/893]: Loss: 0.5846, CE: 0.0859
[15:26:11.259] Epoch [19/30] Iteration [180/893]: Loss: 0.5942, CE: 0.1098
[15:26:16.697] Epoch [19/30] Iteration [190/893]: Loss: 0.5875, CE: 0.0933
[15:26:22.134] Epoch [19/30] Iteration [200/893]: Loss: 0.6007, CE: 0.1260
[15:26:27.591] Epoch [19/30] Iteration [210/893]: Loss: 0.5975, CE: 0.1179
[15:26:33.132] Epoch [19/30] Iteration [220/893]: Loss: 0.5691, CE: 0.0727
[15:26:38.613] Epoch [19/30] Iteration [230/893]: Loss: 0.5728, CE: 0.0577
[15:26:44.076] Epoch [19/30] Iteration [240/893]: Loss: 0.5875, CE: 0.0971
[15:26:49.515] Epoch [19/30] Iteration [250/893]: Loss: 0.5859, CE: 0.0897
[15:26:54.946] Epoch [19/30] Iteration [260/893]: Loss: 0.5916, CE: 0.1036
[15:27:00.382] Epoch [19/30] Iteration [270/893]: Loss: 0.5618, CE: 0.0886
[15:27:05.841] Epoch [19/30] Iteration [280/893]: Loss: 0.5819, CE: 0.0795
[15:27:11.278] Epoch [19/30] Iteration [290/893]: Loss: 0.5742, CE: 0.0852
[15:27:16.716] Epoch [19/30] Iteration [300/893]: Loss: 0.6134, CE: 0.1577
[15:27:22.139] Epoch [19/30] Iteration [310/893]: Loss: 0.5906, CE: 0.1011
[15:27:27.618] Epoch [19/30] Iteration [320/893]: Loss: 0.5882, CE: 0.0949
[15:27:33.066] Epoch [19/30] Iteration [330/893]: Loss: 0.5895, CE: 0.0984
[15:27:38.505] Epoch [19/30] Iteration [340/893]: Loss: 0.6022, CE: 0.1305
[15:27:43.958] Epoch [19/30] Iteration [350/893]: Loss: 0.5908, CE: 0.1015
[15:27:49.405] Epoch [19/30] Iteration [360/893]: Loss: 0.6004, CE: 0.1263
[15:27:54.838] Epoch [19/30] Iteration [370/893]: Loss: 0.5887, CE: 0.0966
[15:28:00.324] Epoch [19/30] Iteration [380/893]: Loss: 0.5822, CE: 0.0800
[15:28:05.777] Epoch [19/30] Iteration [390/893]: Loss: 0.5847, CE: 0.0865
[15:28:11.217] Epoch [19/30] Iteration [400/893]: Loss: 0.5924, CE: 0.1056
[15:28:16.673] Epoch [19/30] Iteration [410/893]: Loss: 0.5981, CE: 0.1200
[15:28:22.157] Epoch [19/30] Iteration [420/893]: Loss: 0.5819, CE: 0.0794
[15:28:27.589] Epoch [19/30] Iteration [430/893]: Loss: 0.6010, CE: 0.1270
[15:28:33.060] Epoch [19/30] Iteration [440/893]: Loss: 0.5788, CE: 0.0729
[15:28:38.531] Epoch [19/30] Iteration [450/893]: Loss: 0.5801, CE: 0.0782
[15:28:43.978] Epoch [19/30] Iteration [460/893]: Loss: 0.6047, CE: 0.1363
[15:28:49.481] Epoch [19/30] Iteration [470/893]: Loss: 0.6007, CE: 0.1262
[15:28:54.955] Epoch [19/30] Iteration [480/893]: Loss: 0.5856, CE: 0.0910
[15:29:00.453] Epoch [19/30] Iteration [490/893]: Loss: 0.6114, CE: 0.1526
[15:29:05.894] Epoch [19/30] Iteration [500/893]: Loss: 0.5847, CE: 0.0862
[15:29:11.326] Epoch [19/30] Iteration [510/893]: Loss: 0.6027, CE: 0.1316
[15:29:16.770] Epoch [19/30] Iteration [520/893]: Loss: 0.5837, CE: 0.0855
[15:29:22.222] Epoch [19/30] Iteration [530/893]: Loss: 0.5986, CE: 0.1210
[15:29:27.691] Epoch [19/30] Iteration [540/893]: Loss: 0.5882, CE: 0.0951
[15:29:33.167] Epoch [19/30] Iteration [550/893]: Loss: 0.5847, CE: 0.0863
[15:29:38.650] Epoch [19/30] Iteration [560/893]: Loss: 0.5544, CE: 0.0651
[15:29:44.125] Epoch [19/30] Iteration [570/893]: Loss: 0.6059, CE: 0.1390
[15:29:49.603] Epoch [19/30] Iteration [580/893]: Loss: 0.5733, CE: 0.0583
[15:29:55.046] Epoch [19/30] Iteration [590/893]: Loss: 0.5878, CE: 0.0952
[15:30:00.522] Epoch [19/30] Iteration [600/893]: Loss: 0.5999, CE: 0.1277
[15:30:05.993] Epoch [19/30] Iteration [610/893]: Loss: 0.6039, CE: 0.1339
[15:30:11.436] Epoch [19/30] Iteration [620/893]: Loss: 0.5923, CE: 0.1140
[15:30:16.877] Epoch [19/30] Iteration [630/893]: Loss: 0.5837, CE: 0.0840
[15:30:22.344] Epoch [19/30] Iteration [640/893]: Loss: 0.5873, CE: 0.0927
[15:30:27.818] Epoch [19/30] Iteration [650/893]: Loss: 0.5935, CE: 0.1084
[15:30:33.287] Epoch [19/30] Iteration [660/893]: Loss: 0.5927, CE: 0.1062
[15:30:38.723] Epoch [19/30] Iteration [670/893]: Loss: 0.5976, CE: 0.1200
[15:30:44.189] Epoch [19/30] Iteration [680/893]: Loss: 0.5707, CE: 0.0516
[15:30:49.641] Epoch [19/30] Iteration [690/893]: Loss: 0.5831, CE: 0.0989
[15:30:55.096] Epoch [19/30] Iteration [700/893]: Loss: 0.6124, CE: 0.1553
[15:31:00.533] Epoch [19/30] Iteration [710/893]: Loss: 0.6043, CE: 0.1351
[15:31:06.001] Epoch [19/30] Iteration [720/893]: Loss: 0.5863, CE: 0.0903
[15:31:11.509] Epoch [19/30] Iteration [730/893]: Loss: 0.5811, CE: 0.0774
[15:31:16.958] Epoch [19/30] Iteration [740/893]: Loss: 0.5918, CE: 0.1041
[15:31:22.384] Epoch [19/30] Iteration [750/893]: Loss: 0.5587, CE: 0.0356
[15:31:27.928] Epoch [19/30] Iteration [760/893]: Loss: 0.5884, CE: 0.0957
[15:31:33.399] Epoch [19/30] Iteration [770/893]: Loss: 0.5959, CE: 0.1143
[15:31:38.866] Epoch [19/30] Iteration [780/893]: Loss: 0.5794, CE: 0.0733
[15:31:44.318] Epoch [19/30] Iteration [790/893]: Loss: 0.5719, CE: 0.0547
[15:31:49.790] Epoch [19/30] Iteration [800/893]: Loss: 0.5995, CE: 0.1245
[15:31:55.241] Epoch [19/30] Iteration [810/893]: Loss: 0.5888, CE: 0.0966
[15:32:00.716] Epoch [19/30] Iteration [820/893]: Loss: 0.5957, CE: 0.1306
[15:32:06.152] Epoch [19/30] Iteration [830/893]: Loss: 0.5982, CE: 0.1197
[15:32:11.630] Epoch [19/30] Iteration [840/893]: Loss: 0.5811, CE: 0.0814
[15:32:17.070] Epoch [19/30] Iteration [850/893]: Loss: 0.5892, CE: 0.0979
[15:32:22.521] Epoch [19/30] Iteration [860/893]: Loss: 0.5927, CE: 0.1060
[15:32:27.948] Epoch [19/30] Iteration [870/893]: Loss: 0.5858, CE: 0.0890
[15:32:33.406] Epoch [19/30] Iteration [880/893]: Loss: 0.5714, CE: 0.0542
[15:32:38.854] Epoch [19/30] Iteration [890/893]: Loss: 0.5945, CE: 0.1114
[15:32:40.597] Epoch [19/30] Average Loss: 0.5906, CE: 0.1054, Dice: 0.9141
[15:32:40.770] Saved continual learning checkpoint to ./universal/synapse_to_kits23_tpgm\epoch_19_continual.pth
[15:32:40.771] Updating surgical weights at epoch 20
[15:33:04.373] Surgical weights range: [0.0002, 1.0000]
[15:33:04.373] Applying TPGM projection update at epoch 20
[15:33:13.385] TPGM iteration 19/50 completed, loss: 0.0774
[15:33:22.897] TPGM iteration 39/50 completed, loss: 0.1357
[15:33:48.949] Epoch [20/30] Iteration [0/893]: Loss: 0.5830, CE: 0.0824
[15:33:54.356] Epoch [20/30] Iteration [10/893]: Loss: 0.6189, CE: 0.1716
[15:33:59.784] Epoch [20/30] Iteration [20/893]: Loss: 0.5976, CE: 0.1185
[15:34:05.221] Epoch [20/30] Iteration [30/893]: Loss: 0.5957, CE: 0.1193
[15:34:10.674] Epoch [20/30] Iteration [40/893]: Loss: 0.6064, CE: 0.1409
[15:34:16.114] Epoch [20/30] Iteration [50/893]: Loss: 0.6037, CE: 0.1825
[15:34:21.585] Epoch [20/30] Iteration [60/893]: Loss: 0.6026, CE: 0.1308
[15:34:27.019] Epoch [20/30] Iteration [70/893]: Loss: 0.5906, CE: 0.1010
[15:34:32.504] Epoch [20/30] Iteration [80/893]: Loss: 0.5980, CE: 0.1194
[15:34:37.965] Epoch [20/30] Iteration [90/893]: Loss: 0.5703, CE: 0.0515
[15:34:43.466] Epoch [20/30] Iteration [100/893]: Loss: 0.5712, CE: 0.0529
[15:34:48.898] Epoch [20/30] Iteration [110/893]: Loss: 0.5758, CE: 0.0648
[15:34:54.371] Epoch [20/30] Iteration [120/893]: Loss: 0.5851, CE: 0.0876
[15:34:59.840] Epoch [20/30] Iteration [130/893]: Loss: 0.5764, CE: 0.0657
[15:35:05.298] Epoch [20/30] Iteration [140/893]: Loss: 0.5982, CE: 0.1200
[15:35:10.733] Epoch [20/30] Iteration [150/893]: Loss: 0.6140, CE: 0.1595
[15:35:16.188] Epoch [20/30] Iteration [160/893]: Loss: 0.5841, CE: 0.0850
[15:35:21.616] Epoch [20/30] Iteration [170/893]: Loss: 0.5786, CE: 0.0713
[15:35:27.088] Epoch [20/30] Iteration [180/893]: Loss: 0.5789, CE: 0.0723
[15:35:32.509] Epoch [20/30] Iteration [190/893]: Loss: 0.5890, CE: 0.0970
[15:35:37.945] Epoch [20/30] Iteration [200/893]: Loss: 0.5867, CE: 0.0912
[15:35:43.391] Epoch [20/30] Iteration [210/893]: Loss: 0.6010, CE: 0.1269
[15:35:48.818] Epoch [20/30] Iteration [220/893]: Loss: 0.5788, CE: 0.0722
[15:35:54.260] Epoch [20/30] Iteration [230/893]: Loss: 0.5804, CE: 0.0758
[15:35:59.728] Epoch [20/30] Iteration [240/893]: Loss: 0.5985, CE: 0.1213
[15:36:05.177] Epoch [20/30] Iteration [250/893]: Loss: 0.5771, CE: 0.0824
[15:36:10.621] Epoch [20/30] Iteration [260/893]: Loss: 0.5804, CE: 0.0757
[15:36:16.060] Epoch [20/30] Iteration [270/893]: Loss: 0.5843, CE: 0.0855
[15:36:21.538] Epoch [20/30] Iteration [280/893]: Loss: 0.5831, CE: 0.0824
[15:36:27.007] Epoch [20/30] Iteration [290/893]: Loss: 0.6014, CE: 0.1276
[15:36:32.565] Epoch [20/30] Iteration [300/893]: Loss: 0.5808, CE: 0.0768
[15:36:38.006] Epoch [20/30] Iteration [310/893]: Loss: 0.6074, CE: 0.1428
[15:36:43.499] Epoch [20/30] Iteration [320/893]: Loss: 0.6095, CE: 0.1486
[15:36:48.957] Epoch [20/30] Iteration [330/893]: Loss: 0.5984, CE: 0.1203
[15:36:54.392] Epoch [20/30] Iteration [340/893]: Loss: 0.5843, CE: 0.0855
[15:36:59.843] Epoch [20/30] Iteration [350/893]: Loss: 0.6151, CE: 0.1621
[15:37:05.280] Epoch [20/30] Iteration [360/893]: Loss: 0.5908, CE: 0.1017
[15:37:10.746] Epoch [20/30] Iteration [370/893]: Loss: 0.5804, CE: 0.0758
[15:37:16.187] Epoch [20/30] Iteration [380/893]: Loss: 0.5963, CE: 0.1153
[15:37:21.614] Epoch [20/30] Iteration [390/893]: Loss: 0.5916, CE: 0.1037
[15:37:27.065] Epoch [20/30] Iteration [400/893]: Loss: 0.5935, CE: 0.1093
[15:37:32.479] Epoch [20/30] Iteration [410/893]: Loss: 0.6068, CE: 0.1414
[15:37:37.951] Epoch [20/30] Iteration [420/893]: Loss: 0.5924, CE: 0.1056
[15:37:43.393] Epoch [20/30] Iteration [430/893]: Loss: 0.5958, CE: 0.1174
[15:37:48.853] Epoch [20/30] Iteration [440/893]: Loss: 0.5971, CE: 0.1230
[15:37:54.322] Epoch [20/30] Iteration [450/893]: Loss: 0.6116, CE: 0.1532
[15:37:59.777] Epoch [20/30] Iteration [460/893]: Loss: 0.6158, CE: 0.1638
[15:38:05.251] Epoch [20/30] Iteration [470/893]: Loss: 0.5763, CE: 0.0732
[15:38:10.710] Epoch [20/30] Iteration [480/893]: Loss: 0.5939, CE: 0.1106
[15:38:16.172] Epoch [20/30] Iteration [490/893]: Loss: 0.5976, CE: 0.1184
[15:38:21.663] Epoch [20/30] Iteration [500/893]: Loss: 0.5883, CE: 0.0951
[15:38:27.110] Epoch [20/30] Iteration [510/893]: Loss: 0.5812, CE: 0.0777
[15:38:32.595] Epoch [20/30] Iteration [520/893]: Loss: 0.5859, CE: 0.0894
[15:38:38.043] Epoch [20/30] Iteration [530/893]: Loss: 0.5890, CE: 0.0987
[15:38:43.496] Epoch [20/30] Iteration [540/893]: Loss: 0.5989, CE: 0.1219
[15:38:48.934] Epoch [20/30] Iteration [550/893]: Loss: 0.5981, CE: 0.1196
[15:38:54.386] Epoch [20/30] Iteration [560/893]: Loss: 0.5804, CE: 0.0762
[15:38:59.825] Epoch [20/30] Iteration [570/893]: Loss: 0.5881, CE: 0.0958
[15:39:05.279] Epoch [20/30] Iteration [580/893]: Loss: 0.5939, CE: 0.1090
[15:39:10.736] Epoch [20/30] Iteration [590/893]: Loss: 0.5847, CE: 0.0957
[15:39:16.241] Epoch [20/30] Iteration [600/893]: Loss: 0.5884, CE: 0.0953
[15:39:21.695] Epoch [20/30] Iteration [610/893]: Loss: 0.5918, CE: 0.1038
[15:39:27.179] Epoch [20/30] Iteration [620/893]: Loss: 0.6073, CE: 0.1426
[15:39:32.640] Epoch [20/30] Iteration [630/893]: Loss: 0.6088, CE: 0.1525
[15:39:38.101] Epoch [20/30] Iteration [640/893]: Loss: 0.6142, CE: 0.1740
[15:39:43.626] Epoch [20/30] Iteration [650/893]: Loss: 0.6002, CE: 0.1248
[15:39:49.087] Epoch [20/30] Iteration [660/893]: Loss: 0.5887, CE: 0.0963
[15:39:54.519] Epoch [20/30] Iteration [670/893]: Loss: 0.5907, CE: 0.1012
[15:40:00.003] Epoch [20/30] Iteration [680/893]: Loss: 0.5842, CE: 0.0851
[15:40:05.464] Epoch [20/30] Iteration [690/893]: Loss: 0.5753, CE: 0.0767
[15:40:10.933] Epoch [20/30] Iteration [700/893]: Loss: 0.5941, CE: 0.1100
[15:40:16.437] Epoch [20/30] Iteration [710/893]: Loss: 0.5775, CE: 0.0693
[15:40:21.889] Epoch [20/30] Iteration [720/893]: Loss: 0.5957, CE: 0.1138
[15:40:27.362] Epoch [20/30] Iteration [730/893]: Loss: 0.6013, CE: 0.1301
[15:40:32.817] Epoch [20/30] Iteration [740/893]: Loss: 0.5976, CE: 0.1186
[15:40:38.274] Epoch [20/30] Iteration [750/893]: Loss: 0.5739, CE: 0.0755
[15:40:43.729] Epoch [20/30] Iteration [760/893]: Loss: 0.5775, CE: 0.0684
[15:40:49.171] Epoch [20/30] Iteration [770/893]: Loss: 0.5901, CE: 0.0999
[15:40:54.671] Epoch [20/30] Iteration [780/893]: Loss: 0.5656, CE: 0.0394
[15:41:00.138] Epoch [20/30] Iteration [790/893]: Loss: 0.5580, CE: 0.0972
[15:41:05.575] Epoch [20/30] Iteration [800/893]: Loss: 0.5546, CE: 0.0864
[15:41:11.076] Epoch [20/30] Iteration [810/893]: Loss: 0.5974, CE: 0.1191
[15:41:16.521] Epoch [20/30] Iteration [820/893]: Loss: 0.5903, CE: 0.1002
[15:41:22.002] Epoch [20/30] Iteration [830/893]: Loss: 0.5768, CE: 0.0671
[15:41:27.461] Epoch [20/30] Iteration [840/893]: Loss: 0.5750, CE: 0.0641
[15:41:32.915] Epoch [20/30] Iteration [850/893]: Loss: 0.6038, CE: 0.1344
[15:41:38.375] Epoch [20/30] Iteration [860/893]: Loss: 0.5757, CE: 0.0647
[15:41:43.812] Epoch [20/30] Iteration [870/893]: Loss: 0.5897, CE: 0.0995
[15:41:49.252] Epoch [20/30] Iteration [880/893]: Loss: 0.6024, CE: 0.1323
[15:41:54.715] Epoch [20/30] Iteration [890/893]: Loss: 0.5955, CE: 0.1132
[15:41:56.402] Epoch [20/30] Average Loss: 0.5905, CE: 0.1054, Dice: 0.9139
[15:42:17.594] Epoch [21/30] Iteration [0/893]: Loss: 0.5844, CE: 0.0858
[15:42:23.030] Epoch [21/30] Iteration [10/893]: Loss: 0.5880, CE: 0.0948
[15:42:28.479] Epoch [21/30] Iteration [20/893]: Loss: 0.5809, CE: 0.0769
[15:42:33.927] Epoch [21/30] Iteration [30/893]: Loss: 0.6148, CE: 0.1618
[15:42:39.349] Epoch [21/30] Iteration [40/893]: Loss: 0.5785, CE: 0.0840
[15:42:41.934] Epoch [21/30] Iteration [50/893]: Loss: 0.5782, CE: 0.0701
[15:42:47.378] Epoch [21/30] Iteration [60/893]: Loss: 0.6056, CE: 0.1387
[15:42:52.837] Epoch [21/30] Iteration [70/893]: Loss: 0.5911, CE: 0.1027
[15:42:58.295] Epoch [21/30] Iteration [80/893]: Loss: 0.5781, CE: 0.0701
[15:43:03.744] Epoch [21/30] Iteration [90/893]: Loss: 0.6028, CE: 0.1326
[15:43:09.175] Epoch [21/30] Iteration [100/893]: Loss: 0.5810, CE: 0.0773
[15:43:14.596] Epoch [21/30] Iteration [110/893]: Loss: 0.5678, CE: 0.0602
[15:43:20.027] Epoch [21/30] Iteration [120/893]: Loss: 0.5721, CE: 0.0655
[15:43:25.523] Epoch [21/30] Iteration [130/893]: Loss: 0.5977, CE: 0.1183
[15:43:30.934] Epoch [21/30] Iteration [140/893]: Loss: 0.5913, CE: 0.1026
[15:43:36.368] Epoch [21/30] Iteration [150/893]: Loss: 0.6064, CE: 0.1401
[15:43:41.828] Epoch [21/30] Iteration [160/893]: Loss: 0.5590, CE: 0.0408
[15:43:47.258] Epoch [21/30] Iteration [170/893]: Loss: 0.6113, CE: 0.1525
[15:43:52.684] Epoch [21/30] Iteration [180/893]: Loss: 0.5843, CE: 0.0853
[15:43:58.133] Epoch [21/30] Iteration [190/893]: Loss: 0.5898, CE: 0.1012
[15:44:03.576] Epoch [21/30] Iteration [200/893]: Loss: 0.5472, CE: 0.0659
[15:44:09.022] Epoch [21/30] Iteration [210/893]: Loss: 0.5303, CE: 0.0430
[15:44:14.470] Epoch [21/30] Iteration [220/893]: Loss: 0.5714, CE: 0.0535
[15:44:19.938] Epoch [21/30] Iteration [230/893]: Loss: 0.6236, CE: 0.1831
[15:44:25.353] Epoch [21/30] Iteration [240/893]: Loss: 0.6087, CE: 0.1488
[15:44:30.799] Epoch [21/30] Iteration [250/893]: Loss: 0.5765, CE: 0.0729
[15:44:36.260] Epoch [21/30] Iteration [260/893]: Loss: 0.5768, CE: 0.0668
[15:44:41.776] Epoch [21/30] Iteration [270/893]: Loss: 0.5792, CE: 0.0938
[15:44:47.198] Epoch [21/30] Iteration [280/893]: Loss: 0.5867, CE: 0.0932
[15:44:52.654] Epoch [21/30] Iteration [290/893]: Loss: 0.5790, CE: 0.0758
[15:44:58.126] Epoch [21/30] Iteration [300/893]: Loss: 0.6152, CE: 0.1624
[15:45:03.580] Epoch [21/30] Iteration [310/893]: Loss: 0.6018, CE: 0.1289
[15:45:09.017] Epoch [21/30] Iteration [320/893]: Loss: 0.6176, CE: 0.1681
[15:45:14.499] Epoch [21/30] Iteration [330/893]: Loss: 0.5797, CE: 0.0741
[15:45:19.939] Epoch [21/30] Iteration [340/893]: Loss: 0.5983, CE: 0.1229
[15:45:25.393] Epoch [21/30] Iteration [350/893]: Loss: 0.5909, CE: 0.1022
[15:45:30.875] Epoch [21/30] Iteration [360/893]: Loss: 0.5878, CE: 0.0941
[15:45:36.371] Epoch [21/30] Iteration [370/893]: Loss: 0.5929, CE: 0.1206
[15:45:41.823] Epoch [21/30] Iteration [380/893]: Loss: 0.5804, CE: 0.0778
[15:45:47.314] Epoch [21/30] Iteration [390/893]: Loss: 0.5737, CE: 0.0592
[15:45:52.754] Epoch [21/30] Iteration [400/893]: Loss: 0.5775, CE: 0.0686
[15:45:58.186] Epoch [21/30] Iteration [410/893]: Loss: 0.5962, CE: 0.1150
[15:46:03.675] Epoch [21/30] Iteration [420/893]: Loss: 0.5950, CE: 0.1118
[15:46:09.141] Epoch [21/30] Iteration [430/893]: Loss: 0.5743, CE: 0.0607
[15:46:14.589] Epoch [21/30] Iteration [440/893]: Loss: 0.5812, CE: 0.1009
[15:46:20.070] Epoch [21/30] Iteration [450/893]: Loss: 0.5890, CE: 0.0987
[15:46:25.531] Epoch [21/30] Iteration [460/893]: Loss: 0.6052, CE: 0.1415
[15:46:30.975] Epoch [21/30] Iteration [470/893]: Loss: 0.5999, CE: 0.1259
[15:46:36.414] Epoch [21/30] Iteration [480/893]: Loss: 0.5880, CE: 0.0953
[15:46:41.891] Epoch [21/30] Iteration [490/893]: Loss: 0.5777, CE: 0.0724
[15:46:47.337] Epoch [21/30] Iteration [500/893]: Loss: 0.5999, CE: 0.1243
[15:46:52.814] Epoch [21/30] Iteration [510/893]: Loss: 0.5901, CE: 0.1001
[15:46:58.280] Epoch [21/30] Iteration [520/893]: Loss: 0.5842, CE: 0.0851
[15:47:03.735] Epoch [21/30] Iteration [530/893]: Loss: 0.5773, CE: 0.0737
[15:47:09.226] Epoch [21/30] Iteration [540/893]: Loss: 0.5949, CE: 0.1140
[15:47:14.670] Epoch [21/30] Iteration [550/893]: Loss: 0.6065, CE: 0.1407
[15:47:20.207] Epoch [21/30] Iteration [560/893]: Loss: 0.5959, CE: 0.1260
[15:47:25.659] Epoch [21/30] Iteration [570/893]: Loss: 0.5914, CE: 0.1036
[15:47:31.163] Epoch [21/30] Iteration [580/893]: Loss: 0.5800, CE: 0.0746
[15:47:36.607] Epoch [21/30] Iteration [590/893]: Loss: 0.5882, CE: 0.0953
[15:47:42.053] Epoch [21/30] Iteration [600/893]: Loss: 0.6392, CE: 0.2223
[15:47:47.536] Epoch [21/30] Iteration [610/893]: Loss: 0.5723, CE: 0.0734
[15:47:53.049] Epoch [21/30] Iteration [620/893]: Loss: 0.5811, CE: 0.0822
[15:47:58.511] Epoch [21/30] Iteration [630/893]: Loss: 0.5918, CE: 0.1039
[15:48:03.995] Epoch [21/30] Iteration [640/893]: Loss: 0.5858, CE: 0.0908
[15:48:09.435] Epoch [21/30] Iteration [650/893]: Loss: 0.5828, CE: 0.0817
[15:48:14.895] Epoch [21/30] Iteration [660/893]: Loss: 0.5784, CE: 0.0705
[15:48:20.397] Epoch [21/30] Iteration [670/893]: Loss: 0.5831, CE: 0.0822
[15:48:25.903] Epoch [21/30] Iteration [680/893]: Loss: 0.5816, CE: 0.0793
[15:48:31.363] Epoch [21/30] Iteration [690/893]: Loss: 0.5674, CE: 0.0880
[15:48:36.795] Epoch [21/30] Iteration [700/893]: Loss: 0.5834, CE: 0.0903
[15:48:42.247] Epoch [21/30] Iteration [710/893]: Loss: 0.5895, CE: 0.0984
[15:48:47.709] Epoch [21/30] Iteration [720/893]: Loss: 0.5966, CE: 0.1238
[15:48:53.168] Epoch [21/30] Iteration [730/893]: Loss: 0.6057, CE: 0.1446
[15:48:58.641] Epoch [21/30] Iteration [740/893]: Loss: 0.5803, CE: 0.0754
[15:49:04.147] Epoch [21/30] Iteration [750/893]: Loss: 0.5940, CE: 0.1100
[15:49:09.614] Epoch [21/30] Iteration [760/893]: Loss: 0.6007, CE: 0.1265
[15:49:15.144] Epoch [21/30] Iteration [770/893]: Loss: 0.5916, CE: 0.1036
[15:49:20.694] Epoch [21/30] Iteration [780/893]: Loss: 0.5941, CE: 0.1110
[15:49:26.209] Epoch [21/30] Iteration [790/893]: Loss: 0.6093, CE: 0.1474
[15:49:31.668] Epoch [21/30] Iteration [800/893]: Loss: 0.5831, CE: 0.0827
[15:49:37.156] Epoch [21/30] Iteration [810/893]: Loss: 0.5994, CE: 0.1230
[15:49:42.667] Epoch [21/30] Iteration [820/893]: Loss: 0.5978, CE: 0.1188
[15:49:48.220] Epoch [21/30] Iteration [830/893]: Loss: 0.5897, CE: 0.1027
[15:49:53.675] Epoch [21/30] Iteration [840/893]: Loss: 0.6484, CE: 0.2448
[15:49:59.187] Epoch [21/30] Iteration [850/893]: Loss: 0.6022, CE: 0.1378
[15:50:04.635] Epoch [21/30] Iteration [860/893]: Loss: 0.6015, CE: 0.1285
[15:50:10.245] Epoch [21/30] Iteration [870/893]: Loss: 0.5848, CE: 0.0865
[15:50:15.822] Epoch [21/30] Iteration [880/893]: Loss: 0.5771, CE: 0.0677
[15:50:21.341] Epoch [21/30] Iteration [890/893]: Loss: 0.6019, CE: 0.1290
[15:50:23.020] Epoch [21/30] Average Loss: 0.5903, CE: 0.1051, Dice: 0.9138
[15:50:44.572] Epoch [22/30] Iteration [0/893]: Loss: 0.5972, CE: 0.1181
[15:50:50.088] Epoch [22/30] Iteration [10/893]: Loss: 0.5966, CE: 0.1160
[15:50:55.588] Epoch [22/30] Iteration [20/893]: Loss: 0.6138, CE: 0.1676
[15:51:01.070] Epoch [22/30] Iteration [30/893]: Loss: 0.5772, CE: 0.0683
[15:51:06.553] Epoch [22/30] Iteration [40/893]: Loss: 0.5559, CE: 0.0367
[15:51:12.008] Epoch [22/30] Iteration [50/893]: Loss: 0.5798, CE: 0.0754
[15:51:17.444] Epoch [22/30] Iteration [60/893]: Loss: 0.5894, CE: 0.0984
[15:51:22.892] Epoch [22/30] Iteration [70/893]: Loss: 0.5735, CE: 0.0585
[15:51:28.382] Epoch [22/30] Iteration [80/893]: Loss: 0.5849, CE: 0.0971
[15:51:33.827] Epoch [22/30] Iteration [90/893]: Loss: 0.5656, CE: 0.0389
[15:51:39.316] Epoch [22/30] Iteration [100/893]: Loss: 0.5982, CE: 0.1308
[15:51:44.749] Epoch [22/30] Iteration [110/893]: Loss: 0.6136, CE: 0.1583
[15:51:50.190] Epoch [22/30] Iteration [120/893]: Loss: 0.5875, CE: 0.0956
[15:51:55.694] Epoch [22/30] Iteration [130/893]: Loss: 0.5707, CE: 0.0538
[15:52:01.135] Epoch [22/30] Iteration [140/893]: Loss: 0.6095, CE: 0.1481
[15:52:06.567] Epoch [22/30] Iteration [150/893]: Loss: 0.5869, CE: 0.0920
[15:52:12.061] Epoch [22/30] Iteration [160/893]: Loss: 0.5923, CE: 0.1080
[15:52:17.499] Epoch [22/30] Iteration [170/893]: Loss: 0.5873, CE: 0.0929
[15:52:22.942] Epoch [22/30] Iteration [180/893]: Loss: 0.5742, CE: 0.0602
[15:52:28.360] Epoch [22/30] Iteration [190/893]: Loss: 0.6115, CE: 0.1550
[15:52:33.798] Epoch [22/30] Iteration [200/893]: Loss: 0.5925, CE: 0.1061
[15:52:39.271] Epoch [22/30] Iteration [210/893]: Loss: 0.5722, CE: 0.0758
[15:52:44.734] Epoch [22/30] Iteration [220/893]: Loss: 0.5769, CE: 0.0707
[15:52:50.175] Epoch [22/30] Iteration [230/893]: Loss: 0.6209, CE: 0.1765
[15:52:55.626] Epoch [22/30] Iteration [240/893]: Loss: 0.5938, CE: 0.1089
[15:53:01.115] Epoch [22/30] Iteration [250/893]: Loss: 0.5835, CE: 0.0842
[15:53:06.607] Epoch [22/30] Iteration [260/893]: Loss: 0.5993, CE: 0.1226
[15:53:12.043] Epoch [22/30] Iteration [270/893]: Loss: 0.6084, CE: 0.1457
[15:53:17.495] Epoch [22/30] Iteration [280/893]: Loss: 0.5770, CE: 0.0681
[15:53:22.940] Epoch [22/30] Iteration [290/893]: Loss: 0.5695, CE: 0.0562
[15:53:28.394] Epoch [22/30] Iteration [300/893]: Loss: 0.5903, CE: 0.1004
[15:53:33.848] Epoch [22/30] Iteration [310/893]: Loss: 0.5914, CE: 0.1034
[15:53:39.301] Epoch [22/30] Iteration [320/893]: Loss: 0.5682, CE: 0.0456
[15:53:44.769] Epoch [22/30] Iteration [330/893]: Loss: 0.5882, CE: 0.0952
[15:53:50.249] Epoch [22/30] Iteration [340/893]: Loss: 0.5795, CE: 0.0825
[15:53:55.769] Epoch [22/30] Iteration [350/893]: Loss: 0.5980, CE: 0.1195
[15:54:01.252] Epoch [22/30] Iteration [360/893]: Loss: 0.5943, CE: 0.1152
[15:54:06.733] Epoch [22/30] Iteration [370/893]: Loss: 0.5871, CE: 0.0937
[15:54:12.177] Epoch [22/30] Iteration [380/893]: Loss: 0.5874, CE: 0.0930
[15:54:17.614] Epoch [22/30] Iteration [390/893]: Loss: 0.5909, CE: 0.1019
[15:54:23.064] Epoch [22/30] Iteration [400/893]: Loss: 0.5883, CE: 0.0952
[15:54:28.491] Epoch [22/30] Iteration [410/893]: Loss: 0.6349, CE: 0.2270
[15:54:33.961] Epoch [22/30] Iteration [420/893]: Loss: 0.5800, CE: 0.0784
[15:54:39.426] Epoch [22/30] Iteration [430/893]: Loss: 0.6119, CE: 0.1542
[15:54:44.920] Epoch [22/30] Iteration [440/893]: Loss: 0.5901, CE: 0.0997
[15:54:50.432] Epoch [22/30] Iteration [450/893]: Loss: 0.6231, CE: 0.1820
[15:54:55.991] Epoch [22/30] Iteration [460/893]: Loss: 0.5908, CE: 0.1019
[15:55:01.433] Epoch [22/30] Iteration [470/893]: Loss: 0.5681, CE: 0.0521
[15:55:06.953] Epoch [22/30] Iteration [480/893]: Loss: 0.5918, CE: 0.1044
[15:55:12.463] Epoch [22/30] Iteration [490/893]: Loss: 0.5754, CE: 0.0636
[15:55:18.032] Epoch [22/30] Iteration [500/893]: Loss: 0.6038, CE: 0.1339
[15:55:23.560] Epoch [22/30] Iteration [510/893]: Loss: 0.5904, CE: 0.1007
[15:55:29.039] Epoch [22/30] Iteration [520/893]: Loss: 0.5642, CE: 0.0892
[15:55:34.537] Epoch [22/30] Iteration [530/893]: Loss: 0.5823, CE: 0.0924
[15:55:40.044] Epoch [22/30] Iteration [540/893]: Loss: 0.5946, CE: 0.1109
[15:55:45.537] Epoch [22/30] Iteration [550/893]: Loss: 0.6058, CE: 0.1400
[15:55:51.004] Epoch [22/30] Iteration [560/893]: Loss: 0.6105, CE: 0.1504
[15:55:56.502] Epoch [22/30] Iteration [570/893]: Loss: 0.5950, CE: 0.1120
[15:56:01.980] Epoch [22/30] Iteration [580/893]: Loss: 0.6018, CE: 0.1320
[15:56:07.471] Epoch [22/30] Iteration [590/893]: Loss: 0.5280, CE: 0.0487
[15:56:12.948] Epoch [22/30] Iteration [600/893]: Loss: 0.6023, CE: 0.1308
[15:56:18.418] Epoch [22/30] Iteration [610/893]: Loss: 0.6071, CE: 0.1422
[15:56:23.852] Epoch [22/30] Iteration [620/893]: Loss: 0.5834, CE: 0.0830
[15:56:29.313] Epoch [22/30] Iteration [630/893]: Loss: 0.5933, CE: 0.1085
[15:56:34.736] Epoch [22/30] Iteration [640/893]: Loss: 0.5783, CE: 0.0751
[15:56:40.155] Epoch [22/30] Iteration [650/893]: Loss: 0.6020, CE: 0.1294
[15:56:45.624] Epoch [22/30] Iteration [660/893]: Loss: 0.5966, CE: 0.1259
[15:56:51.088] Epoch [22/30] Iteration [670/893]: Loss: 0.5932, CE: 0.1074
[15:56:56.530] Epoch [22/30] Iteration [680/893]: Loss: 0.6063, CE: 0.1403
[15:57:02.019] Epoch [22/30] Iteration [690/893]: Loss: 0.5878, CE: 0.0947
[15:57:07.503] Epoch [22/30] Iteration [700/893]: Loss: 0.5826, CE: 0.0947
[15:57:12.933] Epoch [22/30] Iteration [710/893]: Loss: 0.5892, CE: 0.0976
[15:57:18.435] Epoch [22/30] Iteration [720/893]: Loss: 0.5727, CE: 0.0585
[15:57:23.884] Epoch [22/30] Iteration [730/893]: Loss: 0.5763, CE: 0.0661
[15:57:29.324] Epoch [22/30] Iteration [740/893]: Loss: 0.5928, CE: 0.1065
[15:57:34.776] Epoch [22/30] Iteration [750/893]: Loss: 0.5882, CE: 0.1026
[15:57:40.280] Epoch [22/30] Iteration [760/893]: Loss: 0.5985, CE: 0.1233
[15:57:45.784] Epoch [22/30] Iteration [770/893]: Loss: 0.5861, CE: 0.0899
[15:57:51.257] Epoch [22/30] Iteration [780/893]: Loss: 0.6169, CE: 0.1664
[15:57:56.722] Epoch [22/30] Iteration [790/893]: Loss: 0.5767, CE: 0.0670
[15:58:02.194] Epoch [22/30] Iteration [800/893]: Loss: 0.5772, CE: 0.0681
[15:58:07.644] Epoch [22/30] Iteration [810/893]: Loss: 0.6018, CE: 0.1290
[15:58:13.128] Epoch [22/30] Iteration [820/893]: Loss: 0.5893, CE: 0.1012
[15:58:18.569] Epoch [22/30] Iteration [830/893]: Loss: 0.5881, CE: 0.0964
[15:58:24.050] Epoch [22/30] Iteration [840/893]: Loss: 0.5734, CE: 0.0585
[15:58:29.517] Epoch [22/30] Iteration [850/893]: Loss: 0.6004, CE: 0.1253
[15:58:34.989] Epoch [22/30] Iteration [860/893]: Loss: 0.5803, CE: 0.0781
[15:58:40.428] Epoch [22/30] Iteration [870/893]: Loss: 0.5830, CE: 0.0821
[15:58:45.888] Epoch [22/30] Iteration [880/893]: Loss: 0.5820, CE: 0.0975
[15:58:51.326] Epoch [22/30] Iteration [890/893]: Loss: 0.5906, CE: 0.1043
[15:58:53.021] Epoch [22/30] Average Loss: 0.5905, CE: 0.1050, Dice: 0.9142
[15:59:14.421] Epoch [23/30] Iteration [0/893]: Loss: 0.5805, CE: 0.0866
[15:59:19.874] Epoch [23/30] Iteration [10/893]: Loss: 0.5871, CE: 0.0922
[15:59:25.356] Epoch [23/30] Iteration [20/893]: Loss: 0.6110, CE: 0.1516
[15:59:30.827] Epoch [23/30] Iteration [30/893]: Loss: 0.5763, CE: 0.0665
[15:59:36.320] Epoch [23/30] Iteration [40/893]: Loss: 0.5830, CE: 0.0886
[15:59:41.778] Epoch [23/30] Iteration [50/893]: Loss: 0.5872, CE: 0.0931
[15:59:47.225] Epoch [23/30] Iteration [60/893]: Loss: 0.5853, CE: 0.0893
[15:59:52.640] Epoch [23/30] Iteration [70/893]: Loss: 0.5704, CE: 0.0567
[15:59:58.112] Epoch [23/30] Iteration [80/893]: Loss: 0.6263, CE: 0.1996
[16:00:03.566] Epoch [23/30] Iteration [90/893]: Loss: 0.5777, CE: 0.0691
[16:00:08.980] Epoch [23/30] Iteration [100/893]: Loss: 0.6078, CE: 0.1440
[16:00:14.425] Epoch [23/30] Iteration [110/893]: Loss: 0.5934, CE: 0.1128
[16:00:19.888] Epoch [23/30] Iteration [120/893]: Loss: 0.5952, CE: 0.1124
[16:00:25.320] Epoch [23/30] Iteration [130/893]: Loss: 0.5940, CE: 0.1103
[16:00:30.743] Epoch [23/30] Iteration [140/893]: Loss: 0.5796, CE: 0.1007
[16:00:36.175] Epoch [23/30] Iteration [150/893]: Loss: 0.5947, CE: 0.1189
[16:00:41.610] Epoch [23/30] Iteration [160/893]: Loss: 0.5785, CE: 0.0757
[16:00:47.081] Epoch [23/30] Iteration [170/893]: Loss: 0.5868, CE: 0.0932
[16:00:52.505] Epoch [23/30] Iteration [180/893]: Loss: 0.5967, CE: 0.1163
[16:00:57.958] Epoch [23/30] Iteration [190/893]: Loss: 0.6112, CE: 0.1522
[16:01:03.377] Epoch [23/30] Iteration [200/893]: Loss: 0.5953, CE: 0.1167
[16:01:08.838] Epoch [23/30] Iteration [210/893]: Loss: 0.5784, CE: 0.0781
[16:01:14.272] Epoch [23/30] Iteration [220/893]: Loss: 0.5668, CE: 0.0508
[16:01:19.707] Epoch [23/30] Iteration [230/893]: Loss: 0.6082, CE: 0.1459
[16:01:25.176] Epoch [23/30] Iteration [240/893]: Loss: 0.5728, CE: 0.0571
[16:01:30.663] Epoch [23/30] Iteration [250/893]: Loss: 0.6154, CE: 0.1631
[16:01:36.135] Epoch [23/30] Iteration [260/893]: Loss: 0.6079, CE: 0.1599
[16:01:41.614] Epoch [23/30] Iteration [270/893]: Loss: 0.5752, CE: 0.0949
[16:01:47.117] Epoch [23/30] Iteration [280/893]: Loss: 0.5810, CE: 0.0775
[16:01:52.601] Epoch [23/30] Iteration [290/893]: Loss: 0.5880, CE: 0.0946
[16:01:58.028] Epoch [23/30] Iteration [300/893]: Loss: 0.5732, CE: 0.0578
[16:02:03.492] Epoch [23/30] Iteration [310/893]: Loss: 0.6091, CE: 0.1469
[16:02:08.959] Epoch [23/30] Iteration [320/893]: Loss: 0.6319, CE: 0.2036
[16:02:14.396] Epoch [23/30] Iteration [330/893]: Loss: 0.5758, CE: 0.0810
[16:02:19.848] Epoch [23/30] Iteration [340/893]: Loss: 0.6022, CE: 0.1298
[16:02:25.307] Epoch [23/30] Iteration [350/893]: Loss: 0.5857, CE: 0.0888
[16:02:30.772] Epoch [23/30] Iteration [360/893]: Loss: 0.5787, CE: 0.0914
[16:02:36.207] Epoch [23/30] Iteration [370/893]: Loss: 0.5854, CE: 0.0882
[16:02:41.647] Epoch [23/30] Iteration [380/893]: Loss: 0.5968, CE: 0.1166
[16:02:47.106] Epoch [23/30] Iteration [390/893]: Loss: 0.5954, CE: 0.1128
[16:02:52.544] Epoch [23/30] Iteration [400/893]: Loss: 0.5718, CE: 0.0856
[16:02:58.027] Epoch [23/30] Iteration [410/893]: Loss: 0.5687, CE: 0.0469
[16:03:03.485] Epoch [23/30] Iteration [420/893]: Loss: 0.5622, CE: 0.0827
[16:03:08.933] Epoch [23/30] Iteration [430/893]: Loss: 0.5594, CE: 0.0647
[16:03:14.396] Epoch [23/30] Iteration [440/893]: Loss: 0.6077, CE: 0.1435
[16:03:19.872] Epoch [23/30] Iteration [450/893]: Loss: 0.5862, CE: 0.0902
[16:03:25.356] Epoch [23/30] Iteration [460/893]: Loss: 0.5747, CE: 0.0845
[16:03:30.807] Epoch [23/30] Iteration [470/893]: Loss: 0.5951, CE: 0.1158
[16:03:36.241] Epoch [23/30] Iteration [480/893]: Loss: 0.5907, CE: 0.1014
[16:03:41.714] Epoch [23/30] Iteration [490/893]: Loss: 0.6081, CE: 0.1450
[16:03:47.170] Epoch [23/30] Iteration [500/893]: Loss: 0.6190, CE: 0.1715
[16:03:52.622] Epoch [23/30] Iteration [510/893]: Loss: 0.5829, CE: 0.1080
[16:03:58.075] Epoch [23/30] Iteration [520/893]: Loss: 0.5725, CE: 0.0567
[16:04:03.580] Epoch [23/30] Iteration [530/893]: Loss: 0.5819, CE: 0.0793
[16:04:09.034] Epoch [23/30] Iteration [540/893]: Loss: 0.6425, CE: 0.2303
[16:04:14.492] Epoch [23/30] Iteration [550/893]: Loss: 0.5955, CE: 0.1135
[16:04:19.952] Epoch [23/30] Iteration [560/893]: Loss: 0.6144, CE: 0.1602
[16:04:25.425] Epoch [23/30] Iteration [570/893]: Loss: 0.5560, CE: 0.0398
[16:04:30.872] Epoch [23/30] Iteration [580/893]: Loss: 0.5817, CE: 0.0790
[16:04:36.366] Epoch [23/30] Iteration [590/893]: Loss: 0.5741, CE: 0.0605
[16:04:41.812] Epoch [23/30] Iteration [600/893]: Loss: 0.5761, CE: 0.0925
[16:04:47.265] Epoch [23/30] Iteration [610/893]: Loss: 0.5943, CE: 0.1102
[16:04:52.747] Epoch [23/30] Iteration [620/893]: Loss: 0.5950, CE: 0.1119
[16:04:58.201] Epoch [23/30] Iteration [630/893]: Loss: 0.5746, CE: 0.0623
[16:05:03.634] Epoch [23/30] Iteration [640/893]: Loss: 0.6014, CE: 0.1288
[16:05:09.090] Epoch [23/30] Iteration [650/893]: Loss: 0.6131, CE: 0.1570
[16:05:14.554] Epoch [23/30] Iteration [660/893]: Loss: 0.5873, CE: 0.1050
[16:05:20.006] Epoch [23/30] Iteration [670/893]: Loss: 0.5858, CE: 0.0890
[16:05:25.443] Epoch [23/30] Iteration [680/893]: Loss: 0.5766, CE: 0.1005
[16:05:30.927] Epoch [23/30] Iteration [690/893]: Loss: 0.5881, CE: 0.0947
[16:05:36.365] Epoch [23/30] Iteration [700/893]: Loss: 0.5850, CE: 0.0871
[16:05:41.823] Epoch [23/30] Iteration [710/893]: Loss: 0.6126, CE: 0.1576
[16:05:47.255] Epoch [23/30] Iteration [720/893]: Loss: 0.5849, CE: 0.0869
[16:05:52.708] Epoch [23/30] Iteration [730/893]: Loss: 0.5887, CE: 0.0976
[16:05:58.185] Epoch [23/30] Iteration [740/893]: Loss: 0.5956, CE: 0.1139
[16:06:03.659] Epoch [23/30] Iteration [750/893]: Loss: 0.5917, CE: 0.1071
[16:06:09.111] Epoch [23/30] Iteration [760/893]: Loss: 0.5969, CE: 0.1171
[16:06:14.598] Epoch [23/30] Iteration [770/893]: Loss: 0.5799, CE: 0.0758
[16:06:20.034] Epoch [23/30] Iteration [780/893]: Loss: 0.5756, CE: 0.0637
[16:06:25.487] Epoch [23/30] Iteration [790/893]: Loss: 0.5993, CE: 0.1227
[16:06:30.937] Epoch [23/30] Iteration [800/893]: Loss: 0.5885, CE: 0.0958
[16:06:36.388] Epoch [23/30] Iteration [810/893]: Loss: 0.5763, CE: 0.0656
[16:06:41.855] Epoch [23/30] Iteration [820/893]: Loss: 0.5723, CE: 0.0558
[16:06:47.326] Epoch [23/30] Iteration [830/893]: Loss: 0.5864, CE: 0.1038
[16:06:52.797] Epoch [23/30] Iteration [840/893]: Loss: 0.5974, CE: 0.1181
[16:06:58.251] Epoch [23/30] Iteration [850/893]: Loss: 0.6167, CE: 0.1659
[16:07:03.703] Epoch [23/30] Iteration [860/893]: Loss: 0.5794, CE: 0.1212
[16:07:09.189] Epoch [23/30] Iteration [870/893]: Loss: 0.5778, CE: 0.0767
[16:07:14.634] Epoch [23/30] Iteration [880/893]: Loss: 0.6163, CE: 0.1652
[16:07:20.086] Epoch [23/30] Iteration [890/893]: Loss: 0.6083, CE: 0.1617
[16:07:21.793] Epoch [23/30] Average Loss: 0.5905, CE: 0.1051, Dice: 0.9141
[16:07:43.470] Epoch [24/30] Iteration [0/893]: Loss: 0.6283, CE: 0.1950
[16:07:48.941] Epoch [24/30] Iteration [10/893]: Loss: 0.6142, CE: 0.1605
[16:07:54.427] Epoch [24/30] Iteration [20/893]: Loss: 0.5833, CE: 0.0833
[16:07:59.873] Epoch [24/30] Iteration [30/893]: Loss: 0.5902, CE: 0.1001
[16:08:05.311] Epoch [24/30] Iteration [40/893]: Loss: 0.6405, CE: 0.2308
[16:08:10.744] Epoch [24/30] Iteration [50/893]: Loss: 0.5970, CE: 0.1171
[16:08:16.195] Epoch [24/30] Iteration [60/893]: Loss: 0.5834, CE: 0.0868
[16:08:21.729] Epoch [24/30] Iteration [70/893]: Loss: 0.5754, CE: 0.0859
[16:08:27.166] Epoch [24/30] Iteration [80/893]: Loss: 0.5768, CE: 0.0672
[16:08:32.607] Epoch [24/30] Iteration [90/893]: Loss: 0.6021, CE: 0.1299
[16:08:38.047] Epoch [24/30] Iteration [100/893]: Loss: 0.5918, CE: 0.1042
[16:08:43.558] Epoch [24/30] Iteration [110/893]: Loss: 0.5828, CE: 0.0822
[16:08:48.996] Epoch [24/30] Iteration [120/893]: Loss: 0.5908, CE: 0.1029
[16:08:54.470] Epoch [24/30] Iteration [130/893]: Loss: 0.6161, CE: 0.1666
[16:08:59.917] Epoch [24/30] Iteration [140/893]: Loss: 0.5869, CE: 0.0917
[16:09:05.368] Epoch [24/30] Iteration [150/893]: Loss: 0.5962, CE: 0.1148
[16:09:10.856] Epoch [24/30] Iteration [160/893]: Loss: 0.5920, CE: 0.1049
[16:09:16.339] Epoch [24/30] Iteration [170/893]: Loss: 0.6091, CE: 0.1537
[16:09:21.800] Epoch [24/30] Iteration [180/893]: Loss: 0.5977, CE: 0.1189
[16:09:27.242] Epoch [24/30] Iteration [190/893]: Loss: 0.5929, CE: 0.1070
[16:09:32.671] Epoch [24/30] Iteration [200/893]: Loss: 0.5823, CE: 0.0908
[16:09:38.143] Epoch [24/30] Iteration [210/893]: Loss: 0.5905, CE: 0.1005
[16:09:43.572] Epoch [24/30] Iteration [220/893]: Loss: 0.5889, CE: 0.0985
[16:09:49.051] Epoch [24/30] Iteration [230/893]: Loss: 0.5688, CE: 0.0485
[16:09:54.489] Epoch [24/30] Iteration [240/893]: Loss: 0.6055, CE: 0.1378
[16:09:59.974] Epoch [24/30] Iteration [250/893]: Loss: 0.5838, CE: 0.0845
[16:10:05.441] Epoch [24/30] Iteration [260/893]: Loss: 0.5883, CE: 0.0953
[16:10:10.922] Epoch [24/30] Iteration [270/893]: Loss: 0.6026, CE: 0.1308
[16:10:16.359] Epoch [24/30] Iteration [280/893]: Loss: 0.5654, CE: 0.0611
[16:10:21.860] Epoch [24/30] Iteration [290/893]: Loss: 0.5954, CE: 0.1132
[16:10:27.303] Epoch [24/30] Iteration [300/893]: Loss: 0.5777, CE: 0.0692
[16:10:32.752] Epoch [24/30] Iteration [310/893]: Loss: 0.5913, CE: 0.1034
[16:10:38.197] Epoch [24/30] Iteration [320/893]: Loss: 0.6316, CE: 0.2030
[16:10:43.657] Epoch [24/30] Iteration [330/893]: Loss: 0.5828, CE: 0.0818
[16:10:49.103] Epoch [24/30] Iteration [340/893]: Loss: 0.6063, CE: 0.1403
[16:10:54.577] Epoch [24/30] Iteration [350/893]: Loss: 0.5981, CE: 0.1200
[16:11:00.118] Epoch [24/30] Iteration [360/893]: Loss: 0.5892, CE: 0.0979
[16:11:05.564] Epoch [24/30] Iteration [370/893]: Loss: 0.5785, CE: 0.0888
[16:11:11.030] Epoch [24/30] Iteration [380/893]: Loss: 0.5905, CE: 0.1008
[16:11:16.508] Epoch [24/30] Iteration [390/893]: Loss: 0.5919, CE: 0.1044
[16:11:21.949] Epoch [24/30] Iteration [400/893]: Loss: 0.5836, CE: 0.0836
[16:11:27.397] Epoch [24/30] Iteration [410/893]: Loss: 0.5945, CE: 0.1108
[16:11:32.853] Epoch [24/30] Iteration [420/893]: Loss: 0.6022, CE: 0.1304
[16:11:38.326] Epoch [24/30] Iteration [430/893]: Loss: 0.5984, CE: 0.1204
[16:11:43.761] Epoch [24/30] Iteration [440/893]: Loss: 0.5858, CE: 0.0892
[16:11:49.235] Epoch [24/30] Iteration [450/893]: Loss: 0.6087, CE: 0.1459
[16:11:54.669] Epoch [24/30] Iteration [460/893]: Loss: 0.5902, CE: 0.1000
[16:12:00.122] Epoch [24/30] Iteration [470/893]: Loss: 0.5947, CE: 0.1113
[16:12:05.562] Epoch [24/30] Iteration [480/893]: Loss: 0.5862, CE: 0.0904
[16:12:11.001] Epoch [24/30] Iteration [490/893]: Loss: 0.6061, CE: 0.1409
[16:12:16.442] Epoch [24/30] Iteration [500/893]: Loss: 0.5712, CE: 0.0787
[16:12:21.920] Epoch [24/30] Iteration [510/893]: Loss: 0.5748, CE: 0.0620
[16:12:27.356] Epoch [24/30] Iteration [520/893]: Loss: 0.5723, CE: 0.0681
[16:12:32.802] Epoch [24/30] Iteration [530/893]: Loss: 0.5926, CE: 0.1063
[16:12:38.243] Epoch [24/30] Iteration [540/893]: Loss: 0.5942, CE: 0.1102
[16:12:43.683] Epoch [24/30] Iteration [550/893]: Loss: 0.6006, CE: 0.1262
[16:12:49.136] Epoch [24/30] Iteration [560/893]: Loss: 0.5898, CE: 0.1011
[16:12:54.623] Epoch [24/30] Iteration [570/893]: Loss: 0.5988, CE: 0.1276
[16:13:00.074] Epoch [24/30] Iteration [580/893]: Loss: 0.5750, CE: 0.0623
[16:13:05.520] Epoch [24/30] Iteration [590/893]: Loss: 0.6069, CE: 0.1413
[16:13:10.952] Epoch [24/30] Iteration [600/893]: Loss: 0.6160, CE: 0.1667
[16:13:16.427] Epoch [24/30] Iteration [610/893]: Loss: 0.5886, CE: 0.0959
[16:13:21.936] Epoch [24/30] Iteration [620/893]: Loss: 0.5994, CE: 0.1239
[16:13:27.405] Epoch [24/30] Iteration [630/893]: Loss: 0.5850, CE: 0.0872
[16:13:32.866] Epoch [24/30] Iteration [640/893]: Loss: 0.5826, CE: 0.0830
[16:13:38.318] Epoch [24/30] Iteration [650/893]: Loss: 0.6040, CE: 0.1343
[16:13:43.767] Epoch [24/30] Iteration [660/893]: Loss: 0.5921, CE: 0.1047
[16:13:49.259] Epoch [24/30] Iteration [670/893]: Loss: 0.5807, CE: 0.0787
[16:13:54.758] Epoch [24/30] Iteration [680/893]: Loss: 0.6185, CE: 0.1705
[16:14:00.193] Epoch [24/30] Iteration [690/893]: Loss: 0.5877, CE: 0.0938
[16:14:05.638] Epoch [24/30] Iteration [700/893]: Loss: 0.5644, CE: 0.0438
[16:14:11.162] Epoch [24/30] Iteration [710/893]: Loss: 0.5929, CE: 0.1065
[16:14:16.645] Epoch [24/30] Iteration [720/893]: Loss: 0.5998, CE: 0.1239
[16:14:22.085] Epoch [24/30] Iteration [730/893]: Loss: 0.6005, CE: 0.1263
[16:14:27.542] Epoch [24/30] Iteration [740/893]: Loss: 0.5673, CE: 0.1016
[16:14:32.994] Epoch [24/30] Iteration [750/893]: Loss: 0.5857, CE: 0.0952
[16:14:38.429] Epoch [24/30] Iteration [760/893]: Loss: 0.5813, CE: 0.0794
[16:14:43.881] Epoch [24/30] Iteration [770/893]: Loss: 0.5900, CE: 0.0998
[16:14:49.316] Epoch [24/30] Iteration [780/893]: Loss: 0.5832, CE: 0.0826
[16:14:54.801] Epoch [24/30] Iteration [790/893]: Loss: 0.6066, CE: 0.1409
[16:15:00.270] Epoch [24/30] Iteration [800/893]: Loss: 0.5753, CE: 0.1162
[16:15:05.731] Epoch [24/30] Iteration [810/893]: Loss: 0.6157, CE: 0.1637
[16:15:11.172] Epoch [24/30] Iteration [820/893]: Loss: 0.5762, CE: 0.0657
[16:15:16.623] Epoch [24/30] Iteration [830/893]: Loss: 0.5929, CE: 0.1070
[16:15:22.065] Epoch [24/30] Iteration [840/893]: Loss: 0.5978, CE: 0.1205
[16:15:27.518] Epoch [24/30] Iteration [850/893]: Loss: 0.5841, CE: 0.1055
[16:15:33.007] Epoch [24/30] Iteration [860/893]: Loss: 0.5971, CE: 0.1511
[16:15:38.471] Epoch [24/30] Iteration [870/893]: Loss: 0.5999, CE: 0.1279
[16:15:43.935] Epoch [24/30] Iteration [880/893]: Loss: 0.6060, CE: 0.1472
[16:15:49.384] Epoch [24/30] Iteration [890/893]: Loss: 0.5919, CE: 0.1045
[16:15:51.095] Epoch [24/30] Average Loss: 0.5904, CE: 0.1051, Dice: 0.9140
[16:15:51.283] Saved continual learning checkpoint to ./universal/synapse_to_kits23_tpgm\epoch_24_continual.pth
[16:15:51.283] Applying TPGM projection update at epoch 25
[16:16:00.466] TPGM iteration 19/50 completed, loss: 0.0407
[16:16:10.192] TPGM iteration 39/50 completed, loss: 0.0505
[16:16:36.928] Epoch [25/30] Iteration [0/893]: Loss: 0.6051, CE: 0.1402
[16:16:42.382] Epoch [25/30] Iteration [10/893]: Loss: 0.6105, CE: 0.1761
[16:16:47.850] Epoch [25/30] Iteration [20/893]: Loss: 0.5722, CE: 0.0593
[16:16:53.288] Epoch [25/30] Iteration [30/893]: Loss: 0.5777, CE: 0.0693
[16:16:58.721] Epoch [25/30] Iteration [40/893]: Loss: 0.6038, CE: 0.1359
[16:17:04.153] Epoch [25/30] Iteration [50/893]: Loss: 0.5974, CE: 0.1193
[16:17:09.582] Epoch [25/30] Iteration [60/893]: Loss: 0.5811, CE: 0.0778
[16:17:15.036] Epoch [25/30] Iteration [70/893]: Loss: 0.5976, CE: 0.1201
[16:17:20.470] Epoch [25/30] Iteration [80/893]: Loss: 0.6043, CE: 0.1367
[16:17:25.918] Epoch [25/30] Iteration [90/893]: Loss: 0.5902, CE: 0.1003
[16:17:31.380] Epoch [25/30] Iteration [100/893]: Loss: 0.6099, CE: 0.1544
[16:17:36.860] Epoch [25/30] Iteration [110/893]: Loss: 0.5685, CE: 0.0481
[16:17:42.293] Epoch [25/30] Iteration [120/893]: Loss: 0.5917, CE: 0.1038
[16:17:47.744] Epoch [25/30] Iteration [130/893]: Loss: 0.5798, CE: 0.1083
[16:17:53.217] Epoch [25/30] Iteration [140/893]: Loss: 0.5876, CE: 0.0965
[16:17:58.639] Epoch [25/30] Iteration [150/893]: Loss: 0.5893, CE: 0.0979
[16:18:04.067] Epoch [25/30] Iteration [160/893]: Loss: 0.5943, CE: 0.1117
[16:18:09.498] Epoch [25/30] Iteration [170/893]: Loss: 0.5842, CE: 0.0855
[16:18:14.925] Epoch [25/30] Iteration [180/893]: Loss: 0.6071, CE: 0.1421
[16:18:20.378] Epoch [25/30] Iteration [190/893]: Loss: 0.5911, CE: 0.1032
[16:18:25.808] Epoch [25/30] Iteration [200/893]: Loss: 0.6030, CE: 0.1411
[16:18:31.240] Epoch [25/30] Iteration [210/893]: Loss: 0.5826, CE: 0.0828
[16:18:36.719] Epoch [25/30] Iteration [220/893]: Loss: 0.5912, CE: 0.1066
[16:18:42.273] Epoch [25/30] Iteration [230/893]: Loss: 0.5897, CE: 0.0988
[16:18:47.796] Epoch [25/30] Iteration [240/893]: Loss: 0.5792, CE: 0.0727
[16:18:53.236] Epoch [25/30] Iteration [250/893]: Loss: 0.5642, CE: 0.0448
[16:18:58.683] Epoch [25/30] Iteration [260/893]: Loss: 0.6022, CE: 0.1376
[16:19:04.153] Epoch [25/30] Iteration [270/893]: Loss: 0.5749, CE: 0.0623
[16:19:09.593] Epoch [25/30] Iteration [280/893]: Loss: 0.5956, CE: 0.1134
[16:19:15.020] Epoch [25/30] Iteration [290/893]: Loss: 0.5824, CE: 0.0805
[16:19:20.490] Epoch [25/30] Iteration [300/893]: Loss: 0.5700, CE: 0.1059
[16:19:25.935] Epoch [25/30] Iteration [310/893]: Loss: 0.5868, CE: 0.0919
[16:19:31.427] Epoch [25/30] Iteration [320/893]: Loss: 0.5769, CE: 0.0673
[16:19:36.900] Epoch [25/30] Iteration [330/893]: Loss: 0.5800, CE: 0.0748
[16:19:42.368] Epoch [25/30] Iteration [340/893]: Loss: 0.5992, CE: 0.1225
[16:19:47.837] Epoch [25/30] Iteration [350/893]: Loss: 0.5924, CE: 0.1086
[16:19:53.294] Epoch [25/30] Iteration [360/893]: Loss: 0.5886, CE: 0.0961
[16:19:58.735] Epoch [25/30] Iteration [370/893]: Loss: 0.5894, CE: 0.0984
[16:20:04.251] Epoch [25/30] Iteration [380/893]: Loss: 0.5545, CE: 0.0464
[16:20:09.716] Epoch [25/30] Iteration [390/893]: Loss: 0.5884, CE: 0.0959
[16:20:15.181] Epoch [25/30] Iteration [400/893]: Loss: 0.5943, CE: 0.1189
[16:20:20.649] Epoch [25/30] Iteration [410/893]: Loss: 0.5912, CE: 0.1027
[16:20:26.105] Epoch [25/30] Iteration [420/893]: Loss: 0.5939, CE: 0.1097
[16:20:31.544] Epoch [25/30] Iteration [430/893]: Loss: 0.5776, CE: 0.0688
[16:20:37.014] Epoch [25/30] Iteration [440/893]: Loss: 0.5824, CE: 0.0808
[16:20:42.469] Epoch [25/30] Iteration [450/893]: Loss: 0.5790, CE: 0.0726
[16:20:47.932] Epoch [25/30] Iteration [460/893]: Loss: 0.5638, CE: 0.0343
[16:20:53.489] Epoch [25/30] Iteration [470/893]: Loss: 0.5987, CE: 0.1211
[16:20:59.037] Epoch [25/30] Iteration [480/893]: Loss: 0.5760, CE: 0.0650
[16:21:04.525] Epoch [25/30] Iteration [490/893]: Loss: 0.5958, CE: 0.1145
[16:21:10.005] Epoch [25/30] Iteration [500/893]: Loss: 0.5768, CE: 0.0672
[16:21:15.451] Epoch [25/30] Iteration [510/893]: Loss: 0.6145, CE: 0.1607
[16:21:20.896] Epoch [25/30] Iteration [520/893]: Loss: 0.6114, CE: 0.1530
[16:21:26.349] Epoch [25/30] Iteration [530/893]: Loss: 0.5993, CE: 0.1226
[16:21:31.804] Epoch [25/30] Iteration [540/893]: Loss: 0.5863, CE: 0.0942
[16:21:37.257] Epoch [25/30] Iteration [550/893]: Loss: 0.5964, CE: 0.1153
[16:21:42.717] Epoch [25/30] Iteration [560/893]: Loss: 0.5666, CE: 0.0418
[16:21:48.185] Epoch [25/30] Iteration [570/893]: Loss: 0.5774, CE: 0.0689
[16:21:53.641] Epoch [25/30] Iteration [580/893]: Loss: 0.5993, CE: 0.1281
[16:21:59.157] Epoch [25/30] Iteration [590/893]: Loss: 0.5896, CE: 0.1000
[16:22:04.595] Epoch [25/30] Iteration [600/893]: Loss: 0.5813, CE: 0.0778
[16:22:10.056] Epoch [25/30] Iteration [610/893]: Loss: 0.5929, CE: 0.1070
[16:22:15.520] Epoch [25/30] Iteration [620/893]: Loss: 0.5921, CE: 0.1048
[16:22:20.984] Epoch [25/30] Iteration [630/893]: Loss: 0.5626, CE: 0.1026
[16:22:26.480] Epoch [25/30] Iteration [640/893]: Loss: 0.5903, CE: 0.1006
[16:22:31.921] Epoch [25/30] Iteration [650/893]: Loss: 0.6094, CE: 0.1480
[16:22:37.373] Epoch [25/30] Iteration [660/893]: Loss: 0.5981, CE: 0.1199
[16:22:42.821] Epoch [25/30] Iteration [670/893]: Loss: 0.6001, CE: 0.1246
[16:22:48.283] Epoch [25/30] Iteration [680/893]: Loss: 0.6002, CE: 0.1250
[16:22:53.725] Epoch [25/30] Iteration [690/893]: Loss: 0.5807, CE: 0.0768
[16:22:59.149] Epoch [25/30] Iteration [700/893]: Loss: 0.5900, CE: 0.0997
[16:23:04.610] Epoch [25/30] Iteration [710/893]: Loss: 0.5850, CE: 0.0871
[16:23:10.075] Epoch [25/30] Iteration [720/893]: Loss: 0.6042, CE: 0.1347
[16:23:15.509] Epoch [25/30] Iteration [730/893]: Loss: 0.5801, CE: 0.0748
[16:23:20.954] Epoch [25/30] Iteration [740/893]: Loss: 0.5827, CE: 0.0946
[16:23:26.383] Epoch [25/30] Iteration [750/893]: Loss: 0.6179, CE: 0.1688
[16:23:31.928] Epoch [25/30] Iteration [760/893]: Loss: 0.5737, CE: 0.0594
[16:23:37.402] Epoch [25/30] Iteration [770/893]: Loss: 0.6000, CE: 0.1337
[16:23:42.860] Epoch [25/30] Iteration [780/893]: Loss: 0.5877, CE: 0.0938
[16:23:48.311] Epoch [25/30] Iteration [790/893]: Loss: 0.6031, CE: 0.1432
[16:23:53.739] Epoch [25/30] Iteration [800/893]: Loss: 0.5786, CE: 0.0778
[16:23:59.191] Epoch [25/30] Iteration [810/893]: Loss: 0.5772, CE: 0.0678
[16:24:04.677] Epoch [25/30] Iteration [820/893]: Loss: 0.5919, CE: 0.1041
[16:24:10.158] Epoch [25/30] Iteration [830/893]: Loss: 0.5953, CE: 0.1125
[16:24:15.621] Epoch [25/30] Iteration [840/893]: Loss: 0.5587, CE: 0.0405
[16:24:21.104] Epoch [25/30] Iteration [850/893]: Loss: 0.5984, CE: 0.1205
[16:24:26.586] Epoch [25/30] Iteration [860/893]: Loss: 0.6012, CE: 0.1292
[16:24:32.042] Epoch [25/30] Iteration [870/893]: Loss: 0.5835, CE: 0.0833
[16:24:37.513] Epoch [25/30] Iteration [880/893]: Loss: 0.5859, CE: 0.0999
[16:24:43.005] Epoch [25/30] Iteration [890/893]: Loss: 0.5760, CE: 0.0647
[16:24:44.654] Epoch [25/30] Average Loss: 0.5906, CE: 0.1054, Dice: 0.9142
[16:25:05.783] Epoch [26/30] Iteration [0/893]: Loss: 0.5929, CE: 0.1076
[16:25:11.239] Epoch [26/30] Iteration [10/893]: Loss: 0.6337, CE: 0.2084
[16:25:16.726] Epoch [26/30] Iteration [20/893]: Loss: 0.5826, CE: 0.0811
[16:25:22.195] Epoch [26/30] Iteration [30/893]: Loss: 0.5809, CE: 0.0772
[16:25:27.654] Epoch [26/30] Iteration [40/893]: Loss: 0.5857, CE: 0.0888
[16:25:33.099] Epoch [26/30] Iteration [50/893]: Loss: 0.5990, CE: 0.1220
[16:25:38.542] Epoch [26/30] Iteration [60/893]: Loss: 0.5940, CE: 0.1096
[16:25:44.007] Epoch [26/30] Iteration [70/893]: Loss: 0.5861, CE: 0.0932
[16:25:49.468] Epoch [26/30] Iteration [80/893]: Loss: 0.5975, CE: 0.1183
[16:25:54.938] Epoch [26/30] Iteration [90/893]: Loss: 0.5848, CE: 0.0870
[16:26:00.435] Epoch [26/30] Iteration [100/893]: Loss: 0.5901, CE: 0.1008
[16:26:05.935] Epoch [26/30] Iteration [110/893]: Loss: 0.5868, CE: 0.0915
[16:26:11.392] Epoch [26/30] Iteration [120/893]: Loss: 0.5873, CE: 0.0930
[16:26:16.865] Epoch [26/30] Iteration [130/893]: Loss: 0.5839, CE: 0.0988
[16:26:22.289] Epoch [26/30] Iteration [140/893]: Loss: 0.6051, CE: 0.1373
[16:26:27.804] Epoch [26/30] Iteration [150/893]: Loss: 0.6409, CE: 0.2264
[16:26:33.245] Epoch [26/30] Iteration [160/893]: Loss: 0.5991, CE: 0.1230
[16:26:38.700] Epoch [26/30] Iteration [170/893]: Loss: 0.5594, CE: 0.0372
[16:26:44.158] Epoch [26/30] Iteration [180/893]: Loss: 0.5774, CE: 0.0685
[16:26:49.697] Epoch [26/30] Iteration [190/893]: Loss: 0.6133, CE: 0.1574
[16:26:55.210] Epoch [26/30] Iteration [200/893]: Loss: 0.5756, CE: 0.0670
[16:27:00.784] Epoch [26/30] Iteration [210/893]: Loss: 0.5741, CE: 0.0606
[16:27:06.308] Epoch [26/30] Iteration [220/893]: Loss: 0.5990, CE: 0.1219
[16:27:11.774] Epoch [26/30] Iteration [230/893]: Loss: 0.6072, CE: 0.1455
[16:27:17.287] Epoch [26/30] Iteration [240/893]: Loss: 0.6076, CE: 0.1544
[16:27:22.720] Epoch [26/30] Iteration [250/893]: Loss: 0.5927, CE: 0.1127
[16:27:28.150] Epoch [26/30] Iteration [260/893]: Loss: 0.6026, CE: 0.1308
[16:27:33.677] Epoch [26/30] Iteration [270/893]: Loss: 0.5835, CE: 0.0837
[16:27:39.190] Epoch [26/30] Iteration [280/893]: Loss: 0.5762, CE: 0.0654
[16:27:44.705] Epoch [26/30] Iteration [290/893]: Loss: 0.5736, CE: 0.0668
[16:27:50.209] Epoch [26/30] Iteration [300/893]: Loss: 0.5809, CE: 0.0769
[16:27:55.719] Epoch [26/30] Iteration [310/893]: Loss: 0.5761, CE: 0.0649
[16:28:01.181] Epoch [26/30] Iteration [320/893]: Loss: 0.5939, CE: 0.1096
[16:28:06.644] Epoch [26/30] Iteration [330/893]: Loss: 0.5983, CE: 0.1203
[16:28:12.128] Epoch [26/30] Iteration [340/893]: Loss: 0.5714, CE: 0.0672
[16:28:17.572] Epoch [26/30] Iteration [350/893]: Loss: 0.5907, CE: 0.1014
[16:28:23.014] Epoch [26/30] Iteration [360/893]: Loss: 0.6090, CE: 0.1491
[16:28:28.476] Epoch [26/30] Iteration [370/893]: Loss: 0.6120, CE: 0.1546
[16:28:33.936] Epoch [26/30] Iteration [380/893]: Loss: 0.6058, CE: 0.1397
[16:28:39.394] Epoch [26/30] Iteration [390/893]: Loss: 0.5911, CE: 0.1108
[16:28:44.860] Epoch [26/30] Iteration [400/893]: Loss: 0.6129, CE: 0.1563
[16:28:50.346] Epoch [26/30] Iteration [410/893]: Loss: 0.5918, CE: 0.1110
[16:28:55.794] Epoch [26/30] Iteration [420/893]: Loss: 0.5744, CE: 0.0613
[16:29:01.262] Epoch [26/30] Iteration [430/893]: Loss: 0.6110, CE: 0.1519
[16:29:06.705] Epoch [26/30] Iteration [440/893]: Loss: 0.5866, CE: 0.0914
[16:29:12.177] Epoch [26/30] Iteration [450/893]: Loss: 0.5895, CE: 0.0984
[16:29:17.663] Epoch [26/30] Iteration [460/893]: Loss: 0.5879, CE: 0.0943
[16:29:23.158] Epoch [26/30] Iteration [470/893]: Loss: 0.6155, CE: 0.1635
[16:29:28.616] Epoch [26/30] Iteration [480/893]: Loss: 0.5712, CE: 0.0720
[16:29:34.119] Epoch [26/30] Iteration [490/893]: Loss: 0.6327, CE: 0.2057
[16:29:39.597] Epoch [26/30] Iteration [500/893]: Loss: 0.5766, CE: 0.0670
[16:29:45.113] Epoch [26/30] Iteration [510/893]: Loss: 0.6079, CE: 0.1441
[16:29:50.553] Epoch [26/30] Iteration [520/893]: Loss: 0.5972, CE: 0.1173
[16:29:56.007] Epoch [26/30] Iteration [530/893]: Loss: 0.6110, CE: 0.1575
[16:30:01.471] Epoch [26/30] Iteration [540/893]: Loss: 0.5848, CE: 0.1100
[16:30:06.935] Epoch [26/30] Iteration [550/893]: Loss: 0.5898, CE: 0.0994
[16:30:12.397] Epoch [26/30] Iteration [560/893]: Loss: 0.6100, CE: 0.1493
[16:30:17.871] Epoch [26/30] Iteration [570/893]: Loss: 0.5717, CE: 0.0572
[16:30:23.317] Epoch [26/30] Iteration [580/893]: Loss: 0.6085, CE: 0.1459
[16:30:28.756] Epoch [26/30] Iteration [590/893]: Loss: 0.5927, CE: 0.1219
[16:30:34.226] Epoch [26/30] Iteration [600/893]: Loss: 0.6022, CE: 0.1301
[16:30:39.686] Epoch [26/30] Iteration [610/893]: Loss: 0.6062, CE: 0.1397
[16:30:45.131] Epoch [26/30] Iteration [620/893]: Loss: 0.5794, CE: 0.0735
[16:30:50.595] Epoch [26/30] Iteration [630/893]: Loss: 0.5713, CE: 0.0561
[16:30:56.052] Epoch [26/30] Iteration [640/893]: Loss: 0.6032, CE: 0.1344
[16:31:01.504] Epoch [26/30] Iteration [650/893]: Loss: 0.5866, CE: 0.0910
[16:31:06.944] Epoch [26/30] Iteration [660/893]: Loss: 0.5835, CE: 0.0874
[16:31:12.409] Epoch [26/30] Iteration [670/893]: Loss: 0.5951, CE: 0.1121
[16:31:17.870] Epoch [26/30] Iteration [680/893]: Loss: 0.5874, CE: 0.0931
[16:31:23.330] Epoch [26/30] Iteration [690/893]: Loss: 0.5899, CE: 0.1131
[16:31:28.759] Epoch [26/30] Iteration [700/893]: Loss: 0.5850, CE: 0.0872
[16:31:34.235] Epoch [26/30] Iteration [710/893]: Loss: 0.6469, CE: 0.2411
[16:31:39.733] Epoch [26/30] Iteration [720/893]: Loss: 0.5742, CE: 0.0604
[16:31:45.220] Epoch [26/30] Iteration [730/893]: Loss: 0.5802, CE: 0.0750
[16:31:50.682] Epoch [26/30] Iteration [740/893]: Loss: 0.5884, CE: 0.0956
[16:31:56.164] Epoch [26/30] Iteration [750/893]: Loss: 0.5868, CE: 0.0921
[16:32:01.688] Epoch [26/30] Iteration [760/893]: Loss: 0.5834, CE: 0.0833
[16:32:07.160] Epoch [26/30] Iteration [770/893]: Loss: 0.5661, CE: 0.0536
[16:32:12.638] Epoch [26/30] Iteration [780/893]: Loss: 0.5783, CE: 0.0706
[16:32:18.099] Epoch [26/30] Iteration [790/893]: Loss: 0.5931, CE: 0.1074
[16:32:23.549] Epoch [26/30] Iteration [800/893]: Loss: 0.5865, CE: 0.0909
[16:32:29.022] Epoch [26/30] Iteration [810/893]: Loss: 0.5908, CE: 0.1016
[16:32:34.470] Epoch [26/30] Iteration [820/893]: Loss: 0.5725, CE: 0.0559
[16:32:39.920] Epoch [26/30] Iteration [830/893]: Loss: 0.5752, CE: 0.0629
[16:32:45.376] Epoch [26/30] Iteration [840/893]: Loss: 0.6222, CE: 0.1842
[16:32:50.857] Epoch [26/30] Iteration [850/893]: Loss: 0.6191, CE: 0.1723
[16:32:56.288] Epoch [26/30] Iteration [860/893]: Loss: 0.5801, CE: 0.0749
[16:33:01.746] Epoch [26/30] Iteration [870/893]: Loss: 0.6029, CE: 0.1649
[16:33:07.202] Epoch [26/30] Iteration [880/893]: Loss: 0.5908, CE: 0.1037
[16:33:12.657] Epoch [26/30] Iteration [890/893]: Loss: 0.5765, CE: 0.0719
[16:33:14.329] Epoch [26/30] Average Loss: 0.5905, CE: 0.1051, Dice: 0.9141
[16:33:35.020] Epoch [27/30] Iteration [0/893]: Loss: 0.5797, CE: 0.0742
[16:33:40.460] Epoch [27/30] Iteration [10/893]: Loss: 0.5984, CE: 0.1208
[16:33:45.916] Epoch [27/30] Iteration [20/893]: Loss: 0.6012, CE: 0.1273
[16:33:51.379] Epoch [27/30] Iteration [30/893]: Loss: 0.6061, CE: 0.1406
[16:33:56.835] Epoch [27/30] Iteration [40/893]: Loss: 0.6100, CE: 0.1543
[16:34:02.332] Epoch [27/30] Iteration [50/893]: Loss: 0.5921, CE: 0.1061
[16:34:07.782] Epoch [27/30] Iteration [60/893]: Loss: 0.5805, CE: 0.0972
[16:34:13.221] Epoch [27/30] Iteration [70/893]: Loss: 0.5939, CE: 0.1091
[16:34:18.671] Epoch [27/30] Iteration [80/893]: Loss: 0.5962, CE: 0.1149
[16:34:24.125] Epoch [27/30] Iteration [90/893]: Loss: 0.5690, CE: 0.0635
[16:34:29.622] Epoch [27/30] Iteration [100/893]: Loss: 0.5833, CE: 0.0827
[16:34:35.096] Epoch [27/30] Iteration [110/893]: Loss: 0.6019, CE: 0.1293
[16:34:40.546] Epoch [27/30] Iteration [120/893]: Loss: 0.5881, CE: 0.1174
[16:34:46.050] Epoch [27/30] Iteration [130/893]: Loss: 0.5735, CE: 0.0592
[16:34:51.629] Epoch [27/30] Iteration [140/893]: Loss: 0.6089, CE: 0.1466
[16:34:57.214] Epoch [27/30] Iteration [150/893]: Loss: 0.5839, CE: 0.0846
[16:35:02.676] Epoch [27/30] Iteration [160/893]: Loss: 0.6393, CE: 0.2224
[16:35:08.163] Epoch [27/30] Iteration [170/893]: Loss: 0.5841, CE: 0.0849
[16:35:13.635] Epoch [27/30] Iteration [180/893]: Loss: 0.5899, CE: 0.1052
[16:35:19.084] Epoch [27/30] Iteration [190/893]: Loss: 0.5459, CE: 0.0787
[16:35:24.572] Epoch [27/30] Iteration [200/893]: Loss: 0.5707, CE: 0.0862
[16:35:30.071] Epoch [27/30] Iteration [210/893]: Loss: 0.6212, CE: 0.1771
[16:35:35.500] Epoch [27/30] Iteration [220/893]: Loss: 0.6169, CE: 0.1666
[16:35:40.973] Epoch [27/30] Iteration [230/893]: Loss: 0.6069, CE: 0.1447
[16:35:46.436] Epoch [27/30] Iteration [240/893]: Loss: 0.5837, CE: 0.0840
[16:35:51.953] Epoch [27/30] Iteration [250/893]: Loss: 0.5835, CE: 0.0837
[16:35:57.393] Epoch [27/30] Iteration [260/893]: Loss: 0.5816, CE: 0.0786
[16:36:02.909] Epoch [27/30] Iteration [270/893]: Loss: 0.5955, CE: 0.1143
[16:36:08.363] Epoch [27/30] Iteration [280/893]: Loss: 0.6078, CE: 0.1436
[16:36:13.818] Epoch [27/30] Iteration [290/893]: Loss: 0.5842, CE: 0.0851
[16:36:19.274] Epoch [27/30] Iteration [300/893]: Loss: 0.5835, CE: 0.0852
[16:36:24.716] Epoch [27/30] Iteration [310/893]: Loss: 0.5948, CE: 0.1341
[16:36:30.174] Epoch [27/30] Iteration [320/893]: Loss: 0.5847, CE: 0.0864
[16:36:35.654] Epoch [27/30] Iteration [330/893]: Loss: 0.5770, CE: 0.0730
[16:36:41.110] Epoch [27/30] Iteration [340/893]: Loss: 0.5908, CE: 0.1041
[16:36:46.562] Epoch [27/30] Iteration [350/893]: Loss: 0.5984, CE: 0.1203
[16:36:52.032] Epoch [27/30] Iteration [360/893]: Loss: 0.5879, CE: 0.1086
[16:36:57.468] Epoch [27/30] Iteration [370/893]: Loss: 0.5899, CE: 0.0997
[16:37:02.945] Epoch [27/30] Iteration [380/893]: Loss: 0.5955, CE: 0.1147
[16:37:08.425] Epoch [27/30] Iteration [390/893]: Loss: 0.5885, CE: 0.0960
[16:37:13.897] Epoch [27/30] Iteration [400/893]: Loss: 0.5763, CE: 0.0733
[16:37:19.352] Epoch [27/30] Iteration [410/893]: Loss: 0.5921, CE: 0.1162
[16:37:24.788] Epoch [27/30] Iteration [420/893]: Loss: 0.5785, CE: 0.0736
[16:37:30.289] Epoch [27/30] Iteration [430/893]: Loss: 0.5931, CE: 0.1072
[16:37:35.728] Epoch [27/30] Iteration [440/893]: Loss: 0.5863, CE: 0.1145
[16:37:41.180] Epoch [27/30] Iteration [450/893]: Loss: 0.5739, CE: 0.0599
[16:37:46.691] Epoch [27/30] Iteration [460/893]: Loss: 0.5784, CE: 0.0816
[16:37:52.147] Epoch [27/30] Iteration [470/893]: Loss: 0.5790, CE: 0.0723
[16:37:57.580] Epoch [27/30] Iteration [480/893]: Loss: 0.5856, CE: 0.0971
[16:38:03.085] Epoch [27/30] Iteration [490/893]: Loss: 0.6051, CE: 0.1400
[16:38:08.540] Epoch [27/30] Iteration [500/893]: Loss: 0.6016, CE: 0.1411
[16:38:14.003] Epoch [27/30] Iteration [510/893]: Loss: 0.5800, CE: 0.0747
[16:38:19.461] Epoch [27/30] Iteration [520/893]: Loss: 0.6121, CE: 0.1558
[16:38:24.935] Epoch [27/30] Iteration [530/893]: Loss: 0.5826, CE: 0.0820
[16:38:30.395] Epoch [27/30] Iteration [540/893]: Loss: 0.5743, CE: 0.0608
[16:38:35.894] Epoch [27/30] Iteration [550/893]: Loss: 0.5978, CE: 0.1190
[16:38:41.358] Epoch [27/30] Iteration [560/893]: Loss: 0.5963, CE: 0.1152
[16:38:46.817] Epoch [27/30] Iteration [570/893]: Loss: 0.6122, CE: 0.1550
[16:38:52.270] Epoch [27/30] Iteration [580/893]: Loss: 0.6026, CE: 0.1313
[16:38:57.786] Epoch [27/30] Iteration [590/893]: Loss: 0.6200, CE: 0.1741
[16:39:03.260] Epoch [27/30] Iteration [600/893]: Loss: 0.5998, CE: 0.1239
[16:39:08.768] Epoch [27/30] Iteration [610/893]: Loss: 0.5660, CE: 0.0643
[16:39:14.212] Epoch [27/30] Iteration [620/893]: Loss: 0.5786, CE: 0.0727
[16:39:19.678] Epoch [27/30] Iteration [630/893]: Loss: 0.5832, CE: 0.0826
[16:39:25.142] Epoch [27/30] Iteration [640/893]: Loss: 0.5642, CE: 0.0377
[16:39:30.604] Epoch [27/30] Iteration [650/893]: Loss: 0.5884, CE: 0.0970
[16:39:36.068] Epoch [27/30] Iteration [660/893]: Loss: 0.5920, CE: 0.1044
[16:39:41.509] Epoch [27/30] Iteration [670/893]: Loss: 0.5871, CE: 0.0925
[16:39:46.938] Epoch [27/30] Iteration [680/893]: Loss: 0.5830, CE: 0.0911
[16:39:52.384] Epoch [27/30] Iteration [690/893]: Loss: 0.5864, CE: 0.0916
[16:39:57.833] Epoch [27/30] Iteration [700/893]: Loss: 0.5718, CE: 0.0548
[16:40:03.286] Epoch [27/30] Iteration [710/893]: Loss: 0.5833, CE: 0.0841
[16:40:08.737] Epoch [27/30] Iteration [720/893]: Loss: 0.5819, CE: 0.1077
[16:40:14.185] Epoch [27/30] Iteration [730/893]: Loss: 0.5772, CE: 0.0676
[16:40:19.660] Epoch [27/30] Iteration [740/893]: Loss: 0.5899, CE: 0.0993
[16:40:25.103] Epoch [27/30] Iteration [750/893]: Loss: 0.6067, CE: 0.1412
[16:40:30.530] Epoch [27/30] Iteration [760/893]: Loss: 0.5981, CE: 0.1309
[16:40:36.008] Epoch [27/30] Iteration [770/893]: Loss: 0.5770, CE: 0.0674
[16:40:41.440] Epoch [27/30] Iteration [780/893]: Loss: 0.5880, CE: 0.0952
[16:40:46.866] Epoch [27/30] Iteration [790/893]: Loss: 0.5944, CE: 0.1105
[16:40:52.306] Epoch [27/30] Iteration [800/893]: Loss: 0.5626, CE: 0.0329
[16:40:57.748] Epoch [27/30] Iteration [810/893]: Loss: 0.5888, CE: 0.0964
[16:41:03.179] Epoch [27/30] Iteration [820/893]: Loss: 0.5917, CE: 0.1035
[16:41:08.618] Epoch [27/30] Iteration [830/893]: Loss: 0.6105, CE: 0.1505
[16:41:14.039] Epoch [27/30] Iteration [840/893]: Loss: 0.5843, CE: 0.0853
[16:41:19.483] Epoch [27/30] Iteration [850/893]: Loss: 0.5880, CE: 0.0950
[16:41:24.924] Epoch [27/30] Iteration [860/893]: Loss: 0.5801, CE: 0.0749
[16:41:30.408] Epoch [27/30] Iteration [870/893]: Loss: 0.5882, CE: 0.0956
[16:41:35.848] Epoch [27/30] Iteration [880/893]: Loss: 0.5843, CE: 0.0853
[16:41:41.361] Epoch [27/30] Iteration [890/893]: Loss: 0.5980, CE: 0.1209
[16:41:43.023] Epoch [27/30] Average Loss: 0.5903, CE: 0.1050, Dice: 0.9138
[16:42:03.653] Epoch [28/30] Iteration [0/893]: Loss: 0.6077, CE: 0.1436
[16:42:09.111] Epoch [28/30] Iteration [10/893]: Loss: 0.5890, CE: 0.0986
[16:42:14.543] Epoch [28/30] Iteration [20/893]: Loss: 0.5877, CE: 0.0947
[16:42:19.979] Epoch [28/30] Iteration [30/893]: Loss: 0.6103, CE: 0.1501
[16:42:25.424] Epoch [28/30] Iteration [40/893]: Loss: 0.5912, CE: 0.1026
[16:42:30.879] Epoch [28/30] Iteration [50/893]: Loss: 0.6065, CE: 0.1406
[16:42:36.315] Epoch [28/30] Iteration [60/893]: Loss: 0.5770, CE: 0.0685
[16:42:41.775] Epoch [28/30] Iteration [70/893]: Loss: 0.5896, CE: 0.1171
[16:42:47.222] Epoch [28/30] Iteration [80/893]: Loss: 0.6016, CE: 0.1284
[16:42:52.655] Epoch [28/30] Iteration [90/893]: Loss: 0.5781, CE: 0.0706
[16:42:58.105] Epoch [28/30] Iteration [100/893]: Loss: 0.5983, CE: 0.1202
[16:43:03.544] Epoch [28/30] Iteration [110/893]: Loss: 0.5865, CE: 0.0910
[16:43:09.032] Epoch [28/30] Iteration [120/893]: Loss: 0.5869, CE: 0.0924
[16:43:14.477] Epoch [28/30] Iteration [130/893]: Loss: 0.5886, CE: 0.0961
[16:43:19.913] Epoch [28/30] Iteration [140/893]: Loss: 0.6122, CE: 0.1547
[16:43:25.343] Epoch [28/30] Iteration [150/893]: Loss: 0.5958, CE: 0.1141
[16:43:30.814] Epoch [28/30] Iteration [160/893]: Loss: 0.5946, CE: 0.1116
[16:43:36.234] Epoch [28/30] Iteration [170/893]: Loss: 0.6152, CE: 0.1624
[16:43:41.675] Epoch [28/30] Iteration [180/893]: Loss: 0.5890, CE: 0.0970
[16:43:47.139] Epoch [28/30] Iteration [190/893]: Loss: 0.5770, CE: 0.0672
[16:43:52.602] Epoch [28/30] Iteration [200/893]: Loss: 0.5886, CE: 0.0963
[16:43:58.054] Epoch [28/30] Iteration [210/893]: Loss: 0.5940, CE: 0.1114
[16:44:03.499] Epoch [28/30] Iteration [220/893]: Loss: 0.5722, CE: 0.0597
[16:44:08.929] Epoch [28/30] Iteration [230/893]: Loss: 0.5700, CE: 0.0717
[16:44:14.387] Epoch [28/30] Iteration [240/893]: Loss: 0.5795, CE: 0.0759
[16:44:19.811] Epoch [28/30] Iteration [250/893]: Loss: 0.5910, CE: 0.1019
[16:44:25.303] Epoch [28/30] Iteration [260/893]: Loss: 0.6051, CE: 0.1369
[16:44:30.774] Epoch [28/30] Iteration [270/893]: Loss: 0.5817, CE: 0.0793
[16:44:36.242] Epoch [28/30] Iteration [280/893]: Loss: 0.6347, CE: 0.2115
[16:44:41.718] Epoch [28/30] Iteration [290/893]: Loss: 0.6089, CE: 0.1466
[16:44:47.209] Epoch [28/30] Iteration [300/893]: Loss: 0.6052, CE: 0.1374
[16:44:52.650] Epoch [28/30] Iteration [310/893]: Loss: 0.5838, CE: 0.0844
[16:44:58.113] Epoch [28/30] Iteration [320/893]: Loss: 0.6075, CE: 0.1433
[16:45:03.613] Epoch [28/30] Iteration [330/893]: Loss: 0.5884, CE: 0.1072
[16:45:09.049] Epoch [28/30] Iteration [340/893]: Loss: 0.6121, CE: 0.1544
[16:45:14.519] Epoch [28/30] Iteration [350/893]: Loss: 0.5911, CE: 0.1022
[16:45:20.029] Epoch [28/30] Iteration [360/893]: Loss: 0.5710, CE: 0.0528
[16:45:25.471] Epoch [28/30] Iteration [370/893]: Loss: 0.5962, CE: 0.1151
[16:45:30.932] Epoch [28/30] Iteration [380/893]: Loss: 0.5855, CE: 0.0886
[16:45:36.381] Epoch [28/30] Iteration [390/893]: Loss: 0.6080, CE: 0.1443
[16:45:41.863] Epoch [28/30] Iteration [400/893]: Loss: 0.5779, CE: 0.0696
[16:45:47.339] Epoch [28/30] Iteration [410/893]: Loss: 0.5708, CE: 0.0519
[16:45:52.789] Epoch [28/30] Iteration [420/893]: Loss: 0.5766, CE: 0.0848
[16:45:58.236] Epoch [28/30] Iteration [430/893]: Loss: 0.5967, CE: 0.1166
[16:46:03.694] Epoch [28/30] Iteration [440/893]: Loss: 0.5881, CE: 0.0951
[16:46:09.127] Epoch [28/30] Iteration [450/893]: Loss: 0.5886, CE: 0.0964
[16:46:14.588] Epoch [28/30] Iteration [460/893]: Loss: 0.5943, CE: 0.1107
[16:46:20.021] Epoch [28/30] Iteration [470/893]: Loss: 0.5997, CE: 0.1261
[16:46:25.470] Epoch [28/30] Iteration [480/893]: Loss: 0.5882, CE: 0.0950
[16:46:30.903] Epoch [28/30] Iteration [490/893]: Loss: 0.6017, CE: 0.1287
[16:46:36.388] Epoch [28/30] Iteration [500/893]: Loss: 0.5838, CE: 0.0848
[16:46:41.845] Epoch [28/30] Iteration [510/893]: Loss: 0.6025, CE: 0.1305
[16:46:47.299] Epoch [28/30] Iteration [520/893]: Loss: 0.5957, CE: 0.1266
[16:46:52.798] Epoch [28/30] Iteration [530/893]: Loss: 0.5785, CE: 0.0710
[16:46:58.244] Epoch [28/30] Iteration [540/893]: Loss: 0.5824, CE: 0.0806
[16:47:03.678] Epoch [28/30] Iteration [550/893]: Loss: 0.5943, CE: 0.1102
[16:47:09.127] Epoch [28/30] Iteration [560/893]: Loss: 0.5944, CE: 0.1128
[16:47:14.603] Epoch [28/30] Iteration [570/893]: Loss: 0.5721, CE: 0.0588
[16:47:20.062] Epoch [28/30] Iteration [580/893]: Loss: 0.6130, CE: 0.1568
[16:47:25.525] Epoch [28/30] Iteration [590/893]: Loss: 0.5834, CE: 0.0834
[16:47:30.962] Epoch [28/30] Iteration [600/893]: Loss: 0.6059, CE: 0.1395
[16:47:36.426] Epoch [28/30] Iteration [610/893]: Loss: 0.6188, CE: 0.1738
[16:47:41.895] Epoch [28/30] Iteration [620/893]: Loss: 0.6048, CE: 0.1363
[16:47:47.408] Epoch [28/30] Iteration [630/893]: Loss: 0.5777, CE: 0.0692
[16:47:52.863] Epoch [28/30] Iteration [640/893]: Loss: 0.6042, CE: 0.1348
[16:47:58.314] Epoch [28/30] Iteration [650/893]: Loss: 0.5895, CE: 0.0983
[16:48:03.761] Epoch [28/30] Iteration [660/893]: Loss: 0.6240, CE: 0.1842
[16:48:09.227] Epoch [28/30] Iteration [670/893]: Loss: 0.5847, CE: 0.0868
[16:48:14.670] Epoch [28/30] Iteration [680/893]: Loss: 0.6013, CE: 0.1279
[16:48:20.166] Epoch [28/30] Iteration [690/893]: Loss: 0.5716, CE: 0.0718
[16:48:25.624] Epoch [28/30] Iteration [700/893]: Loss: 0.6062, CE: 0.1424
[16:48:31.066] Epoch [28/30] Iteration [710/893]: Loss: 0.5876, CE: 0.0947
[16:48:36.562] Epoch [28/30] Iteration [720/893]: Loss: 0.5776, CE: 0.0688
[16:48:42.045] Epoch [28/30] Iteration [730/893]: Loss: 0.5879, CE: 0.0943
[16:48:47.490] Epoch [28/30] Iteration [740/893]: Loss: 0.5850, CE: 0.0872
[16:48:52.934] Epoch [28/30] Iteration [750/893]: Loss: 0.5837, CE: 0.0944
[16:48:58.481] Epoch [28/30] Iteration [760/893]: Loss: 0.5909, CE: 0.1024
[16:49:03.938] Epoch [28/30] Iteration [770/893]: Loss: 0.5751, CE: 0.0622
[16:49:09.396] Epoch [28/30] Iteration [780/893]: Loss: 0.6015, CE: 0.1285
[16:49:14.865] Epoch [28/30] Iteration [790/893]: Loss: 0.6050, CE: 0.1368
[16:49:20.342] Epoch [28/30] Iteration [800/893]: Loss: 0.5667, CE: 0.0489
[16:49:25.785] Epoch [28/30] Iteration [810/893]: Loss: 0.5740, CE: 0.0612
[16:49:31.237] Epoch [28/30] Iteration [820/893]: Loss: 0.5791, CE: 0.0739
[16:49:36.703] Epoch [28/30] Iteration [830/893]: Loss: 0.5919, CE: 0.1166
[16:49:42.159] Epoch [28/30] Iteration [840/893]: Loss: 0.6108, CE: 0.1513
[16:49:47.587] Epoch [28/30] Iteration [850/893]: Loss: 0.6173, CE: 0.1675
[16:49:53.055] Epoch [28/30] Iteration [860/893]: Loss: 0.5831, CE: 0.0824
[16:49:58.537] Epoch [28/30] Iteration [870/893]: Loss: 0.5935, CE: 0.1089
[16:50:04.019] Epoch [28/30] Iteration [880/893]: Loss: 0.5682, CE: 0.0452
[16:50:09.483] Epoch [28/30] Iteration [890/893]: Loss: 0.5922, CE: 0.1058
[16:50:11.135] Epoch [28/30] Average Loss: 0.5905, CE: 0.1050, Dice: 0.9143
[16:50:32.297] Epoch [29/30] Iteration [0/893]: Loss: 0.5985, CE: 0.1259
[16:50:37.740] Epoch [29/30] Iteration [10/893]: Loss: 0.5916, CE: 0.1376
[16:50:43.165] Epoch [29/30] Iteration [20/893]: Loss: 0.5724, CE: 0.0558
[16:50:48.627] Epoch [29/30] Iteration [30/893]: Loss: 0.5972, CE: 0.1176
[16:50:54.065] Epoch [29/30] Iteration [40/893]: Loss: 0.5992, CE: 0.1225
[16:50:59.502] Epoch [29/30] Iteration [50/893]: Loss: 0.6134, CE: 0.1597
[16:51:04.984] Epoch [29/30] Iteration [60/893]: Loss: 0.5875, CE: 0.0933
[16:51:10.437] Epoch [29/30] Iteration [70/893]: Loss: 0.5941, CE: 0.1098
[16:51:15.869] Epoch [29/30] Iteration [80/893]: Loss: 0.5953, CE: 0.1134
[16:51:21.342] Epoch [29/30] Iteration [90/893]: Loss: 0.5883, CE: 0.0954
[16:51:26.783] Epoch [29/30] Iteration [100/893]: Loss: 0.5860, CE: 0.0900
[16:51:32.217] Epoch [29/30] Iteration [110/893]: Loss: 0.5937, CE: 0.1087
[16:51:37.701] Epoch [29/30] Iteration [120/893]: Loss: 0.6009, CE: 0.1266
[16:51:43.153] Epoch [29/30] Iteration [130/893]: Loss: 0.5830, CE: 0.0826
[16:51:48.571] Epoch [29/30] Iteration [140/893]: Loss: 0.6148, CE: 0.1613
[16:51:54.012] Epoch [29/30] Iteration [150/893]: Loss: 0.6255, CE: 0.1877
[16:51:59.461] Epoch [29/30] Iteration [160/893]: Loss: 0.5959, CE: 0.1252
[16:52:04.965] Epoch [29/30] Iteration [170/893]: Loss: 0.5974, CE: 0.1180
[16:52:10.454] Epoch [29/30] Iteration [180/893]: Loss: 0.5777, CE: 0.0690
[16:52:15.911] Epoch [29/30] Iteration [190/893]: Loss: 0.5888, CE: 0.0967
[16:52:21.347] Epoch [29/30] Iteration [200/893]: Loss: 0.5781, CE: 0.0702
[16:52:26.804] Epoch [29/30] Iteration [210/893]: Loss: 0.5770, CE: 0.0673
[16:52:32.285] Epoch [29/30] Iteration [220/893]: Loss: 0.5885, CE: 0.0964
[16:52:37.762] Epoch [29/30] Iteration [230/893]: Loss: 0.5982, CE: 0.1199
[16:52:43.202] Epoch [29/30] Iteration [240/893]: Loss: 0.5768, CE: 0.0669
[16:52:48.649] Epoch [29/30] Iteration [250/893]: Loss: 0.5843, CE: 0.0854
[16:52:54.082] Epoch [29/30] Iteration [260/893]: Loss: 0.6032, CE: 0.1325
[16:52:59.519] Epoch [29/30] Iteration [270/893]: Loss: 0.5756, CE: 0.0653
[16:53:05.011] Epoch [29/30] Iteration [280/893]: Loss: 0.5829, CE: 0.0819
[16:53:10.454] Epoch [29/30] Iteration [290/893]: Loss: 0.6047, CE: 0.1360
[16:53:15.885] Epoch [29/30] Iteration [300/893]: Loss: 0.6078, CE: 0.1456
[16:53:21.329] Epoch [29/30] Iteration [310/893]: Loss: 0.5812, CE: 0.0776
[16:53:26.777] Epoch [29/30] Iteration [320/893]: Loss: 0.5805, CE: 0.0758
[16:53:32.278] Epoch [29/30] Iteration [330/893]: Loss: 0.5940, CE: 0.1096
[16:53:37.753] Epoch [29/30] Iteration [340/893]: Loss: 0.5871, CE: 0.1216
[16:53:43.203] Epoch [29/30] Iteration [350/893]: Loss: 0.6152, CE: 0.1623
[16:53:48.639] Epoch [29/30] Iteration [360/893]: Loss: 0.5950, CE: 0.1118
[16:53:54.085] Epoch [29/30] Iteration [370/893]: Loss: 0.5945, CE: 0.1108
[16:53:59.545] Epoch [29/30] Iteration [380/893]: Loss: 0.5878, CE: 0.0940
[16:54:05.018] Epoch [29/30] Iteration [390/893]: Loss: 0.6063, CE: 0.1411
[16:54:10.479] Epoch [29/30] Iteration [400/893]: Loss: 0.6044, CE: 0.1356
[16:54:15.951] Epoch [29/30] Iteration [410/893]: Loss: 0.5711, CE: 0.0624
[16:54:21.388] Epoch [29/30] Iteration [420/893]: Loss: 0.6146, CE: 0.1607
[16:54:26.884] Epoch [29/30] Iteration [430/893]: Loss: 0.6089, CE: 0.1465
[16:54:32.338] Epoch [29/30] Iteration [440/893]: Loss: 0.6057, CE: 0.1401
[16:54:37.817] Epoch [29/30] Iteration [450/893]: Loss: 0.5987, CE: 0.1244
[16:54:43.285] Epoch [29/30] Iteration [460/893]: Loss: 0.6463, CE: 0.2397
[16:54:48.728] Epoch [29/30] Iteration [470/893]: Loss: 0.6031, CE: 0.1319
[16:54:54.163] Epoch [29/30] Iteration [480/893]: Loss: 0.5890, CE: 0.0970
[16:54:59.622] Epoch [29/30] Iteration [490/893]: Loss: 0.5952, CE: 0.1186
[16:55:05.060] Epoch [29/30] Iteration [500/893]: Loss: 0.6034, CE: 0.1330
[16:55:10.524] Epoch [29/30] Iteration [510/893]: Loss: 0.5832, CE: 0.0825
[16:55:15.982] Epoch [29/30] Iteration [520/893]: Loss: 0.5974, CE: 0.1205
[16:55:21.429] Epoch [29/30] Iteration [530/893]: Loss: 0.5948, CE: 0.1115
[16:55:26.909] Epoch [29/30] Iteration [540/893]: Loss: 0.5804, CE: 0.0774
[16:55:32.352] Epoch [29/30] Iteration [550/893]: Loss: 0.5751, CE: 0.0872
[16:55:37.819] Epoch [29/30] Iteration [560/893]: Loss: 0.5943, CE: 0.1103
[16:55:43.275] Epoch [29/30] Iteration [570/893]: Loss: 0.5917, CE: 0.1037
[16:55:48.739] Epoch [29/30] Iteration [580/893]: Loss: 0.5817, CE: 0.0805
[16:55:54.203] Epoch [29/30] Iteration [590/893]: Loss: 0.6395, CE: 0.2227
[16:55:59.723] Epoch [29/30] Iteration [600/893]: Loss: 0.5804, CE: 0.0781
[16:56:05.177] Epoch [29/30] Iteration [610/893]: Loss: 0.5774, CE: 0.0689
[16:56:10.604] Epoch [29/30] Iteration [620/893]: Loss: 0.5771, CE: 0.0694
[16:56:16.087] Epoch [29/30] Iteration [630/893]: Loss: 0.5737, CE: 0.0731
[16:56:21.531] Epoch [29/30] Iteration [640/893]: Loss: 0.6166, CE: 0.1658
[16:56:27.000] Epoch [29/30] Iteration [650/893]: Loss: 0.5808, CE: 0.0765
[16:56:32.437] Epoch [29/30] Iteration [660/893]: Loss: 0.6347, CE: 0.2125
[16:56:37.886] Epoch [29/30] Iteration [670/893]: Loss: 0.6047, CE: 0.1363
[16:56:43.352] Epoch [29/30] Iteration [680/893]: Loss: 0.5621, CE: 0.1052
[16:56:48.791] Epoch [29/30] Iteration [690/893]: Loss: 0.5872, CE: 0.0925
[16:56:54.235] Epoch [29/30] Iteration [700/893]: Loss: 0.5884, CE: 0.1298
[16:56:59.674] Epoch [29/30] Iteration [710/893]: Loss: 0.5880, CE: 0.0948
[16:57:05.115] Epoch [29/30] Iteration [720/893]: Loss: 0.5717, CE: 0.0550
[16:57:10.583] Epoch [29/30] Iteration [730/893]: Loss: 0.5838, CE: 0.0981
[16:57:16.012] Epoch [29/30] Iteration [740/893]: Loss: 0.5902, CE: 0.1031
[16:57:21.505] Epoch [29/30] Iteration [750/893]: Loss: 0.5722, CE: 0.0568
[16:57:26.949] Epoch [29/30] Iteration [760/893]: Loss: 0.6005, CE: 0.1256
[16:57:32.405] Epoch [29/30] Iteration [770/893]: Loss: 0.5770, CE: 0.1280
[16:57:37.861] Epoch [29/30] Iteration [780/893]: Loss: 0.5886, CE: 0.1114
[16:57:43.317] Epoch [29/30] Iteration [790/893]: Loss: 0.5837, CE: 0.0839
[16:57:48.770] Epoch [29/30] Iteration [800/893]: Loss: 0.6178, CE: 0.1755
[16:57:54.251] Epoch [29/30] Iteration [810/893]: Loss: 0.6043, CE: 0.1352
[16:57:59.691] Epoch [29/30] Iteration [820/893]: Loss: 0.6024, CE: 0.1306
[16:58:05.144] Epoch [29/30] Iteration [830/893]: Loss: 0.5923, CE: 0.1053
[16:58:10.611] Epoch [29/30] Iteration [840/893]: Loss: 0.5820, CE: 0.0799
[16:58:16.061] Epoch [29/30] Iteration [850/893]: Loss: 0.5955, CE: 0.1200
[16:58:21.491] Epoch [29/30] Iteration [860/893]: Loss: 0.5798, CE: 0.0743
[16:58:26.922] Epoch [29/30] Iteration [870/893]: Loss: 0.5884, CE: 0.0958
[16:58:32.362] Epoch [29/30] Iteration [880/893]: Loss: 0.5795, CE: 0.0739
[16:58:37.820] Epoch [29/30] Iteration [890/893]: Loss: 0.5743, CE: 0.0607
[16:58:39.517] Epoch [29/30] Average Loss: 0.5906, CE: 0.1052, Dice: 0.9142
[16:58:39.683] Saved continual learning checkpoint to ./universal/synapse_to_kits23_tpgm\epoch_29_continual.pth
[16:58:39.844] Saved final continual model to ./universal/synapse_to_kits23_tpgm\final_continual_model.pth
[16:06:43.493] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=5, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', dataset_fraction=1.0, enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=False, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[16:06:43.493] Continual Learning: 9 -> 12 classes
[16:06:43.493] Task offset: 9, New classes: 4
[16:06:43.493] Dataset fraction: 1.0
[16:06:43.493] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[16:06:43.493] Old task classes: 9, New task classes: 4
[16:06:43.590] Identified output layer keys: ['cswin_unet.output.weight']
[16:06:43.591] Loaded 462 backbone layers from pretrained model.
[16:06:43.591] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[16:06:43.626] Successfully adapted model for continual learning.
[16:06:43.627] Pretrained weights loaded and adapted for continual learning
[16:06:43.635] Using full dataset: 95221 samples
[16:06:43.637] Surgical fine-tuning enabled for continual learning
[16:06:43.639] 2976 iterations per epoch. 14880 max iterations
[16:06:43.640] Updating surgical weights at epoch 0
[16:07:09.857] Surgical weights range: [0.0002, 1.0000]
[16:07:33.563] Epoch [0/5] Iteration [0/2976]: Loss: 0.6014, CE: 0.1275
[16:07:38.024] Epoch [0/5] Iteration [10/2976]: Loss: 0.6426, CE: 0.2596
[16:07:42.547] Epoch [0/5] Iteration [20/2976]: Loss: 0.5532, CE: 0.0789
[16:07:47.066] Epoch [0/5] Iteration [30/2976]: Loss: 0.4965, CE: 0.0959
[16:07:51.593] Epoch [0/5] Iteration [40/2976]: Loss: 0.2927, CE: 0.1375
[16:07:56.089] Epoch [0/5] Iteration [50/2976]: Loss: 0.2764, CE: 0.1776
[16:08:00.599] Epoch [0/5] Iteration [60/2976]: Loss: 0.1665, CE: 0.0570
[16:08:05.111] Epoch [0/5] Iteration [70/2976]: Loss: 0.2331, CE: 0.1693
[16:08:09.609] Epoch [0/5] Iteration [80/2976]: Loss: 0.1921, CE: 0.0954
[16:08:14.129] Epoch [0/5] Iteration [90/2976]: Loss: 0.1334, CE: 0.0786
[16:08:18.635] Epoch [0/5] Iteration [100/2976]: Loss: 0.1700, CE: 0.1713
[16:08:23.141] Epoch [0/5] Iteration [110/2976]: Loss: 0.1953, CE: 0.1086
[16:08:27.642] Epoch [0/5] Iteration [120/2976]: Loss: 0.2152, CE: 0.1586
[16:08:32.165] Epoch [0/5] Iteration [130/2976]: Loss: 0.1398, CE: 0.0953
[16:08:36.682] Epoch [0/5] Iteration [140/2976]: Loss: 0.2085, CE: 0.1400
[16:08:41.183] Epoch [0/5] Iteration [150/2976]: Loss: 0.2034, CE: 0.1287
[16:08:45.717] Epoch [0/5] Iteration [160/2976]: Loss: 0.1378, CE: 0.0913
[16:08:50.223] Epoch [0/5] Iteration [170/2976]: Loss: 0.2167, CE: 0.1629
[16:08:54.739] Epoch [0/5] Iteration [180/2976]: Loss: 0.2073, CE: 0.1395
[16:08:59.259] Epoch [0/5] Iteration [190/2976]: Loss: 0.1987, CE: 0.1178
[16:09:03.770] Epoch [0/5] Iteration [200/2976]: Loss: 0.1213, CE: 0.0494
[16:09:08.283] Epoch [0/5] Iteration [210/2976]: Loss: 0.1430, CE: 0.1031
[16:09:12.803] Epoch [0/5] Iteration [220/2976]: Loss: 0.1560, CE: 0.1364
[16:09:17.328] Epoch [0/5] Iteration [230/2976]: Loss: 0.1966, CE: 0.1103
[16:09:21.849] Epoch [0/5] Iteration [240/2976]: Loss: 0.1802, CE: 0.0710
[16:09:26.382] Epoch [0/5] Iteration [250/2976]: Loss: 0.1468, CE: 0.1133
[16:09:30.896] Epoch [0/5] Iteration [260/2976]: Loss: 0.2347, CE: 0.2071
[16:09:35.423] Epoch [0/5] Iteration [270/2976]: Loss: 0.2010, CE: 0.1235
[16:09:39.942] Epoch [0/5] Iteration [280/2976]: Loss: 0.1706, CE: 0.1719
[16:09:44.462] Epoch [0/5] Iteration [290/2976]: Loss: 0.1965, CE: 0.1123
[16:09:48.987] Epoch [0/5] Iteration [300/2976]: Loss: 0.1854, CE: 0.0846
[16:09:53.497] Epoch [0/5] Iteration [310/2976]: Loss: 0.2204, CE: 0.1716
[16:09:58.025] Epoch [0/5] Iteration [320/2976]: Loss: 0.2197, CE: 0.1705
[16:10:02.542] Epoch [0/5] Iteration [330/2976]: Loss: 0.1937, CE: 0.1050
[16:10:07.070] Epoch [0/5] Iteration [340/2976]: Loss: 0.1955, CE: 0.1087
[16:10:11.586] Epoch [0/5] Iteration [350/2976]: Loss: 0.1923, CE: 0.1020
[16:10:16.120] Epoch [0/5] Iteration [360/2976]: Loss: 0.1795, CE: 0.0695
[16:10:20.638] Epoch [0/5] Iteration [370/2976]: Loss: 0.1751, CE: 0.0594
[16:10:25.168] Epoch [0/5] Iteration [380/2976]: Loss: 0.1793, CE: 0.1943
[16:10:29.684] Epoch [0/5] Iteration [390/2976]: Loss: 0.1992, CE: 0.1192
[16:10:34.223] Epoch [0/5] Iteration [400/2976]: Loss: 0.1927, CE: 0.1014
[16:10:38.721] Epoch [0/5] Iteration [410/2976]: Loss: 0.1890, CE: 0.0939
[16:10:43.248] Epoch [0/5] Iteration [420/2976]: Loss: 0.1898, CE: 0.0958
[16:10:47.760] Epoch [0/5] Iteration [430/2976]: Loss: 0.1216, CE: 0.0482
[16:10:52.287] Epoch [0/5] Iteration [440/2976]: Loss: 0.1935, CE: 0.1044
[16:10:56.808] Epoch [0/5] Iteration [450/2976]: Loss: 0.1894, CE: 0.0948
[16:11:01.335] Epoch [0/5] Iteration [460/2976]: Loss: 0.2025, CE: 0.1274
[16:11:05.855] Epoch [0/5] Iteration [470/2976]: Loss: 0.2039, CE: 0.1313
[16:11:10.387] Epoch [0/5] Iteration [480/2976]: Loss: 0.1913, CE: 0.0994
[16:11:14.906] Epoch [0/5] Iteration [490/2976]: Loss: 0.1746, CE: 0.1793
[16:11:19.462] Epoch [0/5] Iteration [500/2976]: Loss: 0.2128, CE: 0.1534
[16:11:23.965] Epoch [0/5] Iteration [510/2976]: Loss: 0.2279, CE: 0.1899
[16:11:28.505] Epoch [0/5] Iteration [520/2976]: Loss: 0.1957, CE: 0.1093
[16:11:33.107] Epoch [0/5] Iteration [530/2976]: Loss: 0.2035, CE: 0.1301
[16:11:37.637] Epoch [0/5] Iteration [540/2976]: Loss: 0.2354, CE: 0.2085
[16:11:42.175] Epoch [0/5] Iteration [550/2976]: Loss: 0.2124, CE: 0.1516
[16:11:46.711] Epoch [0/5] Iteration [560/2976]: Loss: 0.2199, CE: 0.1712
[16:11:51.216] Epoch [0/5] Iteration [570/2976]: Loss: 0.2258, CE: 0.1791
[16:11:55.751] Epoch [0/5] Iteration [580/2976]: Loss: 0.2335, CE: 0.2045
[16:12:00.283] Epoch [0/5] Iteration [590/2976]: Loss: 0.2343, CE: 0.2059
[16:12:04.811] Epoch [0/5] Iteration [600/2976]: Loss: 0.1841, CE: 0.0819
[16:12:09.333] Epoch [0/5] Iteration [610/2976]: Loss: 0.2106, CE: 0.1476
[16:12:13.852] Epoch [0/5] Iteration [620/2976]: Loss: 0.2416, CE: 0.2249
[16:12:18.364] Epoch [0/5] Iteration [630/2976]: Loss: 0.2203, CE: 0.1718
[16:12:22.907] Epoch [0/5] Iteration [640/2976]: Loss: 0.2109, CE: 0.1483
[16:12:27.416] Epoch [0/5] Iteration [650/2976]: Loss: 0.1567, CE: 0.1378
[16:12:31.930] Epoch [0/5] Iteration [660/2976]: Loss: 0.2219, CE: 0.1755
[16:12:36.456] Epoch [0/5] Iteration [670/2976]: Loss: 0.1859, CE: 0.0850
[16:12:40.985] Epoch [0/5] Iteration [680/2976]: Loss: 0.2192, CE: 0.1675
[16:12:45.488] Epoch [0/5] Iteration [690/2976]: Loss: 0.1638, CE: 0.1548
[16:12:50.011] Epoch [0/5] Iteration [700/2976]: Loss: 0.2297, CE: 0.1954
[16:12:54.533] Epoch [0/5] Iteration [710/2976]: Loss: 0.1937, CE: 0.1056
[16:12:59.049] Epoch [0/5] Iteration [720/2976]: Loss: 0.2173, CE: 0.1645
[16:13:03.556] Epoch [0/5] Iteration [730/2976]: Loss: 0.1952, CE: 0.1080
[16:13:08.083] Epoch [0/5] Iteration [740/2976]: Loss: 0.2184, CE: 0.1635
[16:13:12.614] Epoch [0/5] Iteration [750/2976]: Loss: 0.2044, CE: 0.1322
[16:13:17.144] Epoch [0/5] Iteration [760/2976]: Loss: 0.2045, CE: 0.1313
[16:13:21.672] Epoch [0/5] Iteration [770/2976]: Loss: 0.1481, CE: 0.1167
[16:13:26.206] Epoch [0/5] Iteration [780/2976]: Loss: 0.2390, CE: 0.2183
[16:13:30.724] Epoch [0/5] Iteration [790/2976]: Loss: 0.2086, CE: 0.1404
[16:13:35.251] Epoch [0/5] Iteration [800/2976]: Loss: 0.2557, CE: 0.2596
[16:13:39.776] Epoch [0/5] Iteration [810/2976]: Loss: 0.2082, CE: 0.1398
[16:13:44.297] Epoch [0/5] Iteration [820/2976]: Loss: 0.2162, CE: 0.1607
[16:13:48.808] Epoch [0/5] Iteration [830/2976]: Loss: 0.2019, CE: 0.1258
[16:13:53.336] Epoch [0/5] Iteration [840/2976]: Loss: 0.1593, CE: 0.1439
[16:13:57.856] Epoch [0/5] Iteration [850/2976]: Loss: 0.2421, CE: 0.2255
[16:14:02.378] Epoch [0/5] Iteration [860/2976]: Loss: 0.1277, CE: 0.0626
[16:14:06.900] Epoch [0/5] Iteration [870/2976]: Loss: 0.1854, CE: 0.0838
[16:14:11.427] Epoch [0/5] Iteration [880/2976]: Loss: 0.2236, CE: 0.1804
[16:14:15.934] Epoch [0/5] Iteration [890/2976]: Loss: 0.1884, CE: 0.0926
[16:14:20.467] Epoch [0/5] Iteration [900/2976]: Loss: 0.2042, CE: 0.1317
[16:14:24.977] Epoch [0/5] Iteration [910/2976]: Loss: 0.2345, CE: 0.2073
[16:14:29.493] Epoch [0/5] Iteration [920/2976]: Loss: 0.1956, CE: 0.1103
[16:14:33.999] Epoch [0/5] Iteration [930/2976]: Loss: 0.1945, CE: 0.1030
[16:14:38.530] Epoch [0/5] Iteration [940/2976]: Loss: 0.1288, CE: 0.0683
[16:14:43.048] Epoch [0/5] Iteration [950/2976]: Loss: 0.1548, CE: 0.1335
[16:14:47.569] Epoch [0/5] Iteration [960/2976]: Loss: 0.1837, CE: 0.0809
[16:14:52.078] Epoch [0/5] Iteration [970/2976]: Loss: 0.2179, CE: 0.1650
[16:14:56.605] Epoch [0/5] Iteration [980/2976]: Loss: 0.1737, CE: 0.0561
[16:15:01.117] Epoch [0/5] Iteration [990/2976]: Loss: 0.1755, CE: 0.0604
[16:15:05.644] Epoch [0/5] Iteration [1000/2976]: Loss: 0.2179, CE: 0.1660
[16:15:10.161] Epoch [0/5] Iteration [1010/2976]: Loss: 0.1831, CE: 0.0777
[16:15:14.697] Epoch [0/5] Iteration [1020/2976]: Loss: 0.1990, CE: 0.1168
[16:15:19.233] Epoch [0/5] Iteration [1030/2976]: Loss: 0.1971, CE: 0.1142
[16:15:23.815] Epoch [0/5] Iteration [1040/2976]: Loss: 0.2301, CE: 0.1963
[16:15:28.342] Epoch [0/5] Iteration [1050/2976]: Loss: 0.2030, CE: 0.1279
[16:15:32.876] Epoch [0/5] Iteration [1060/2976]: Loss: 0.2507, CE: 0.2474
[16:15:37.404] Epoch [0/5] Iteration [1070/2976]: Loss: 0.1965, CE: 0.1131
[16:15:41.964] Epoch [0/5] Iteration [1080/2976]: Loss: 0.1914, CE: 0.2244
[16:15:46.478] Epoch [0/5] Iteration [1090/2976]: Loss: 0.1883, CE: 0.0873
[16:15:50.994] Epoch [0/5] Iteration [1100/2976]: Loss: 0.2238, CE: 0.1805
[16:15:55.501] Epoch [0/5] Iteration [1110/2976]: Loss: 0.2056, CE: 0.1349
[16:16:00.019] Epoch [0/5] Iteration [1120/2976]: Loss: 0.2133, CE: 0.1543
[16:16:04.525] Epoch [0/5] Iteration [1130/2976]: Loss: 0.1932, CE: 0.1024
[16:16:09.063] Epoch [0/5] Iteration [1140/2976]: Loss: 0.1864, CE: 0.0877
[16:16:13.678] Epoch [0/5] Iteration [1150/2976]: Loss: 0.2191, CE: 0.1681
[16:16:18.201] Epoch [0/5] Iteration [1160/2976]: Loss: 0.2003, CE: 0.1218
[16:16:22.719] Epoch [0/5] Iteration [1170/2976]: Loss: 0.1924, CE: 0.1012
[16:16:27.244] Epoch [0/5] Iteration [1180/2976]: Loss: 0.1874, CE: 0.0898
[16:16:31.760] Epoch [0/5] Iteration [1190/2976]: Loss: 0.2168, CE: 0.1631
[16:16:36.282] Epoch [0/5] Iteration [1200/2976]: Loss: 0.2152, CE: 0.1592
[16:16:40.802] Epoch [0/5] Iteration [1210/2976]: Loss: 0.2139, CE: 0.1557
[16:16:45.365] Epoch [0/5] Iteration [1220/2976]: Loss: 0.1448, CE: 0.1078
[16:16:49.889] Epoch [0/5] Iteration [1230/2976]: Loss: 0.2105, CE: 0.1467
[16:16:54.430] Epoch [0/5] Iteration [1240/2976]: Loss: 0.1419, CE: 0.1008
[16:16:58.947] Epoch [0/5] Iteration [1250/2976]: Loss: 0.1962, CE: 0.1112
[16:17:03.492] Epoch [0/5] Iteration [1260/2976]: Loss: 0.1597, CE: 0.1442
[16:17:08.017] Epoch [0/5] Iteration [1270/2976]: Loss: 0.1917, CE: 0.1008
[16:17:12.556] Epoch [0/5] Iteration [1280/2976]: Loss: 0.1819, CE: 0.0755
[16:17:17.069] Epoch [0/5] Iteration [1290/2976]: Loss: 0.2126, CE: 0.1527
[16:17:21.597] Epoch [0/5] Iteration [1300/2976]: Loss: 0.2084, CE: 0.1419
[16:17:26.117] Epoch [0/5] Iteration [1310/2976]: Loss: 0.1991, CE: 0.1190
[16:17:30.650] Epoch [0/5] Iteration [1320/2976]: Loss: 0.1859, CE: 0.0862
[16:17:35.164] Epoch [0/5] Iteration [1330/2976]: Loss: 0.1908, CE: 0.0921
[16:17:39.703] Epoch [0/5] Iteration [1340/2976]: Loss: 0.1974, CE: 0.1148
[16:17:44.229] Epoch [0/5] Iteration [1350/2976]: Loss: 0.2613, CE: 0.2737
[16:17:48.791] Epoch [0/5] Iteration [1360/2976]: Loss: 0.2159, CE: 0.1606
[16:17:53.331] Epoch [0/5] Iteration [1370/2976]: Loss: 0.1944, CE: 0.1062
[16:17:57.874] Epoch [0/5] Iteration [1380/2976]: Loss: 0.1224, CE: 0.0490
[16:18:02.392] Epoch [0/5] Iteration [1390/2976]: Loss: 0.2045, CE: 0.1327
[16:18:06.925] Epoch [0/5] Iteration [1400/2976]: Loss: 0.1449, CE: 0.1066
[16:18:11.451] Epoch [0/5] Iteration [1410/2976]: Loss: 0.1494, CE: 0.1192
[16:18:15.978] Epoch [0/5] Iteration [1420/2976]: Loss: 0.2042, CE: 0.1318
[16:18:20.488] Epoch [0/5] Iteration [1430/2976]: Loss: 0.1949, CE: 0.1076
[16:18:25.003] Epoch [0/5] Iteration [1440/2976]: Loss: 0.2218, CE: 0.1757
[16:18:29.502] Epoch [0/5] Iteration [1450/2976]: Loss: 0.2328, CE: 0.2024
[16:18:34.019] Epoch [0/5] Iteration [1460/2976]: Loss: 0.2060, CE: 0.1361
[16:18:38.534] Epoch [0/5] Iteration [1470/2976]: Loss: 0.2375, CE: 0.2150
[16:18:43.066] Epoch [0/5] Iteration [1480/2976]: Loss: 0.2011, CE: 0.1225
[16:18:47.577] Epoch [0/5] Iteration [1490/2976]: Loss: 0.2070, CE: 0.1386
[16:18:52.090] Epoch [0/5] Iteration [1500/2976]: Loss: 0.2309, CE: 0.1963
[16:18:56.596] Epoch [0/5] Iteration [1510/2976]: Loss: 0.2031, CE: 0.1293
[16:19:01.123] Epoch [0/5] Iteration [1520/2976]: Loss: 0.2009, CE: 0.1194
[16:19:05.632] Epoch [0/5] Iteration [1530/2976]: Loss: 0.1759, CE: 0.1857
[16:19:10.160] Epoch [0/5] Iteration [1540/2976]: Loss: 0.1509, CE: 0.1237
[16:19:14.682] Epoch [0/5] Iteration [1550/2976]: Loss: 0.1488, CE: 0.1120
[16:19:19.227] Epoch [0/5] Iteration [1560/2976]: Loss: 0.1909, CE: 0.0988
[16:19:23.797] Epoch [0/5] Iteration [1570/2976]: Loss: 0.2196, CE: 0.1672
[16:19:28.330] Epoch [0/5] Iteration [1580/2976]: Loss: 0.2433, CE: 0.2294
[16:19:32.876] Epoch [0/5] Iteration [1590/2976]: Loss: 0.2043, CE: 0.1322
[16:19:37.395] Epoch [0/5] Iteration [1600/2976]: Loss: 0.1752, CE: 0.0594
[16:19:41.911] Epoch [0/5] Iteration [1610/2976]: Loss: 0.2091, CE: 0.1406
[16:19:46.440] Epoch [0/5] Iteration [1620/2976]: Loss: 0.1853, CE: 0.0844
[16:19:50.958] Epoch [0/5] Iteration [1630/2976]: Loss: 0.1935, CE: 0.1052
[16:19:55.485] Epoch [0/5] Iteration [1640/2976]: Loss: 0.1497, CE: 0.1165
[16:19:59.993] Epoch [0/5] Iteration [1650/2976]: Loss: 0.1655, CE: 0.1598
[16:20:04.510] Epoch [0/5] Iteration [1660/2976]: Loss: 0.1833, CE: 0.0798
[16:20:09.029] Epoch [0/5] Iteration [1670/2976]: Loss: 0.2654, CE: 0.2839
[16:20:13.564] Epoch [0/5] Iteration [1680/2976]: Loss: 0.1616, CE: 0.1499
[16:20:18.148] Epoch [0/5] Iteration [1690/2976]: Loss: 0.2528, CE: 0.2521
[16:20:22.660] Epoch [0/5] Iteration [1700/2976]: Loss: 0.1571, CE: 0.1389
[16:20:27.170] Epoch [0/5] Iteration [1710/2976]: Loss: 0.2050, CE: 0.1334
[16:20:31.695] Epoch [0/5] Iteration [1720/2976]: Loss: 0.2535, CE: 0.2531
[16:20:36.209] Epoch [0/5] Iteration [1730/2976]: Loss: 0.1853, CE: 0.0850
[16:20:40.747] Epoch [0/5] Iteration [1740/2976]: Loss: 0.2061, CE: 0.1367
[16:20:45.256] Epoch [0/5] Iteration [1750/2976]: Loss: 0.2087, CE: 0.1429
[16:20:49.784] Epoch [0/5] Iteration [1760/2976]: Loss: 0.2028, CE: 0.1284
[16:20:54.286] Epoch [0/5] Iteration [1770/2976]: Loss: 0.2006, CE: 0.1229
[16:20:58.813] Epoch [0/5] Iteration [1780/2976]: Loss: 0.2156, CE: 0.1599
[16:21:03.320] Epoch [0/5] Iteration [1790/2976]: Loss: 0.1978, CE: 0.1159
[16:21:07.858] Epoch [0/5] Iteration [1800/2976]: Loss: 0.1978, CE: 0.1147
[16:21:12.373] Epoch [0/5] Iteration [1810/2976]: Loss: 0.2135, CE: 0.1551
[16:21:16.893] Epoch [0/5] Iteration [1820/2976]: Loss: 0.2010, CE: 0.1237
[16:21:21.400] Epoch [0/5] Iteration [1830/2976]: Loss: 0.2437, CE: 0.2286
[16:21:25.919] Epoch [0/5] Iteration [1840/2976]: Loss: 0.1320, CE: 0.0703
[16:21:30.426] Epoch [0/5] Iteration [1850/2976]: Loss: 0.1962, CE: 0.1099
[16:21:34.950] Epoch [0/5] Iteration [1860/2976]: Loss: 0.1531, CE: 0.1289
[16:21:39.464] Epoch [0/5] Iteration [1870/2976]: Loss: 0.2072, CE: 0.1372
[16:21:43.998] Epoch [0/5] Iteration [1880/2976]: Loss: 0.1983, CE: 0.1164
[16:21:48.504] Epoch [0/5] Iteration [1890/2976]: Loss: 0.1384, CE: 0.0917
[16:21:53.030] Epoch [0/5] Iteration [1900/2976]: Loss: 0.1988, CE: 0.1182
[16:21:57.540] Epoch [0/5] Iteration [1910/2976]: Loss: 0.1418, CE: 0.0986
[16:22:02.062] Epoch [0/5] Iteration [1920/2976]: Loss: 0.2268, CE: 0.1868
[16:22:06.566] Epoch [0/5] Iteration [1930/2976]: Loss: 0.1896, CE: 0.0934
[16:22:11.094] Epoch [0/5] Iteration [1940/2976]: Loss: 0.2053, CE: 0.1337
[16:22:15.625] Epoch [0/5] Iteration [1950/2976]: Loss: 0.1910, CE: 0.0991
[16:22:20.152] Epoch [0/5] Iteration [1960/2976]: Loss: 0.2035, CE: 0.1294
[16:22:24.658] Epoch [0/5] Iteration [1970/2976]: Loss: 0.2151, CE: 0.1576
[16:22:29.180] Epoch [0/5] Iteration [1980/2976]: Loss: 0.1926, CE: 0.1030
[16:22:33.693] Epoch [0/5] Iteration [1990/2976]: Loss: 0.1352, CE: 0.0834
[16:22:38.222] Epoch [0/5] Iteration [2000/2976]: Loss: 0.1873, CE: 0.0895
[16:22:42.738] Epoch [0/5] Iteration [2010/2976]: Loss: 0.1978, CE: 0.1159
[16:22:47.259] Epoch [0/5] Iteration [2020/2976]: Loss: 0.2233, CE: 0.1776
[16:22:51.771] Epoch [0/5] Iteration [2030/2976]: Loss: 0.2224, CE: 0.1772
[16:22:56.322] Epoch [0/5] Iteration [2040/2976]: Loss: 0.1858, CE: 0.0852
[16:23:00.829] Epoch [0/5] Iteration [2050/2976]: Loss: 0.1395, CE: 0.0944
[16:23:05.355] Epoch [0/5] Iteration [2060/2976]: Loss: 0.1961, CE: 0.1118
[16:23:09.865] Epoch [0/5] Iteration [2070/2976]: Loss: 0.1895, CE: 0.0931
[16:23:14.383] Epoch [0/5] Iteration [2080/2976]: Loss: 0.1625, CE: 0.1519
[16:23:18.894] Epoch [0/5] Iteration [2090/2976]: Loss: 0.2023, CE: 0.1273
[16:23:23.425] Epoch [0/5] Iteration [2100/2976]: Loss: 0.2105, CE: 0.1473
[16:23:27.929] Epoch [0/5] Iteration [2110/2976]: Loss: 0.2206, CE: 0.1727
[16:23:32.459] Epoch [0/5] Iteration [2120/2976]: Loss: 0.2109, CE: 0.1485
[16:23:36.976] Epoch [0/5] Iteration [2130/2976]: Loss: 0.1947, CE: 0.1064
[16:23:41.498] Epoch [0/5] Iteration [2140/2976]: Loss: 0.2122, CE: 0.1516
[16:23:46.014] Epoch [0/5] Iteration [2150/2976]: Loss: 0.1841, CE: 0.0822
[16:23:50.546] Epoch [0/5] Iteration [2160/2976]: Loss: 0.1794, CE: 0.0699
[16:23:55.058] Epoch [0/5] Iteration [2170/2976]: Loss: 0.1817, CE: 0.0758
[16:23:59.593] Epoch [0/5] Iteration [2180/2976]: Loss: 0.2074, CE: 0.1392
[16:24:04.095] Epoch [0/5] Iteration [2190/2976]: Loss: 0.1369, CE: 0.0861
[16:24:08.620] Epoch [0/5] Iteration [2200/2976]: Loss: 0.1821, CE: 0.0768
[16:24:13.138] Epoch [0/5] Iteration [2210/2976]: Loss: 0.2019, CE: 0.1240
[16:24:17.655] Epoch [0/5] Iteration [2220/2976]: Loss: 0.1863, CE: 0.0868
[16:24:22.199] Epoch [0/5] Iteration [2230/2976]: Loss: 0.1803, CE: 0.1923
[16:24:26.730] Epoch [0/5] Iteration [2240/2976]: Loss: 0.1768, CE: 0.0639
[16:24:31.260] Epoch [0/5] Iteration [2250/2976]: Loss: 0.2401, CE: 0.2205
[16:24:35.775] Epoch [0/5] Iteration [2260/2976]: Loss: 0.0604, CE: 0.0220
[16:24:40.271] Epoch [0/5] Iteration [2270/2976]: Loss: 0.2061, CE: 0.1363
[16:24:44.778] Epoch [0/5] Iteration [2280/2976]: Loss: 0.1680, CE: 0.1653
[16:24:49.290] Epoch [0/5] Iteration [2290/2976]: Loss: 0.1880, CE: 0.0919
[16:24:53.828] Epoch [0/5] Iteration [2300/2976]: Loss: 0.1699, CE: 0.1711
[16:24:58.378] Epoch [0/5] Iteration [2310/2976]: Loss: 0.2273, CE: 0.1896
[16:25:02.911] Epoch [0/5] Iteration [2320/2976]: Loss: 0.1877, CE: 0.0874
[16:25:07.411] Epoch [0/5] Iteration [2330/2976]: Loss: 0.2134, CE: 0.1546
[16:25:11.944] Epoch [0/5] Iteration [2340/2976]: Loss: 0.2274, CE: 0.1879
[16:25:16.463] Epoch [0/5] Iteration [2350/2976]: Loss: 0.2028, CE: 0.1274
[16:25:20.984] Epoch [0/5] Iteration [2360/2976]: Loss: 0.1984, CE: 0.1162
[16:25:25.503] Epoch [0/5] Iteration [2370/2976]: Loss: 0.1988, CE: 0.1178
[16:25:30.044] Epoch [0/5] Iteration [2380/2976]: Loss: 0.2511, CE: 0.2465
[16:25:34.557] Epoch [0/5] Iteration [2390/2976]: Loss: 0.2585, CE: 0.2668
[16:25:39.087] Epoch [0/5] Iteration [2400/2976]: Loss: 0.2162, CE: 0.1612
[16:25:43.663] Epoch [0/5] Iteration [2410/2976]: Loss: 0.2045, CE: 0.1328
[16:25:48.211] Epoch [0/5] Iteration [2420/2976]: Loss: 0.1883, CE: 0.0908
[16:25:52.721] Epoch [0/5] Iteration [2430/2976]: Loss: 0.1923, CE: 0.0993
[16:25:57.240] Epoch [0/5] Iteration [2440/2976]: Loss: 0.1658, CE: 0.1594
[16:26:01.749] Epoch [0/5] Iteration [2450/2976]: Loss: 0.2253, CE: 0.1838
[16:26:06.257] Epoch [0/5] Iteration [2460/2976]: Loss: 0.2229, CE: 0.1785
[16:26:10.787] Epoch [0/5] Iteration [2470/2976]: Loss: 0.2070, CE: 0.1390
[16:26:15.304] Epoch [0/5] Iteration [2480/2976]: Loss: 0.2143, CE: 0.1551
[16:26:19.814] Epoch [0/5] Iteration [2490/2976]: Loss: 0.2032, CE: 0.1284
[16:26:24.328] Epoch [0/5] Iteration [2500/2976]: Loss: 0.2288, CE: 0.1926
[16:26:28.863] Epoch [0/5] Iteration [2510/2976]: Loss: 0.1904, CE: 0.0975
[16:26:33.422] Epoch [0/5] Iteration [2520/2976]: Loss: 0.1994, CE: 0.1176
[16:26:37.958] Epoch [0/5] Iteration [2530/2976]: Loss: 0.2162, CE: 0.1558
[16:26:42.481] Epoch [0/5] Iteration [2540/2976]: Loss: 0.2452, CE: 0.2331
[16:26:46.999] Epoch [0/5] Iteration [2550/2976]: Loss: 0.1985, CE: 0.1175
[16:26:51.546] Epoch [0/5] Iteration [2560/2976]: Loss: 0.2016, CE: 0.1248
[16:26:56.048] Epoch [0/5] Iteration [2570/2976]: Loss: 0.2175, CE: 0.1646
[16:27:00.586] Epoch [0/5] Iteration [2580/2976]: Loss: 0.1801, CE: 0.0681
[16:27:05.100] Epoch [0/5] Iteration [2590/2976]: Loss: 0.1414, CE: 0.0996
[16:27:09.619] Epoch [0/5] Iteration [2600/2976]: Loss: 0.1445, CE: 0.1077
[16:27:14.120] Epoch [0/5] Iteration [2610/2976]: Loss: 0.2130, CE: 0.1538
[16:27:18.638] Epoch [0/5] Iteration [2620/2976]: Loss: 0.1257, CE: 0.0604
[16:27:23.135] Epoch [0/5] Iteration [2630/2976]: Loss: 0.2096, CE: 0.1454
[16:27:27.652] Epoch [0/5] Iteration [2640/2976]: Loss: 0.1983, CE: 0.1152
[16:27:32.167] Epoch [0/5] Iteration [2650/2976]: Loss: 0.1579, CE: 0.1386
[16:27:36.698] Epoch [0/5] Iteration [2660/2976]: Loss: 0.2090, CE: 0.1436
[16:27:41.198] Epoch [0/5] Iteration [2670/2976]: Loss: 0.2112, CE: 0.1489
[16:27:45.730] Epoch [0/5] Iteration [2680/2976]: Loss: 0.2002, CE: 0.1218
[16:27:50.231] Epoch [0/5] Iteration [2690/2976]: Loss: 0.2181, CE: 0.1666
[16:27:54.750] Epoch [0/5] Iteration [2700/2976]: Loss: 0.1614, CE: 0.1489
[16:27:59.273] Epoch [0/5] Iteration [2710/2976]: Loss: 0.2071, CE: 0.1388
[16:28:03.792] Epoch [0/5] Iteration [2720/2976]: Loss: 0.1886, CE: 0.0876
[16:28:08.301] Epoch [0/5] Iteration [2730/2976]: Loss: 0.1863, CE: 0.0864
[16:28:12.840] Epoch [0/5] Iteration [2740/2976]: Loss: 0.1839, CE: 0.0809
[16:28:17.346] Epoch [0/5] Iteration [2750/2976]: Loss: 0.1943, CE: 0.1070
[16:28:21.866] Epoch [0/5] Iteration [2760/2976]: Loss: 0.1245, CE: 0.0559
[16:28:26.383] Epoch [0/5] Iteration [2770/2976]: Loss: 0.1987, CE: 0.1158
[16:28:30.900] Epoch [0/5] Iteration [2780/2976]: Loss: 0.2176, CE: 0.1638
[16:28:35.418] Epoch [0/5] Iteration [2790/2976]: Loss: 0.2248, CE: 0.1827
[16:28:39.944] Epoch [0/5] Iteration [2800/2976]: Loss: 0.2044, CE: 0.1319
[16:28:44.465] Epoch [0/5] Iteration [2810/2976]: Loss: 0.1441, CE: 0.1053
[16:28:49.000] Epoch [0/5] Iteration [2820/2976]: Loss: 0.2168, CE: 0.1589
[16:28:53.509] Epoch [0/5] Iteration [2830/2976]: Loss: 0.1863, CE: 0.0856
[16:28:58.051] Epoch [0/5] Iteration [2840/2976]: Loss: 0.2126, CE: 0.1528
[16:29:02.570] Epoch [0/5] Iteration [2850/2976]: Loss: 0.2134, CE: 0.1535
[16:29:07.166] Epoch [0/5] Iteration [2860/2976]: Loss: 0.1911, CE: 0.0990
[16:29:11.680] Epoch [0/5] Iteration [2870/2976]: Loss: 0.1929, CE: 0.1025
[16:29:16.201] Epoch [0/5] Iteration [2880/2976]: Loss: 0.1938, CE: 0.1062
[16:29:20.723] Epoch [0/5] Iteration [2890/2976]: Loss: 0.1185, CE: 0.0407
[16:29:25.257] Epoch [0/5] Iteration [2900/2976]: Loss: 0.2235, CE: 0.1801
[16:29:29.778] Epoch [0/5] Iteration [2910/2976]: Loss: 0.1474, CE: 0.1132
[16:29:34.291] Epoch [0/5] Iteration [2920/2976]: Loss: 0.2202, CE: 0.1717
[16:29:38.797] Epoch [0/5] Iteration [2930/2976]: Loss: 0.1849, CE: 0.0836
[16:29:43.348] Epoch [0/5] Iteration [2940/2976]: Loss: 0.1636, CE: 0.1549
[16:29:47.880] Epoch [0/5] Iteration [2950/2976]: Loss: 0.2141, CE: 0.1565
[16:29:52.407] Epoch [0/5] Iteration [2960/2976]: Loss: 0.2144, CE: 0.1557
[16:29:56.907] Epoch [0/5] Iteration [2970/2976]: Loss: 0.1632, CE: 0.1537
[16:29:59.772] Epoch [0/5] Average Loss: 0.1986, CE: 0.1313, Dice: 0.2435
[16:30:21.332] Epoch [1/5] Iteration [0/2976]: Loss: 0.1657, CE: 0.1599
[16:30:25.821] Epoch [1/5] Iteration [10/2976]: Loss: 0.2351, CE: 0.2092
[16:30:30.300] Epoch [1/5] Iteration [20/2976]: Loss: 0.2239, CE: 0.1803
[16:30:34.782] Epoch [1/5] Iteration [30/2976]: Loss: 0.2213, CE: 0.1742
[16:30:39.260] Epoch [1/5] Iteration [40/2976]: Loss: 0.2373, CE: 0.2116
[16:30:43.765] Epoch [1/5] Iteration [50/2976]: Loss: 0.1910, CE: 0.0987
[16:30:48.237] Epoch [1/5] Iteration [60/2976]: Loss: 0.2029, CE: 0.1272
[16:30:52.722] Epoch [1/5] Iteration [70/2976]: Loss: 0.1341, CE: 0.0815
[16:30:57.216] Epoch [1/5] Iteration [80/2976]: Loss: 0.2218, CE: 0.1748
[16:31:01.744] Epoch [1/5] Iteration [90/2976]: Loss: 0.1481, CE: 0.1159
[16:31:06.247] Epoch [1/5] Iteration [100/2976]: Loss: 0.1997, CE: 0.1208
[16:31:10.740] Epoch [1/5] Iteration [110/2976]: Loss: 0.1928, CE: 0.1013
[16:31:15.240] Epoch [1/5] Iteration [120/2976]: Loss: 0.2231, CE: 0.1743
[16:31:19.755] Epoch [1/5] Iteration [130/2976]: Loss: 0.2334, CE: 0.2044
[16:31:24.261] Epoch [1/5] Iteration [140/2976]: Loss: 0.1736, CE: 0.1795
[16:31:28.795] Epoch [1/5] Iteration [150/2976]: Loss: 0.1289, CE: 0.0680
[16:31:33.322] Epoch [1/5] Iteration [160/2976]: Loss: 0.1954, CE: 0.1042
[16:31:37.838] Epoch [1/5] Iteration [170/2976]: Loss: 0.1357, CE: 0.0858
[16:31:42.340] Epoch [1/5] Iteration [180/2976]: Loss: 0.1877, CE: 0.0903
[16:31:46.836] Epoch [1/5] Iteration [190/2976]: Loss: 0.2026, CE: 0.1277
[16:31:51.359] Epoch [1/5] Iteration [200/2976]: Loss: 0.1831, CE: 0.0797
[16:31:55.879] Epoch [1/5] Iteration [210/2976]: Loss: 0.2442, CE: 0.2309
[16:32:00.389] Epoch [1/5] Iteration [220/2976]: Loss: 0.1462, CE: 0.1119
[16:32:04.919] Epoch [1/5] Iteration [230/2976]: Loss: 0.2004, CE: 0.1222
[16:32:09.422] Epoch [1/5] Iteration [240/2976]: Loss: 0.2028, CE: 0.1251
[16:32:13.927] Epoch [1/5] Iteration [250/2976]: Loss: 0.1989, CE: 0.1159
[16:32:18.427] Epoch [1/5] Iteration [260/2976]: Loss: 0.2030, CE: 0.1289
[16:32:22.979] Epoch [1/5] Iteration [270/2976]: Loss: 0.2070, CE: 0.1385
[16:32:27.484] Epoch [1/5] Iteration [280/2976]: Loss: 0.1786, CE: 0.1928
[16:32:31.995] Epoch [1/5] Iteration [290/2976]: Loss: 0.2423, CE: 0.2259
[16:32:36.502] Epoch [1/5] Iteration [300/2976]: Loss: 0.2086, CE: 0.1418
[16:32:41.021] Epoch [1/5] Iteration [310/2976]: Loss: 0.1862, CE: 0.0869
[16:32:45.524] Epoch [1/5] Iteration [320/2976]: Loss: 0.2111, CE: 0.1485
[16:32:50.065] Epoch [1/5] Iteration [330/2976]: Loss: 0.2016, CE: 0.1250
[16:32:54.593] Epoch [1/5] Iteration [340/2976]: Loss: 0.2037, CE: 0.1301
[16:32:59.119] Epoch [1/5] Iteration [350/2976]: Loss: 0.1815, CE: 0.1989
[16:33:03.639] Epoch [1/5] Iteration [360/2976]: Loss: 0.2074, CE: 0.1386
[16:33:08.150] Epoch [1/5] Iteration [370/2976]: Loss: 0.2090, CE: 0.1432
[16:33:12.651] Epoch [1/5] Iteration [380/2976]: Loss: 0.2061, CE: 0.1360
[16:33:17.189] Epoch [1/5] Iteration [390/2976]: Loss: 0.2149, CE: 0.1586
[16:33:21.700] Epoch [1/5] Iteration [400/2976]: Loss: 0.2249, CE: 0.1831
[16:33:26.230] Epoch [1/5] Iteration [410/2976]: Loss: 0.2083, CE: 0.1358
[16:33:30.742] Epoch [1/5] Iteration [420/2976]: Loss: 0.2601, CE: 0.2699
[16:33:35.316] Epoch [1/5] Iteration [430/2976]: Loss: 0.1771, CE: 0.0617
[16:33:39.819] Epoch [1/5] Iteration [440/2976]: Loss: 0.2368, CE: 0.2118
[16:33:44.380] Epoch [1/5] Iteration [450/2976]: Loss: 0.2219, CE: 0.1750
[16:33:48.899] Epoch [1/5] Iteration [460/2976]: Loss: 0.2017, CE: 0.1253
[16:33:53.452] Epoch [1/5] Iteration [470/2976]: Loss: 0.2049, CE: 0.1332
[16:33:57.960] Epoch [1/5] Iteration [480/2976]: Loss: 0.1577, CE: 0.1364
[16:34:02.478] Epoch [1/5] Iteration [490/2976]: Loss: 0.1957, CE: 0.1108
[16:34:06.999] Epoch [1/5] Iteration [500/2976]: Loss: 0.1394, CE: 0.0947
[16:34:11.525] Epoch [1/5] Iteration [510/2976]: Loss: 0.1331, CE: 0.0790
[16:34:16.049] Epoch [1/5] Iteration [520/2976]: Loss: 0.2155, CE: 0.1600
[16:34:20.590] Epoch [1/5] Iteration [530/2976]: Loss: 0.1095, CE: 0.0192
[16:34:25.124] Epoch [1/5] Iteration [540/2976]: Loss: 0.1781, CE: 0.0660
[16:34:29.653] Epoch [1/5] Iteration [550/2976]: Loss: 0.2119, CE: 0.1514
[16:34:34.158] Epoch [1/5] Iteration [560/2976]: Loss: 0.1963, CE: 0.1105
[16:34:38.686] Epoch [1/5] Iteration [570/2976]: Loss: 0.2020, CE: 0.1247
[16:34:43.180] Epoch [1/5] Iteration [580/2976]: Loss: 0.1894, CE: 0.0940
[16:34:47.691] Epoch [1/5] Iteration [590/2976]: Loss: 0.2176, CE: 0.1634
[16:34:52.201] Epoch [1/5] Iteration [600/2976]: Loss: 0.2081, CE: 0.1395
[16:34:56.729] Epoch [1/5] Iteration [610/2976]: Loss: 0.1547, CE: 0.1243
[16:35:01.242] Epoch [1/5] Iteration [620/2976]: Loss: 0.2189, CE: 0.1686
[16:35:05.782] Epoch [1/5] Iteration [630/2976]: Loss: 0.1983, CE: 0.1143
[16:35:10.334] Epoch [1/5] Iteration [640/2976]: Loss: 0.1549, CE: 0.1336
[16:35:14.876] Epoch [1/5] Iteration [650/2976]: Loss: 0.1822, CE: 0.0772
[16:35:19.390] Epoch [1/5] Iteration [660/2976]: Loss: 0.2077, CE: 0.1390
[16:35:23.920] Epoch [1/5] Iteration [670/2976]: Loss: 0.2130, CE: 0.1538
[16:35:28.448] Epoch [1/5] Iteration [680/2976]: Loss: 0.1328, CE: 0.0782
[16:35:32.973] Epoch [1/5] Iteration [690/2976]: Loss: 0.2052, CE: 0.1317
[16:35:37.486] Epoch [1/5] Iteration [700/2976]: Loss: 0.2095, CE: 0.1447
[16:35:42.006] Epoch [1/5] Iteration [710/2976]: Loss: 0.2113, CE: 0.1483
[16:35:46.506] Epoch [1/5] Iteration [720/2976]: Loss: 0.2006, CE: 0.1223
[16:35:51.038] Epoch [1/5] Iteration [730/2976]: Loss: 0.2145, CE: 0.1576
[16:35:55.557] Epoch [1/5] Iteration [740/2976]: Loss: 0.1677, CE: 0.1637
[16:36:00.118] Epoch [1/5] Iteration [750/2976]: Loss: 0.2137, CE: 0.1543
[16:36:04.639] Epoch [1/5] Iteration [760/2976]: Loss: 0.1811, CE: 0.0743
[16:36:09.172] Epoch [1/5] Iteration [770/2976]: Loss: 0.1764, CE: 0.0626
[16:36:13.691] Epoch [1/5] Iteration [780/2976]: Loss: 0.0738, CE: 0.0548
[16:36:18.246] Epoch [1/5] Iteration [790/2976]: Loss: 0.2172, CE: 0.1642
[16:36:22.757] Epoch [1/5] Iteration [800/2976]: Loss: 0.1814, CE: 0.0695
[16:36:27.281] Epoch [1/5] Iteration [810/2976]: Loss: 0.1685, CE: 0.1673
[16:36:31.794] Epoch [1/5] Iteration [820/2976]: Loss: 0.2260, CE: 0.1863
[16:36:36.325] Epoch [1/5] Iteration [830/2976]: Loss: 0.2123, CE: 0.1513
[16:36:40.910] Epoch [1/5] Iteration [840/2976]: Loss: 0.1993, CE: 0.1184
[16:36:45.427] Epoch [1/5] Iteration [850/2976]: Loss: 0.1503, CE: 0.1216
[16:36:49.948] Epoch [1/5] Iteration [860/2976]: Loss: 0.1992, CE: 0.1179
[16:36:54.474] Epoch [1/5] Iteration [870/2976]: Loss: 0.2202, CE: 0.1713
[16:36:59.000] Epoch [1/5] Iteration [880/2976]: Loss: 0.1190, CE: 0.0442
[16:37:03.517] Epoch [1/5] Iteration [890/2976]: Loss: 0.1469, CE: 0.1135
[16:37:08.063] Epoch [1/5] Iteration [900/2976]: Loss: 0.1359, CE: 0.0862
[16:37:12.606] Epoch [1/5] Iteration [910/2976]: Loss: 0.1858, CE: 0.0855
[16:37:17.130] Epoch [1/5] Iteration [920/2976]: Loss: 0.2325, CE: 0.2000
[16:37:21.652] Epoch [1/5] Iteration [930/2976]: Loss: 0.1946, CE: 0.1077
[16:37:26.174] Epoch [1/5] Iteration [940/2976]: Loss: 0.2231, CE: 0.1785
[16:37:30.711] Epoch [1/5] Iteration [950/2976]: Loss: 0.1612, CE: 0.1493
[16:37:35.222] Epoch [1/5] Iteration [960/2976]: Loss: 0.1953, CE: 0.1092
[16:37:39.759] Epoch [1/5] Iteration [970/2976]: Loss: 0.1927, CE: 0.1031
[16:37:44.295] Epoch [1/5] Iteration [980/2976]: Loss: 0.1358, CE: 0.0842
[16:37:48.820] Epoch [1/5] Iteration [990/2976]: Loss: 0.1939, CE: 0.1054
[16:37:53.366] Epoch [1/5] Iteration [1000/2976]: Loss: 0.1987, CE: 0.1184
[16:37:57.925] Epoch [1/5] Iteration [1010/2976]: Loss: 0.2380, CE: 0.2162
[16:38:02.463] Epoch [1/5] Iteration [1020/2976]: Loss: 0.1959, CE: 0.1109
[16:38:06.997] Epoch [1/5] Iteration [1030/2976]: Loss: 0.1910, CE: 0.0987
[16:38:11.506] Epoch [1/5] Iteration [1040/2976]: Loss: 0.1831, CE: 0.0796
[16:38:16.095] Epoch [1/5] Iteration [1050/2976]: Loss: 0.2019, CE: 0.1236
[16:38:20.620] Epoch [1/5] Iteration [1060/2976]: Loss: 0.2330, CE: 0.2035
[16:38:25.148] Epoch [1/5] Iteration [1070/2976]: Loss: 0.2067, CE: 0.1378
[16:38:29.666] Epoch [1/5] Iteration [1080/2976]: Loss: 0.2028, CE: 0.1282
[16:38:34.232] Epoch [1/5] Iteration [1090/2976]: Loss: 0.2054, CE: 0.1347
[16:38:38.746] Epoch [1/5] Iteration [1100/2976]: Loss: 0.2268, CE: 0.1882
[16:38:43.286] Epoch [1/5] Iteration [1110/2976]: Loss: 0.1487, CE: 0.1169
[16:38:47.791] Epoch [1/5] Iteration [1120/2976]: Loss: 0.2018, CE: 0.1238
[16:38:52.341] Epoch [1/5] Iteration [1130/2976]: Loss: 0.1964, CE: 0.1121
[16:38:56.842] Epoch [1/5] Iteration [1140/2976]: Loss: 0.1321, CE: 0.0771
[16:39:01.396] Epoch [1/5] Iteration [1150/2976]: Loss: 0.2035, CE: 0.1297
[16:39:05.928] Epoch [1/5] Iteration [1160/2976]: Loss: 0.1378, CE: 0.0906
[16:39:10.487] Epoch [1/5] Iteration [1170/2976]: Loss: 0.1865, CE: 0.0867
[16:39:15.022] Epoch [1/5] Iteration [1180/2976]: Loss: 0.2134, CE: 0.1543
[16:39:19.572] Epoch [1/5] Iteration [1190/2976]: Loss: 0.2196, CE: 0.1691
[16:39:24.101] Epoch [1/5] Iteration [1200/2976]: Loss: 0.1905, CE: 0.0975
[16:39:28.631] Epoch [1/5] Iteration [1210/2976]: Loss: 0.1997, CE: 0.1203
[16:39:33.173] Epoch [1/5] Iteration [1220/2976]: Loss: 0.1928, CE: 0.1020
[16:39:37.692] Epoch [1/5] Iteration [1230/2976]: Loss: 0.2189, CE: 0.1688
[16:39:42.226] Epoch [1/5] Iteration [1240/2976]: Loss: 0.1792, CE: 0.0697
[16:39:46.748] Epoch [1/5] Iteration [1250/2976]: Loss: 0.1319, CE: 0.0761
[16:39:51.259] Epoch [1/5] Iteration [1260/2976]: Loss: 0.2137, CE: 0.1535
[16:39:55.790] Epoch [1/5] Iteration [1270/2976]: Loss: 0.2225, CE: 0.1770
[16:40:00.289] Epoch [1/5] Iteration [1280/2976]: Loss: 0.2123, CE: 0.1517
[16:40:04.821] Epoch [1/5] Iteration [1290/2976]: Loss: 0.2010, CE: 0.1238
[16:40:09.345] Epoch [1/5] Iteration [1300/2976]: Loss: 0.1820, CE: 0.0744
[16:40:13.878] Epoch [1/5] Iteration [1310/2976]: Loss: 0.1680, CE: 0.1657
[16:40:18.410] Epoch [1/5] Iteration [1320/2976]: Loss: 0.1404, CE: 0.0964
[16:40:22.956] Epoch [1/5] Iteration [1330/2976]: Loss: 0.2055, CE: 0.1350
[16:40:27.464] Epoch [1/5] Iteration [1340/2976]: Loss: 0.1798, CE: 0.0705
[16:40:31.992] Epoch [1/5] Iteration [1350/2976]: Loss: 0.1778, CE: 0.0655
[16:40:36.496] Epoch [1/5] Iteration [1360/2976]: Loss: 0.1661, CE: 0.1618
[16:40:41.049] Epoch [1/5] Iteration [1370/2976]: Loss: 0.2147, CE: 0.1561
[16:40:45.586] Epoch [1/5] Iteration [1380/2976]: Loss: 0.1989, CE: 0.2429
[16:40:50.106] Epoch [1/5] Iteration [1390/2976]: Loss: 0.1925, CE: 0.1022
[16:40:54.628] Epoch [1/5] Iteration [1400/2976]: Loss: 0.1887, CE: 0.0932
[16:40:59.159] Epoch [1/5] Iteration [1410/2976]: Loss: 0.2117, CE: 0.1505
[16:41:03.697] Epoch [1/5] Iteration [1420/2976]: Loss: 0.1748, CE: 0.0586
[16:41:08.244] Epoch [1/5] Iteration [1430/2976]: Loss: 0.2212, CE: 0.1717
[16:41:12.767] Epoch [1/5] Iteration [1440/2976]: Loss: 0.1487, CE: 0.1171
[16:41:17.288] Epoch [1/5] Iteration [1450/2976]: Loss: 0.1533, CE: 0.1253
[16:41:21.797] Epoch [1/5] Iteration [1460/2976]: Loss: 0.1775, CE: 0.0652
[16:41:26.370] Epoch [1/5] Iteration [1470/2976]: Loss: 0.2189, CE: 0.1634
[16:41:30.907] Epoch [1/5] Iteration [1480/2976]: Loss: 0.1950, CE: 0.1090
[16:41:35.431] Epoch [1/5] Iteration [1490/2976]: Loss: 0.1877, CE: 0.0909
[16:41:39.940] Epoch [1/5] Iteration [1500/2976]: Loss: 0.2267, CE: 0.1879
[16:41:44.465] Epoch [1/5] Iteration [1510/2976]: Loss: 0.1967, CE: 0.1097
[16:41:48.990] Epoch [1/5] Iteration [1520/2976]: Loss: 0.1845, CE: 0.0817
[16:41:53.521] Epoch [1/5] Iteration [1530/2976]: Loss: 0.1683, CE: 0.1666
[16:41:58.035] Epoch [1/5] Iteration [1540/2976]: Loss: 0.2071, CE: 0.1366
[16:42:02.581] Epoch [1/5] Iteration [1550/2976]: Loss: 0.1432, CE: 0.1020
[16:42:07.105] Epoch [1/5] Iteration [1560/2976]: Loss: 0.2065, CE: 0.1341
[16:42:11.635] Epoch [1/5] Iteration [1570/2976]: Loss: 0.2018, CE: 0.1255
[16:42:16.173] Epoch [1/5] Iteration [1580/2976]: Loss: 0.2345, CE: 0.2035
[16:42:20.711] Epoch [1/5] Iteration [1590/2976]: Loss: 0.1698, CE: 0.1695
[16:42:25.227] Epoch [1/5] Iteration [1600/2976]: Loss: 0.2182, CE: 0.1620
[16:42:29.760] Epoch [1/5] Iteration [1610/2976]: Loss: 0.2130, CE: 0.1537
[16:42:34.269] Epoch [1/5] Iteration [1620/2976]: Loss: 0.2388, CE: 0.2177
[16:42:38.805] Epoch [1/5] Iteration [1630/2976]: Loss: 0.1894, CE: 0.0946
[16:42:43.356] Epoch [1/5] Iteration [1640/2976]: Loss: 0.1844, CE: 0.0824
[16:42:47.910] Epoch [1/5] Iteration [1650/2976]: Loss: 0.1876, CE: 0.0905
[16:42:52.457] Epoch [1/5] Iteration [1660/2976]: Loss: 0.2145, CE: 0.1567
[16:42:56.993] Epoch [1/5] Iteration [1670/2976]: Loss: 0.2161, CE: 0.1605
[16:43:01.510] Epoch [1/5] Iteration [1680/2976]: Loss: 0.1979, CE: 0.1163
[16:43:06.055] Epoch [1/5] Iteration [1690/2976]: Loss: 0.2118, CE: 0.1508
[16:43:10.579] Epoch [1/5] Iteration [1700/2976]: Loss: 0.1363, CE: 0.0875
[16:43:15.096] Epoch [1/5] Iteration [1710/2976]: Loss: 0.2297, CE: 0.1934
[16:43:19.606] Epoch [1/5] Iteration [1720/2976]: Loss: 0.1872, CE: 0.0895
[16:43:24.132] Epoch [1/5] Iteration [1730/2976]: Loss: 0.1793, CE: 0.0697
[16:43:28.656] Epoch [1/5] Iteration [1740/2976]: Loss: 0.2370, CE: 0.2137
[16:43:33.188] Epoch [1/5] Iteration [1750/2976]: Loss: 0.1824, CE: 0.0774
[16:43:37.713] Epoch [1/5] Iteration [1760/2976]: Loss: 0.2155, CE: 0.1544
[16:43:42.255] Epoch [1/5] Iteration [1770/2976]: Loss: 0.2199, CE: 0.1710
[16:43:46.763] Epoch [1/5] Iteration [1780/2976]: Loss: 0.2205, CE: 0.1692
[16:43:51.303] Epoch [1/5] Iteration [1790/2976]: Loss: 0.1319, CE: 0.0763
[16:43:55.823] Epoch [1/5] Iteration [1800/2976]: Loss: 0.1894, CE: 0.0947
[16:44:00.365] Epoch [1/5] Iteration [1810/2976]: Loss: 0.2053, CE: 0.1344
[16:44:04.905] Epoch [1/5] Iteration [1820/2976]: Loss: 0.2406, CE: 0.2225
[16:44:09.451] Epoch [1/5] Iteration [1830/2976]: Loss: 0.1967, CE: 0.1107
[16:44:14.003] Epoch [1/5] Iteration [1840/2976]: Loss: 0.1728, CE: 0.0523
[16:44:18.547] Epoch [1/5] Iteration [1850/2976]: Loss: 0.1855, CE: 0.0825
[16:44:23.082] Epoch [1/5] Iteration [1860/2976]: Loss: 0.2213, CE: 0.1742
[16:44:27.602] Epoch [1/5] Iteration [1870/2976]: Loss: 0.1464, CE: 0.1121
[16:44:32.129] Epoch [1/5] Iteration [1880/2976]: Loss: 0.2058, CE: 0.1355
[16:44:36.668] Epoch [1/5] Iteration [1890/2976]: Loss: 0.1843, CE: 0.0811
[16:44:41.177] Epoch [1/5] Iteration [1900/2976]: Loss: 0.2122, CE: 0.1519
[16:44:45.707] Epoch [1/5] Iteration [1910/2976]: Loss: 0.1232, CE: 0.0545
[16:44:50.221] Epoch [1/5] Iteration [1920/2976]: Loss: 0.2360, CE: 0.2103
[16:44:54.757] Epoch [1/5] Iteration [1930/2976]: Loss: 0.2251, CE: 0.1836
[16:44:59.260] Epoch [1/5] Iteration [1940/2976]: Loss: 0.1985, CE: 0.1173
[16:45:03.786] Epoch [1/5] Iteration [1950/2976]: Loss: 0.2111, CE: 0.1485
[16:45:08.303] Epoch [1/5] Iteration [1960/2976]: Loss: 0.2122, CE: 0.1512
[16:45:12.834] Epoch [1/5] Iteration [1970/2976]: Loss: 0.2112, CE: 0.1495
[16:45:17.345] Epoch [1/5] Iteration [1980/2976]: Loss: 0.2256, CE: 0.1848
[16:45:21.897] Epoch [1/5] Iteration [1990/2976]: Loss: 0.1992, CE: 0.1196
[16:45:26.435] Epoch [1/5] Iteration [2000/2976]: Loss: 0.1988, CE: 0.1181
[16:45:30.966] Epoch [1/5] Iteration [2010/2976]: Loss: 0.2144, CE: 0.1572
[16:45:35.476] Epoch [1/5] Iteration [2020/2976]: Loss: 0.1979, CE: 0.1164
[16:45:40.006] Epoch [1/5] Iteration [2030/2976]: Loss: 0.1710, CE: 0.0475
[16:45:44.524] Epoch [1/5] Iteration [2040/2976]: Loss: 0.1893, CE: 0.0944
[16:45:49.062] Epoch [1/5] Iteration [2050/2976]: Loss: 0.1688, CE: 0.0423
[16:45:53.566] Epoch [1/5] Iteration [2060/2976]: Loss: 0.2040, CE: 0.1313
[16:45:58.105] Epoch [1/5] Iteration [2070/2976]: Loss: 0.2044, CE: 0.1325
[16:46:02.624] Epoch [1/5] Iteration [2080/2976]: Loss: 0.1903, CE: 0.0951
[16:46:07.179] Epoch [1/5] Iteration [2090/2976]: Loss: 0.1365, CE: 0.0880
[16:46:11.721] Epoch [1/5] Iteration [2100/2976]: Loss: 0.2084, CE: 0.1424
[16:46:16.242] Epoch [1/5] Iteration [2110/2976]: Loss: 0.1865, CE: 0.0880
[16:46:20.769] Epoch [1/5] Iteration [2120/2976]: Loss: 0.2087, CE: 0.1424
[16:46:25.324] Epoch [1/5] Iteration [2130/2976]: Loss: 0.1496, CE: 0.1200
[16:46:29.833] Epoch [1/5] Iteration [2140/2976]: Loss: 0.2053, CE: 0.1340
[16:46:34.352] Epoch [1/5] Iteration [2150/2976]: Loss: 0.1809, CE: 0.0737
[16:46:38.886] Epoch [1/5] Iteration [2160/2976]: Loss: 0.2133, CE: 0.1535
[16:46:43.424] Epoch [1/5] Iteration [2170/2976]: Loss: 0.2300, CE: 0.1959
[16:46:47.971] Epoch [1/5] Iteration [2180/2976]: Loss: 0.1942, CE: 0.1064
[16:46:52.496] Epoch [1/5] Iteration [2190/2976]: Loss: 0.1954, CE: 0.1101
[16:46:57.018] Epoch [1/5] Iteration [2200/2976]: Loss: 0.2043, CE: 0.1285
[16:47:01.622] Epoch [1/5] Iteration [2210/2976]: Loss: 0.1966, CE: 0.1128
[16:47:06.125] Epoch [1/5] Iteration [2220/2976]: Loss: 0.1923, CE: 0.1024
[16:47:10.670] Epoch [1/5] Iteration [2230/2976]: Loss: 0.2090, CE: 0.1434
[16:47:15.175] Epoch [1/5] Iteration [2240/2976]: Loss: 0.2135, CE: 0.1549
[16:47:19.760] Epoch [1/5] Iteration [2250/2976]: Loss: 0.2064, CE: 0.1371
[16:47:24.319] Epoch [1/5] Iteration [2260/2976]: Loss: 0.2077, CE: 0.1395
[16:47:28.853] Epoch [1/5] Iteration [2270/2976]: Loss: 0.2070, CE: 0.1338
[16:47:33.383] Epoch [1/5] Iteration [2280/2976]: Loss: 0.2467, CE: 0.2374
[16:47:37.918] Epoch [1/5] Iteration [2290/2976]: Loss: 0.1355, CE: 0.0851
[16:47:42.441] Epoch [1/5] Iteration [2300/2976]: Loss: 0.1914, CE: 0.0994
[16:47:46.980] Epoch [1/5] Iteration [2310/2976]: Loss: 0.2516, CE: 0.2500
[16:47:51.505] Epoch [1/5] Iteration [2320/2976]: Loss: 0.2921, CE: 0.3492
[16:47:56.039] Epoch [1/5] Iteration [2330/2976]: Loss: 0.2269, CE: 0.1867
[16:48:00.541] Epoch [1/5] Iteration [2340/2976]: Loss: 0.2004, CE: 0.1222
[16:48:05.066] Epoch [1/5] Iteration [2350/2976]: Loss: 0.2181, CE: 0.1659
[16:48:09.599] Epoch [1/5] Iteration [2360/2976]: Loss: 0.1764, CE: 0.0626
[16:48:14.134] Epoch [1/5] Iteration [2370/2976]: Loss: 0.1797, CE: 0.0710
[16:48:18.635] Epoch [1/5] Iteration [2380/2976]: Loss: 0.2195, CE: 0.1673
[16:48:23.179] Epoch [1/5] Iteration [2390/2976]: Loss: 0.2010, CE: 0.1215
[16:48:27.707] Epoch [1/5] Iteration [2400/2976]: Loss: 0.1362, CE: 0.0870
[16:48:32.263] Epoch [1/5] Iteration [2410/2976]: Loss: 0.1833, CE: 0.0796
[16:48:36.768] Epoch [1/5] Iteration [2420/2976]: Loss: 0.1634, CE: 0.1541
[16:48:41.306] Epoch [1/5] Iteration [2430/2976]: Loss: 0.1547, CE: 0.1333
[16:48:45.831] Epoch [1/5] Iteration [2440/2976]: Loss: 0.1888, CE: 0.0936
[16:48:50.375] Epoch [1/5] Iteration [2450/2976]: Loss: 0.2068, CE: 0.1384
[16:48:54.899] Epoch [1/5] Iteration [2460/2976]: Loss: 0.1371, CE: 0.0891
[16:48:59.416] Epoch [1/5] Iteration [2470/2976]: Loss: 0.2390, CE: 0.2183
[16:49:03.924] Epoch [1/5] Iteration [2480/2976]: Loss: 0.2544, CE: 0.2539
[16:49:08.463] Epoch [1/5] Iteration [2490/2976]: Loss: 0.1878, CE: 0.0910
[16:49:12.976] Epoch [1/5] Iteration [2500/2976]: Loss: 0.2190, CE: 0.1680
[16:49:17.494] Epoch [1/5] Iteration [2510/2976]: Loss: 0.1943, CE: 0.1071
[16:49:22.033] Epoch [1/5] Iteration [2520/2976]: Loss: 0.1801, CE: 0.1907
[16:49:26.577] Epoch [1/5] Iteration [2530/2976]: Loss: 0.2135, CE: 0.1527
[16:49:31.101] Epoch [1/5] Iteration [2540/2976]: Loss: 0.1351, CE: 0.0812
[16:49:35.644] Epoch [1/5] Iteration [2550/2976]: Loss: 0.1848, CE: 0.0833
[16:49:40.164] Epoch [1/5] Iteration [2560/2976]: Loss: 0.1549, CE: 0.1336
[16:49:44.717] Epoch [1/5] Iteration [2570/2976]: Loss: 0.2200, CE: 0.1709
[16:49:49.251] Epoch [1/5] Iteration [2580/2976]: Loss: 0.2256, CE: 0.1842
[16:49:53.817] Epoch [1/5] Iteration [2590/2976]: Loss: 0.2345, CE: 0.2050
[16:49:58.353] Epoch [1/5] Iteration [2600/2976]: Loss: 0.2050, CE: 0.1327
[16:50:02.888] Epoch [1/5] Iteration [2610/2976]: Loss: 0.1922, CE: 0.1011
[16:50:07.393] Epoch [1/5] Iteration [2620/2976]: Loss: 0.1783, CE: 0.1911
[16:50:11.910] Epoch [1/5] Iteration [2630/2976]: Loss: 0.1623, CE: 0.1483
[16:50:16.460] Epoch [1/5] Iteration [2640/2976]: Loss: 0.2130, CE: 0.1536
[16:50:20.984] Epoch [1/5] Iteration [2650/2976]: Loss: 0.2060, CE: 0.1361
[16:50:25.509] Epoch [1/5] Iteration [2660/2976]: Loss: 0.1561, CE: 0.1360
[16:50:30.086] Epoch [1/5] Iteration [2670/2976]: Loss: 0.2425, CE: 0.2274
[16:50:34.602] Epoch [1/5] Iteration [2680/2976]: Loss: 0.1859, CE: 0.0860
[16:50:39.125] Epoch [1/5] Iteration [2690/2976]: Loss: 0.1985, CE: 0.1172
[16:50:43.661] Epoch [1/5] Iteration [2700/2976]: Loss: 0.1333, CE: 0.0801
[16:50:48.216] Epoch [1/5] Iteration [2710/2976]: Loss: 0.2089, CE: 0.1425
[16:50:52.727] Epoch [1/5] Iteration [2720/2976]: Loss: 0.2179, CE: 0.1655
[16:50:57.276] Epoch [1/5] Iteration [2730/2976]: Loss: 0.1856, CE: 0.0846
[16:51:01.783] Epoch [1/5] Iteration [2740/2976]: Loss: 0.2095, CE: 0.1441
[16:51:06.318] Epoch [1/5] Iteration [2750/2976]: Loss: 0.1976, CE: 0.1139
[16:51:10.840] Epoch [1/5] Iteration [2760/2976]: Loss: 0.2046, CE: 0.1330
[16:51:15.366] Epoch [1/5] Iteration [2770/2976]: Loss: 0.1635, CE: 0.1552
[16:51:19.864] Epoch [1/5] Iteration [2780/2976]: Loss: 0.1571, CE: 0.1375
[16:51:24.411] Epoch [1/5] Iteration [2790/2976]: Loss: 0.1598, CE: 0.1457
[16:51:28.917] Epoch [1/5] Iteration [2800/2976]: Loss: 0.2151, CE: 0.1570
[16:51:33.474] Epoch [1/5] Iteration [2810/2976]: Loss: 0.1200, CE: 0.0462
[16:51:37.982] Epoch [1/5] Iteration [2820/2976]: Loss: 0.2154, CE: 0.1596
[16:51:42.527] Epoch [1/5] Iteration [2830/2976]: Loss: 0.1423, CE: 0.1024
[16:51:47.069] Epoch [1/5] Iteration [2840/2976]: Loss: 0.1434, CE: 0.1041
[16:51:51.622] Epoch [1/5] Iteration [2850/2976]: Loss: 0.1906, CE: 0.0939
[16:51:56.165] Epoch [1/5] Iteration [2860/2976]: Loss: 0.1919, CE: 0.1010
[16:52:00.708] Epoch [1/5] Iteration [2870/2976]: Loss: 0.2145, CE: 0.1559
[16:52:05.228] Epoch [1/5] Iteration [2880/2976]: Loss: 0.1852, CE: 0.0841
[16:52:09.763] Epoch [1/5] Iteration [2890/2976]: Loss: 0.2059, CE: 0.1318
[16:52:14.284] Epoch [1/5] Iteration [2900/2976]: Loss: 0.1994, CE: 0.1199
[16:52:18.804] Epoch [1/5] Iteration [2910/2976]: Loss: 0.1974, CE: 0.1146
[16:52:23.313] Epoch [1/5] Iteration [2920/2976]: Loss: 0.2391, CE: 0.2170
[16:52:27.849] Epoch [1/5] Iteration [2930/2976]: Loss: 0.1876, CE: 0.0857
[16:52:32.380] Epoch [1/5] Iteration [2940/2976]: Loss: 0.1820, CE: 0.0754
[16:52:36.931] Epoch [1/5] Iteration [2950/2976]: Loss: 0.1877, CE: 0.0881
[16:52:41.480] Epoch [1/5] Iteration [2960/2976]: Loss: 0.2092, CE: 0.1442
[16:52:46.005] Epoch [1/5] Iteration [2970/2976]: Loss: 0.2380, CE: 0.2158
[16:52:48.808] Epoch [1/5] Average Loss: 0.1933, CE: 0.1310, Dice: 0.2349
[16:53:09.733] Epoch [2/5] Iteration [0/2976]: Loss: 0.1784, CE: 0.0678
[16:53:14.233] Epoch [2/5] Iteration [10/2976]: Loss: 0.1424, CE: 0.1008
[16:53:18.717] Epoch [2/5] Iteration [20/2976]: Loss: 0.2018, CE: 0.1249
[16:53:23.219] Epoch [2/5] Iteration [30/2976]: Loss: 0.2240, CE: 0.1809
[16:53:27.705] Epoch [2/5] Iteration [40/2976]: Loss: 0.2005, CE: 0.1205
[16:53:32.202] Epoch [2/5] Iteration [50/2976]: Loss: 0.1566, CE: 0.1361
[16:53:36.679] Epoch [2/5] Iteration [60/2976]: Loss: 0.2248, CE: 0.1830
[16:53:41.182] Epoch [2/5] Iteration [70/2976]: Loss: 0.2295, CE: 0.1918
[16:53:45.672] Epoch [2/5] Iteration [80/2976]: Loss: 0.1650, CE: 0.1590
[16:53:50.174] Epoch [2/5] Iteration [90/2976]: Loss: 0.1899, CE: 0.0958
[16:53:54.697] Epoch [2/5] Iteration [100/2976]: Loss: 0.1548, CE: 0.1330
[16:53:59.220] Epoch [2/5] Iteration [110/2976]: Loss: 0.2094, CE: 0.1446
[16:54:03.760] Epoch [2/5] Iteration [120/2976]: Loss: 0.2043, CE: 0.1320
[16:54:08.284] Epoch [2/5] Iteration [130/2976]: Loss: 0.2196, CE: 0.1662
[16:54:12.799] Epoch [2/5] Iteration [140/2976]: Loss: 0.2471, CE: 0.2389
[16:54:17.322] Epoch [2/5] Iteration [150/2976]: Loss: 0.1907, CE: 0.0958
[16:54:21.831] Epoch [2/5] Iteration [160/2976]: Loss: 0.1843, CE: 0.2065
[16:54:26.364] Epoch [2/5] Iteration [170/2976]: Loss: 0.1703, CE: 0.0479
[16:54:30.866] Epoch [2/5] Iteration [180/2976]: Loss: 0.2016, CE: 0.1257
[16:54:35.383] Epoch [2/5] Iteration [190/2976]: Loss: 0.1302, CE: 0.0722
[16:54:39.885] Epoch [2/5] Iteration [200/2976]: Loss: 0.2126, CE: 0.1519
[16:54:44.404] Epoch [2/5] Iteration [210/2976]: Loss: 0.2185, CE: 0.1670
[16:54:48.929] Epoch [2/5] Iteration [220/2976]: Loss: 0.2555, CE: 0.2592
[16:54:53.438] Epoch [2/5] Iteration [230/2976]: Loss: 0.2320, CE: 0.2012
[16:54:57.939] Epoch [2/5] Iteration [240/2976]: Loss: 0.2225, CE: 0.1775
[16:55:02.463] Epoch [2/5] Iteration [250/2976]: Loss: 0.2213, CE: 0.1737
[16:55:06.963] Epoch [2/5] Iteration [260/2976]: Loss: 0.1928, CE: 0.1034
[16:55:11.496] Epoch [2/5] Iteration [270/2976]: Loss: 0.1901, CE: 0.0971
[16:55:16.017] Epoch [2/5] Iteration [280/2976]: Loss: 0.1983, CE: 0.1160
[16:55:20.547] Epoch [2/5] Iteration [290/2976]: Loss: 0.1541, CE: 0.1316
[16:55:25.085] Epoch [2/5] Iteration [300/2976]: Loss: 0.1505, CE: 0.1220
[16:55:29.604] Epoch [2/5] Iteration [310/2976]: Loss: 0.1819, CE: 0.0764
[16:55:34.127] Epoch [2/5] Iteration [320/2976]: Loss: 0.1854, CE: 0.0848
[16:55:38.650] Epoch [2/5] Iteration [330/2976]: Loss: 0.1941, CE: 0.1059
[16:55:43.167] Epoch [2/5] Iteration [340/2976]: Loss: 0.2129, CE: 0.1524
[16:55:47.701] Epoch [2/5] Iteration [350/2976]: Loss: 0.2127, CE: 0.1525
[16:55:52.212] Epoch [2/5] Iteration [360/2976]: Loss: 0.1934, CE: 0.1042
[16:55:56.752] Epoch [2/5] Iteration [370/2976]: Loss: 0.2320, CE: 0.2003
[16:56:01.288] Epoch [2/5] Iteration [380/2976]: Loss: 0.1918, CE: 0.1005
[16:56:05.884] Epoch [2/5] Iteration [390/2976]: Loss: 0.1564, CE: 0.1371
[16:56:10.394] Epoch [2/5] Iteration [400/2976]: Loss: 0.2007, CE: 0.1231
[16:56:14.918] Epoch [2/5] Iteration [410/2976]: Loss: 0.2126, CE: 0.1525
[16:56:19.452] Epoch [2/5] Iteration [420/2976]: Loss: 0.2369, CE: 0.2130
[16:56:23.974] Epoch [2/5] Iteration [430/2976]: Loss: 0.1301, CE: 0.0704
[16:56:28.505] Epoch [2/5] Iteration [440/2976]: Loss: 0.1848, CE: 0.0796
[16:56:33.012] Epoch [2/5] Iteration [450/2976]: Loss: 0.2409, CE: 0.2234
[16:56:37.512] Epoch [2/5] Iteration [460/2976]: Loss: 0.1808, CE: 0.0716
[16:56:42.043] Epoch [2/5] Iteration [470/2976]: Loss: 0.2080, CE: 0.1410
[16:56:46.574] Epoch [2/5] Iteration [480/2976]: Loss: 0.1930, CE: 0.2284
[16:56:51.103] Epoch [2/5] Iteration [490/2976]: Loss: 0.1986, CE: 0.1172
[16:56:55.611] Epoch [2/5] Iteration [500/2976]: Loss: 0.2501, CE: 0.2460
[16:57:00.146] Epoch [2/5] Iteration [510/2976]: Loss: 0.1895, CE: 0.0942
[16:57:04.673] Epoch [2/5] Iteration [520/2976]: Loss: 0.2204, CE: 0.1699
[16:57:09.187] Epoch [2/5] Iteration [530/2976]: Loss: 0.2015, CE: 0.1246
[16:57:13.721] Epoch [2/5] Iteration [540/2976]: Loss: 0.1975, CE: 0.1148
[16:57:18.263] Epoch [2/5] Iteration [550/2976]: Loss: 0.2622, CE: 0.2755
[16:57:22.765] Epoch [2/5] Iteration [560/2976]: Loss: 0.1973, CE: 0.1144
[16:57:27.327] Epoch [2/5] Iteration [570/2976]: Loss: 0.2149, CE: 0.1582
[16:57:31.828] Epoch [2/5] Iteration [580/2976]: Loss: 0.2229, CE: 0.1772
[16:57:36.370] Epoch [2/5] Iteration [590/2976]: Loss: 0.2067, CE: 0.1385
[16:57:40.874] Epoch [2/5] Iteration [600/2976]: Loss: 0.1943, CE: 0.1070
[16:57:45.396] Epoch [2/5] Iteration [610/2976]: Loss: 0.2242, CE: 0.1811
[16:57:49.933] Epoch [2/5] Iteration [620/2976]: Loss: 0.2119, CE: 0.1463
[16:57:54.456] Epoch [2/5] Iteration [630/2976]: Loss: 0.2179, CE: 0.1655
[16:57:58.994] Epoch [2/5] Iteration [640/2976]: Loss: 0.1350, CE: 0.0826
[16:58:03.514] Epoch [2/5] Iteration [650/2976]: Loss: 0.1915, CE: 0.1003
[16:58:08.020] Epoch [2/5] Iteration [660/2976]: Loss: 0.2097, CE: 0.1418
[16:58:12.548] Epoch [2/5] Iteration [670/2976]: Loss: 0.2098, CE: 0.1451
[16:58:17.092] Epoch [2/5] Iteration [680/2976]: Loss: 0.2090, CE: 0.1425
[16:58:21.610] Epoch [2/5] Iteration [690/2976]: Loss: 0.2009, CE: 0.1227
[16:58:26.139] Epoch [2/5] Iteration [700/2976]: Loss: 0.1927, CE: 0.0996
[16:58:30.686] Epoch [2/5] Iteration [710/2976]: Loss: 0.2129, CE: 0.1534
[16:58:35.238] Epoch [2/5] Iteration [720/2976]: Loss: 0.2296, CE: 0.1950
[16:58:39.783] Epoch [2/5] Iteration [730/2976]: Loss: 0.2025, CE: 0.1253
[16:58:44.323] Epoch [2/5] Iteration [740/2976]: Loss: 0.2094, CE: 0.1443
[16:58:48.861] Epoch [2/5] Iteration [750/2976]: Loss: 0.1153, CE: 0.0341
[16:58:53.386] Epoch [2/5] Iteration [760/2976]: Loss: 0.2200, CE: 0.1610
[16:58:57.909] Epoch [2/5] Iteration [770/2976]: Loss: 0.1845, CE: 0.0828
[16:59:02.411] Epoch [2/5] Iteration [780/2976]: Loss: 0.2170, CE: 0.1633
[16:59:06.953] Epoch [2/5] Iteration [790/2976]: Loss: 0.2114, CE: 0.1494
[16:59:11.457] Epoch [2/5] Iteration [800/2976]: Loss: 0.1943, CE: 0.1070
[16:59:15.989] Epoch [2/5] Iteration [810/2976]: Loss: 0.2189, CE: 0.1663
[16:59:20.495] Epoch [2/5] Iteration [820/2976]: Loss: 0.2442, CE: 0.2312
[16:59:25.050] Epoch [2/5] Iteration [830/2976]: Loss: 0.2492, CE: 0.2432
[16:59:29.571] Epoch [2/5] Iteration [840/2976]: Loss: 0.2121, CE: 0.1502
[16:59:34.114] Epoch [2/5] Iteration [850/2976]: Loss: 0.1911, CE: 0.0980
[16:59:38.624] Epoch [2/5] Iteration [860/2976]: Loss: 0.1249, CE: 0.0590
[16:59:43.141] Epoch [2/5] Iteration [870/2976]: Loss: 0.1917, CE: 0.1000
[16:59:47.656] Epoch [2/5] Iteration [880/2976]: Loss: 0.1615, CE: 0.1499
[16:59:52.211] Epoch [2/5] Iteration [890/2976]: Loss: 0.1949, CE: 0.1089
[16:59:56.743] Epoch [2/5] Iteration [900/2976]: Loss: 0.1949, CE: 0.1085
[17:00:01.265] Epoch [2/5] Iteration [910/2976]: Loss: 0.2070, CE: 0.1388
[17:00:05.813] Epoch [2/5] Iteration [920/2976]: Loss: 0.2167, CE: 0.1633
[17:00:10.351] Epoch [2/5] Iteration [930/2976]: Loss: 0.1717, CE: 0.0506
[17:00:14.862] Epoch [2/5] Iteration [940/2976]: Loss: 0.1891, CE: 0.0925
[17:00:19.395] Epoch [2/5] Iteration [950/2976]: Loss: 0.1818, CE: 0.0763
[17:00:23.948] Epoch [2/5] Iteration [960/2976]: Loss: 0.1972, CE: 0.1140
[17:00:28.478] Epoch [2/5] Iteration [970/2976]: Loss: 0.1862, CE: 0.0864
[17:00:32.999] Epoch [2/5] Iteration [980/2976]: Loss: 0.1929, CE: 0.1007
[17:00:37.543] Epoch [2/5] Iteration [990/2976]: Loss: 0.1898, CE: 0.0957
[17:00:42.061] Epoch [2/5] Iteration [1000/2976]: Loss: 0.1740, CE: 0.1792
[17:00:46.597] Epoch [2/5] Iteration [1010/2976]: Loss: 0.2067, CE: 0.1374
[17:00:51.127] Epoch [2/5] Iteration [1020/2976]: Loss: 0.1350, CE: 0.0842
[17:00:55.646] Epoch [2/5] Iteration [1030/2976]: Loss: 0.1454, CE: 0.1101
[17:01:00.152] Epoch [2/5] Iteration [1040/2976]: Loss: 0.1693, CE: 0.1690
[17:01:04.724] Epoch [2/5] Iteration [1050/2976]: Loss: 0.2075, CE: 0.1373
[17:01:09.248] Epoch [2/5] Iteration [1060/2976]: Loss: 0.1827, CE: 0.0780
[17:01:13.780] Epoch [2/5] Iteration [1070/2976]: Loss: 0.2147, CE: 0.1579
[17:01:18.288] Epoch [2/5] Iteration [1080/2976]: Loss: 0.2168, CE: 0.1632
[17:01:22.822] Epoch [2/5] Iteration [1090/2976]: Loss: 0.2447, CE: 0.2326
[17:01:27.340] Epoch [2/5] Iteration [1100/2976]: Loss: 0.1959, CE: 0.1113
[17:01:31.868] Epoch [2/5] Iteration [1110/2976]: Loss: 0.1487, CE: 0.1178
[17:01:36.399] Epoch [2/5] Iteration [1120/2976]: Loss: 0.2337, CE: 0.2049
[17:01:40.940] Epoch [2/5] Iteration [1130/2976]: Loss: 0.2312, CE: 0.1973
[17:01:45.464] Epoch [2/5] Iteration [1140/2976]: Loss: 0.1773, CE: 0.0614
[17:01:49.988] Epoch [2/5] Iteration [1150/2976]: Loss: 0.1760, CE: 0.0614
[17:01:54.492] Epoch [2/5] Iteration [1160/2976]: Loss: 0.1982, CE: 0.1168
[17:01:59.030] Epoch [2/5] Iteration [1170/2976]: Loss: 0.2013, CE: 0.1248
[17:02:03.559] Epoch [2/5] Iteration [1180/2976]: Loss: 0.1861, CE: 0.0864
[17:02:08.100] Epoch [2/5] Iteration [1190/2976]: Loss: 0.1966, CE: 0.1126
[17:02:12.617] Epoch [2/5] Iteration [1200/2976]: Loss: 0.2225, CE: 0.1776
[17:02:17.140] Epoch [2/5] Iteration [1210/2976]: Loss: 0.2814, CE: 0.3241
[17:02:21.673] Epoch [2/5] Iteration [1220/2976]: Loss: 0.2103, CE: 0.1468
[17:02:26.205] Epoch [2/5] Iteration [1230/2976]: Loss: 0.1383, CE: 0.0880
[17:02:30.729] Epoch [2/5] Iteration [1240/2976]: Loss: 0.1671, CE: 0.1635
[17:02:35.245] Epoch [2/5] Iteration [1250/2976]: Loss: 0.2105, CE: 0.1472
[17:02:39.758] Epoch [2/5] Iteration [1260/2976]: Loss: 0.2260, CE: 0.1840
[17:02:44.296] Epoch [2/5] Iteration [1270/2976]: Loss: 0.1487, CE: 0.1173
[17:02:48.803] Epoch [2/5] Iteration [1280/2976]: Loss: 0.2075, CE: 0.1399
[17:02:53.359] Epoch [2/5] Iteration [1290/2976]: Loss: 0.2039, CE: 0.1314
[17:02:57.886] Epoch [2/5] Iteration [1300/2976]: Loss: 0.2104, CE: 0.1475
[17:03:02.445] Epoch [2/5] Iteration [1310/2976]: Loss: 0.1844, CE: 0.2071
[17:03:06.982] Epoch [2/5] Iteration [1320/2976]: Loss: 0.1607, CE: 0.1474
[17:03:11.521] Epoch [2/5] Iteration [1330/2976]: Loss: 0.2000, CE: 0.1194
[17:03:16.060] Epoch [2/5] Iteration [1340/2976]: Loss: 0.2268, CE: 0.1826
[17:03:20.605] Epoch [2/5] Iteration [1350/2976]: Loss: 0.1579, CE: 0.1410
[17:03:25.135] Epoch [2/5] Iteration [1360/2976]: Loss: 0.2091, CE: 0.1442
[17:03:29.666] Epoch [2/5] Iteration [1370/2976]: Loss: 0.1838, CE: 0.0811
[17:03:34.182] Epoch [2/5] Iteration [1380/2976]: Loss: 0.1841, CE: 0.0820
[17:03:38.721] Epoch [2/5] Iteration [1390/2976]: Loss: 0.1868, CE: 0.0872
[17:03:43.243] Epoch [2/5] Iteration [1400/2976]: Loss: 0.2169, CE: 0.1628
[17:03:47.784] Epoch [2/5] Iteration [1410/2976]: Loss: 0.1698, CE: 0.1704
[17:03:52.302] Epoch [2/5] Iteration [1420/2976]: Loss: 0.1835, CE: 0.0801
[17:03:56.831] Epoch [2/5] Iteration [1430/2976]: Loss: 0.2154, CE: 0.1593
[17:04:01.361] Epoch [2/5] Iteration [1440/2976]: Loss: 0.1176, CE: 0.0407
[17:04:05.903] Epoch [2/5] Iteration [1450/2976]: Loss: 0.1853, CE: 0.0845
[17:04:10.457] Epoch [2/5] Iteration [1460/2976]: Loss: 0.1311, CE: 0.0716
[17:04:14.973] Epoch [2/5] Iteration [1470/2976]: Loss: 0.1940, CE: 0.1062
[17:04:19.535] Epoch [2/5] Iteration [1480/2976]: Loss: 0.1780, CE: 0.0656
[17:04:24.100] Epoch [2/5] Iteration [1490/2976]: Loss: 0.1762, CE: 0.0620
[17:04:28.634] Epoch [2/5] Iteration [1500/2976]: Loss: 0.1112, CE: 0.0246
[17:04:33.164] Epoch [2/5] Iteration [1510/2976]: Loss: 0.2385, CE: 0.2169
[17:04:37.703] Epoch [2/5] Iteration [1520/2976]: Loss: 0.2150, CE: 0.1585
[17:04:42.260] Epoch [2/5] Iteration [1530/2976]: Loss: 0.2150, CE: 0.1581
[17:04:46.768] Epoch [2/5] Iteration [1540/2976]: Loss: 0.2059, CE: 0.1355
[17:04:51.384] Epoch [2/5] Iteration [1550/2976]: Loss: 0.2087, CE: 0.1430
[17:04:55.899] Epoch [2/5] Iteration [1560/2976]: Loss: 0.0685, CE: 0.0387
[17:05:00.444] Epoch [2/5] Iteration [1570/2976]: Loss: 0.1914, CE: 0.0998
[17:05:04.957] Epoch [2/5] Iteration [1580/2976]: Loss: 0.1915, CE: 0.0997
[17:05:09.494] Epoch [2/5] Iteration [1590/2976]: Loss: 0.1684, CE: 0.1669
[17:05:14.026] Epoch [2/5] Iteration [1600/2976]: Loss: 0.2125, CE: 0.1528
[17:05:18.586] Epoch [2/5] Iteration [1610/2976]: Loss: 0.0962, CE: 0.1034
[17:05:23.117] Epoch [2/5] Iteration [1620/2976]: Loss: 0.1371, CE: 0.0889
[17:05:27.649] Epoch [2/5] Iteration [1630/2976]: Loss: 0.2077, CE: 0.1408
[17:05:32.176] Epoch [2/5] Iteration [1640/2976]: Loss: 0.2027, CE: 0.1258
[17:05:36.722] Epoch [2/5] Iteration [1650/2976]: Loss: 0.1521, CE: 0.1262
[17:05:41.246] Epoch [2/5] Iteration [1660/2976]: Loss: 0.1894, CE: 0.0942
[17:05:45.805] Epoch [2/5] Iteration [1670/2976]: Loss: 0.2407, CE: 0.2229
[17:05:50.338] Epoch [2/5] Iteration [1680/2976]: Loss: 0.1902, CE: 0.0952
[17:05:54.872] Epoch [2/5] Iteration [1690/2976]: Loss: 0.1262, CE: 0.0621
[17:05:59.414] Epoch [2/5] Iteration [1700/2976]: Loss: 0.2064, CE: 0.1375
[17:06:03.970] Epoch [2/5] Iteration [1710/2976]: Loss: 0.2232, CE: 0.1791
[17:06:08.515] Epoch [2/5] Iteration [1720/2976]: Loss: 0.1501, CE: 0.1205
[17:06:13.049] Epoch [2/5] Iteration [1730/2976]: Loss: 0.2293, CE: 0.1903
[17:06:17.601] Epoch [2/5] Iteration [1740/2976]: Loss: 0.1761, CE: 0.0617
[17:06:22.142] Epoch [2/5] Iteration [1750/2976]: Loss: 0.1944, CE: 0.1073
[17:06:26.667] Epoch [2/5] Iteration [1760/2976]: Loss: 0.1335, CE: 0.0801
[17:06:31.212] Epoch [2/5] Iteration [1770/2976]: Loss: 0.1470, CE: 0.1123
[17:06:35.746] Epoch [2/5] Iteration [1780/2976]: Loss: 0.2362, CE: 0.2105
[17:06:40.299] Epoch [2/5] Iteration [1790/2976]: Loss: 0.1329, CE: 0.0785
[17:06:44.810] Epoch [2/5] Iteration [1800/2976]: Loss: 0.1868, CE: 0.0860
[17:06:49.351] Epoch [2/5] Iteration [1810/2976]: Loss: 0.1862, CE: 0.0871
[17:06:53.902] Epoch [2/5] Iteration [1820/2976]: Loss: 0.1539, CE: 0.1313
[17:06:58.447] Epoch [2/5] Iteration [1830/2976]: Loss: 0.2445, CE: 0.2315
[17:07:02.968] Epoch [2/5] Iteration [1840/2976]: Loss: 0.1925, CE: 0.1026
[17:07:07.513] Epoch [2/5] Iteration [1850/2976]: Loss: 0.2314, CE: 0.1994
[17:07:12.072] Epoch [2/5] Iteration [1860/2976]: Loss: 0.1860, CE: 0.0845
[17:07:16.604] Epoch [2/5] Iteration [1870/2976]: Loss: 0.2117, CE: 0.1507
[17:07:21.113] Epoch [2/5] Iteration [1880/2976]: Loss: 0.1918, CE: 0.1011
[17:07:25.666] Epoch [2/5] Iteration [1890/2976]: Loss: 0.1833, CE: 0.2027
[17:07:30.180] Epoch [2/5] Iteration [1900/2976]: Loss: 0.1687, CE: 0.1681
[17:07:34.726] Epoch [2/5] Iteration [1910/2976]: Loss: 0.1544, CE: 0.1315
[17:07:39.254] Epoch [2/5] Iteration [1920/2976]: Loss: 0.1986, CE: 0.1166
[17:07:43.787] Epoch [2/5] Iteration [1930/2976]: Loss: 0.2054, CE: 0.1351
[17:07:48.304] Epoch [2/5] Iteration [1940/2976]: Loss: 0.2222, CE: 0.1761
[17:07:52.858] Epoch [2/5] Iteration [1950/2976]: Loss: 0.2152, CE: 0.1585
[17:07:57.383] Epoch [2/5] Iteration [1960/2976]: Loss: 0.1832, CE: 0.0790
[17:08:01.921] Epoch [2/5] Iteration [1970/2976]: Loss: 0.1921, CE: 0.1015
[17:08:06.463] Epoch [2/5] Iteration [1980/2976]: Loss: 0.1871, CE: 0.0875
[17:08:11.017] Epoch [2/5] Iteration [1990/2976]: Loss: 0.2460, CE: 0.2354
[17:08:15.551] Epoch [2/5] Iteration [2000/2976]: Loss: 0.2060, CE: 0.1361
[17:08:20.075] Epoch [2/5] Iteration [2010/2976]: Loss: 0.1829, CE: 0.0790
[17:08:24.595] Epoch [2/5] Iteration [2020/2976]: Loss: 0.1986, CE: 0.1159
[17:08:29.140] Epoch [2/5] Iteration [2030/2976]: Loss: 0.1777, CE: 0.0649
[17:08:33.678] Epoch [2/5] Iteration [2040/2976]: Loss: 0.1506, CE: 0.1227
[17:08:38.218] Epoch [2/5] Iteration [2050/2976]: Loss: 0.1936, CE: 0.1053
[17:08:42.761] Epoch [2/5] Iteration [2060/2976]: Loss: 0.1843, CE: 0.0803
[17:08:47.290] Epoch [2/5] Iteration [2070/2976]: Loss: 0.1614, CE: 0.1482
[17:08:51.849] Epoch [2/5] Iteration [2080/2976]: Loss: 0.2021, CE: 0.1263
[17:08:56.380] Epoch [2/5] Iteration [2090/2976]: Loss: 0.2034, CE: 0.1302
[17:09:00.899] Epoch [2/5] Iteration [2100/2976]: Loss: 0.1890, CE: 0.0939
[17:09:05.439] Epoch [2/5] Iteration [2110/2976]: Loss: 0.2077, CE: 0.1398
[17:09:09.975] Epoch [2/5] Iteration [2120/2976]: Loss: 0.2169, CE: 0.1628
[17:09:14.501] Epoch [2/5] Iteration [2130/2976]: Loss: 0.1968, CE: 0.1128
[17:09:19.018] Epoch [2/5] Iteration [2140/2976]: Loss: 0.2148, CE: 0.1579
[17:09:23.550] Epoch [2/5] Iteration [2150/2976]: Loss: 0.2287, CE: 0.1928
[17:09:28.064] Epoch [2/5] Iteration [2160/2976]: Loss: 0.1466, CE: 0.1114
[17:09:32.605] Epoch [2/5] Iteration [2170/2976]: Loss: 0.2123, CE: 0.1521
[17:09:37.133] Epoch [2/5] Iteration [2180/2976]: Loss: 0.1442, CE: 0.1066
[17:09:41.672] Epoch [2/5] Iteration [2190/2976]: Loss: 0.2016, CE: 0.1251
[17:09:46.188] Epoch [2/5] Iteration [2200/2976]: Loss: 0.2579, CE: 0.2640
[17:09:50.727] Epoch [2/5] Iteration [2210/2976]: Loss: 0.2013, CE: 0.1236
[17:09:55.252] Epoch [2/5] Iteration [2220/2976]: Loss: 0.1901, CE: 0.0968
[17:09:59.784] Epoch [2/5] Iteration [2230/2976]: Loss: 0.2009, CE: 0.1233
[17:10:04.338] Epoch [2/5] Iteration [2240/2976]: Loss: 0.2207, CE: 0.1719
[17:10:08.884] Epoch [2/5] Iteration [2250/2976]: Loss: 0.1964, CE: 0.1122
[17:10:13.407] Epoch [2/5] Iteration [2260/2976]: Loss: 0.1913, CE: 0.0996
[17:10:17.942] Epoch [2/5] Iteration [2270/2976]: Loss: 0.1348, CE: 0.0837
[17:10:22.479] Epoch [2/5] Iteration [2280/2976]: Loss: 0.1940, CE: 0.1061
[17:10:27.005] Epoch [2/5] Iteration [2290/2976]: Loss: 0.1740, CE: 0.0550
[17:10:31.526] Epoch [2/5] Iteration [2300/2976]: Loss: 0.2056, CE: 0.1353
[17:10:36.040] Epoch [2/5] Iteration [2310/2976]: Loss: 0.1505, CE: 0.1207
[17:10:40.604] Epoch [2/5] Iteration [2320/2976]: Loss: 0.1997, CE: 0.1200
[17:10:45.145] Epoch [2/5] Iteration [2330/2976]: Loss: 0.1268, CE: 0.0632
[17:10:49.666] Epoch [2/5] Iteration [2340/2976]: Loss: 0.1546, CE: 0.1327
[17:10:54.217] Epoch [2/5] Iteration [2350/2976]: Loss: 0.2046, CE: 0.1325
[17:10:58.736] Epoch [2/5] Iteration [2360/2976]: Loss: 0.2095, CE: 0.1444
[17:11:03.270] Epoch [2/5] Iteration [2370/2976]: Loss: 0.1919, CE: 0.1012
[17:11:07.791] Epoch [2/5] Iteration [2380/2976]: Loss: 0.1799, CE: 0.0716
[17:11:12.311] Epoch [2/5] Iteration [2390/2976]: Loss: 0.2007, CE: 0.1223
[17:11:16.862] Epoch [2/5] Iteration [2400/2976]: Loss: 0.2109, CE: 0.1483
[17:11:21.398] Epoch [2/5] Iteration [2410/2976]: Loss: 0.2233, CE: 0.1791
[17:11:25.914] Epoch [2/5] Iteration [2420/2976]: Loss: 0.2113, CE: 0.1486
[17:11:30.442] Epoch [2/5] Iteration [2430/2976]: Loss: 0.2025, CE: 0.1271
[17:11:34.973] Epoch [2/5] Iteration [2440/2976]: Loss: 0.1510, CE: 0.1224
[17:11:39.555] Epoch [2/5] Iteration [2450/2976]: Loss: 0.2107, CE: 0.1468
[17:11:44.074] Epoch [2/5] Iteration [2460/2976]: Loss: 0.1444, CE: 0.1075
[17:11:48.614] Epoch [2/5] Iteration [2470/2976]: Loss: 0.2017, CE: 0.1256
[17:11:53.160] Epoch [2/5] Iteration [2480/2976]: Loss: 0.1945, CE: 0.1071
[17:11:57.704] Epoch [2/5] Iteration [2490/2976]: Loss: 0.1942, CE: 0.1055
[17:12:02.228] Epoch [2/5] Iteration [2500/2976]: Loss: 0.2301, CE: 0.1938
[17:12:06.768] Epoch [2/5] Iteration [2510/2976]: Loss: 0.1840, CE: 0.0807
[17:12:11.294] Epoch [2/5] Iteration [2520/2976]: Loss: 0.1737, CE: 0.1807
[17:12:15.814] Epoch [2/5] Iteration [2530/2976]: Loss: 0.2329, CE: 0.1989
[17:12:20.328] Epoch [2/5] Iteration [2540/2976]: Loss: 0.1731, CE: 0.0527
[17:12:24.868] Epoch [2/5] Iteration [2550/2976]: Loss: 0.1866, CE: 0.0874
[17:12:29.393] Epoch [2/5] Iteration [2560/2976]: Loss: 0.1832, CE: 0.0797
[17:12:33.928] Epoch [2/5] Iteration [2570/2976]: Loss: 0.2083, CE: 0.1420
[17:12:38.443] Epoch [2/5] Iteration [2580/2976]: Loss: 0.2013, CE: 0.1209
[17:12:42.993] Epoch [2/5] Iteration [2590/2976]: Loss: 0.1822, CE: 0.0767
[17:12:47.513] Epoch [2/5] Iteration [2600/2976]: Loss: 0.1610, CE: 0.1476
[17:12:52.054] Epoch [2/5] Iteration [2610/2976]: Loss: 0.2516, CE: 0.2498
[17:12:56.580] Epoch [2/5] Iteration [2620/2976]: Loss: 0.2220, CE: 0.1756
[17:13:01.115] Epoch [2/5] Iteration [2630/2976]: Loss: 0.2213, CE: 0.1741
[17:13:05.662] Epoch [2/5] Iteration [2640/2976]: Loss: 0.2283, CE: 0.1918
[17:13:10.200] Epoch [2/5] Iteration [2650/2976]: Loss: 0.1593, CE: 0.1441
[17:13:14.705] Epoch [2/5] Iteration [2660/2976]: Loss: 0.1970, CE: 0.1137
[17:13:19.277] Epoch [2/5] Iteration [2670/2976]: Loss: 0.2084, CE: 0.1412
[17:13:23.805] Epoch [2/5] Iteration [2680/2976]: Loss: 0.1830, CE: 0.0775
[17:13:28.330] Epoch [2/5] Iteration [2690/2976]: Loss: 0.2103, CE: 0.1466
[17:13:32.849] Epoch [2/5] Iteration [2700/2976]: Loss: 0.2237, CE: 0.1735
[17:13:37.389] Epoch [2/5] Iteration [2710/2976]: Loss: 0.1914, CE: 0.0996
[17:13:41.982] Epoch [2/5] Iteration [2720/2976]: Loss: 0.1812, CE: 0.0722
[17:13:46.517] Epoch [2/5] Iteration [2730/2976]: Loss: 0.2050, CE: 0.1339
[17:13:51.053] Epoch [2/5] Iteration [2740/2976]: Loss: 0.2069, CE: 0.1387
[17:13:55.594] Epoch [2/5] Iteration [2750/2976]: Loss: 0.1963, CE: 0.1119
[17:14:00.142] Epoch [2/5] Iteration [2760/2976]: Loss: 0.1915, CE: 0.0946
[17:14:04.679] Epoch [2/5] Iteration [2770/2976]: Loss: 0.1973, CE: 0.1142
[17:14:09.233] Epoch [2/5] Iteration [2780/2976]: Loss: 0.1953, CE: 0.1097
[17:14:13.779] Epoch [2/5] Iteration [2790/2976]: Loss: 0.1959, CE: 0.1109
[17:14:18.326] Epoch [2/5] Iteration [2800/2976]: Loss: 0.1957, CE: 0.1096
[17:14:22.866] Epoch [2/5] Iteration [2810/2976]: Loss: 0.2306, CE: 0.1972
[17:14:27.383] Epoch [2/5] Iteration [2820/2976]: Loss: 0.2053, CE: 0.1328
[17:14:31.928] Epoch [2/5] Iteration [2830/2976]: Loss: 0.1814, CE: 0.0748
[17:14:36.457] Epoch [2/5] Iteration [2840/2976]: Loss: 0.1426, CE: 0.1024
[17:14:41.004] Epoch [2/5] Iteration [2850/2976]: Loss: 0.2100, CE: 0.1458
[17:14:45.517] Epoch [2/5] Iteration [2860/2976]: Loss: 0.1657, CE: 0.1563
[17:14:50.068] Epoch [2/5] Iteration [2870/2976]: Loss: 0.2147, CE: 0.1514
[17:14:54.588] Epoch [2/5] Iteration [2880/2976]: Loss: 0.2143, CE: 0.1556
[17:14:59.120] Epoch [2/5] Iteration [2890/2976]: Loss: 0.1877, CE: 0.0902
[17:15:03.639] Epoch [2/5] Iteration [2900/2976]: Loss: 0.2161, CE: 0.1559
[17:15:08.166] Epoch [2/5] Iteration [2910/2976]: Loss: 0.1944, CE: 0.1065
[17:15:12.685] Epoch [2/5] Iteration [2920/2976]: Loss: 0.1167, CE: 0.0377
[17:15:17.236] Epoch [2/5] Iteration [2930/2976]: Loss: 0.2068, CE: 0.1376
[17:15:21.769] Epoch [2/5] Iteration [2940/2976]: Loss: 0.1876, CE: 0.0903
[17:15:26.323] Epoch [2/5] Iteration [2950/2976]: Loss: 0.2360, CE: 0.2089
[17:15:30.851] Epoch [2/5] Iteration [2960/2976]: Loss: 0.2131, CE: 0.1526
[17:15:35.431] Epoch [2/5] Iteration [2970/2976]: Loss: 0.1275, CE: 0.0620
[17:15:38.279] Epoch [2/5] Average Loss: 0.1941, CE: 0.1310, Dice: 0.2361
[17:15:59.283] Epoch [3/5] Iteration [0/2976]: Loss: 0.1421, CE: 0.1020
[17:16:03.768] Epoch [3/5] Iteration [10/2976]: Loss: 0.2075, CE: 0.1385
[17:16:08.237] Epoch [3/5] Iteration [20/2976]: Loss: 0.1908, CE: 0.0984
[17:16:12.718] Epoch [3/5] Iteration [30/2976]: Loss: 0.1867, CE: 0.0885
[17:16:17.227] Epoch [3/5] Iteration [40/2976]: Loss: 0.2182, CE: 0.1667
[17:16:21.729] Epoch [3/5] Iteration [50/2976]: Loss: 0.2050, CE: 0.1332
[17:16:26.215] Epoch [3/5] Iteration [60/2976]: Loss: 0.1501, CE: 0.1213
[17:16:30.718] Epoch [3/5] Iteration [70/2976]: Loss: 0.1996, CE: 0.1199
[17:16:35.215] Epoch [3/5] Iteration [80/2976]: Loss: 0.0842, CE: 0.0823
[17:16:39.718] Epoch [3/5] Iteration [90/2976]: Loss: 0.1253, CE: 0.0592
[17:16:44.225] Epoch [3/5] Iteration [100/2976]: Loss: 0.1960, CE: 0.1107
[17:16:48.724] Epoch [3/5] Iteration [110/2976]: Loss: 0.1925, CE: 0.1024
[17:16:53.240] Epoch [3/5] Iteration [120/2976]: Loss: 0.2180, CE: 0.1652
[17:16:57.756] Epoch [3/5] Iteration [130/2976]: Loss: 0.1789, CE: 0.0688
[17:17:02.262] Epoch [3/5] Iteration [140/2976]: Loss: 0.1988, CE: 0.1183
[17:17:06.769] Epoch [3/5] Iteration [150/2976]: Loss: 0.1425, CE: 0.1024
[17:17:11.309] Epoch [3/5] Iteration [160/2976]: Loss: 0.2268, CE: 0.1845
[17:17:15.823] Epoch [3/5] Iteration [170/2976]: Loss: 0.2061, CE: 0.1365
[17:17:20.355] Epoch [3/5] Iteration [180/2976]: Loss: 0.2293, CE: 0.1935
[17:17:24.850] Epoch [3/5] Iteration [190/2976]: Loss: 0.1943, CE: 0.1073
[17:17:29.394] Epoch [3/5] Iteration [200/2976]: Loss: 0.1506, CE: 0.1212
[17:17:33.898] Epoch [3/5] Iteration [210/2976]: Loss: 0.1799, CE: 0.0714
[17:17:38.432] Epoch [3/5] Iteration [220/2976]: Loss: 0.2294, CE: 0.1934
[17:17:42.929] Epoch [3/5] Iteration [230/2976]: Loss: 0.2014, CE: 0.1252
[17:17:47.460] Epoch [3/5] Iteration [240/2976]: Loss: 0.1864, CE: 0.0873
[17:17:51.964] Epoch [3/5] Iteration [250/2976]: Loss: 0.1271, CE: 0.0630
[17:17:56.475] Epoch [3/5] Iteration [260/2976]: Loss: 0.1826, CE: 0.0779
[17:18:01.003] Epoch [3/5] Iteration [270/2976]: Loss: 0.1941, CE: 0.1069
[17:18:05.519] Epoch [3/5] Iteration [280/2976]: Loss: 0.1930, CE: 0.1039
[17:18:10.023] Epoch [3/5] Iteration [290/2976]: Loss: 0.2107, CE: 0.1480
[17:18:14.540] Epoch [3/5] Iteration [300/2976]: Loss: 0.1913, CE: 0.0969
[17:18:19.066] Epoch [3/5] Iteration [310/2976]: Loss: 0.2032, CE: 0.1290
[17:18:23.588] Epoch [3/5] Iteration [320/2976]: Loss: 0.2031, CE: 0.1283
[17:18:28.101] Epoch [3/5] Iteration [330/2976]: Loss: 0.2283, CE: 0.1908
[17:18:32.621] Epoch [3/5] Iteration [340/2976]: Loss: 0.1310, CE: 0.0736
[17:18:37.138] Epoch [3/5] Iteration [350/2976]: Loss: 0.1356, CE: 0.0848
[17:18:41.660] Epoch [3/5] Iteration [360/2976]: Loss: 0.2288, CE: 0.1932
[17:18:46.171] Epoch [3/5] Iteration [370/2976]: Loss: 0.1979, CE: 0.1163
[17:18:50.688] Epoch [3/5] Iteration [380/2976]: Loss: 0.1779, CE: 0.0662
[17:18:55.208] Epoch [3/5] Iteration [390/2976]: Loss: 0.2295, CE: 0.1936
[17:18:59.740] Epoch [3/5] Iteration [400/2976]: Loss: 0.1317, CE: 0.0756
[17:19:04.250] Epoch [3/5] Iteration [410/2976]: Loss: 0.2149, CE: 0.1572
[17:19:08.761] Epoch [3/5] Iteration [420/2976]: Loss: 0.1217, CE: 0.0505
[17:19:13.285] Epoch [3/5] Iteration [430/2976]: Loss: 0.1852, CE: 0.2088
[17:19:17.809] Epoch [3/5] Iteration [440/2976]: Loss: 0.2097, CE: 0.1408
[17:19:22.307] Epoch [3/5] Iteration [450/2976]: Loss: 0.2041, CE: 0.1317
[17:19:26.830] Epoch [3/5] Iteration [460/2976]: Loss: 0.1841, CE: 0.0808
[17:19:31.344] Epoch [3/5] Iteration [470/2976]: Loss: 0.1913, CE: 0.0997
[17:19:35.889] Epoch [3/5] Iteration [480/2976]: Loss: 0.2043, CE: 0.1317
[17:19:40.423] Epoch [3/5] Iteration [490/2976]: Loss: 0.1825, CE: 0.0780
[17:19:44.957] Epoch [3/5] Iteration [500/2976]: Loss: 0.1455, CE: 0.1076
[17:19:49.483] Epoch [3/5] Iteration [510/2976]: Loss: 0.1951, CE: 0.1090
[17:19:54.035] Epoch [3/5] Iteration [520/2976]: Loss: 0.1962, CE: 0.1114
[17:19:58.554] Epoch [3/5] Iteration [530/2976]: Loss: 0.1801, CE: 0.0717
[17:20:03.082] Epoch [3/5] Iteration [540/2976]: Loss: 0.2067, CE: 0.1382
[17:20:07.596] Epoch [3/5] Iteration [550/2976]: Loss: 0.1886, CE: 0.0929
[17:20:12.130] Epoch [3/5] Iteration [560/2976]: Loss: 0.1333, CE: 0.0798
[17:20:16.635] Epoch [3/5] Iteration [570/2976]: Loss: 0.2064, CE: 0.1371
[17:20:21.165] Epoch [3/5] Iteration [580/2976]: Loss: 0.2097, CE: 0.1450
[17:20:25.664] Epoch [3/5] Iteration [590/2976]: Loss: 0.1808, CE: 0.0723
[17:20:30.217] Epoch [3/5] Iteration [600/2976]: Loss: 0.1503, CE: 0.1222
[17:20:34.739] Epoch [3/5] Iteration [610/2976]: Loss: 0.2246, CE: 0.1828
[17:20:39.292] Epoch [3/5] Iteration [620/2976]: Loss: 0.2314, CE: 0.1978
[17:20:43.805] Epoch [3/5] Iteration [630/2976]: Loss: 0.2043, CE: 0.1311
[17:20:48.339] Epoch [3/5] Iteration [640/2976]: Loss: 0.2139, CE: 0.1555
[17:20:52.852] Epoch [3/5] Iteration [650/2976]: Loss: 0.1885, CE: 0.0920
[17:20:57.382] Epoch [3/5] Iteration [660/2976]: Loss: 0.2119, CE: 0.1502
[17:21:01.904] Epoch [3/5] Iteration [670/2976]: Loss: 0.1929, CE: 0.1038
[17:21:06.461] Epoch [3/5] Iteration [680/2976]: Loss: 0.1646, CE: 0.1566
[17:21:11.009] Epoch [3/5] Iteration [690/2976]: Loss: 0.1665, CE: 0.1618
[17:21:15.543] Epoch [3/5] Iteration [700/2976]: Loss: 0.1887, CE: 0.0929
[17:21:20.053] Epoch [3/5] Iteration [710/2976]: Loss: 0.1974, CE: 0.1149
[17:21:24.591] Epoch [3/5] Iteration [720/2976]: Loss: 0.1907, CE: 0.0982
[17:21:29.152] Epoch [3/5] Iteration [730/2976]: Loss: 0.1960, CE: 0.1115
[17:21:33.692] Epoch [3/5] Iteration [740/2976]: Loss: 0.2106, CE: 0.1472
[17:21:38.212] Epoch [3/5] Iteration [750/2976]: Loss: 0.1288, CE: 0.0681
[17:21:42.762] Epoch [3/5] Iteration [760/2976]: Loss: 0.2052, CE: 0.1342
[17:21:47.270] Epoch [3/5] Iteration [770/2976]: Loss: 0.1991, CE: 0.1190
[17:21:51.802] Epoch [3/5] Iteration [780/2976]: Loss: 0.1872, CE: 0.0892
[17:21:56.314] Epoch [3/5] Iteration [790/2976]: Loss: 0.2022, CE: 0.1267
[17:22:00.837] Epoch [3/5] Iteration [800/2976]: Loss: 0.2203, CE: 0.1695
[17:22:05.365] Epoch [3/5] Iteration [810/2976]: Loss: 0.2040, CE: 0.1306
[17:22:09.891] Epoch [3/5] Iteration [820/2976]: Loss: 0.2032, CE: 0.1272
[17:22:14.441] Epoch [3/5] Iteration [830/2976]: Loss: 0.2232, CE: 0.1790
[17:22:18.988] Epoch [3/5] Iteration [840/2976]: Loss: 0.2322, CE: 0.2013
[17:22:23.516] Epoch [3/5] Iteration [850/2976]: Loss: 0.1376, CE: 0.0904
[17:22:28.063] Epoch [3/5] Iteration [860/2976]: Loss: 0.2041, CE: 0.1314
[17:22:32.590] Epoch [3/5] Iteration [870/2976]: Loss: 0.2059, CE: 0.1353
[17:22:37.111] Epoch [3/5] Iteration [880/2976]: Loss: 0.1930, CE: 0.1036
[17:22:41.694] Epoch [3/5] Iteration [890/2976]: Loss: 0.2123, CE: 0.1486
[17:22:46.228] Epoch [3/5] Iteration [900/2976]: Loss: 0.1265, CE: 0.0613
[17:22:50.742] Epoch [3/5] Iteration [910/2976]: Loss: 0.1894, CE: 0.0950
[17:22:55.278] Epoch [3/5] Iteration [920/2976]: Loss: 0.1817, CE: 0.0757
[17:22:59.805] Epoch [3/5] Iteration [930/2976]: Loss: 0.2048, CE: 0.1334
[17:23:04.341] Epoch [3/5] Iteration [940/2976]: Loss: 0.2046, CE: 0.1328
[17:23:08.874] Epoch [3/5] Iteration [950/2976]: Loss: 0.2207, CE: 0.1688
[17:23:13.403] Epoch [3/5] Iteration [960/2976]: Loss: 0.2001, CE: 0.1208
[17:23:17.936] Epoch [3/5] Iteration [970/2976]: Loss: 0.2622, CE: 0.2751
[17:23:22.480] Epoch [3/5] Iteration [980/2976]: Loss: 0.1214, CE: 0.0498
[17:23:27.018] Epoch [3/5] Iteration [990/2976]: Loss: 0.2254, CE: 0.1845
[17:23:31.542] Epoch [3/5] Iteration [1000/2976]: Loss: 0.1568, CE: 0.1382
[17:23:36.066] Epoch [3/5] Iteration [1010/2976]: Loss: 0.2001, CE: 0.1215
[17:23:40.588] Epoch [3/5] Iteration [1020/2976]: Loss: 0.1805, CE: 0.1968
[17:23:45.109] Epoch [3/5] Iteration [1030/2976]: Loss: 0.2118, CE: 0.1502
[17:23:49.660] Epoch [3/5] Iteration [1040/2976]: Loss: 0.1967, CE: 0.1131
[17:23:54.193] Epoch [3/5] Iteration [1050/2976]: Loss: 0.2107, CE: 0.1469
[17:23:58.725] Epoch [3/5] Iteration [1060/2976]: Loss: 0.1425, CE: 0.1021
[17:24:03.250] Epoch [3/5] Iteration [1070/2976]: Loss: 0.2347, CE: 0.2078
[17:24:07.771] Epoch [3/5] Iteration [1080/2976]: Loss: 0.1859, CE: 0.0845
[17:24:12.307] Epoch [3/5] Iteration [1090/2976]: Loss: 0.2024, CE: 0.1264
[17:24:16.846] Epoch [3/5] Iteration [1100/2976]: Loss: 0.2041, CE: 0.1317
[17:24:21.358] Epoch [3/5] Iteration [1110/2976]: Loss: 0.2369, CE: 0.2127
[17:24:25.892] Epoch [3/5] Iteration [1120/2976]: Loss: 0.2125, CE: 0.1523
[17:24:30.442] Epoch [3/5] Iteration [1130/2976]: Loss: 0.2065, CE: 0.1374
[17:24:34.979] Epoch [3/5] Iteration [1140/2976]: Loss: 0.2023, CE: 0.1262
[17:24:39.499] Epoch [3/5] Iteration [1150/2976]: Loss: 0.1743, CE: 0.0576
[17:24:44.034] Epoch [3/5] Iteration [1160/2976]: Loss: 0.1941, CE: 0.1064
[17:24:48.567] Epoch [3/5] Iteration [1170/2976]: Loss: 0.2220, CE: 0.1757
[17:24:53.136] Epoch [3/5] Iteration [1180/2976]: Loss: 0.1962, CE: 0.1104
[17:24:57.676] Epoch [3/5] Iteration [1190/2976]: Loss: 0.2109, CE: 0.1487
[17:25:02.208] Epoch [3/5] Iteration [1200/2976]: Loss: 0.1379, CE: 0.0888
[17:25:06.729] Epoch [3/5] Iteration [1210/2976]: Loss: 0.2156, CE: 0.1600
[17:25:11.267] Epoch [3/5] Iteration [1220/2976]: Loss: 0.2537, CE: 0.2547
[17:25:15.791] Epoch [3/5] Iteration [1230/2976]: Loss: 0.2039, CE: 0.1312
[17:25:20.352] Epoch [3/5] Iteration [1240/2976]: Loss: 0.1928, CE: 0.1034
[17:25:24.879] Epoch [3/5] Iteration [1250/2976]: Loss: 0.1387, CE: 0.0870
[17:25:29.405] Epoch [3/5] Iteration [1260/2976]: Loss: 0.1990, CE: 0.1191
[17:25:33.915] Epoch [3/5] Iteration [1270/2976]: Loss: 0.2249, CE: 0.1830
[17:25:38.486] Epoch [3/5] Iteration [1280/2976]: Loss: 0.2179, CE: 0.1649
[17:25:43.025] Epoch [3/5] Iteration [1290/2976]: Loss: 0.2218, CE: 0.1758
[17:25:47.570] Epoch [3/5] Iteration [1300/2976]: Loss: 0.2180, CE: 0.1652
[17:25:52.091] Epoch [3/5] Iteration [1310/2976]: Loss: 0.1947, CE: 0.1077
[17:25:56.623] Epoch [3/5] Iteration [1320/2976]: Loss: 0.2310, CE: 0.1985
[17:26:01.169] Epoch [3/5] Iteration [1330/2976]: Loss: 0.2100, CE: 0.1460
[17:26:05.702] Epoch [3/5] Iteration [1340/2976]: Loss: 0.1932, CE: 0.1042
[17:26:10.230] Epoch [3/5] Iteration [1350/2976]: Loss: 0.2230, CE: 0.1774
[17:26:14.758] Epoch [3/5] Iteration [1360/2976]: Loss: 0.2280, CE: 0.1899
[17:26:19.292] Epoch [3/5] Iteration [1370/2976]: Loss: 0.1914, CE: 0.1001
[17:26:23.831] Epoch [3/5] Iteration [1380/2976]: Loss: 0.1386, CE: 0.0929
[17:26:28.367] Epoch [3/5] Iteration [1390/2976]: Loss: 0.2223, CE: 0.1764
[17:26:32.927] Epoch [3/5] Iteration [1400/2976]: Loss: 0.1951, CE: 0.1093
[17:26:37.445] Epoch [3/5] Iteration [1410/2976]: Loss: 0.1682, CE: 0.1635
[17:26:42.001] Epoch [3/5] Iteration [1420/2976]: Loss: 0.1671, CE: 0.1624
[17:26:46.542] Epoch [3/5] Iteration [1430/2976]: Loss: 0.2053, CE: 0.1335
[17:26:51.106] Epoch [3/5] Iteration [1440/2976]: Loss: 0.2553, CE: 0.2593
[17:26:55.669] Epoch [3/5] Iteration [1450/2976]: Loss: 0.2247, CE: 0.1821
[17:27:00.224] Epoch [3/5] Iteration [1460/2976]: Loss: 0.2033, CE: 0.1299
[17:27:04.748] Epoch [3/5] Iteration [1470/2976]: Loss: 0.2309, CE: 0.1976
[17:27:09.293] Epoch [3/5] Iteration [1480/2976]: Loss: 0.2085, CE: 0.1420
[17:27:13.821] Epoch [3/5] Iteration [1490/2976]: Loss: 0.2257, CE: 0.1844
[17:27:18.383] Epoch [3/5] Iteration [1500/2976]: Loss: 0.1841, CE: 0.0818
[17:27:22.947] Epoch [3/5] Iteration [1510/2976]: Loss: 0.2092, CE: 0.1422
[17:27:27.517] Epoch [3/5] Iteration [1520/2976]: Loss: 0.1686, CE: 0.1643
[17:27:32.066] Epoch [3/5] Iteration [1530/2976]: Loss: 0.2326, CE: 0.1997
[17:27:36.626] Epoch [3/5] Iteration [1540/2976]: Loss: 0.1417, CE: 0.1004
[17:27:41.167] Epoch [3/5] Iteration [1550/2976]: Loss: 0.1459, CE: 0.1085
[17:27:45.716] Epoch [3/5] Iteration [1560/2976]: Loss: 0.1812, CE: 0.0727
[17:27:50.276] Epoch [3/5] Iteration [1570/2976]: Loss: 0.1544, CE: 0.1288
[17:27:54.843] Epoch [3/5] Iteration [1580/2976]: Loss: 0.1329, CE: 0.0788
[17:27:59.366] Epoch [3/5] Iteration [1590/2976]: Loss: 0.2077, CE: 0.1405
[17:28:03.919] Epoch [3/5] Iteration [1600/2976]: Loss: 0.2123, CE: 0.1509
[17:28:08.438] Epoch [3/5] Iteration [1610/2976]: Loss: 0.2301, CE: 0.1932
[17:28:12.989] Epoch [3/5] Iteration [1620/2976]: Loss: 0.2162, CE: 0.1612
[17:28:17.524] Epoch [3/5] Iteration [1630/2976]: Loss: 0.2246, CE: 0.1795
[17:28:22.061] Epoch [3/5] Iteration [1640/2976]: Loss: 0.1990, CE: 0.1188
[17:28:26.578] Epoch [3/5] Iteration [1650/2976]: Loss: 0.1839, CE: 0.0789
[17:28:31.135] Epoch [3/5] Iteration [1660/2976]: Loss: 0.2046, CE: 0.1331
[17:28:35.654] Epoch [3/5] Iteration [1670/2976]: Loss: 0.2107, CE: 0.1484
[17:28:40.206] Epoch [3/5] Iteration [1680/2976]: Loss: 0.2237, CE: 0.1802
[17:28:44.745] Epoch [3/5] Iteration [1690/2976]: Loss: 0.1426, CE: 0.1023
[17:28:49.300] Epoch [3/5] Iteration [1700/2976]: Loss: 0.1797, CE: 0.0704
[17:28:53.834] Epoch [3/5] Iteration [1710/2976]: Loss: 0.2569, CE: 0.2631
[17:28:58.362] Epoch [3/5] Iteration [1720/2976]: Loss: 0.1899, CE: 0.0945
[17:29:02.884] Epoch [3/5] Iteration [1730/2976]: Loss: 0.1287, CE: 0.0681
[17:29:07.423] Epoch [3/5] Iteration [1740/2976]: Loss: 0.1846, CE: 0.0829
[17:29:11.972] Epoch [3/5] Iteration [1750/2976]: Loss: 0.2028, CE: 0.1274
[17:29:16.519] Epoch [3/5] Iteration [1760/2976]: Loss: 0.2125, CE: 0.1518
[17:29:21.070] Epoch [3/5] Iteration [1770/2976]: Loss: 0.1976, CE: 0.1155
[17:29:25.603] Epoch [3/5] Iteration [1780/2976]: Loss: 0.2333, CE: 0.2046
[17:29:30.153] Epoch [3/5] Iteration [1790/2976]: Loss: 0.1880, CE: 0.0904
[17:29:34.712] Epoch [3/5] Iteration [1800/2976]: Loss: 0.1838, CE: 0.0798
[17:29:39.246] Epoch [3/5] Iteration [1810/2976]: Loss: 0.2050, CE: 0.1335
[17:29:43.817] Epoch [3/5] Iteration [1820/2976]: Loss: 0.2069, CE: 0.1385
[17:29:48.352] Epoch [3/5] Iteration [1830/2976]: Loss: 0.1686, CE: 0.1676
[17:29:52.935] Epoch [3/5] Iteration [1840/2976]: Loss: 0.2162, CE: 0.1611
[17:29:57.477] Epoch [3/5] Iteration [1850/2976]: Loss: 0.2031, CE: 0.1288
[17:30:02.029] Epoch [3/5] Iteration [1860/2976]: Loss: 0.1920, CE: 0.1007
[17:30:06.586] Epoch [3/5] Iteration [1870/2976]: Loss: 0.2013, CE: 0.1237
[17:30:11.119] Epoch [3/5] Iteration [1880/2976]: Loss: 0.1697, CE: 0.1705
[17:30:15.670] Epoch [3/5] Iteration [1890/2976]: Loss: 0.1598, CE: 0.1453
[17:30:20.212] Epoch [3/5] Iteration [1900/2976]: Loss: 0.1868, CE: 0.0872
[17:30:24.765] Epoch [3/5] Iteration [1910/2976]: Loss: 0.1761, CE: 0.1860
[17:30:29.316] Epoch [3/5] Iteration [1920/2976]: Loss: 0.1967, CE: 0.1132
[17:30:33.872] Epoch [3/5] Iteration [1930/2976]: Loss: 0.1440, CE: 0.1063
[17:30:38.458] Epoch [3/5] Iteration [1940/2976]: Loss: 0.1998, CE: 0.2431
[17:30:42.997] Epoch [3/5] Iteration [1950/2976]: Loss: 0.1927, CE: 0.1028
[17:30:47.548] Epoch [3/5] Iteration [1960/2976]: Loss: 0.1917, CE: 0.1006
[17:30:52.098] Epoch [3/5] Iteration [1970/2976]: Loss: 0.2100, CE: 0.1415
[17:30:56.664] Epoch [3/5] Iteration [1980/2976]: Loss: 0.1974, CE: 0.1147
[17:31:01.236] Epoch [3/5] Iteration [1990/2976]: Loss: 0.1959, CE: 0.1099
[17:31:05.787] Epoch [3/5] Iteration [2000/2976]: Loss: 0.2188, CE: 0.1672
[17:31:10.330] Epoch [3/5] Iteration [2010/2976]: Loss: 0.1868, CE: 0.0887
[17:31:14.889] Epoch [3/5] Iteration [2020/2976]: Loss: 0.1980, CE: 0.1155
[17:31:19.427] Epoch [3/5] Iteration [2030/2976]: Loss: 0.1924, CE: 0.1024
[17:31:23.966] Epoch [3/5] Iteration [2040/2976]: Loss: 0.2432, CE: 0.2212
[17:31:28.507] Epoch [3/5] Iteration [2050/2976]: Loss: 0.2008, CE: 0.1218
[17:31:33.153] Epoch [3/5] Iteration [2060/2976]: Loss: 0.2022, CE: 0.1256
[17:31:37.701] Epoch [3/5] Iteration [2070/2976]: Loss: 0.1896, CE: 0.0948
[17:31:42.247] Epoch [3/5] Iteration [2080/2976]: Loss: 0.1926, CE: 0.1030
[17:31:46.799] Epoch [3/5] Iteration [2090/2976]: Loss: 0.2006, CE: 0.1183
[17:31:51.348] Epoch [3/5] Iteration [2100/2976]: Loss: 0.1507, CE: 0.1228
[17:31:55.878] Epoch [3/5] Iteration [2110/2976]: Loss: 0.2438, CE: 0.2270
[17:32:00.443] Epoch [3/5] Iteration [2120/2976]: Loss: 0.1356, CE: 0.0840
[17:32:04.999] Epoch [3/5] Iteration [2130/2976]: Loss: 0.1965, CE: 0.1107
[17:32:09.552] Epoch [3/5] Iteration [2140/2976]: Loss: 0.2541, CE: 0.2557
[17:32:14.104] Epoch [3/5] Iteration [2150/2976]: Loss: 0.2113, CE: 0.1497
[17:32:18.657] Epoch [3/5] Iteration [2160/2976]: Loss: 0.1954, CE: 0.1096
[17:32:23.209] Epoch [3/5] Iteration [2170/2976]: Loss: 0.2220, CE: 0.1764
[17:32:27.774] Epoch [3/5] Iteration [2180/2976]: Loss: 0.1734, CE: 0.0547
[17:32:32.310] Epoch [3/5] Iteration [2190/2976]: Loss: 0.1965, CE: 0.1127
[17:32:36.891] Epoch [3/5] Iteration [2200/2976]: Loss: 0.1326, CE: 0.0780
[17:32:41.437] Epoch [3/5] Iteration [2210/2976]: Loss: 0.1895, CE: 0.0949
[17:32:45.998] Epoch [3/5] Iteration [2220/2976]: Loss: 0.2175, CE: 0.1625
[17:32:50.518] Epoch [3/5] Iteration [2230/2976]: Loss: 0.1758, CE: 0.0606
[17:32:55.076] Epoch [3/5] Iteration [2240/2976]: Loss: 0.2156, CE: 0.1588
[17:32:59.651] Epoch [3/5] Iteration [2250/2976]: Loss: 0.2214, CE: 0.1745
[17:33:04.200] Epoch [3/5] Iteration [2260/2976]: Loss: 0.2194, CE: 0.1684
[17:33:08.785] Epoch [3/5] Iteration [2270/2976]: Loss: 0.2072, CE: 0.1364
[17:33:13.332] Epoch [3/5] Iteration [2280/2976]: Loss: 0.2188, CE: 0.1679
[17:33:17.871] Epoch [3/5] Iteration [2290/2976]: Loss: 0.2281, CE: 0.1914
[17:33:22.419] Epoch [3/5] Iteration [2300/2976]: Loss: 0.2143, CE: 0.1567
[17:33:26.953] Epoch [3/5] Iteration [2310/2976]: Loss: 0.2114, CE: 0.1500
[17:33:31.519] Epoch [3/5] Iteration [2320/2976]: Loss: 0.1489, CE: 0.1178
[17:33:36.072] Epoch [3/5] Iteration [2330/2976]: Loss: 0.2222, CE: 0.1757
[17:33:40.618] Epoch [3/5] Iteration [2340/2976]: Loss: 0.2017, CE: 0.1252
[17:33:45.161] Epoch [3/5] Iteration [2350/2976]: Loss: 0.2247, CE: 0.1829
[17:33:49.692] Epoch [3/5] Iteration [2360/2976]: Loss: 0.2079, CE: 0.1408
[17:33:54.225] Epoch [3/5] Iteration [2370/2976]: Loss: 0.1783, CE: 0.0677
[17:33:58.776] Epoch [3/5] Iteration [2380/2976]: Loss: 0.2372, CE: 0.2095
[17:34:03.328] Epoch [3/5] Iteration [2390/2976]: Loss: 0.1885, CE: 0.0925
[17:34:07.866] Epoch [3/5] Iteration [2400/2976]: Loss: 0.2237, CE: 0.1802
[17:34:12.403] Epoch [3/5] Iteration [2410/2976]: Loss: 0.2023, CE: 0.1237
[17:34:16.949] Epoch [3/5] Iteration [2420/2976]: Loss: 0.2049, CE: 0.1338
[17:34:21.491] Epoch [3/5] Iteration [2430/2976]: Loss: 0.2171, CE: 0.1640
[17:34:26.063] Epoch [3/5] Iteration [2440/2976]: Loss: 0.0813, CE: 0.0745
[17:34:30.639] Epoch [3/5] Iteration [2450/2976]: Loss: 0.2254, CE: 0.1845
[17:34:35.193] Epoch [3/5] Iteration [2460/2976]: Loss: 0.1844, CE: 0.0811
[17:34:39.748] Epoch [3/5] Iteration [2470/2976]: Loss: 0.2041, CE: 0.1320
[17:34:44.313] Epoch [3/5] Iteration [2480/2976]: Loss: 0.1265, CE: 0.0621
[17:34:48.884] Epoch [3/5] Iteration [2490/2976]: Loss: 0.2447, CE: 0.2325
[17:34:53.428] Epoch [3/5] Iteration [2500/2976]: Loss: 0.1992, CE: 0.1176
[17:34:58.014] Epoch [3/5] Iteration [2510/2976]: Loss: 0.1968, CE: 0.1128
[17:35:02.566] Epoch [3/5] Iteration [2520/2976]: Loss: 0.1324, CE: 0.0766
[17:35:07.108] Epoch [3/5] Iteration [2530/2976]: Loss: 0.2037, CE: 0.1296
[17:35:11.653] Epoch [3/5] Iteration [2540/2976]: Loss: 0.1797, CE: 0.0689
[17:35:16.197] Epoch [3/5] Iteration [2550/2976]: Loss: 0.1572, CE: 0.1394
[17:35:20.740] Epoch [3/5] Iteration [2560/2976]: Loss: 0.2168, CE: 0.1606
[17:35:25.279] Epoch [3/5] Iteration [2570/2976]: Loss: 0.1846, CE: 0.0813
[17:35:29.806] Epoch [3/5] Iteration [2580/2976]: Loss: 0.1909, CE: 0.0895
[17:35:34.360] Epoch [3/5] Iteration [2590/2976]: Loss: 0.1937, CE: 0.1059
[17:35:38.898] Epoch [3/5] Iteration [2600/2976]: Loss: 0.1446, CE: 0.1082
[17:35:43.450] Epoch [3/5] Iteration [2610/2976]: Loss: 0.2020, CE: 0.1260
[17:35:48.007] Epoch [3/5] Iteration [2620/2976]: Loss: 0.1814, CE: 0.0749
[17:35:52.567] Epoch [3/5] Iteration [2630/2976]: Loss: 0.1864, CE: 0.0864
[17:35:57.136] Epoch [3/5] Iteration [2640/2976]: Loss: 0.1484, CE: 0.1170
[17:36:01.688] Epoch [3/5] Iteration [2650/2976]: Loss: 0.1897, CE: 0.0957
[17:36:06.241] Epoch [3/5] Iteration [2660/2976]: Loss: 0.2247, CE: 0.1822
[17:36:10.786] Epoch [3/5] Iteration [2670/2976]: Loss: 0.2298, CE: 0.1885
[17:36:15.334] Epoch [3/5] Iteration [2680/2976]: Loss: 0.1851, CE: 0.0824
[17:36:19.865] Epoch [3/5] Iteration [2690/2976]: Loss: 0.1306, CE: 0.0722
[17:36:24.406] Epoch [3/5] Iteration [2700/2976]: Loss: 0.2039, CE: 0.1304
[17:36:28.954] Epoch [3/5] Iteration [2710/2976]: Loss: 0.2309, CE: 0.1977
[17:36:33.505] Epoch [3/5] Iteration [2720/2976]: Loss: 0.1482, CE: 0.1171
[17:36:38.057] Epoch [3/5] Iteration [2730/2976]: Loss: 0.2186, CE: 0.1678
[17:36:42.610] Epoch [3/5] Iteration [2740/2976]: Loss: 0.1524, CE: 0.1273
[17:36:47.154] Epoch [3/5] Iteration [2750/2976]: Loss: 0.2123, CE: 0.1520
[17:36:51.725] Epoch [3/5] Iteration [2760/2976]: Loss: 0.1930, CE: 0.1033
[17:36:56.267] Epoch [3/5] Iteration [2770/2976]: Loss: 0.2066, CE: 0.1378
[17:37:00.826] Epoch [3/5] Iteration [2780/2976]: Loss: 0.1967, CE: 0.1109
[17:37:05.360] Epoch [3/5] Iteration [2790/2976]: Loss: 0.1899, CE: 0.0957
[17:37:09.921] Epoch [3/5] Iteration [2800/2976]: Loss: 0.1940, CE: 0.1062
[17:37:14.460] Epoch [3/5] Iteration [2810/2976]: Loss: 0.1960, CE: 0.1079
[17:37:19.025] Epoch [3/5] Iteration [2820/2976]: Loss: 0.2164, CE: 0.1610
[17:37:23.573] Epoch [3/5] Iteration [2830/2976]: Loss: 0.1654, CE: 0.1599
[17:37:28.138] Epoch [3/5] Iteration [2840/2976]: Loss: 0.2164, CE: 0.1583
[17:37:32.686] Epoch [3/5] Iteration [2850/2976]: Loss: 0.2323, CE: 0.2017
[17:37:37.250] Epoch [3/5] Iteration [2860/2976]: Loss: 0.2059, CE: 0.1358
[17:37:41.790] Epoch [3/5] Iteration [2870/2976]: Loss: 0.2296, CE: 0.1933
[17:37:46.349] Epoch [3/5] Iteration [2880/2976]: Loss: 0.1883, CE: 0.0919
[17:37:50.886] Epoch [3/5] Iteration [2890/2976]: Loss: 0.2035, CE: 0.1298
[17:37:55.441] Epoch [3/5] Iteration [2900/2976]: Loss: 0.2538, CE: 0.2556
[17:37:59.988] Epoch [3/5] Iteration [2910/2976]: Loss: 0.2463, CE: 0.2367
[17:38:04.547] Epoch [3/5] Iteration [2920/2976]: Loss: 0.2443, CE: 0.2271
[17:38:09.090] Epoch [3/5] Iteration [2930/2976]: Loss: 0.2118, CE: 0.1501
[17:38:13.629] Epoch [3/5] Iteration [2940/2976]: Loss: 0.1834, CE: 0.0798
[17:38:18.159] Epoch [3/5] Iteration [2950/2976]: Loss: 0.2089, CE: 0.1412
[17:38:22.690] Epoch [3/5] Iteration [2960/2976]: Loss: 0.2094, CE: 0.1451
[17:38:27.237] Epoch [3/5] Iteration [2970/2976]: Loss: 0.2063, CE: 0.1367
[17:38:30.102] Epoch [3/5] Average Loss: 0.1934, CE: 0.1310, Dice: 0.2350
[17:38:51.298] Epoch [4/5] Iteration [0/2976]: Loss: 0.1992, CE: 0.1186
[17:38:55.784] Epoch [4/5] Iteration [10/2976]: Loss: 0.2175, CE: 0.1645
[17:39:00.291] Epoch [4/5] Iteration [20/2976]: Loss: 0.1338, CE: 0.0785
[17:39:04.781] Epoch [4/5] Iteration [30/2976]: Loss: 0.2465, CE: 0.2372
[17:39:09.285] Epoch [4/5] Iteration [40/2976]: Loss: 0.2099, CE: 0.1464
[17:39:13.789] Epoch [4/5] Iteration [50/2976]: Loss: 0.2263, CE: 0.1869
[17:39:18.303] Epoch [4/5] Iteration [60/2976]: Loss: 0.1929, CE: 0.1019
[17:39:22.820] Epoch [4/5] Iteration [70/2976]: Loss: 0.1883, CE: 0.0922
[17:39:27.320] Epoch [4/5] Iteration [80/2976]: Loss: 0.1340, CE: 0.0802
[17:39:31.836] Epoch [4/5] Iteration [90/2976]: Loss: 0.1918, CE: 0.1007
[17:39:36.346] Epoch [4/5] Iteration [100/2976]: Loss: 0.2029, CE: 0.1249
[17:39:40.849] Epoch [4/5] Iteration [110/2976]: Loss: 0.1526, CE: 0.1274
[17:39:45.359] Epoch [4/5] Iteration [120/2976]: Loss: 0.2213, CE: 0.1744
[17:39:49.866] Epoch [4/5] Iteration [130/2976]: Loss: 0.1937, CE: 0.1058
[17:39:54.384] Epoch [4/5] Iteration [140/2976]: Loss: 0.1410, CE: 0.0988
[17:39:58.885] Epoch [4/5] Iteration [150/2976]: Loss: 0.1791, CE: 0.1930
[17:40:03.407] Epoch [4/5] Iteration [160/2976]: Loss: 0.2381, CE: 0.2160
[17:40:07.929] Epoch [4/5] Iteration [170/2976]: Loss: 0.1255, CE: 0.0605
[17:40:12.450] Epoch [4/5] Iteration [180/2976]: Loss: 0.1983, CE: 0.1167
[17:40:16.945] Epoch [4/5] Iteration [190/2976]: Loss: 0.1477, CE: 0.1155
[17:40:21.460] Epoch [4/5] Iteration [200/2976]: Loss: 0.1829, CE: 0.0777
[17:40:25.978] Epoch [4/5] Iteration [210/2976]: Loss: 0.1966, CE: 0.1130
[17:40:30.508] Epoch [4/5] Iteration [220/2976]: Loss: 0.1899, CE: 0.0925
[17:40:35.021] Epoch [4/5] Iteration [230/2976]: Loss: 0.1943, CE: 0.1072
[17:40:39.626] Epoch [4/5] Iteration [240/2976]: Loss: 0.1584, CE: 0.1424
[17:40:44.139] Epoch [4/5] Iteration [250/2976]: Loss: 0.1854, CE: 0.0850
[17:40:48.670] Epoch [4/5] Iteration [260/2976]: Loss: 0.2399, CE: 0.2201
[17:40:53.182] Epoch [4/5] Iteration [270/2976]: Loss: 0.1363, CE: 0.0873
[17:40:57.717] Epoch [4/5] Iteration [280/2976]: Loss: 0.1779, CE: 0.0612
[17:41:02.239] Epoch [4/5] Iteration [290/2976]: Loss: 0.1871, CE: 0.0869
[17:41:06.773] Epoch [4/5] Iteration [300/2976]: Loss: 0.1799, CE: 0.0712
[17:41:11.274] Epoch [4/5] Iteration [310/2976]: Loss: 0.1517, CE: 0.1248
[17:41:15.797] Epoch [4/5] Iteration [320/2976]: Loss: 0.1577, CE: 0.1406
[17:41:20.319] Epoch [4/5] Iteration [330/2976]: Loss: 0.2326, CE: 0.2000
[17:41:24.846] Epoch [4/5] Iteration [340/2976]: Loss: 0.2378, CE: 0.2132
[17:41:29.371] Epoch [4/5] Iteration [350/2976]: Loss: 0.1849, CE: 0.0829
[17:41:33.897] Epoch [4/5] Iteration [360/2976]: Loss: 0.2250, CE: 0.1835
[17:41:38.428] Epoch [4/5] Iteration [370/2976]: Loss: 0.2084, CE: 0.1421
[17:41:42.959] Epoch [4/5] Iteration [380/2976]: Loss: 0.1980, CE: 0.1146
[17:41:47.489] Epoch [4/5] Iteration [390/2976]: Loss: 0.2084, CE: 0.1423
[17:41:52.008] Epoch [4/5] Iteration [400/2976]: Loss: 0.2061, CE: 0.1352
[17:41:56.554] Epoch [4/5] Iteration [410/2976]: Loss: 0.1281, CE: 0.0641
[17:42:01.081] Epoch [4/5] Iteration [420/2976]: Loss: 0.1945, CE: 0.1059
[17:42:05.607] Epoch [4/5] Iteration [430/2976]: Loss: 0.2102, CE: 0.1467
[17:42:10.140] Epoch [4/5] Iteration [440/2976]: Loss: 0.2053, CE: 0.1332
[17:42:14.679] Epoch [4/5] Iteration [450/2976]: Loss: 0.2192, CE: 0.1690
[17:42:19.210] Epoch [4/5] Iteration [460/2976]: Loss: 0.1467, CE: 0.1103
[17:42:23.744] Epoch [4/5] Iteration [470/2976]: Loss: 0.2182, CE: 0.1664
[17:42:28.280] Epoch [4/5] Iteration [480/2976]: Loss: 0.1705, CE: 0.1724
[17:42:32.813] Epoch [4/5] Iteration [490/2976]: Loss: 0.2259, CE: 0.1858
[17:42:37.364] Epoch [4/5] Iteration [500/2976]: Loss: 0.1953, CE: 0.1098
[17:42:41.884] Epoch [4/5] Iteration [510/2976]: Loss: 0.2081, CE: 0.1397
[17:42:46.430] Epoch [4/5] Iteration [520/2976]: Loss: 0.1816, CE: 0.0734
[17:42:50.973] Epoch [4/5] Iteration [530/2976]: Loss: 0.1851, CE: 0.0825
[17:42:55.521] Epoch [4/5] Iteration [540/2976]: Loss: 0.2201, CE: 0.1711
[17:43:00.036] Epoch [4/5] Iteration [550/2976]: Loss: 0.2333, CE: 0.2001
[17:43:04.605] Epoch [4/5] Iteration [560/2976]: Loss: 0.2259, CE: 0.1823
[17:43:09.136] Epoch [4/5] Iteration [570/2976]: Loss: 0.1498, CE: 0.1207
[17:43:13.680] Epoch [4/5] Iteration [580/2976]: Loss: 0.2218, CE: 0.1724
[17:43:18.209] Epoch [4/5] Iteration [590/2976]: Loss: 0.2048, CE: 0.1315
[17:43:22.752] Epoch [4/5] Iteration [600/2976]: Loss: 0.2011, CE: 0.1238
[17:43:27.322] Epoch [4/5] Iteration [610/2976]: Loss: 0.1352, CE: 0.0803
[17:43:31.862] Epoch [4/5] Iteration [620/2976]: Loss: 0.2130, CE: 0.1518
[17:43:36.423] Epoch [4/5] Iteration [630/2976]: Loss: 0.1527, CE: 0.1261
[17:43:40.960] Epoch [4/5] Iteration [640/2976]: Loss: 0.2110, CE: 0.1480
[17:43:45.503] Epoch [4/5] Iteration [650/2976]: Loss: 0.2118, CE: 0.1510
[17:43:50.045] Epoch [4/5] Iteration [660/2976]: Loss: 0.1465, CE: 0.1125
[17:43:54.575] Epoch [4/5] Iteration [670/2976]: Loss: 0.2163, CE: 0.1620
[17:43:59.110] Epoch [4/5] Iteration [680/2976]: Loss: 0.2169, CE: 0.1635
[17:44:03.643] Epoch [4/5] Iteration [690/2976]: Loss: 0.2529, CE: 0.2532
[17:44:08.174] Epoch [4/5] Iteration [700/2976]: Loss: 0.1224, CE: 0.0526
[17:44:12.705] Epoch [4/5] Iteration [710/2976]: Loss: 0.2057, CE: 0.1350
[17:44:17.258] Epoch [4/5] Iteration [720/2976]: Loss: 0.2060, CE: 0.1353
[17:44:21.842] Epoch [4/5] Iteration [730/2976]: Loss: 0.2098, CE: 0.1455
[17:44:26.370] Epoch [4/5] Iteration [740/2976]: Loss: 0.1903, CE: 0.0973
[17:44:30.879] Epoch [4/5] Iteration [750/2976]: Loss: 0.2098, CE: 0.1456
[17:44:35.445] Epoch [4/5] Iteration [760/2976]: Loss: 0.1695, CE: 0.1689
[17:44:39.970] Epoch [4/5] Iteration [770/2976]: Loss: 0.2024, CE: 0.1273
[17:44:44.537] Epoch [4/5] Iteration [780/2976]: Loss: 0.1960, CE: 0.1111
[17:44:49.083] Epoch [4/5] Iteration [790/2976]: Loss: 0.1846, CE: 0.0830
[17:44:53.622] Epoch [4/5] Iteration [800/2976]: Loss: 0.2223, CE: 0.1775
[17:44:58.170] Epoch [4/5] Iteration [810/2976]: Loss: 0.1460, CE: 0.1102
[17:45:02.717] Epoch [4/5] Iteration [820/2976]: Loss: 0.2096, CE: 0.1449
[17:45:07.265] Epoch [4/5] Iteration [830/2976]: Loss: 0.2055, CE: 0.1349
[17:45:11.795] Epoch [4/5] Iteration [840/2976]: Loss: 0.2173, CE: 0.1637
[17:45:16.323] Epoch [4/5] Iteration [850/2976]: Loss: 0.2379, CE: 0.2114
[17:45:20.874] Epoch [4/5] Iteration [860/2976]: Loss: 0.2323, CE: 0.1998
[17:45:25.440] Epoch [4/5] Iteration [870/2976]: Loss: 0.1963, CE: 0.1121
[17:45:29.984] Epoch [4/5] Iteration [880/2976]: Loss: 0.2140, CE: 0.1560
[17:45:34.532] Epoch [4/5] Iteration [890/2976]: Loss: 0.2028, CE: 0.1283
[17:45:39.090] Epoch [4/5] Iteration [900/2976]: Loss: 0.2029, CE: 0.1278
[17:45:43.629] Epoch [4/5] Iteration [910/2976]: Loss: 0.1899, CE: 0.0954
[17:45:48.186] Epoch [4/5] Iteration [920/2976]: Loss: 0.1939, CE: 0.1061
[17:45:52.729] Epoch [4/5] Iteration [930/2976]: Loss: 0.2086, CE: 0.1425
[17:45:57.269] Epoch [4/5] Iteration [940/2976]: Loss: 0.1904, CE: 0.0961
[17:46:01.811] Epoch [4/5] Iteration [950/2976]: Loss: 0.2207, CE: 0.1726
[17:46:06.349] Epoch [4/5] Iteration [960/2976]: Loss: 0.1252, CE: 0.0555
[17:46:10.884] Epoch [4/5] Iteration [970/2976]: Loss: 0.2252, CE: 0.1822
[17:46:15.418] Epoch [4/5] Iteration [980/2976]: Loss: 0.2042, CE: 0.1291
[17:46:19.975] Epoch [4/5] Iteration [990/2976]: Loss: 0.2219, CE: 0.1756
[17:46:24.514] Epoch [4/5] Iteration [1000/2976]: Loss: 0.1384, CE: 0.0919
[17:46:29.052] Epoch [4/5] Iteration [1010/2976]: Loss: 0.1288, CE: 0.0686
[17:46:33.582] Epoch [4/5] Iteration [1020/2976]: Loss: 0.2023, CE: 0.1272
[17:46:38.134] Epoch [4/5] Iteration [1030/2976]: Loss: 0.1359, CE: 0.0855
[17:46:42.662] Epoch [4/5] Iteration [1040/2976]: Loss: 0.2325, CE: 0.2013
[17:46:47.197] Epoch [4/5] Iteration [1050/2976]: Loss: 0.1337, CE: 0.0799
[17:46:51.752] Epoch [4/5] Iteration [1060/2976]: Loss: 0.2205, CE: 0.1708
[17:46:56.288] Epoch [4/5] Iteration [1070/2976]: Loss: 0.1817, CE: 0.0748
[17:47:00.837] Epoch [4/5] Iteration [1080/2976]: Loss: 0.1372, CE: 0.0895
[17:47:05.373] Epoch [4/5] Iteration [1090/2976]: Loss: 0.2168, CE: 0.1620
[17:47:09.943] Epoch [4/5] Iteration [1100/2976]: Loss: 0.1935, CE: 0.1034
[17:47:14.494] Epoch [4/5] Iteration [1110/2976]: Loss: 0.2003, CE: 0.1219
[17:47:19.048] Epoch [4/5] Iteration [1120/2976]: Loss: 0.2239, CE: 0.1809
[17:47:23.653] Epoch [4/5] Iteration [1130/2976]: Loss: 0.2126, CE: 0.1511
[17:47:28.224] Epoch [4/5] Iteration [1140/2976]: Loss: 0.1960, CE: 0.1109
[17:47:32.836] Epoch [4/5] Iteration [1150/2976]: Loss: 0.1874, CE: 0.0900
[17:47:37.393] Epoch [4/5] Iteration [1160/2976]: Loss: 0.2203, CE: 0.1719
[17:47:41.995] Epoch [4/5] Iteration [1170/2976]: Loss: 0.1963, CE: 0.1060
[17:47:46.551] Epoch [4/5] Iteration [1180/2976]: Loss: 0.1435, CE: 0.1046
[17:47:51.106] Epoch [4/5] Iteration [1190/2976]: Loss: 0.1622, CE: 0.1517
[17:47:55.678] Epoch [4/5] Iteration [1200/2976]: Loss: 0.1643, CE: 0.1571
[17:48:00.221] Epoch [4/5] Iteration [1210/2976]: Loss: 0.2435, CE: 0.2288
[17:48:04.805] Epoch [4/5] Iteration [1220/2976]: Loss: 0.1988, CE: 0.1181
[17:48:09.379] Epoch [4/5] Iteration [1230/2976]: Loss: 0.1749, CE: 0.0588
[17:48:13.945] Epoch [4/5] Iteration [1240/2976]: Loss: 0.2150, CE: 0.1587
[17:48:18.491] Epoch [4/5] Iteration [1250/2976]: Loss: 0.1912, CE: 0.0982
[17:48:23.042] Epoch [4/5] Iteration [1260/2976]: Loss: 0.1948, CE: 0.1083
[17:48:27.619] Epoch [4/5] Iteration [1270/2976]: Loss: 0.1827, CE: 0.0778
[17:48:32.220] Epoch [4/5] Iteration [1280/2976]: Loss: 0.1897, CE: 0.0947
[17:48:36.777] Epoch [4/5] Iteration [1290/2976]: Loss: 0.2271, CE: 0.1877
[17:48:41.386] Epoch [4/5] Iteration [1300/2976]: Loss: 0.1953, CE: 0.1063
[17:48:45.941] Epoch [4/5] Iteration [1310/2976]: Loss: 0.2090, CE: 0.1433
[17:48:50.518] Epoch [4/5] Iteration [1320/2976]: Loss: 0.2172, CE: 0.1634
[17:48:55.084] Epoch [4/5] Iteration [1330/2976]: Loss: 0.2035, CE: 0.1302
[17:48:59.632] Epoch [4/5] Iteration [1340/2976]: Loss: 0.2096, CE: 0.1317
[17:49:04.178] Epoch [4/5] Iteration [1350/2976]: Loss: 0.2256, CE: 0.1846
[17:49:08.740] Epoch [4/5] Iteration [1360/2976]: Loss: 0.2111, CE: 0.1493
[17:49:13.270] Epoch [4/5] Iteration [1370/2976]: Loss: 0.1969, CE: 0.1137
[17:49:17.799] Epoch [4/5] Iteration [1380/2976]: Loss: 0.2084, CE: 0.1420
[17:49:22.325] Epoch [4/5] Iteration [1390/2976]: Loss: 0.2463, CE: 0.2365
[17:49:26.932] Epoch [4/5] Iteration [1400/2976]: Loss: 0.2042, CE: 0.1314
[17:49:31.490] Epoch [4/5] Iteration [1410/2976]: Loss: 0.2079, CE: 0.1403
[17:49:36.042] Epoch [4/5] Iteration [1420/2976]: Loss: 0.2096, CE: 0.1454
[17:49:40.585] Epoch [4/5] Iteration [1430/2976]: Loss: 0.2052, CE: 0.1343
[17:49:45.132] Epoch [4/5] Iteration [1440/2976]: Loss: 0.1963, CE: 0.1121
[17:49:49.664] Epoch [4/5] Iteration [1450/2976]: Loss: 0.1277, CE: 0.0630
[17:49:54.217] Epoch [4/5] Iteration [1460/2976]: Loss: 0.1727, CE: 0.1756
[17:49:58.770] Epoch [4/5] Iteration [1470/2976]: Loss: 0.2155, CE: 0.1586
[17:50:03.296] Epoch [4/5] Iteration [1480/2976]: Loss: 0.2075, CE: 0.1397
[17:50:07.819] Epoch [4/5] Iteration [1490/2976]: Loss: 0.2293, CE: 0.1936
[17:50:12.361] Epoch [4/5] Iteration [1500/2976]: Loss: 0.2120, CE: 0.1490
[17:50:16.905] Epoch [4/5] Iteration [1510/2976]: Loss: 0.1956, CE: 0.1102
[17:50:21.462] Epoch [4/5] Iteration [1520/2976]: Loss: 0.1828, CE: 0.0782
[17:50:25.994] Epoch [4/5] Iteration [1530/2976]: Loss: 0.2131, CE: 0.1522
[17:50:30.543] Epoch [4/5] Iteration [1540/2976]: Loss: 0.2075, CE: 0.1386
[17:50:35.065] Epoch [4/5] Iteration [1550/2976]: Loss: 0.1939, CE: 0.1061
[17:50:39.621] Epoch [4/5] Iteration [1560/2976]: Loss: 0.2089, CE: 0.1390
[17:50:44.161] Epoch [4/5] Iteration [1570/2976]: Loss: 0.2014, CE: 0.1248
[17:50:48.712] Epoch [4/5] Iteration [1580/2976]: Loss: 0.2051, CE: 0.1333
[17:50:53.254] Epoch [4/5] Iteration [1590/2976]: Loss: 0.1869, CE: 0.0888
[17:50:57.811] Epoch [4/5] Iteration [1600/2976]: Loss: 0.1963, CE: 0.1123
[17:51:02.340] Epoch [4/5] Iteration [1610/2976]: Loss: 0.1994, CE: 0.1187
[17:51:06.889] Epoch [4/5] Iteration [1620/2976]: Loss: 0.2079, CE: 0.1394
[17:51:11.454] Epoch [4/5] Iteration [1630/2976]: Loss: 0.2044, CE: 0.1318
[17:51:15.978] Epoch [4/5] Iteration [1640/2976]: Loss: 0.2006, CE: 0.1194
[17:51:20.511] Epoch [4/5] Iteration [1650/2976]: Loss: 0.1778, CE: 0.1908
[17:51:25.057] Epoch [4/5] Iteration [1660/2976]: Loss: 0.2064, CE: 0.1351
[17:51:29.616] Epoch [4/5] Iteration [1670/2976]: Loss: 0.2205, CE: 0.1726
[17:51:34.153] Epoch [4/5] Iteration [1680/2976]: Loss: 0.2497, CE: 0.2447
[17:51:38.691] Epoch [4/5] Iteration [1690/2976]: Loss: 0.2301, CE: 0.1932
[17:51:43.226] Epoch [4/5] Iteration [1700/2976]: Loss: 0.2129, CE: 0.1535
[17:51:47.762] Epoch [4/5] Iteration [1710/2976]: Loss: 0.2294, CE: 0.1947
[17:51:52.310] Epoch [4/5] Iteration [1720/2976]: Loss: 0.1630, CE: 0.1534
[17:51:56.840] Epoch [4/5] Iteration [1730/2976]: Loss: 0.2081, CE: 0.1416
[17:52:01.398] Epoch [4/5] Iteration [1740/2976]: Loss: 0.2083, CE: 0.1418
[17:52:05.930] Epoch [4/5] Iteration [1750/2976]: Loss: 0.1699, CE: 0.1682
[17:52:10.465] Epoch [4/5] Iteration [1760/2976]: Loss: 0.1545, CE: 0.1313
[17:52:14.992] Epoch [4/5] Iteration [1770/2976]: Loss: 0.1709, CE: 0.0482
[17:52:19.542] Epoch [4/5] Iteration [1780/2976]: Loss: 0.1260, CE: 0.0618
[17:52:24.093] Epoch [4/5] Iteration [1790/2976]: Loss: 0.1969, CE: 0.1142
[17:52:28.632] Epoch [4/5] Iteration [1800/2976]: Loss: 0.2363, CE: 0.2115
[17:52:33.167] Epoch [4/5] Iteration [1810/2976]: Loss: 0.1951, CE: 0.1092
[17:52:37.710] Epoch [4/5] Iteration [1820/2976]: Loss: 0.1974, CE: 0.1147
[17:52:42.227] Epoch [4/5] Iteration [1830/2976]: Loss: 0.1540, CE: 0.1280
[17:52:46.786] Epoch [4/5] Iteration [1840/2976]: Loss: 0.2509, CE: 0.2479
[17:52:51.325] Epoch [4/5] Iteration [1850/2976]: Loss: 0.2213, CE: 0.1740
[17:52:55.882] Epoch [4/5] Iteration [1860/2976]: Loss: 0.1681, CE: 0.1651
[17:53:00.425] Epoch [4/5] Iteration [1870/2976]: Loss: 0.2229, CE: 0.1778
[17:53:04.972] Epoch [4/5] Iteration [1880/2976]: Loss: 0.2025, CE: 0.1278
[17:53:09.511] Epoch [4/5] Iteration [1890/2976]: Loss: 0.2149, CE: 0.1554
[17:53:14.074] Epoch [4/5] Iteration [1900/2976]: Loss: 0.1323, CE: 0.0766
[17:53:18.630] Epoch [4/5] Iteration [1910/2976]: Loss: 0.1930, CE: 0.1035
[17:53:23.199] Epoch [4/5] Iteration [1920/2976]: Loss: 0.2332, CE: 0.2033
[17:53:27.721] Epoch [4/5] Iteration [1930/2976]: Loss: 0.1866, CE: 0.0879
[17:53:32.260] Epoch [4/5] Iteration [1940/2976]: Loss: 0.2211, CE: 0.1735
[17:53:36.800] Epoch [4/5] Iteration [1950/2976]: Loss: 0.2153, CE: 0.1595
[17:53:41.349] Epoch [4/5] Iteration [1960/2976]: Loss: 0.2120, CE: 0.1514
[17:53:45.878] Epoch [4/5] Iteration [1970/2976]: Loss: 0.1949, CE: 0.1078
[17:53:50.443] Epoch [4/5] Iteration [1980/2976]: Loss: 0.1594, CE: 0.1445
[17:53:54.979] Epoch [4/5] Iteration [1990/2976]: Loss: 0.2000, CE: 0.1168
[17:53:59.538] Epoch [4/5] Iteration [2000/2976]: Loss: 0.2060, CE: 0.1361
[17:54:04.090] Epoch [4/5] Iteration [2010/2976]: Loss: 0.1712, CE: 0.0497
[17:54:08.632] Epoch [4/5] Iteration [2020/2976]: Loss: 0.2094, CE: 0.1402
[17:54:13.179] Epoch [4/5] Iteration [2030/2976]: Loss: 0.1887, CE: 0.0930
[17:54:17.730] Epoch [4/5] Iteration [2040/2976]: Loss: 0.2100, CE: 0.1455
[17:54:22.275] Epoch [4/5] Iteration [2050/2976]: Loss: 0.1985, CE: 0.1147
[17:54:26.821] Epoch [4/5] Iteration [2060/2976]: Loss: 0.2055, CE: 0.1351
[17:54:31.337] Epoch [4/5] Iteration [2070/2976]: Loss: 0.2302, CE: 0.1964
[17:54:35.879] Epoch [4/5] Iteration [2080/2976]: Loss: 0.1371, CE: 0.0888
[17:54:40.416] Epoch [4/5] Iteration [2090/2976]: Loss: 0.1962, CE: 0.1120
[17:54:44.949] Epoch [4/5] Iteration [2100/2976]: Loss: 0.2202, CE: 0.1706
[17:54:49.493] Epoch [4/5] Iteration [2110/2976]: Loss: 0.2254, CE: 0.1844
[17:54:54.062] Epoch [4/5] Iteration [2120/2976]: Loss: 0.2717, CE: 0.2999
[17:54:58.607] Epoch [4/5] Iteration [2130/2976]: Loss: 0.1853, CE: 0.0833
[17:55:03.164] Epoch [4/5] Iteration [2140/2976]: Loss: 0.2149, CE: 0.1574
[17:55:07.707] Epoch [4/5] Iteration [2150/2976]: Loss: 0.2038, CE: 0.1271
[17:55:12.247] Epoch [4/5] Iteration [2160/2976]: Loss: 0.2063, CE: 0.1365
[17:55:16.796] Epoch [4/5] Iteration [2170/2976]: Loss: 0.2102, CE: 0.1467
[17:55:21.361] Epoch [4/5] Iteration [2180/2976]: Loss: 0.1609, CE: 0.1471
[17:55:25.888] Epoch [4/5] Iteration [2190/2976]: Loss: 0.2675, CE: 0.2887
[17:55:30.424] Epoch [4/5] Iteration [2200/2976]: Loss: 0.2168, CE: 0.1634
[17:55:34.961] Epoch [4/5] Iteration [2210/2976]: Loss: 0.1438, CE: 0.1057
[17:55:39.529] Epoch [4/5] Iteration [2220/2976]: Loss: 0.2158, CE: 0.1582
[17:55:44.073] Epoch [4/5] Iteration [2230/2976]: Loss: 0.1754, CE: 0.0597
[17:55:48.614] Epoch [4/5] Iteration [2240/2976]: Loss: 0.1791, CE: 0.0694
[17:55:53.142] Epoch [4/5] Iteration [2250/2976]: Loss: 0.1798, CE: 0.0713
[17:55:57.688] Epoch [4/5] Iteration [2260/2976]: Loss: 0.1974, CE: 0.1143
[17:56:02.211] Epoch [4/5] Iteration [2270/2976]: Loss: 0.2089, CE: 0.1435
[17:56:06.765] Epoch [4/5] Iteration [2280/2976]: Loss: 0.1835, CE: 0.0800
[17:56:11.296] Epoch [4/5] Iteration [2290/2976]: Loss: 0.1524, CE: 0.1251
[17:56:15.850] Epoch [4/5] Iteration [2300/2976]: Loss: 0.2280, CE: 0.1916
[17:56:20.417] Epoch [4/5] Iteration [2310/2976]: Loss: 0.2176, CE: 0.1652
[17:56:24.954] Epoch [4/5] Iteration [2320/2976]: Loss: 0.1566, CE: 0.1369
[17:56:29.494] Epoch [4/5] Iteration [2330/2976]: Loss: 0.2236, CE: 0.1768
[17:56:34.041] Epoch [4/5] Iteration [2340/2976]: Loss: 0.2108, CE: 0.1476
[17:56:38.590] Epoch [4/5] Iteration [2350/2976]: Loss: 0.1814, CE: 0.0744
[17:56:43.128] Epoch [4/5] Iteration [2360/2976]: Loss: 0.1374, CE: 0.0856
[17:56:47.710] Epoch [4/5] Iteration [2370/2976]: Loss: 0.2186, CE: 0.1674
[17:56:52.254] Epoch [4/5] Iteration [2380/2976]: Loss: 0.2216, CE: 0.1750
[17:56:56.781] Epoch [4/5] Iteration [2390/2976]: Loss: 0.2019, CE: 0.1259
[17:57:01.322] Epoch [4/5] Iteration [2400/2976]: Loss: 0.2097, CE: 0.1447
[17:57:05.850] Epoch [4/5] Iteration [2410/2976]: Loss: 0.2373, CE: 0.2143
[17:57:10.401] Epoch [4/5] Iteration [2420/2976]: Loss: 0.1322, CE: 0.0770
[17:57:14.934] Epoch [4/5] Iteration [2430/2976]: Loss: 0.1974, CE: 0.1147
[17:57:19.479] Epoch [4/5] Iteration [2440/2976]: Loss: 0.1976, CE: 0.1133
[17:57:24.087] Epoch [4/5] Iteration [2450/2976]: Loss: 0.2292, CE: 0.1933
[17:57:28.633] Epoch [4/5] Iteration [2460/2976]: Loss: 0.1360, CE: 0.0840
[17:57:33.190] Epoch [4/5] Iteration [2470/2976]: Loss: 0.2222, CE: 0.1765
[17:57:37.750] Epoch [4/5] Iteration [2480/2976]: Loss: 0.0619, CE: 0.0264
[17:57:42.305] Epoch [4/5] Iteration [2490/2976]: Loss: 0.2010, CE: 0.1232
[17:57:46.869] Epoch [4/5] Iteration [2500/2976]: Loss: 0.2326, CE: 0.2026
[17:57:51.413] Epoch [4/5] Iteration [2510/2976]: Loss: 0.2266, CE: 0.1867
[17:57:55.964] Epoch [4/5] Iteration [2520/2976]: Loss: 0.1993, CE: 0.1198
[17:58:00.515] Epoch [4/5] Iteration [2530/2976]: Loss: 0.1876, CE: 0.0888
[17:58:05.055] Epoch [4/5] Iteration [2540/2976]: Loss: 0.1954, CE: 0.1073
[17:58:09.605] Epoch [4/5] Iteration [2550/2976]: Loss: 0.2073, CE: 0.1398
[17:58:14.170] Epoch [4/5] Iteration [2560/2976]: Loss: 0.1729, CE: 0.1785
[17:58:18.765] Epoch [4/5] Iteration [2570/2976]: Loss: 0.2030, CE: 0.1273
[17:58:23.323] Epoch [4/5] Iteration [2580/2976]: Loss: 0.2010, CE: 0.1217
[17:58:27.860] Epoch [4/5] Iteration [2590/2976]: Loss: 0.1750, CE: 0.0589
[17:58:32.435] Epoch [4/5] Iteration [2600/2976]: Loss: 0.2276, CE: 0.1905
[17:58:36.987] Epoch [4/5] Iteration [2610/2976]: Loss: 0.2056, CE: 0.1347
[17:58:41.531] Epoch [4/5] Iteration [2620/2976]: Loss: 0.2024, CE: 0.1254
[17:58:46.087] Epoch [4/5] Iteration [2630/2976]: Loss: 0.2415, CE: 0.2248
[17:58:50.613] Epoch [4/5] Iteration [2640/2976]: Loss: 0.2142, CE: 0.1563
[17:58:55.142] Epoch [4/5] Iteration [2650/2976]: Loss: 0.2358, CE: 0.2092
[17:58:59.673] Epoch [4/5] Iteration [2660/2976]: Loss: 0.1810, CE: 0.0737
[17:59:04.198] Epoch [4/5] Iteration [2670/2976]: Loss: 0.1957, CE: 0.1108
[17:59:08.743] Epoch [4/5] Iteration [2680/2976]: Loss: 0.2021, CE: 0.1269
[17:59:13.292] Epoch [4/5] Iteration [2690/2976]: Loss: 0.2010, CE: 0.1228
[17:59:17.838] Epoch [4/5] Iteration [2700/2976]: Loss: 0.2169, CE: 0.1634
[17:59:22.379] Epoch [4/5] Iteration [2710/2976]: Loss: 0.1778, CE: 0.0661
[17:59:26.950] Epoch [4/5] Iteration [2720/2976]: Loss: 0.2341, CE: 0.2050
[17:59:31.489] Epoch [4/5] Iteration [2730/2976]: Loss: 0.2126, CE: 0.1517
[17:59:36.053] Epoch [4/5] Iteration [2740/2976]: Loss: 0.1799, CE: 0.0675
[17:59:40.585] Epoch [4/5] Iteration [2750/2976]: Loss: 0.1968, CE: 0.1135
[17:59:45.133] Epoch [4/5] Iteration [2760/2976]: Loss: 0.2569, CE: 0.2628
[17:59:49.665] Epoch [4/5] Iteration [2770/2976]: Loss: 0.2044, CE: 0.1313
[17:59:54.248] Epoch [4/5] Iteration [2780/2976]: Loss: 0.1460, CE: 0.1104
[17:59:58.804] Epoch [4/5] Iteration [2790/2976]: Loss: 0.2372, CE: 0.2138
[18:00:03.345] Epoch [4/5] Iteration [2800/2976]: Loss: 0.1476, CE: 0.1153
[18:00:07.900] Epoch [4/5] Iteration [2810/2976]: Loss: 0.2060, CE: 0.1309
[18:00:12.444] Epoch [4/5] Iteration [2820/2976]: Loss: 0.2066, CE: 0.1273
[18:00:17.000] Epoch [4/5] Iteration [2830/2976]: Loss: 0.1326, CE: 0.0777
[18:00:21.545] Epoch [4/5] Iteration [2840/2976]: Loss: 0.1876, CE: 0.0902
[18:00:26.082] Epoch [4/5] Iteration [2850/2976]: Loss: 0.2081, CE: 0.1403
[18:00:30.632] Epoch [4/5] Iteration [2860/2976]: Loss: 0.1468, CE: 0.1130
[18:00:35.167] Epoch [4/5] Iteration [2870/2976]: Loss: 0.2583, CE: 0.2666
[18:00:39.719] Epoch [4/5] Iteration [2880/2976]: Loss: 0.2191, CE: 0.1683
[18:00:44.255] Epoch [4/5] Iteration [2890/2976]: Loss: 0.2252, CE: 0.1828
[18:00:48.812] Epoch [4/5] Iteration [2900/2976]: Loss: 0.2054, CE: 0.1348
[18:00:53.372] Epoch [4/5] Iteration [2910/2976]: Loss: 0.2278, CE: 0.1834
[18:00:57.921] Epoch [4/5] Iteration [2920/2976]: Loss: 0.2080, CE: 0.1413
[18:01:02.476] Epoch [4/5] Iteration [2930/2976]: Loss: 0.1915, CE: 0.0984
[18:01:06.998] Epoch [4/5] Iteration [2940/2976]: Loss: 0.1920, CE: 0.1006
[18:01:11.545] Epoch [4/5] Iteration [2950/2976]: Loss: 0.1906, CE: 0.0977
[18:01:16.098] Epoch [4/5] Iteration [2960/2976]: Loss: 0.1579, CE: 0.1406
[18:01:20.653] Epoch [4/5] Iteration [2970/2976]: Loss: 0.2266, CE: 0.1875
[18:01:23.537] Epoch [4/5] Average Loss: 0.1934, CE: 0.1310, Dice: 0.2349
[18:01:23.733] Saved continual learning checkpoint to ./universal/synapse_to_kits23_tpgm\epoch_4_continual.pth
[18:01:23.895] Saved final continual model to ./universal/synapse_to_kits23_tpgm\final_continual_model.pth
[22:55:02.697] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=5, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', dataset_fraction=0.25, enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=False, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[22:55:02.697] Continual Learning: 9 -> 12 classes
[22:55:02.697] Task offset: 9, New classes: 4
[22:55:02.697] Dataset fraction: 0.25
[22:55:02.697] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[22:55:02.697] Old task classes: 9, New task classes: 4
[22:55:02.793] Identified output layer keys: ['cswin_unet.output.weight']
[22:55:02.793] Loaded 462 backbone layers from pretrained model.
[22:55:02.794] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[22:55:02.826] Successfully adapted model for continual learning.
[22:55:02.827] Pretrained weights loaded and adapted for continual learning
[22:55:02.841] Using 23805/95221 samples (25.00% of dataset)
[22:55:02.843] Surgical fine-tuning enabled for continual learning
[22:55:02.846] 744 iterations per epoch. 3720 max iterations
[22:55:29.995] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=5, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', dataset_fraction=0.25, enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=False, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[22:55:29.995] Continual Learning: 9 -> 12 classes
[22:55:29.995] Task offset: 9, New classes: 4
[22:55:29.995] Dataset fraction: 0.25
[22:55:29.995] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[22:55:29.996] Old task classes: 9, New task classes: 4
[22:55:30.038] Identified output layer keys: ['cswin_unet.output.weight']
[22:55:30.039] Loaded 462 backbone layers from pretrained model.
[22:55:30.039] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[22:55:30.072] Successfully adapted model for continual learning.
[22:55:30.073] Pretrained weights loaded and adapted for continual learning
[22:55:30.083] Using 23805/95221 samples (25.00% of dataset)
[22:55:30.085] Surgical fine-tuning enabled for continual learning
[22:55:30.087] 744 iterations per epoch. 3720 max iterations
[22:55:30.088] Updating surgical weights at epoch 0
[22:55:53.696] Surgical weights range: [0.0002, 1.0000]
[22:56:14.022] Epoch [0/5] Iteration [0/744]: Loss: 0.5991, CE: 0.1218
[22:56:18.441] Epoch [0/5] Iteration [10/744]: Loss: 0.6131, CE: 0.1637
[22:56:22.909] Epoch [0/5] Iteration [20/744]: Loss: 0.5387, CE: 0.1051
[22:56:27.368] Epoch [0/5] Iteration [30/744]: Loss: 0.3729, CE: 0.0904
[22:56:31.840] Epoch [0/5] Iteration [40/744]: Loss: 0.2826, CE: 0.1514
[22:56:36.300] Epoch [0/5] Iteration [50/744]: Loss: 0.2414, CE: 0.0987
[22:56:40.763] Epoch [0/5] Iteration [60/744]: Loss: 0.2756, CE: 0.2093
[22:56:45.229] Epoch [0/5] Iteration [70/744]: Loss: 0.2235, CE: 0.1250
[22:56:49.718] Epoch [0/5] Iteration [80/744]: Loss: 0.2198, CE: 0.1505
[22:56:54.205] Epoch [0/5] Iteration [90/744]: Loss: 0.2099, CE: 0.1367
[22:56:58.677] Epoch [0/5] Iteration [100/744]: Loss: 0.1981, CE: 0.1109
[22:57:03.141] Epoch [0/5] Iteration [110/744]: Loss: 0.2396, CE: 0.2145
[22:57:07.619] Epoch [0/5] Iteration [120/744]: Loss: 0.1530, CE: 0.1240
[22:57:12.094] Epoch [0/5] Iteration [130/744]: Loss: 0.0729, CE: 0.0501
[22:57:16.560] Epoch [0/5] Iteration [140/744]: Loss: 0.1981, CE: 0.1117
[22:57:21.031] Epoch [0/5] Iteration [150/744]: Loss: 0.1872, CE: 0.0844
[22:57:25.510] Epoch [0/5] Iteration [160/744]: Loss: 0.1896, CE: 0.0922
[22:57:29.988] Epoch [0/5] Iteration [170/744]: Loss: 0.1987, CE: 0.1142
[22:57:34.496] Epoch [0/5] Iteration [180/744]: Loss: 0.2004, CE: 0.1159
[22:57:38.979] Epoch [0/5] Iteration [190/744]: Loss: 0.2702, CE: 0.2923
[22:57:43.491] Epoch [0/5] Iteration [200/744]: Loss: 0.2109, CE: 0.1436
[22:57:47.977] Epoch [0/5] Iteration [210/744]: Loss: 0.2059, CE: 0.1327
[22:57:52.449] Epoch [0/5] Iteration [220/744]: Loss: 0.2459, CE: 0.2318
[22:57:56.940] Epoch [0/5] Iteration [230/744]: Loss: 0.1616, CE: 0.1447
[22:58:01.423] Epoch [0/5] Iteration [240/744]: Loss: 0.1488, CE: 0.1147
[22:58:05.915] Epoch [0/5] Iteration [250/744]: Loss: 0.2096, CE: 0.1411
[22:58:10.416] Epoch [0/5] Iteration [260/744]: Loss: 0.1419, CE: 0.0971
[22:58:14.907] Epoch [0/5] Iteration [270/744]: Loss: 0.2107, CE: 0.1419
[22:58:19.399] Epoch [0/5] Iteration [280/744]: Loss: 0.1489, CE: 0.1148
[22:58:23.889] Epoch [0/5] Iteration [290/744]: Loss: 0.2186, CE: 0.1644
[22:58:28.372] Epoch [0/5] Iteration [300/744]: Loss: 0.2169, CE: 0.1601
[22:58:32.879] Epoch [0/5] Iteration [310/744]: Loss: 0.1959, CE: 0.1062
[22:58:37.393] Epoch [0/5] Iteration [320/744]: Loss: 0.2063, CE: 0.1327
[22:58:41.894] Epoch [0/5] Iteration [330/744]: Loss: 0.2327, CE: 0.1984
[22:58:46.380] Epoch [0/5] Iteration [340/744]: Loss: 0.1449, CE: 0.1047
[22:58:50.873] Epoch [0/5] Iteration [350/744]: Loss: 0.2192, CE: 0.1649
[22:58:55.383] Epoch [0/5] Iteration [360/744]: Loss: 0.2253, CE: 0.1812
[22:58:59.876] Epoch [0/5] Iteration [370/744]: Loss: 0.2371, CE: 0.2093
[22:59:04.368] Epoch [0/5] Iteration [380/744]: Loss: 0.2038, CE: 0.1266
[22:59:08.864] Epoch [0/5] Iteration [390/744]: Loss: 0.1495, CE: 0.1161
[22:59:13.362] Epoch [0/5] Iteration [400/744]: Loss: 0.2058, CE: 0.1284
[22:59:17.851] Epoch [0/5] Iteration [410/744]: Loss: 0.2265, CE: 0.1818
[22:59:22.356] Epoch [0/5] Iteration [420/744]: Loss: 0.1871, CE: 0.0797
[22:59:26.843] Epoch [0/5] Iteration [430/744]: Loss: 0.2125, CE: 0.1484
[22:59:31.331] Epoch [0/5] Iteration [440/744]: Loss: 0.1930, CE: 0.1006
[22:59:35.829] Epoch [0/5] Iteration [450/744]: Loss: 0.1822, CE: 0.0735
[22:59:40.332] Epoch [0/5] Iteration [460/744]: Loss: 0.2338, CE: 0.2022
[22:59:44.819] Epoch [0/5] Iteration [470/744]: Loss: 0.2131, CE: 0.1507
[22:59:49.306] Epoch [0/5] Iteration [480/744]: Loss: 0.1975, CE: 0.1108
[22:59:53.802] Epoch [0/5] Iteration [490/744]: Loss: 0.1302, CE: 0.0655
[22:59:58.305] Epoch [0/5] Iteration [500/744]: Loss: 0.2036, CE: 0.1274
[23:00:02.780] Epoch [0/5] Iteration [510/744]: Loss: 0.1867, CE: 0.0831
[23:00:07.286] Epoch [0/5] Iteration [520/744]: Loss: 0.2206, CE: 0.1693
[23:00:11.860] Epoch [0/5] Iteration [530/744]: Loss: 0.1542, CE: 0.1287
[23:00:16.372] Epoch [0/5] Iteration [540/744]: Loss: 0.2069, CE: 0.1318
[23:00:20.877] Epoch [0/5] Iteration [550/744]: Loss: 0.2174, CE: 0.1593
[23:00:25.382] Epoch [0/5] Iteration [560/744]: Loss: 0.1796, CE: 0.0675
[23:00:29.879] Epoch [0/5] Iteration [570/744]: Loss: 0.1953, CE: 0.1035
[23:00:34.385] Epoch [0/5] Iteration [580/744]: Loss: 0.1849, CE: 0.0804
[23:00:38.866] Epoch [0/5] Iteration [590/744]: Loss: 0.2202, CE: 0.1683
[23:00:43.366] Epoch [0/5] Iteration [600/744]: Loss: 0.2106, CE: 0.1419
[23:00:47.864] Epoch [0/5] Iteration [610/744]: Loss: 0.2136, CE: 0.1502
[23:00:52.364] Epoch [0/5] Iteration [620/744]: Loss: 0.2163, CE: 0.1584
[23:00:56.866] Epoch [0/5] Iteration [630/744]: Loss: 0.2450, CE: 0.2280
[23:01:01.365] Epoch [0/5] Iteration [640/744]: Loss: 0.1223, CE: 0.0478
[23:01:05.858] Epoch [0/5] Iteration [650/744]: Loss: 0.1726, CE: 0.0486
[23:01:10.361] Epoch [0/5] Iteration [660/744]: Loss: 0.2151, CE: 0.1559
[23:01:14.856] Epoch [0/5] Iteration [670/744]: Loss: 0.1876, CE: 0.0874
[23:01:19.355] Epoch [0/5] Iteration [680/744]: Loss: 0.1759, CE: 0.0573
[23:01:23.850] Epoch [0/5] Iteration [690/744]: Loss: 0.2162, CE: 0.1574
[23:01:28.371] Epoch [0/5] Iteration [700/744]: Loss: 0.2201, CE: 0.1676
[23:01:32.889] Epoch [0/5] Iteration [710/744]: Loss: 0.1450, CE: 0.1030
[23:01:37.402] Epoch [0/5] Iteration [720/744]: Loss: 0.2296, CE: 0.1905
[23:01:41.898] Epoch [0/5] Iteration [730/744]: Loss: 0.2031, CE: 0.1263
[23:01:46.404] Epoch [0/5] Iteration [740/744]: Loss: 0.2174, CE: 0.1599
[23:01:48.449] Epoch [0/5] Average Loss: 0.2167, CE: 0.1397, Dice: 0.2679
[23:02:09.093] Epoch [1/5] Iteration [0/744]: Loss: 0.2033, CE: 0.1267
[23:02:13.575] Epoch [1/5] Iteration [10/744]: Loss: 0.2117, CE: 0.1465
[23:02:18.066] Epoch [1/5] Iteration [20/744]: Loss: 0.1426, CE: 0.0994
[23:02:22.568] Epoch [1/5] Iteration [30/744]: Loss: 0.1874, CE: 0.0853
[23:02:27.062] Epoch [1/5] Iteration [40/744]: Loss: 0.1649, CE: 0.1509
[23:02:31.564] Epoch [1/5] Iteration [50/744]: Loss: 0.1957, CE: 0.1073
[23:02:36.078] Epoch [1/5] Iteration [60/744]: Loss: 0.1302, CE: 0.0688
[23:02:40.573] Epoch [1/5] Iteration [70/744]: Loss: 0.2133, CE: 0.1488
[23:02:45.056] Epoch [1/5] Iteration [80/744]: Loss: 0.2118, CE: 0.1474
[23:02:49.544] Epoch [1/5] Iteration [90/744]: Loss: 0.2025, CE: 0.1228
[23:02:54.047] Epoch [1/5] Iteration [100/744]: Loss: 0.1341, CE: 0.0783
[23:02:58.538] Epoch [1/5] Iteration [110/744]: Loss: 0.1425, CE: 0.0958
[23:03:03.045] Epoch [1/5] Iteration [120/744]: Loss: 0.1839, CE: 0.0772
[23:03:07.537] Epoch [1/5] Iteration [130/744]: Loss: 0.2270, CE: 0.1848
[23:03:12.034] Epoch [1/5] Iteration [140/744]: Loss: 0.1589, CE: 0.1402
[23:03:16.526] Epoch [1/5] Iteration [150/744]: Loss: 0.2084, CE: 0.1364
[23:03:21.037] Epoch [1/5] Iteration [160/744]: Loss: 0.1965, CE: 0.1094
[23:03:25.528] Epoch [1/5] Iteration [170/744]: Loss: 0.2055, CE: 0.1312
[23:03:30.027] Epoch [1/5] Iteration [180/744]: Loss: 0.2364, CE: 0.2084
[23:03:34.534] Epoch [1/5] Iteration [190/744]: Loss: 0.2279, CE: 0.1874
[23:03:39.041] Epoch [1/5] Iteration [200/744]: Loss: 0.2084, CE: 0.1390
[23:03:43.540] Epoch [1/5] Iteration [210/744]: Loss: 0.2337, CE: 0.2027
[23:03:48.049] Epoch [1/5] Iteration [220/744]: Loss: 0.2339, CE: 0.2026
[23:03:52.559] Epoch [1/5] Iteration [230/744]: Loss: 0.1942, CE: 0.1023
[23:03:57.068] Epoch [1/5] Iteration [240/744]: Loss: 0.2058, CE: 0.1310
[23:04:01.559] Epoch [1/5] Iteration [250/744]: Loss: 0.1567, CE: 0.1339
[23:04:06.077] Epoch [1/5] Iteration [260/744]: Loss: 0.2080, CE: 0.1348
[23:04:10.597] Epoch [1/5] Iteration [270/744]: Loss: 0.2197, CE: 0.1661
[23:04:15.107] Epoch [1/5] Iteration [280/744]: Loss: 0.1653, CE: 0.1536
[23:04:19.601] Epoch [1/5] Iteration [290/744]: Loss: 0.1432, CE: 0.1002
[23:04:24.113] Epoch [1/5] Iteration [300/744]: Loss: 0.2026, CE: 0.1231
[23:04:28.600] Epoch [1/5] Iteration [310/744]: Loss: 0.2057, CE: 0.1321
[23:04:33.114] Epoch [1/5] Iteration [320/744]: Loss: 0.1970, CE: 0.1104
[23:04:37.611] Epoch [1/5] Iteration [330/744]: Loss: 0.1934, CE: 0.0973
[23:04:42.122] Epoch [1/5] Iteration [340/744]: Loss: 0.1743, CE: 0.0526
[23:04:46.615] Epoch [1/5] Iteration [350/744]: Loss: 0.1932, CE: 0.1013
[23:04:51.117] Epoch [1/5] Iteration [360/744]: Loss: 0.2040, CE: 0.1275
[23:04:55.615] Epoch [1/5] Iteration [370/744]: Loss: 0.2459, CE: 0.2289
[23:05:00.141] Epoch [1/5] Iteration [380/744]: Loss: 0.2072, CE: 0.1352
[23:05:04.637] Epoch [1/5] Iteration [390/744]: Loss: 0.1468, CE: 0.1068
[23:05:09.151] Epoch [1/5] Iteration [400/744]: Loss: 0.2158, CE: 0.1568
[23:05:13.652] Epoch [1/5] Iteration [410/744]: Loss: 0.2112, CE: 0.1449
[23:05:18.161] Epoch [1/5] Iteration [420/744]: Loss: 0.2287, CE: 0.1897
[23:05:22.672] Epoch [1/5] Iteration [430/744]: Loss: 0.1947, CE: 0.1045
[23:05:27.183] Epoch [1/5] Iteration [440/744]: Loss: 0.1419, CE: 0.0963
[23:05:31.676] Epoch [1/5] Iteration [450/744]: Loss: 0.1834, CE: 0.0748
[23:05:36.187] Epoch [1/5] Iteration [460/744]: Loss: 0.2269, CE: 0.1855
[23:05:40.684] Epoch [1/5] Iteration [470/744]: Loss: 0.1947, CE: 0.1046
[23:05:45.212] Epoch [1/5] Iteration [480/744]: Loss: 0.2138, CE: 0.1517
[23:05:49.716] Epoch [1/5] Iteration [490/744]: Loss: 0.2025, CE: 0.1246
[23:05:54.217] Epoch [1/5] Iteration [500/744]: Loss: 0.2237, CE: 0.1768
[23:05:58.719] Epoch [1/5] Iteration [510/744]: Loss: 0.2317, CE: 0.1966
[23:06:03.232] Epoch [1/5] Iteration [520/744]: Loss: 0.1801, CE: 0.0675
[23:06:07.741] Epoch [1/5] Iteration [530/744]: Loss: 0.2282, CE: 0.1884
[23:06:12.255] Epoch [1/5] Iteration [540/744]: Loss: 0.2115, CE: 0.1459
[23:06:16.752] Epoch [1/5] Iteration [550/744]: Loss: 0.1142, CE: 0.0291
[23:06:21.250] Epoch [1/5] Iteration [560/744]: Loss: 0.2315, CE: 0.1954
[23:06:25.747] Epoch [1/5] Iteration [570/744]: Loss: 0.1725, CE: 0.0499
[23:06:30.252] Epoch [1/5] Iteration [580/744]: Loss: 0.1462, CE: 0.1062
[23:06:34.744] Epoch [1/5] Iteration [590/744]: Loss: 0.1857, CE: 0.0822
[23:06:39.238] Epoch [1/5] Iteration [600/744]: Loss: 0.2367, CE: 0.2069
[23:06:43.747] Epoch [1/5] Iteration [610/744]: Loss: 0.2035, CE: 0.1261
[23:06:48.274] Epoch [1/5] Iteration [620/744]: Loss: 0.2367, CE: 0.2094
[23:06:52.766] Epoch [1/5] Iteration [630/744]: Loss: 0.1881, CE: 0.0883
[23:06:57.287] Epoch [1/5] Iteration [640/744]: Loss: 0.2181, CE: 0.1637
[23:07:01.783] Epoch [1/5] Iteration [650/744]: Loss: 0.2150, CE: 0.1557
[23:07:06.296] Epoch [1/5] Iteration [660/744]: Loss: 0.1997, CE: 0.1170
[23:07:10.800] Epoch [1/5] Iteration [670/744]: Loss: 0.2152, CE: 0.1557
[23:07:15.305] Epoch [1/5] Iteration [680/744]: Loss: 0.2138, CE: 0.1504
[23:07:19.798] Epoch [1/5] Iteration [690/744]: Loss: 0.2438, CE: 0.2267
[23:07:24.322] Epoch [1/5] Iteration [700/744]: Loss: 0.2290, CE: 0.1894
[23:07:28.844] Epoch [1/5] Iteration [710/744]: Loss: 0.1600, CE: 0.1427
[23:07:33.356] Epoch [1/5] Iteration [720/744]: Loss: 0.1893, CE: 0.0914
[23:07:37.865] Epoch [1/5] Iteration [730/744]: Loss: 0.2117, CE: 0.1474
[23:07:42.375] Epoch [1/5] Iteration [740/744]: Loss: 0.2164, CE: 0.1576
[23:07:44.415] Epoch [1/5] Average Loss: 0.1995, CE: 0.1415, Dice: 0.2383
[23:08:04.789] Epoch [2/5] Iteration [0/744]: Loss: 0.2003, CE: 0.1172
[23:08:09.288] Epoch [2/5] Iteration [10/744]: Loss: 0.2149, CE: 0.1530
[23:08:13.780] Epoch [2/5] Iteration [20/744]: Loss: 0.2160, CE: 0.1572
[23:08:18.282] Epoch [2/5] Iteration [30/744]: Loss: 0.2087, CE: 0.1392
[23:08:22.800] Epoch [2/5] Iteration [40/744]: Loss: 0.2136, CE: 0.1506
[23:08:27.292] Epoch [2/5] Iteration [50/744]: Loss: 0.1500, CE: 0.1167
[23:08:31.795] Epoch [2/5] Iteration [60/744]: Loss: 0.1919, CE: 0.0969
[23:08:36.303] Epoch [2/5] Iteration [70/744]: Loss: 0.1923, CE: 0.0983
[23:08:40.803] Epoch [2/5] Iteration [80/744]: Loss: 0.2057, CE: 0.1307
[23:08:45.284] Epoch [2/5] Iteration [90/744]: Loss: 0.1981, CE: 0.1133
[23:08:49.783] Epoch [2/5] Iteration [100/744]: Loss: 0.2141, CE: 0.1536
[23:08:54.270] Epoch [2/5] Iteration [110/744]: Loss: 0.1947, CE: 0.1056
[23:08:58.786] Epoch [2/5] Iteration [120/744]: Loss: 0.1844, CE: 0.0794
[23:09:03.286] Epoch [2/5] Iteration [130/744]: Loss: 0.2152, CE: 0.1568
[23:09:07.790] Epoch [2/5] Iteration [140/744]: Loss: 0.2383, CE: 0.2129
[23:09:12.287] Epoch [2/5] Iteration [150/744]: Loss: 0.2089, CE: 0.1399
[23:09:16.778] Epoch [2/5] Iteration [160/744]: Loss: 0.2020, CE: 0.2474
[23:09:21.274] Epoch [2/5] Iteration [170/744]: Loss: 0.1987, CE: 0.1152
[23:09:25.777] Epoch [2/5] Iteration [180/744]: Loss: 0.1617, CE: 0.1466
[23:09:30.332] Epoch [2/5] Iteration [190/744]: Loss: 0.2054, CE: 0.1295
[23:09:34.842] Epoch [2/5] Iteration [200/744]: Loss: 0.2489, CE: 0.2399
[23:09:39.335] Epoch [2/5] Iteration [210/744]: Loss: 0.2015, CE: 0.1210
[23:09:43.860] Epoch [2/5] Iteration [220/744]: Loss: 0.1465, CE: 0.1068
[23:09:48.371] Epoch [2/5] Iteration [230/744]: Loss: 0.1921, CE: 0.0987
[23:09:52.892] Epoch [2/5] Iteration [240/744]: Loss: 0.2065, CE: 0.1349
[23:09:57.398] Epoch [2/5] Iteration [250/744]: Loss: 0.2168, CE: 0.1580
[23:10:01.911] Epoch [2/5] Iteration [260/744]: Loss: 0.1841, CE: 0.0781
[23:10:06.406] Epoch [2/5] Iteration [270/744]: Loss: 0.2397, CE: 0.2175
[23:10:10.911] Epoch [2/5] Iteration [280/744]: Loss: 0.1941, CE: 0.1032
[23:10:15.410] Epoch [2/5] Iteration [290/744]: Loss: 0.2051, CE: 0.1312
[23:10:19.918] Epoch [2/5] Iteration [300/744]: Loss: 0.1570, CE: 0.1344
[23:10:24.428] Epoch [2/5] Iteration [310/744]: Loss: 0.2171, CE: 0.1582
[23:10:28.952] Epoch [2/5] Iteration [320/744]: Loss: 0.2238, CE: 0.1772
[23:10:33.442] Epoch [2/5] Iteration [330/744]: Loss: 0.2214, CE: 0.1703
[23:10:37.958] Epoch [2/5] Iteration [340/744]: Loss: 0.1921, CE: 0.0969
[23:10:42.455] Epoch [2/5] Iteration [350/744]: Loss: 0.1452, CE: 0.1058
[23:10:46.980] Epoch [2/5] Iteration [360/744]: Loss: 0.1127, CE: 0.0250
[23:10:51.506] Epoch [2/5] Iteration [370/744]: Loss: 0.1590, CE: 0.1404
[23:10:56.009] Epoch [2/5] Iteration [380/744]: Loss: 0.1551, CE: 0.1300
[23:11:00.504] Epoch [2/5] Iteration [390/744]: Loss: 0.2087, CE: 0.1387
[23:11:05.014] Epoch [2/5] Iteration [400/744]: Loss: 0.1936, CE: 0.0988
[23:11:09.501] Epoch [2/5] Iteration [410/744]: Loss: 0.2360, CE: 0.2063
[23:11:14.021] Epoch [2/5] Iteration [420/744]: Loss: 0.1454, CE: 0.1062
[23:11:18.512] Epoch [2/5] Iteration [430/744]: Loss: 0.2131, CE: 0.1493
[23:11:23.029] Epoch [2/5] Iteration [440/744]: Loss: 0.1949, CE: 0.1055
[23:11:27.526] Epoch [2/5] Iteration [450/744]: Loss: 0.2004, CE: 0.1189
[23:11:32.029] Epoch [2/5] Iteration [460/744]: Loss: 0.2388, CE: 0.2144
[23:11:36.545] Epoch [2/5] Iteration [470/744]: Loss: 0.1874, CE: 0.0855
[23:11:41.055] Epoch [2/5] Iteration [480/744]: Loss: 0.1418, CE: 0.0974
[23:11:45.554] Epoch [2/5] Iteration [490/744]: Loss: 0.1870, CE: 0.0823
[23:11:50.077] Epoch [2/5] Iteration [500/744]: Loss: 0.2219, CE: 0.1726
[23:11:54.587] Epoch [2/5] Iteration [510/744]: Loss: 0.1199, CE: 0.0429
[23:11:59.093] Epoch [2/5] Iteration [520/744]: Loss: 0.1544, CE: 0.1279
[23:12:03.600] Epoch [2/5] Iteration [530/744]: Loss: 0.2392, CE: 0.2105
[23:12:08.118] Epoch [2/5] Iteration [540/744]: Loss: 0.1553, CE: 0.1304
[23:12:12.635] Epoch [2/5] Iteration [550/744]: Loss: 0.2057, CE: 0.1316
[23:12:17.138] Epoch [2/5] Iteration [560/744]: Loss: 0.1634, CE: 0.1513
[23:12:21.642] Epoch [2/5] Iteration [570/744]: Loss: 0.2060, CE: 0.1327
[23:12:26.167] Epoch [2/5] Iteration [580/744]: Loss: 0.1936, CE: 0.1021
[23:12:30.673] Epoch [2/5] Iteration [590/744]: Loss: 0.1957, CE: 0.1079
[23:12:35.183] Epoch [2/5] Iteration [600/744]: Loss: 0.1863, CE: 0.0825
[23:12:39.682] Epoch [2/5] Iteration [610/744]: Loss: 0.2213, CE: 0.1702
[23:12:44.217] Epoch [2/5] Iteration [620/744]: Loss: 0.2017, CE: 0.1201
[23:12:48.731] Epoch [2/5] Iteration [630/744]: Loss: 0.2383, CE: 0.2123
[23:12:53.246] Epoch [2/5] Iteration [640/744]: Loss: 0.2119, CE: 0.1470
[23:12:57.751] Epoch [2/5] Iteration [650/744]: Loss: 0.1454, CE: 0.1052
[23:13:02.273] Epoch [2/5] Iteration [660/744]: Loss: 0.1883, CE: 0.0883
[23:13:06.780] Epoch [2/5] Iteration [670/744]: Loss: 0.2068, CE: 0.1351
[23:13:11.294] Epoch [2/5] Iteration [680/744]: Loss: 0.1772, CE: 0.0612
[23:13:15.801] Epoch [2/5] Iteration [690/744]: Loss: 0.1783, CE: 0.1870
[23:13:20.312] Epoch [2/5] Iteration [700/744]: Loss: 0.2100, CE: 0.1418
[23:13:24.813] Epoch [2/5] Iteration [710/744]: Loss: 0.1428, CE: 0.0990
[23:13:29.326] Epoch [2/5] Iteration [720/744]: Loss: 0.1175, CE: 0.0358
[23:13:33.829] Epoch [2/5] Iteration [730/744]: Loss: 0.2474, CE: 0.2347
[23:13:38.350] Epoch [2/5] Iteration [740/744]: Loss: 0.2027, CE: 0.1248
[23:13:40.389] Epoch [2/5] Average Loss: 0.1978, CE: 0.1414, Dice: 0.2354
[23:14:01.095] Epoch [3/5] Iteration [0/744]: Loss: 0.2289, CE: 0.1894
[23:14:05.596] Epoch [3/5] Iteration [10/744]: Loss: 0.2082, CE: 0.1356
[23:14:10.091] Epoch [3/5] Iteration [20/744]: Loss: 0.2167, CE: 0.1593
[23:14:14.582] Epoch [3/5] Iteration [30/744]: Loss: 0.2187, CE: 0.1645
[23:14:19.069] Epoch [3/5] Iteration [40/744]: Loss: 0.2150, CE: 0.1513
[23:14:23.574] Epoch [3/5] Iteration [50/744]: Loss: 0.2398, CE: 0.2145
[23:14:28.067] Epoch [3/5] Iteration [60/744]: Loss: 0.2272, CE: 0.1855
[23:14:32.555] Epoch [3/5] Iteration [70/744]: Loss: 0.1492, CE: 0.1159
[23:14:37.045] Epoch [3/5] Iteration [80/744]: Loss: 0.1511, CE: 0.1214
[23:14:41.554] Epoch [3/5] Iteration [90/744]: Loss: 0.1984, CE: 0.1143
[23:14:46.064] Epoch [3/5] Iteration [100/744]: Loss: 0.2030, CE: 0.1245
[23:14:50.581] Epoch [3/5] Iteration [110/744]: Loss: 0.1952, CE: 0.1029
[23:14:55.092] Epoch [3/5] Iteration [120/744]: Loss: 0.1784, CE: 0.1888
[23:14:59.590] Epoch [3/5] Iteration [130/744]: Loss: 0.1809, CE: 0.0676
[23:15:04.084] Epoch [3/5] Iteration [140/744]: Loss: 0.2129, CE: 0.1480
[23:15:08.585] Epoch [3/5] Iteration [150/744]: Loss: 0.2076, CE: 0.1374
[23:15:13.085] Epoch [3/5] Iteration [160/744]: Loss: 0.2208, CE: 0.1690
[23:15:17.607] Epoch [3/5] Iteration [170/744]: Loss: 0.2209, CE: 0.1699
[23:15:22.099] Epoch [3/5] Iteration [180/744]: Loss: 0.1367, CE: 0.0834
[23:15:26.593] Epoch [3/5] Iteration [190/744]: Loss: 0.2198, CE: 0.1671
[23:15:31.079] Epoch [3/5] Iteration [200/744]: Loss: 0.1975, CE: 0.1102
[23:15:35.588] Epoch [3/5] Iteration [210/744]: Loss: 0.1998, CE: 0.1179
[23:15:40.088] Epoch [3/5] Iteration [220/744]: Loss: 0.1987, CE: 0.1137
[23:15:44.598] Epoch [3/5] Iteration [230/744]: Loss: 0.2317, CE: 0.1969
[23:15:49.104] Epoch [3/5] Iteration [240/744]: Loss: 0.1929, CE: 0.1008
[23:15:53.617] Epoch [3/5] Iteration [250/744]: Loss: 0.2001, CE: 0.1161
[23:15:58.120] Epoch [3/5] Iteration [260/744]: Loss: 0.1949, CE: 0.1058
[23:16:02.637] Epoch [3/5] Iteration [270/744]: Loss: 0.1379, CE: 0.0882
[23:16:07.141] Epoch [3/5] Iteration [280/744]: Loss: 0.2062, CE: 0.1333
[23:16:11.662] Epoch [3/5] Iteration [290/744]: Loss: 0.2251, CE: 0.1808
[23:16:16.169] Epoch [3/5] Iteration [300/744]: Loss: 0.2125, CE: 0.1480
[23:16:20.686] Epoch [3/5] Iteration [310/744]: Loss: 0.2144, CE: 0.1542
[23:16:25.190] Epoch [3/5] Iteration [320/744]: Loss: 0.2230, CE: 0.1746
[23:16:29.712] Epoch [3/5] Iteration [330/744]: Loss: 0.1975, CE: 0.1120
[23:16:34.213] Epoch [3/5] Iteration [340/744]: Loss: 0.1956, CE: 0.1030
[23:16:38.724] Epoch [3/5] Iteration [350/744]: Loss: 0.2075, CE: 0.1362
[23:16:43.229] Epoch [3/5] Iteration [360/744]: Loss: 0.2041, CE: 0.1283
[23:16:47.754] Epoch [3/5] Iteration [370/744]: Loss: 0.1859, CE: 0.2078
[23:16:52.259] Epoch [3/5] Iteration [380/744]: Loss: 0.1409, CE: 0.0941
[23:16:56.772] Epoch [3/5] Iteration [390/744]: Loss: 0.1721, CE: 0.1679
[23:17:01.282] Epoch [3/5] Iteration [400/744]: Loss: 0.1476, CE: 0.1115
[23:17:05.809] Epoch [3/5] Iteration [410/744]: Loss: 0.2152, CE: 0.1562
[23:17:10.311] Epoch [3/5] Iteration [420/744]: Loss: 0.1589, CE: 0.1367
[23:17:14.831] Epoch [3/5] Iteration [430/744]: Loss: 0.1335, CE: 0.0755
[23:17:19.337] Epoch [3/5] Iteration [440/744]: Loss: 0.2833, CE: 0.3248
[23:17:23.851] Epoch [3/5] Iteration [450/744]: Loss: 0.2213, CE: 0.1710
[23:17:28.368] Epoch [3/5] Iteration [460/744]: Loss: 0.2309, CE: 0.1948
[23:17:32.872] Epoch [3/5] Iteration [470/744]: Loss: 0.1526, CE: 0.1244
[23:17:37.387] Epoch [3/5] Iteration [480/744]: Loss: 0.2314, CE: 0.1954
[23:17:41.905] Epoch [3/5] Iteration [490/744]: Loss: 0.1862, CE: 0.0823
[23:17:46.414] Epoch [3/5] Iteration [500/744]: Loss: 0.2009, CE: 0.1205
[23:17:50.932] Epoch [3/5] Iteration [510/744]: Loss: 0.2315, CE: 0.1909
[23:17:55.439] Epoch [3/5] Iteration [520/744]: Loss: 0.2187, CE: 0.1635
[23:17:59.957] Epoch [3/5] Iteration [530/744]: Loss: 0.2271, CE: 0.1854
[23:18:04.470] Epoch [3/5] Iteration [540/744]: Loss: 0.2139, CE: 0.1527
[23:18:08.991] Epoch [3/5] Iteration [550/744]: Loss: 0.2054, CE: 0.1309
[23:18:13.503] Epoch [3/5] Iteration [560/744]: Loss: 0.1454, CE: 0.1055
[23:18:18.021] Epoch [3/5] Iteration [570/744]: Loss: 0.2048, CE: 0.1294
[23:18:22.516] Epoch [3/5] Iteration [580/744]: Loss: 0.2142, CE: 0.1542
[23:18:27.045] Epoch [3/5] Iteration [590/744]: Loss: 0.2199, CE: 0.1642
[23:18:31.612] Epoch [3/5] Iteration [600/744]: Loss: 0.1949, CE: 0.1053
[23:18:36.121] Epoch [3/5] Iteration [610/744]: Loss: 0.2007, CE: 0.1187
[23:18:40.632] Epoch [3/5] Iteration [620/744]: Loss: 0.2237, CE: 0.1770
[23:18:45.151] Epoch [3/5] Iteration [630/744]: Loss: 0.2228, CE: 0.1739
[23:18:49.651] Epoch [3/5] Iteration [640/744]: Loss: 0.1981, CE: 0.1126
[23:18:54.180] Epoch [3/5] Iteration [650/744]: Loss: 0.1522, CE: 0.1239
[23:18:58.689] Epoch [3/5] Iteration [660/744]: Loss: 0.1895, CE: 0.0905
[23:19:03.218] Epoch [3/5] Iteration [670/744]: Loss: 0.2038, CE: 0.1281
[23:19:07.724] Epoch [3/5] Iteration [680/744]: Loss: 0.2022, CE: 0.1228
[23:19:12.258] Epoch [3/5] Iteration [690/744]: Loss: 0.2071, CE: 0.1335
[23:19:16.759] Epoch [3/5] Iteration [700/744]: Loss: 0.1591, CE: 0.1403
[23:19:21.276] Epoch [3/5] Iteration [710/744]: Loss: 0.2319, CE: 0.1975
[23:19:25.777] Epoch [3/5] Iteration [720/744]: Loss: 0.2282, CE: 0.1880
[23:19:30.296] Epoch [3/5] Iteration [730/744]: Loss: 0.1220, CE: 0.0413
[23:19:34.800] Epoch [3/5] Iteration [740/744]: Loss: 0.2117, CE: 0.1469
[23:19:36.921] Epoch [3/5] Average Loss: 0.1988, CE: 0.1415, Dice: 0.2370
[23:19:57.393] Epoch [4/5] Iteration [0/744]: Loss: 0.2089, CE: 0.1400
[23:20:01.897] Epoch [4/5] Iteration [10/744]: Loss: 0.1588, CE: 0.1382
[23:20:06.392] Epoch [4/5] Iteration [20/744]: Loss: 0.2135, CE: 0.1509
[23:20:10.894] Epoch [4/5] Iteration [30/744]: Loss: 0.2075, CE: 0.1360
[23:20:15.397] Epoch [4/5] Iteration [40/744]: Loss: 0.2445, CE: 0.2297
[23:20:19.906] Epoch [4/5] Iteration [50/744]: Loss: 0.1507, CE: 0.1184
[23:20:24.413] Epoch [4/5] Iteration [60/744]: Loss: 0.1376, CE: 0.0859
[23:20:28.918] Epoch [4/5] Iteration [70/744]: Loss: 0.1610, CE: 0.1437
[23:20:33.427] Epoch [4/5] Iteration [80/744]: Loss: 0.1912, CE: 0.0950
[23:20:37.955] Epoch [4/5] Iteration [90/744]: Loss: 0.1371, CE: 0.0848
[23:20:42.451] Epoch [4/5] Iteration [100/744]: Loss: 0.1861, CE: 0.2078
[23:20:46.996] Epoch [4/5] Iteration [110/744]: Loss: 0.1637, CE: 0.1511
[23:20:51.498] Epoch [4/5] Iteration [120/744]: Loss: 0.2042, CE: 0.1222
[23:20:56.002] Epoch [4/5] Iteration [130/744]: Loss: 0.1807, CE: 0.0696
[23:21:00.502] Epoch [4/5] Iteration [140/744]: Loss: 0.1533, CE: 0.1262
[23:21:05.012] Epoch [4/5] Iteration [150/744]: Loss: 0.1994, CE: 0.1155
[23:21:09.508] Epoch [4/5] Iteration [160/744]: Loss: 0.1979, CE: 0.1116
[23:21:14.035] Epoch [4/5] Iteration [170/744]: Loss: 0.2033, CE: 0.1267
[23:21:18.532] Epoch [4/5] Iteration [180/744]: Loss: 0.1600, CE: 0.1422
[23:21:23.043] Epoch [4/5] Iteration [190/744]: Loss: 0.1981, CE: 0.1127
[23:21:27.542] Epoch [4/5] Iteration [200/744]: Loss: 0.2178, CE: 0.1584
[23:21:32.048] Epoch [4/5] Iteration [210/744]: Loss: 0.1436, CE: 0.1026
[23:21:36.534] Epoch [4/5] Iteration [220/744]: Loss: 0.1902, CE: 0.0910
[23:21:41.041] Epoch [4/5] Iteration [230/744]: Loss: 0.2006, CE: 0.1197
[23:21:45.545] Epoch [4/5] Iteration [240/744]: Loss: 0.2143, CE: 0.1504
[23:21:50.065] Epoch [4/5] Iteration [250/744]: Loss: 0.2415, CE: 0.2198
[23:21:54.565] Epoch [4/5] Iteration [260/744]: Loss: 0.2038, CE: 0.1269
[23:21:59.075] Epoch [4/5] Iteration [270/744]: Loss: 0.2225, CE: 0.1744
[23:22:03.591] Epoch [4/5] Iteration [280/744]: Loss: 0.1387, CE: 0.0897
[23:22:08.091] Epoch [4/5] Iteration [290/744]: Loss: 0.2118, CE: 0.1434
[23:22:12.601] Epoch [4/5] Iteration [300/744]: Loss: 0.2575, CE: 0.2617
[23:22:17.105] Epoch [4/5] Iteration [310/744]: Loss: 0.2100, CE: 0.1424
[23:22:21.610] Epoch [4/5] Iteration [320/744]: Loss: 0.2035, CE: 0.1270
[23:22:26.146] Epoch [4/5] Iteration [330/744]: Loss: 0.2204, CE: 0.1690
[23:22:30.645] Epoch [4/5] Iteration [340/744]: Loss: 0.1977, CE: 0.1117
[23:22:35.158] Epoch [4/5] Iteration [350/744]: Loss: 0.1789, CE: 0.1902
[23:22:39.685] Epoch [4/5] Iteration [360/744]: Loss: 0.2144, CE: 0.1527
[23:22:44.203] Epoch [4/5] Iteration [370/744]: Loss: 0.1968, CE: 0.1106
[23:22:48.698] Epoch [4/5] Iteration [380/744]: Loss: 0.2031, CE: 0.1237
[23:22:53.205] Epoch [4/5] Iteration [390/744]: Loss: 0.1849, CE: 0.2053
[23:22:57.696] Epoch [4/5] Iteration [400/744]: Loss: 0.2047, CE: 0.1291
[23:23:02.226] Epoch [4/5] Iteration [410/744]: Loss: 0.2318, CE: 0.1972
[23:23:06.744] Epoch [4/5] Iteration [420/744]: Loss: 0.1939, CE: 0.1025
[23:23:11.258] Epoch [4/5] Iteration [430/744]: Loss: 0.1686, CE: 0.1597
[23:23:15.768] Epoch [4/5] Iteration [440/744]: Loss: 0.2083, CE: 0.1381
[23:23:20.296] Epoch [4/5] Iteration [450/744]: Loss: 0.2166, CE: 0.1575
[23:23:24.806] Epoch [4/5] Iteration [460/744]: Loss: 0.1543, CE: 0.1294
[23:23:29.325] Epoch [4/5] Iteration [470/744]: Loss: 0.1820, CE: 0.0732
[23:23:33.836] Epoch [4/5] Iteration [480/744]: Loss: 0.2266, CE: 0.1843
[23:23:38.337] Epoch [4/5] Iteration [490/744]: Loss: 0.1192, CE: 0.0414
[23:23:42.839] Epoch [4/5] Iteration [500/744]: Loss: 0.1916, CE: 0.0961
[23:23:47.363] Epoch [4/5] Iteration [510/744]: Loss: 0.1979, CE: 0.1129
[23:23:51.883] Epoch [4/5] Iteration [520/744]: Loss: 0.2490, CE: 0.2401
[23:23:56.404] Epoch [4/5] Iteration [530/744]: Loss: 0.1380, CE: 0.0885
[23:24:00.902] Epoch [4/5] Iteration [540/744]: Loss: 0.1822, CE: 0.0737
[23:24:05.438] Epoch [4/5] Iteration [550/744]: Loss: 0.1958, CE: 0.1075
[23:24:09.933] Epoch [4/5] Iteration [560/744]: Loss: 0.2166, CE: 0.1568
[23:24:14.456] Epoch [4/5] Iteration [570/744]: Loss: 0.1879, CE: 0.0867
[23:24:18.967] Epoch [4/5] Iteration [580/744]: Loss: 0.1823, CE: 0.0738
[23:24:23.478] Epoch [4/5] Iteration [590/744]: Loss: 0.1955, CE: 0.1060
[23:24:27.987] Epoch [4/5] Iteration [600/744]: Loss: 0.2107, CE: 0.1423
[23:24:32.509] Epoch [4/5] Iteration [610/744]: Loss: 0.1414, CE: 0.0948
[23:24:37.013] Epoch [4/5] Iteration [620/744]: Loss: 0.2248, CE: 0.1791
[23:24:41.530] Epoch [4/5] Iteration [630/744]: Loss: 0.2112, CE: 0.1456
[23:24:46.034] Epoch [4/5] Iteration [640/744]: Loss: 0.2271, CE: 0.1855
[23:24:50.548] Epoch [4/5] Iteration [650/744]: Loss: 0.1831, CE: 0.0760
[23:24:55.047] Epoch [4/5] Iteration [660/744]: Loss: 0.2520, CE: 0.2473
[23:24:59.562] Epoch [4/5] Iteration [670/744]: Loss: 0.1647, CE: 0.1544
[23:25:04.062] Epoch [4/5] Iteration [680/744]: Loss: 0.2095, CE: 0.1412
[23:25:08.585] Epoch [4/5] Iteration [690/744]: Loss: 0.1427, CE: 0.0981
[23:25:13.116] Epoch [4/5] Iteration [700/744]: Loss: 0.2157, CE: 0.1566
[23:25:17.657] Epoch [4/5] Iteration [710/744]: Loss: 0.2001, CE: 0.1175
[23:25:22.176] Epoch [4/5] Iteration [720/744]: Loss: 0.2199, CE: 0.1670
[23:25:26.687] Epoch [4/5] Iteration [730/744]: Loss: 0.2054, CE: 0.1307
[23:25:31.201] Epoch [4/5] Iteration [740/744]: Loss: 0.2372, CE: 0.2097
[23:25:33.241] Epoch [4/5] Average Loss: 0.1984, CE: 0.1415, Dice: 0.2363
[23:25:33.441] Saved continual learning checkpoint to ./universal/synapse_to_kits23_tpgm\epoch_4_continual.pth
[23:25:33.630] Saved final continual model to ./universal/synapse_to_kits23_tpgm\final_continual_model.pth
[01:01:48.251] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=5, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', dataset_fraction=0.25, enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=False, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[01:01:48.251] Continual Learning: 9 -> 12 classes
[01:01:48.251] Task offset: 9, New classes: 4
[01:01:48.251] Dataset fraction: 0.25
[01:01:48.251] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[01:01:48.251] Old task classes: 9, New task classes: 4
[01:01:48.308] Identified output layer keys: ['cswin_unet.output.weight']
[01:01:48.309] Loaded 462 backbone layers from pretrained model.
[01:01:48.309] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[01:01:48.343] Successfully adapted model for continual learning.
[01:01:48.344] Pretrained weights loaded and adapted for continual learning
[01:01:48.355] Using 23805/95221 samples (25.00% of dataset)
[01:01:48.358] Surgical fine-tuning enabled for continual learning
[01:01:48.362] 744 iterations per epoch. 3720 max iterations
[01:01:48.363] Updating surgical weights at epoch 0
[01:02:13.320] Surgical weights range: [0.0002, 1.0000]
[01:02:39.564] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=5, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', dataset_fraction=0.25, enable_surgical=True, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=False, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[01:02:39.564] Continual Learning: 9 -> 12 classes
[01:02:39.564] Task offset: 9, New classes: 4
[01:02:39.564] Dataset fraction: 0.25
[01:02:39.564] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[01:02:39.564] Old task classes: 9, New task classes: 4
[01:02:39.613] Identified output layer keys: ['cswin_unet.output.weight']
[01:02:39.613] Loaded 462 backbone layers from pretrained model.
[01:02:39.613] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[01:02:39.649] Successfully adapted model for continual learning.
[01:02:39.649] Pretrained weights loaded and adapted for continual learning
[01:02:39.659] Using 23805/95221 samples (25.00% of dataset)
[01:02:39.662] Surgical fine-tuning enabled for continual learning
[01:02:39.664] 744 iterations per epoch. 3720 max iterations
[01:02:39.665] Updating surgical weights at epoch 0
[01:03:03.663] Surgical weights range: [0.0002, 1.0000]
[01:03:47.908] Epoch [0/5] Iteration [0/744]: Loss: 0.5991, CE: 0.1218
[01:03:52.343] Epoch [0/5] Iteration [10/744]: Loss: 0.6131, CE: 0.1637
[01:03:56.826] Epoch [0/5] Iteration [20/744]: Loss: 0.5387, CE: 0.1051
[01:04:01.288] Epoch [0/5] Iteration [30/744]: Loss: 0.3729, CE: 0.0904
[01:04:05.766] Epoch [0/5] Iteration [40/744]: Loss: 0.2826, CE: 0.1514
[01:04:10.254] Epoch [0/5] Iteration [50/744]: Loss: 0.2414, CE: 0.0987
[01:04:14.748] Epoch [0/5] Iteration [60/744]: Loss: 0.2756, CE: 0.2093
[01:04:19.212] Epoch [0/5] Iteration [70/744]: Loss: 0.2235, CE: 0.1250
[01:04:23.688] Epoch [0/5] Iteration [80/744]: Loss: 0.2198, CE: 0.1505
[01:04:28.159] Epoch [0/5] Iteration [90/744]: Loss: 0.2099, CE: 0.1367
[01:04:32.674] Epoch [0/5] Iteration [100/744]: Loss: 0.1981, CE: 0.1109
[01:04:37.150] Epoch [0/5] Iteration [110/744]: Loss: 0.2396, CE: 0.2145
[01:04:41.632] Epoch [0/5] Iteration [120/744]: Loss: 0.1530, CE: 0.1240
[01:04:46.101] Epoch [0/5] Iteration [130/744]: Loss: 0.0729, CE: 0.0501
[01:04:50.593] Epoch [0/5] Iteration [140/744]: Loss: 0.1981, CE: 0.1117
[01:04:55.070] Epoch [0/5] Iteration [150/744]: Loss: 0.1872, CE: 0.0844
[01:04:59.586] Epoch [0/5] Iteration [160/744]: Loss: 0.1896, CE: 0.0922
[01:05:04.097] Epoch [0/5] Iteration [170/744]: Loss: 0.1987, CE: 0.1142
[01:05:08.582] Epoch [0/5] Iteration [180/744]: Loss: 0.2004, CE: 0.1159
[01:05:13.068] Epoch [0/5] Iteration [190/744]: Loss: 0.2702, CE: 0.2923
[01:05:17.552] Epoch [0/5] Iteration [200/744]: Loss: 0.2109, CE: 0.1436
[01:05:22.013] Epoch [0/5] Iteration [210/744]: Loss: 0.2059, CE: 0.1327
[01:05:26.504] Epoch [0/5] Iteration [220/744]: Loss: 0.2459, CE: 0.2318
[01:05:30.974] Epoch [0/5] Iteration [230/744]: Loss: 0.1616, CE: 0.1447
[01:05:35.454] Epoch [0/5] Iteration [240/744]: Loss: 0.1488, CE: 0.1147
[01:05:39.911] Epoch [0/5] Iteration [250/744]: Loss: 0.2096, CE: 0.1411
[01:05:44.405] Epoch [0/5] Iteration [260/744]: Loss: 0.1419, CE: 0.0971
[01:05:48.904] Epoch [0/5] Iteration [270/744]: Loss: 0.2107, CE: 0.1419
[01:05:53.415] Epoch [0/5] Iteration [280/744]: Loss: 0.1489, CE: 0.1148
[01:05:57.915] Epoch [0/5] Iteration [290/744]: Loss: 0.2186, CE: 0.1644
[01:06:02.406] Epoch [0/5] Iteration [300/744]: Loss: 0.2169, CE: 0.1601
[01:06:06.921] Epoch [0/5] Iteration [310/744]: Loss: 0.1959, CE: 0.1062
[01:06:11.423] Epoch [0/5] Iteration [320/744]: Loss: 0.2063, CE: 0.1327
[01:06:15.886] Epoch [0/5] Iteration [330/744]: Loss: 0.2327, CE: 0.1984
[01:06:20.404] Epoch [0/5] Iteration [340/744]: Loss: 0.1449, CE: 0.1047
[01:06:24.899] Epoch [0/5] Iteration [350/744]: Loss: 0.2192, CE: 0.1649
[01:06:29.407] Epoch [0/5] Iteration [360/744]: Loss: 0.2253, CE: 0.1812
[01:06:33.883] Epoch [0/5] Iteration [370/744]: Loss: 0.2371, CE: 0.2093
[01:06:38.429] Epoch [0/5] Iteration [380/744]: Loss: 0.2038, CE: 0.1266
[01:06:42.931] Epoch [0/5] Iteration [390/744]: Loss: 0.1495, CE: 0.1161
[01:06:47.437] Epoch [0/5] Iteration [400/744]: Loss: 0.2058, CE: 0.1284
[01:06:51.934] Epoch [0/5] Iteration [410/744]: Loss: 0.2265, CE: 0.1818
[01:06:56.437] Epoch [0/5] Iteration [420/744]: Loss: 0.1871, CE: 0.0797
[01:07:00.906] Epoch [0/5] Iteration [430/744]: Loss: 0.2125, CE: 0.1484
[01:07:05.421] Epoch [0/5] Iteration [440/744]: Loss: 0.1930, CE: 0.1006
[01:07:09.901] Epoch [0/5] Iteration [450/744]: Loss: 0.1822, CE: 0.0735
[01:07:14.400] Epoch [0/5] Iteration [460/744]: Loss: 0.2338, CE: 0.2022
[01:07:18.910] Epoch [0/5] Iteration [470/744]: Loss: 0.2131, CE: 0.1507
[01:07:23.418] Epoch [0/5] Iteration [480/744]: Loss: 0.1975, CE: 0.1108
[01:07:27.911] Epoch [0/5] Iteration [490/744]: Loss: 0.1302, CE: 0.0655
[01:07:32.422] Epoch [0/5] Iteration [500/744]: Loss: 0.2036, CE: 0.1274
[01:07:36.956] Epoch [0/5] Iteration [510/744]: Loss: 0.1867, CE: 0.0831
[01:07:41.456] Epoch [0/5] Iteration [520/744]: Loss: 0.2206, CE: 0.1693
[01:07:46.034] Epoch [0/5] Iteration [530/744]: Loss: 0.1542, CE: 0.1287
[01:07:50.540] Epoch [0/5] Iteration [540/744]: Loss: 0.2069, CE: 0.1318
[01:07:55.048] Epoch [0/5] Iteration [550/744]: Loss: 0.2174, CE: 0.1593
[01:07:59.542] Epoch [0/5] Iteration [560/744]: Loss: 0.1796, CE: 0.0675
[01:08:04.023] Epoch [0/5] Iteration [570/744]: Loss: 0.1953, CE: 0.1035
[01:08:08.521] Epoch [0/5] Iteration [580/744]: Loss: 0.1849, CE: 0.0804
[01:08:13.020] Epoch [0/5] Iteration [590/744]: Loss: 0.2202, CE: 0.1683
[01:08:17.516] Epoch [0/5] Iteration [600/744]: Loss: 0.2106, CE: 0.1419
[01:08:22.022] Epoch [0/5] Iteration [610/744]: Loss: 0.2136, CE: 0.1502
[01:08:26.544] Epoch [0/5] Iteration [620/744]: Loss: 0.2163, CE: 0.1584
[01:08:31.043] Epoch [0/5] Iteration [630/744]: Loss: 0.2450, CE: 0.2280
[01:08:35.564] Epoch [0/5] Iteration [640/744]: Loss: 0.1223, CE: 0.0478
[01:08:40.068] Epoch [0/5] Iteration [650/744]: Loss: 0.1726, CE: 0.0486
[01:08:44.578] Epoch [0/5] Iteration [660/744]: Loss: 0.2151, CE: 0.1559
[01:08:49.105] Epoch [0/5] Iteration [670/744]: Loss: 0.1876, CE: 0.0874
[01:08:53.604] Epoch [0/5] Iteration [680/744]: Loss: 0.1759, CE: 0.0573
[01:08:58.098] Epoch [0/5] Iteration [690/744]: Loss: 0.2162, CE: 0.1574
[01:09:02.604] Epoch [0/5] Iteration [700/744]: Loss: 0.2201, CE: 0.1676
[01:09:07.096] Epoch [0/5] Iteration [710/744]: Loss: 0.1450, CE: 0.1030
[01:09:11.600] Epoch [0/5] Iteration [720/744]: Loss: 0.2296, CE: 0.1905
[01:09:16.110] Epoch [0/5] Iteration [730/744]: Loss: 0.2031, CE: 0.1263
[01:09:20.621] Epoch [0/5] Iteration [740/744]: Loss: 0.2174, CE: 0.1599
[01:09:22.657] Epoch [0/5] Average Loss: 0.2167, CE: 0.1397, Dice: 0.2679
[01:10:05.501] Epoch [1/5] Iteration [0/744]: Loss: 0.2008, CE: 0.1172
[01:10:09.985] Epoch [1/5] Iteration [10/744]: Loss: 0.2145, CE: 0.1531
[01:10:14.466] Epoch [1/5] Iteration [20/744]: Loss: 0.2158, CE: 0.1572
[01:10:18.956] Epoch [1/5] Iteration [30/744]: Loss: 0.2086, CE: 0.1389
[01:10:23.431] Epoch [1/5] Iteration [40/744]: Loss: 0.2137, CE: 0.1509
[01:10:27.899] Epoch [1/5] Iteration [50/744]: Loss: 0.1502, CE: 0.1169
[01:10:32.362] Epoch [1/5] Iteration [60/744]: Loss: 0.1921, CE: 0.0975
[01:10:36.850] Epoch [1/5] Iteration [70/744]: Loss: 0.1932, CE: 0.0985
[01:10:41.347] Epoch [1/5] Iteration [80/744]: Loss: 0.2055, CE: 0.1304
[01:10:45.847] Epoch [1/5] Iteration [90/744]: Loss: 0.1984, CE: 0.1136
[01:10:50.344] Epoch [1/5] Iteration [100/744]: Loss: 0.2141, CE: 0.1537
[01:10:54.818] Epoch [1/5] Iteration [110/744]: Loss: 0.1949, CE: 0.1054
[01:10:59.320] Epoch [1/5] Iteration [120/744]: Loss: 0.1843, CE: 0.0794
[01:11:03.828] Epoch [1/5] Iteration [130/744]: Loss: 0.2152, CE: 0.1569
[01:11:08.337] Epoch [1/5] Iteration [140/744]: Loss: 0.2381, CE: 0.2126
[01:11:12.826] Epoch [1/5] Iteration [150/744]: Loss: 0.2092, CE: 0.1404
[01:11:17.311] Epoch [1/5] Iteration [160/744]: Loss: 0.2016, CE: 0.2465
[01:11:21.803] Epoch [1/5] Iteration [170/744]: Loss: 0.1993, CE: 0.1154
[01:11:26.288] Epoch [1/5] Iteration [180/744]: Loss: 0.1615, CE: 0.1468
[01:11:30.764] Epoch [1/5] Iteration [190/744]: Loss: 0.2053, CE: 0.1295
[01:11:35.263] Epoch [1/5] Iteration [200/744]: Loss: 0.2486, CE: 0.2390
[01:11:39.761] Epoch [1/5] Iteration [210/744]: Loss: 0.2013, CE: 0.1205
[01:11:44.261] Epoch [1/5] Iteration [220/744]: Loss: 0.1461, CE: 0.1066
[01:11:48.747] Epoch [1/5] Iteration [230/744]: Loss: 0.1921, CE: 0.0987
[01:11:53.248] Epoch [1/5] Iteration [240/744]: Loss: 0.2062, CE: 0.1340
[01:11:57.741] Epoch [1/5] Iteration [250/744]: Loss: 0.2165, CE: 0.1576
[01:12:02.254] Epoch [1/5] Iteration [260/744]: Loss: 0.1841, CE: 0.0781
[01:12:06.741] Epoch [1/5] Iteration [270/744]: Loss: 0.2399, CE: 0.2181
[01:12:11.252] Epoch [1/5] Iteration [280/744]: Loss: 0.1942, CE: 0.1038
[01:12:15.761] Epoch [1/5] Iteration [290/744]: Loss: 0.2052, CE: 0.1312
[01:12:20.262] Epoch [1/5] Iteration [300/744]: Loss: 0.1566, CE: 0.1335
[01:12:24.761] Epoch [1/5] Iteration [310/744]: Loss: 0.2164, CE: 0.1577
[01:12:29.271] Epoch [1/5] Iteration [320/744]: Loss: 0.2238, CE: 0.1771
[01:12:33.758] Epoch [1/5] Iteration [330/744]: Loss: 0.2213, CE: 0.1703
[01:12:38.274] Epoch [1/5] Iteration [340/744]: Loss: 0.1922, CE: 0.0972
[01:12:42.773] Epoch [1/5] Iteration [350/744]: Loss: 0.1453, CE: 0.1058
[01:12:47.296] Epoch [1/5] Iteration [360/744]: Loss: 0.1127, CE: 0.0251
[01:12:51.813] Epoch [1/5] Iteration [370/744]: Loss: 0.1588, CE: 0.1402
[01:12:56.344] Epoch [1/5] Iteration [380/744]: Loss: 0.1556, CE: 0.1310
[01:13:00.831] Epoch [1/5] Iteration [390/744]: Loss: 0.2092, CE: 0.1392
[01:13:05.335] Epoch [1/5] Iteration [400/744]: Loss: 0.1932, CE: 0.0984
[01:13:09.861] Epoch [1/5] Iteration [410/744]: Loss: 0.2356, CE: 0.2053
[01:13:14.365] Epoch [1/5] Iteration [420/744]: Loss: 0.1452, CE: 0.1061
[01:13:18.869] Epoch [1/5] Iteration [430/744]: Loss: 0.2129, CE: 0.1495
[01:13:23.369] Epoch [1/5] Iteration [440/744]: Loss: 0.1948, CE: 0.1055
[01:13:27.862] Epoch [1/5] Iteration [450/744]: Loss: 0.2004, CE: 0.1190
[01:13:32.363] Epoch [1/5] Iteration [460/744]: Loss: 0.2384, CE: 0.2138
[01:13:36.875] Epoch [1/5] Iteration [470/744]: Loss: 0.1872, CE: 0.0851
[01:13:41.365] Epoch [1/5] Iteration [480/744]: Loss: 0.1421, CE: 0.0978
[01:13:45.879] Epoch [1/5] Iteration [490/744]: Loss: 0.1864, CE: 0.0821
[01:13:50.397] Epoch [1/5] Iteration [500/744]: Loss: 0.2219, CE: 0.1727
[01:13:54.911] Epoch [1/5] Iteration [510/744]: Loss: 0.1198, CE: 0.0425
[01:13:59.417] Epoch [1/5] Iteration [520/744]: Loss: 0.1537, CE: 0.1278
[01:14:03.923] Epoch [1/5] Iteration [530/744]: Loss: 0.2382, CE: 0.2098
[01:14:08.425] Epoch [1/5] Iteration [540/744]: Loss: 0.1552, CE: 0.1300
[01:14:12.927] Epoch [1/5] Iteration [550/744]: Loss: 0.2056, CE: 0.1313
[01:14:17.452] Epoch [1/5] Iteration [560/744]: Loss: 0.1634, CE: 0.1513
[01:14:21.972] Epoch [1/5] Iteration [570/744]: Loss: 0.2061, CE: 0.1329
[01:14:26.484] Epoch [1/5] Iteration [580/744]: Loss: 0.1937, CE: 0.1025
[01:14:31.001] Epoch [1/5] Iteration [590/744]: Loss: 0.1955, CE: 0.1077
[01:14:35.513] Epoch [1/5] Iteration [600/744]: Loss: 0.1864, CE: 0.0830
[01:14:40.025] Epoch [1/5] Iteration [610/744]: Loss: 0.2216, CE: 0.1711
[01:14:44.541] Epoch [1/5] Iteration [620/744]: Loss: 0.2019, CE: 0.1200
[01:14:49.049] Epoch [1/5] Iteration [630/744]: Loss: 0.2382, CE: 0.2126
[01:14:53.566] Epoch [1/5] Iteration [640/744]: Loss: 0.2115, CE: 0.1467
[01:14:58.075] Epoch [1/5] Iteration [650/744]: Loss: 0.1453, CE: 0.1059
[01:15:02.579] Epoch [1/5] Iteration [660/744]: Loss: 0.1881, CE: 0.0878
[01:15:07.115] Epoch [1/5] Iteration [670/744]: Loss: 0.2065, CE: 0.1344
[01:15:11.625] Epoch [1/5] Iteration [680/744]: Loss: 0.1772, CE: 0.0610
[01:15:16.134] Epoch [1/5] Iteration [690/744]: Loss: 0.1781, CE: 0.1860
[01:15:20.648] Epoch [1/5] Iteration [700/744]: Loss: 0.2099, CE: 0.1421
[01:15:25.150] Epoch [1/5] Iteration [710/744]: Loss: 0.1425, CE: 0.0983
[01:15:29.651] Epoch [1/5] Iteration [720/744]: Loss: 0.1175, CE: 0.0359
[01:15:34.159] Epoch [1/5] Iteration [730/744]: Loss: 0.2478, CE: 0.2349
[01:15:38.678] Epoch [1/5] Iteration [740/744]: Loss: 0.2029, CE: 0.1255
[01:15:40.717] Epoch [1/5] Average Loss: 0.1978, CE: 0.1415, Dice: 0.2354
[01:16:23.011] Epoch [2/5] Iteration [0/744]: Loss: 0.2088, CE: 0.1400
[01:16:27.469] Epoch [2/5] Iteration [10/744]: Loss: 0.1592, CE: 0.1387
[01:16:31.941] Epoch [2/5] Iteration [20/744]: Loss: 0.2134, CE: 0.1502
[01:16:36.424] Epoch [2/5] Iteration [30/744]: Loss: 0.2076, CE: 0.1366
[01:16:40.924] Epoch [2/5] Iteration [40/744]: Loss: 0.2443, CE: 0.2292
[01:16:45.383] Epoch [2/5] Iteration [50/744]: Loss: 0.1505, CE: 0.1178
[01:16:49.864] Epoch [2/5] Iteration [60/744]: Loss: 0.1375, CE: 0.0859
[01:16:54.329] Epoch [2/5] Iteration [70/744]: Loss: 0.1615, CE: 0.1442
[01:16:58.815] Epoch [2/5] Iteration [80/744]: Loss: 0.1912, CE: 0.0956
[01:17:03.299] Epoch [2/5] Iteration [90/744]: Loss: 0.1367, CE: 0.0845
[01:17:07.787] Epoch [2/5] Iteration [100/744]: Loss: 0.1862, CE: 0.2081
[01:17:12.267] Epoch [2/5] Iteration [110/744]: Loss: 0.1638, CE: 0.1515
[01:17:16.734] Epoch [2/5] Iteration [120/744]: Loss: 0.2042, CE: 0.1223
[01:17:21.236] Epoch [2/5] Iteration [130/744]: Loss: 0.1808, CE: 0.0698
[01:17:25.721] Epoch [2/5] Iteration [140/744]: Loss: 0.1533, CE: 0.1259
[01:17:30.216] Epoch [2/5] Iteration [150/744]: Loss: 0.1993, CE: 0.1154
[01:17:34.691] Epoch [2/5] Iteration [160/744]: Loss: 0.1980, CE: 0.1121
[01:17:39.184] Epoch [2/5] Iteration [170/744]: Loss: 0.2030, CE: 0.1260
[01:17:43.674] Epoch [2/5] Iteration [180/744]: Loss: 0.1602, CE: 0.1423
[01:17:48.262] Epoch [2/5] Iteration [190/744]: Loss: 0.1982, CE: 0.1130
[01:17:52.735] Epoch [2/5] Iteration [200/744]: Loss: 0.2174, CE: 0.1580
[01:17:57.235] Epoch [2/5] Iteration [210/744]: Loss: 0.1436, CE: 0.1025
[01:18:01.726] Epoch [2/5] Iteration [220/744]: Loss: 0.1899, CE: 0.0917
[01:18:06.226] Epoch [2/5] Iteration [230/744]: Loss: 0.2008, CE: 0.1197
[01:18:10.734] Epoch [2/5] Iteration [240/744]: Loss: 0.2141, CE: 0.1511
[01:18:15.217] Epoch [2/5] Iteration [250/744]: Loss: 0.2415, CE: 0.2195
[01:18:19.731] Epoch [2/5] Iteration [260/744]: Loss: 0.2038, CE: 0.1268
[01:18:24.238] Epoch [2/5] Iteration [270/744]: Loss: 0.2223, CE: 0.1736
[01:18:28.753] Epoch [2/5] Iteration [280/744]: Loss: 0.1388, CE: 0.0899
[01:18:33.255] Epoch [2/5] Iteration [290/744]: Loss: 0.2126, CE: 0.1429
[01:18:37.766] Epoch [2/5] Iteration [300/744]: Loss: 0.2580, CE: 0.2624
[01:18:42.284] Epoch [2/5] Iteration [310/744]: Loss: 0.2099, CE: 0.1424
[01:18:46.784] Epoch [2/5] Iteration [320/744]: Loss: 0.2034, CE: 0.1269
[01:18:51.289] Epoch [2/5] Iteration [330/744]: Loss: 0.2211, CE: 0.1707
[01:18:55.794] Epoch [2/5] Iteration [340/744]: Loss: 0.1972, CE: 0.1107
[01:19:00.308] Epoch [2/5] Iteration [350/744]: Loss: 0.1781, CE: 0.1883
[01:19:04.818] Epoch [2/5] Iteration [360/744]: Loss: 0.2142, CE: 0.1524
[01:19:09.328] Epoch [2/5] Iteration [370/744]: Loss: 0.1969, CE: 0.1109
[01:19:13.840] Epoch [2/5] Iteration [380/744]: Loss: 0.2024, CE: 0.1233
[01:19:18.351] Epoch [2/5] Iteration [390/744]: Loss: 0.1844, CE: 0.2044
[01:19:22.858] Epoch [2/5] Iteration [400/744]: Loss: 0.2051, CE: 0.1301
[01:19:27.358] Epoch [2/5] Iteration [410/744]: Loss: 0.2314, CE: 0.1962
[01:19:31.871] Epoch [2/5] Iteration [420/744]: Loss: 0.1938, CE: 0.1024
[01:19:36.370] Epoch [2/5] Iteration [430/744]: Loss: 0.1679, CE: 0.1594
[01:19:40.877] Epoch [2/5] Iteration [440/744]: Loss: 0.2090, CE: 0.1390
[01:19:45.390] Epoch [2/5] Iteration [450/744]: Loss: 0.2167, CE: 0.1569
[01:19:49.916] Epoch [2/5] Iteration [460/744]: Loss: 0.1543, CE: 0.1291
[01:19:54.415] Epoch [2/5] Iteration [470/744]: Loss: 0.1820, CE: 0.0734
[01:19:58.931] Epoch [2/5] Iteration [480/744]: Loss: 0.2270, CE: 0.1852
[01:20:03.437] Epoch [2/5] Iteration [490/744]: Loss: 0.1192, CE: 0.0413
[01:20:07.943] Epoch [2/5] Iteration [500/744]: Loss: 0.1916, CE: 0.0965
[01:20:12.448] Epoch [2/5] Iteration [510/744]: Loss: 0.1978, CE: 0.1127
[01:20:16.970] Epoch [2/5] Iteration [520/744]: Loss: 0.2488, CE: 0.2395
[01:20:21.477] Epoch [2/5] Iteration [530/744]: Loss: 0.1380, CE: 0.0883
[01:20:25.982] Epoch [2/5] Iteration [540/744]: Loss: 0.1822, CE: 0.0736
[01:20:30.476] Epoch [2/5] Iteration [550/744]: Loss: 0.1958, CE: 0.1075
[01:20:35.013] Epoch [2/5] Iteration [560/744]: Loss: 0.2162, CE: 0.1570
[01:20:39.522] Epoch [2/5] Iteration [570/744]: Loss: 0.1876, CE: 0.0864
[01:20:44.044] Epoch [2/5] Iteration [580/744]: Loss: 0.1823, CE: 0.0738
[01:20:48.546] Epoch [2/5] Iteration [590/744]: Loss: 0.1957, CE: 0.1065
[01:20:53.075] Epoch [2/5] Iteration [600/744]: Loss: 0.2111, CE: 0.1421
[01:20:57.586] Epoch [2/5] Iteration [610/744]: Loss: 0.1414, CE: 0.0948
[01:21:02.111] Epoch [2/5] Iteration [620/744]: Loss: 0.2247, CE: 0.1791
[01:21:06.610] Epoch [2/5] Iteration [630/744]: Loss: 0.2113, CE: 0.1458
[01:21:11.135] Epoch [2/5] Iteration [640/744]: Loss: 0.2268, CE: 0.1848
[01:21:15.644] Epoch [2/5] Iteration [650/744]: Loss: 0.1831, CE: 0.0761
[01:21:20.146] Epoch [2/5] Iteration [660/744]: Loss: 0.2522, CE: 0.2480
[01:21:24.650] Epoch [2/5] Iteration [670/744]: Loss: 0.1645, CE: 0.1539
[01:21:29.162] Epoch [2/5] Iteration [680/744]: Loss: 0.2098, CE: 0.1421
[01:21:33.665] Epoch [2/5] Iteration [690/744]: Loss: 0.1432, CE: 0.0980
[01:21:38.181] Epoch [2/5] Iteration [700/744]: Loss: 0.2157, CE: 0.1563
[01:21:42.695] Epoch [2/5] Iteration [710/744]: Loss: 0.2002, CE: 0.1179
[01:21:47.223] Epoch [2/5] Iteration [720/744]: Loss: 0.2197, CE: 0.1667
[01:21:51.736] Epoch [2/5] Iteration [730/744]: Loss: 0.2055, CE: 0.1311
[01:23:41.146] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=5, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', dataset_fraction=0.25, enable_surgical=False, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=False, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[01:23:41.146] Continual Learning: 9 -> 12 classes
[01:23:41.147] Task offset: 9, New classes: 4
[01:23:41.147] Dataset fraction: 0.25
[01:23:41.147] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[01:23:41.147] Old task classes: 9, New task classes: 4
[01:23:41.195] Identified output layer keys: ['cswin_unet.output.weight']
[01:23:41.195] Loaded 462 backbone layers from pretrained model.
[01:23:41.196] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[01:23:41.229] Successfully adapted model for continual learning.
[01:23:41.229] Pretrained weights loaded and adapted for continual learning
[01:23:41.239] Using 23805/95221 samples (25.00% of dataset)
[01:23:41.243] 744 iterations per epoch. 3720 max iterations
[01:24:23.223] Epoch [0/5] Iteration [0/744]: Loss: 0.6196, CE: 0.1729
[01:24:27.495] Epoch [0/5] Iteration [10/744]: Loss: 0.2944, CE: 0.0443
[01:24:31.792] Epoch [0/5] Iteration [20/744]: Loss: 0.1824, CE: 0.2037
[01:24:36.066] Epoch [0/5] Iteration [30/744]: Loss: 0.2368, CE: 0.2162
[01:24:40.351] Epoch [0/5] Iteration [40/744]: Loss: 0.1946, CE: 0.1107
[01:24:44.631] Epoch [0/5] Iteration [50/744]: Loss: 0.1750, CE: 0.0559
[01:24:48.925] Epoch [0/5] Iteration [60/744]: Loss: 0.1407, CE: 0.0993
[01:24:53.207] Epoch [0/5] Iteration [70/744]: Loss: 0.2161, CE: 0.1616
[01:24:57.505] Epoch [0/5] Iteration [80/744]: Loss: 0.1837, CE: 0.0808
[01:25:01.791] Epoch [0/5] Iteration [90/744]: Loss: 0.1838, CE: 0.0813
[01:25:06.093] Epoch [0/5] Iteration [100/744]: Loss: 0.1782, CE: 0.0676
[01:25:10.388] Epoch [0/5] Iteration [110/744]: Loss: 0.1914, CE: 0.1003
[01:25:14.694] Epoch [0/5] Iteration [120/744]: Loss: 0.1716, CE: 0.0505
[01:25:18.987] Epoch [0/5] Iteration [130/744]: Loss: 0.1695, CE: 0.0454
[01:25:23.293] Epoch [0/5] Iteration [140/744]: Loss: 0.1347, CE: 0.0843
[01:25:27.591] Epoch [0/5] Iteration [150/744]: Loss: 0.1913, CE: 0.1003
[01:25:31.903] Epoch [0/5] Iteration [160/744]: Loss: 0.1812, CE: 0.0760
[01:25:36.203] Epoch [0/5] Iteration [170/744]: Loss: 0.1247, CE: 0.0606
[01:25:40.511] Epoch [0/5] Iteration [180/744]: Loss: 0.1787, CE: 0.0719
[01:25:44.809] Epoch [0/5] Iteration [190/744]: Loss: 0.1669, CE: 0.0429
[01:25:49.120] Epoch [0/5] Iteration [200/744]: Loss: 0.1849, CE: 0.0900
[01:25:53.420] Epoch [0/5] Iteration [210/744]: Loss: 0.1685, CE: 0.0529
[01:25:57.736] Epoch [0/5] Iteration [220/744]: Loss: 0.1645, CE: 0.0441
[01:26:02.040] Epoch [0/5] Iteration [230/744]: Loss: 0.1634, CE: 0.0406
[01:26:06.356] Epoch [0/5] Iteration [240/744]: Loss: 0.1876, CE: 0.1042
[01:26:10.668] Epoch [0/5] Iteration [250/744]: Loss: 0.1182, CE: 0.0551
[01:26:14.983] Epoch [0/5] Iteration [260/744]: Loss: 0.1600, CE: 0.0367
[01:26:19.291] Epoch [0/5] Iteration [270/744]: Loss: 0.1686, CE: 0.0580
[01:26:23.609] Epoch [0/5] Iteration [280/744]: Loss: 0.1742, CE: 0.0718
[01:26:27.917] Epoch [0/5] Iteration [290/744]: Loss: 0.1735, CE: 0.0717
[01:26:32.238] Epoch [0/5] Iteration [300/744]: Loss: 0.1639, CE: 0.0480
[01:26:36.551] Epoch [0/5] Iteration [310/744]: Loss: 0.1669, CE: 0.0572
[01:26:40.874] Epoch [0/5] Iteration [320/744]: Loss: 0.1324, CE: 0.0941
[01:26:45.186] Epoch [0/5] Iteration [330/744]: Loss: 0.1747, CE: 0.0755
[01:26:49.511] Epoch [0/5] Iteration [340/744]: Loss: 0.1116, CE: 0.0411
[01:26:53.825] Epoch [0/5] Iteration [350/744]: Loss: 0.1597, CE: 0.0377
[01:26:58.153] Epoch [0/5] Iteration [360/744]: Loss: 0.1146, CE: 0.0502
[01:27:02.473] Epoch [0/5] Iteration [370/744]: Loss: 0.1637, CE: 0.0478
[01:27:06.805] Epoch [0/5] Iteration [380/744]: Loss: 0.1169, CE: 0.0586
[01:27:11.127] Epoch [0/5] Iteration [390/744]: Loss: 0.1670, CE: 0.0568
[01:27:15.464] Epoch [0/5] Iteration [400/744]: Loss: 0.1590, CE: 0.0346
[01:27:19.832] Epoch [0/5] Iteration [410/744]: Loss: 0.1735, CE: 0.0745
[01:27:24.173] Epoch [0/5] Iteration [420/744]: Loss: 0.1702, CE: 0.0668
[01:27:28.498] Epoch [0/5] Iteration [430/744]: Loss: 0.1707, CE: 0.0662
[01:27:32.839] Epoch [0/5] Iteration [440/744]: Loss: 0.2012, CE: 0.1414
[01:27:37.169] Epoch [0/5] Iteration [450/744]: Loss: 0.1810, CE: 0.0950
[01:27:41.512] Epoch [0/5] Iteration [460/744]: Loss: 0.1720, CE: 0.0719
[01:27:45.841] Epoch [0/5] Iteration [470/744]: Loss: 0.1661, CE: 0.0564
[01:27:50.182] Epoch [0/5] Iteration [480/744]: Loss: 0.1646, CE: 0.0492
[01:27:54.512] Epoch [0/5] Iteration [490/744]: Loss: 0.1660, CE: 0.0553
[01:27:58.847] Epoch [0/5] Iteration [500/744]: Loss: 0.1109, CE: 0.0432
[01:28:03.181] Epoch [0/5] Iteration [510/744]: Loss: 0.1228, CE: 0.0727
[01:28:07.532] Epoch [0/5] Iteration [520/744]: Loss: 0.1584, CE: 0.0321
[01:28:11.865] Epoch [0/5] Iteration [530/744]: Loss: 0.1613, CE: 0.0429
[01:28:16.211] Epoch [0/5] Iteration [540/744]: Loss: 0.1597, CE: 0.0404
[01:28:20.548] Epoch [0/5] Iteration [550/744]: Loss: 0.1418, CE: 0.1206
[01:28:24.892] Epoch [0/5] Iteration [560/744]: Loss: 0.1599, CE: 0.0390
[01:28:29.228] Epoch [0/5] Iteration [570/744]: Loss: 0.1590, CE: 0.0402
[01:28:33.590] Epoch [0/5] Iteration [580/744]: Loss: 0.1562, CE: 0.0288
[01:28:37.928] Epoch [0/5] Iteration [590/744]: Loss: 0.1664, CE: 0.0580
[01:28:42.277] Epoch [0/5] Iteration [600/744]: Loss: 0.1667, CE: 0.0603
[01:28:46.620] Epoch [0/5] Iteration [610/744]: Loss: 0.1597, CE: 0.0410
[01:28:50.971] Epoch [0/5] Iteration [620/744]: Loss: 0.1672, CE: 0.0610
[01:28:55.317] Epoch [0/5] Iteration [630/744]: Loss: 0.1584, CE: 0.0333
[01:28:59.659] Epoch [0/5] Iteration [640/744]: Loss: 0.1653, CE: 0.0548
[01:29:04.001] Epoch [0/5] Iteration [650/744]: Loss: 0.1737, CE: 0.0725
[01:29:08.356] Epoch [0/5] Iteration [660/744]: Loss: 0.1158, CE: 0.0550
[01:29:12.702] Epoch [0/5] Iteration [670/744]: Loss: 0.1630, CE: 0.0480
[01:29:17.055] Epoch [0/5] Iteration [680/744]: Loss: 0.1717, CE: 0.0702
[01:29:21.397] Epoch [0/5] Iteration [690/744]: Loss: 0.1759, CE: 0.0813
[01:29:25.751] Epoch [0/5] Iteration [700/744]: Loss: 0.1530, CE: 0.0217
[01:29:30.098] Epoch [0/5] Iteration [710/744]: Loss: 0.1605, CE: 0.0375
[01:29:34.452] Epoch [0/5] Iteration [720/744]: Loss: 0.1077, CE: 0.0319
[01:29:38.802] Epoch [0/5] Iteration [730/744]: Loss: 0.1058, CE: 0.0287
[01:29:43.154] Epoch [0/5] Iteration [740/744]: Loss: 0.1720, CE: 0.0722
[01:29:45.122] Epoch [0/5] Average Loss: 0.1670, CE: 0.0651, Dice: 0.2349
[01:30:27.488] Epoch [1/5] Iteration [0/744]: Loss: 0.1604, CE: 0.0440
[01:30:31.773] Epoch [1/5] Iteration [10/744]: Loss: 0.1651, CE: 0.0559
[01:30:36.072] Epoch [1/5] Iteration [20/744]: Loss: 0.1099, CE: 0.0378
[01:30:40.367] Epoch [1/5] Iteration [30/744]: Loss: 0.1613, CE: 0.0397
[01:30:44.668] Epoch [1/5] Iteration [40/744]: Loss: 0.1188, CE: 0.0618
[01:30:48.966] Epoch [1/5] Iteration [50/744]: Loss: 0.1611, CE: 0.0442
[01:30:53.276] Epoch [1/5] Iteration [60/744]: Loss: 0.1058, CE: 0.0300
[01:30:57.576] Epoch [1/5] Iteration [70/744]: Loss: 0.1684, CE: 0.0618
[01:31:01.888] Epoch [1/5] Iteration [80/744]: Loss: 0.1654, CE: 0.0538
[01:31:06.191] Epoch [1/5] Iteration [90/744]: Loss: 0.1609, CE: 0.0423
[01:31:10.506] Epoch [1/5] Iteration [100/744]: Loss: 0.1079, CE: 0.0318
[01:31:14.812] Epoch [1/5] Iteration [110/744]: Loss: 0.1066, CE: 0.0329
[01:31:19.124] Epoch [1/5] Iteration [120/744]: Loss: 0.1602, CE: 0.0379
[01:31:23.433] Epoch [1/5] Iteration [130/744]: Loss: 0.1630, CE: 0.0515
[01:31:27.754] Epoch [1/5] Iteration [140/744]: Loss: 0.1093, CE: 0.0415
[01:31:32.066] Epoch [1/5] Iteration [150/744]: Loss: 0.1657, CE: 0.0576
[01:31:36.393] Epoch [1/5] Iteration [160/744]: Loss: 0.1573, CE: 0.0367
[01:31:40.717] Epoch [1/5] Iteration [170/744]: Loss: 0.1613, CE: 0.0469
[01:31:45.044] Epoch [1/5] Iteration [180/744]: Loss: 0.1678, CE: 0.0615
[01:31:49.364] Epoch [1/5] Iteration [190/744]: Loss: 0.1675, CE: 0.0621
[01:31:53.693] Epoch [1/5] Iteration [200/744]: Loss: 0.1622, CE: 0.0480
[01:31:58.014] Epoch [1/5] Iteration [210/744]: Loss: 0.1681, CE: 0.0619
[01:32:02.341] Epoch [1/5] Iteration [220/744]: Loss: 0.1726, CE: 0.0764
[01:32:06.669] Epoch [1/5] Iteration [230/744]: Loss: 0.1573, CE: 0.0364
[01:32:11.008] Epoch [1/5] Iteration [240/744]: Loss: 0.1594, CE: 0.0408
[01:32:15.333] Epoch [1/5] Iteration [250/744]: Loss: 0.1082, CE: 0.0398
[01:32:19.671] Epoch [1/5] Iteration [260/744]: Loss: 0.1595, CE: 0.0437
[01:32:24.004] Epoch [1/5] Iteration [270/744]: Loss: 0.1610, CE: 0.0470
[01:32:28.341] Epoch [1/5] Iteration [280/744]: Loss: 0.1119, CE: 0.0477
[01:32:32.675] Epoch [1/5] Iteration [290/744]: Loss: 0.1058, CE: 0.0325
[01:32:37.024] Epoch [1/5] Iteration [300/744]: Loss: 0.1590, CE: 0.0441
[01:32:41.360] Epoch [1/5] Iteration [310/744]: Loss: 0.1579, CE: 0.0414
[01:32:45.714] Epoch [1/5] Iteration [320/744]: Loss: 0.1590, CE: 0.0424
[01:32:50.053] Epoch [1/5] Iteration [330/744]: Loss: 0.1550, CE: 0.0349
[01:32:54.408] Epoch [1/5] Iteration [340/744]: Loss: 0.1558, CE: 0.0324
[01:32:58.749] Epoch [1/5] Iteration [350/744]: Loss: 0.1565, CE: 0.0365
[01:33:03.096] Epoch [1/5] Iteration [360/744]: Loss: 0.1578, CE: 0.0413
[01:33:07.438] Epoch [1/5] Iteration [370/744]: Loss: 0.1622, CE: 0.0599
[01:33:11.791] Epoch [1/5] Iteration [380/744]: Loss: 0.1564, CE: 0.0412
[01:33:16.129] Epoch [1/5] Iteration [390/744]: Loss: 0.1048, CE: 0.0373
[01:33:20.493] Epoch [1/5] Iteration [400/744]: Loss: 0.1587, CE: 0.0471
[01:33:24.841] Epoch [1/5] Iteration [410/744]: Loss: 0.1595, CE: 0.0477
[01:33:29.194] Epoch [1/5] Iteration [420/744]: Loss: 0.1615, CE: 0.0566
[01:33:33.539] Epoch [1/5] Iteration [430/744]: Loss: 0.1581, CE: 0.0447
[01:33:37.902] Epoch [1/5] Iteration [440/744]: Loss: 0.1059, CE: 0.0339
[01:33:42.243] Epoch [1/5] Iteration [450/744]: Loss: 0.1535, CE: 0.0334
[01:33:46.605] Epoch [1/5] Iteration [460/744]: Loss: 0.1590, CE: 0.0504
[01:33:50.956] Epoch [1/5] Iteration [470/744]: Loss: 0.1550, CE: 0.0379
[01:33:55.323] Epoch [1/5] Iteration [480/744]: Loss: 0.1582, CE: 0.0455
[01:33:59.675] Epoch [1/5] Iteration [490/744]: Loss: 0.1592, CE: 0.0452
[01:34:04.038] Epoch [1/5] Iteration [500/744]: Loss: 0.1591, CE: 0.0500
[01:34:08.387] Epoch [1/5] Iteration [510/744]: Loss: 0.1628, CE: 0.0571
[01:34:12.747] Epoch [1/5] Iteration [520/744]: Loss: 0.1533, CE: 0.0332
[01:34:17.097] Epoch [1/5] Iteration [530/744]: Loss: 0.1614, CE: 0.0548
[01:34:21.465] Epoch [1/5] Iteration [540/744]: Loss: 0.1564, CE: 0.0448
[01:34:25.815] Epoch [1/5] Iteration [550/744]: Loss: 0.1055, CE: 0.0300
[01:34:30.176] Epoch [1/5] Iteration [560/744]: Loss: 0.1618, CE: 0.0514
[01:34:34.536] Epoch [1/5] Iteration [570/744]: Loss: 0.1547, CE: 0.0333
[01:34:38.900] Epoch [1/5] Iteration [580/744]: Loss: 0.1051, CE: 0.0379
[01:34:43.259] Epoch [1/5] Iteration [590/744]: Loss: 0.1524, CE: 0.0312
[01:34:47.621] Epoch [1/5] Iteration [600/744]: Loss: 0.1611, CE: 0.0633
[01:34:51.981] Epoch [1/5] Iteration [610/744]: Loss: 0.1568, CE: 0.0433
[01:34:56.343] Epoch [1/5] Iteration [620/744]: Loss: 0.1705, CE: 0.0749
[01:35:00.699] Epoch [1/5] Iteration [630/744]: Loss: 0.1553, CE: 0.0441
[01:35:05.065] Epoch [1/5] Iteration [640/744]: Loss: 0.1594, CE: 0.0478
[01:35:09.432] Epoch [1/5] Iteration [650/744]: Loss: 0.1592, CE: 0.0513
[01:35:13.792] Epoch [1/5] Iteration [660/744]: Loss: 0.1519, CE: 0.0393
[01:35:18.149] Epoch [1/5] Iteration [670/744]: Loss: 0.1584, CE: 0.0486
[01:35:22.519] Epoch [1/5] Iteration [680/744]: Loss: 0.1551, CE: 0.0477
[01:35:26.877] Epoch [1/5] Iteration [690/744]: Loss: 0.1628, CE: 0.0610
[01:35:31.240] Epoch [1/5] Iteration [700/744]: Loss: 0.1581, CE: 0.0518
[01:35:35.600] Epoch [1/5] Iteration [710/744]: Loss: 0.1035, CE: 0.0408
[01:35:39.971] Epoch [1/5] Iteration [720/744]: Loss: 0.1523, CE: 0.0360
[01:35:44.329] Epoch [1/5] Iteration [730/744]: Loss: 0.1531, CE: 0.0434
[01:35:48.695] Epoch [1/5] Iteration [740/744]: Loss: 0.1595, CE: 0.0527
[01:35:50.646] Epoch [1/5] Average Loss: 0.1501, CE: 0.0463, Dice: 0.2192
[01:36:31.980] Epoch [2/5] Iteration [0/744]: Loss: 0.1548, CE: 0.0516
[01:36:36.268] Epoch [2/5] Iteration [10/744]: Loss: 0.1549, CE: 0.0431
[01:36:40.567] Epoch [2/5] Iteration [20/744]: Loss: 0.1549, CE: 0.0492
[01:36:44.859] Epoch [2/5] Iteration [30/744]: Loss: 0.1539, CE: 0.0437
[01:36:49.162] Epoch [2/5] Iteration [40/744]: Loss: 0.1539, CE: 0.0433
[01:36:53.461] Epoch [2/5] Iteration [50/744]: Loss: 0.1562, CE: 0.0529
[01:36:57.773] Epoch [2/5] Iteration [60/744]: Loss: 0.1529, CE: 0.0483
[01:37:02.076] Epoch [2/5] Iteration [70/744]: Loss: 0.0978, CE: 0.0366
[01:37:06.393] Epoch [2/5] Iteration [80/744]: Loss: 0.0980, CE: 0.0329
[01:37:10.698] Epoch [2/5] Iteration [90/744]: Loss: 0.1485, CE: 0.0448
[01:37:15.014] Epoch [2/5] Iteration [100/744]: Loss: 0.1429, CE: 0.0349
[01:37:19.318] Epoch [2/5] Iteration [110/744]: Loss: 0.1462, CE: 0.0340
[01:37:23.633] Epoch [2/5] Iteration [120/744]: Loss: 0.0957, CE: 0.0445
[01:37:27.944] Epoch [2/5] Iteration [130/744]: Loss: 0.1409, CE: 0.0283
[01:37:32.263] Epoch [2/5] Iteration [140/744]: Loss: 0.1433, CE: 0.0367
[01:37:36.575] Epoch [2/5] Iteration [150/744]: Loss: 0.1452, CE: 0.0372
[01:37:40.899] Epoch [2/5] Iteration [160/744]: Loss: 0.1473, CE: 0.0444
[01:37:45.215] Epoch [2/5] Iteration [170/744]: Loss: 0.1408, CE: 0.0411
[01:37:49.541] Epoch [2/5] Iteration [180/744]: Loss: 0.0950, CE: 0.0374
[01:37:53.862] Epoch [2/5] Iteration [190/744]: Loss: 0.1336, CE: 0.0362
[01:37:58.197] Epoch [2/5] Iteration [200/744]: Loss: 0.1422, CE: 0.0438
[01:38:02.521] Epoch [2/5] Iteration [210/744]: Loss: 0.1380, CE: 0.0259
[01:38:06.862] Epoch [2/5] Iteration [220/744]: Loss: 0.1345, CE: 0.0255
[01:38:11.189] Epoch [2/5] Iteration [230/744]: Loss: 0.1328, CE: 0.0378
[01:38:15.536] Epoch [2/5] Iteration [240/744]: Loss: 0.1309, CE: 0.0263
[01:38:19.871] Epoch [2/5] Iteration [250/744]: Loss: 0.1308, CE: 0.0308
[01:38:52.110] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=5, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', dataset_fraction=0.25, enable_surgical=True, surgical_mode='RGN', surgical_update_freq=20, enable_tpgm=False, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[01:38:52.111] Continual Learning: 9 -> 12 classes
[01:38:52.111] Task offset: 9, New classes: 4
[01:38:52.111] Dataset fraction: 0.25
[01:38:52.111] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[01:38:52.111] Old task classes: 9, New task classes: 4
[01:38:52.163] Identified output layer keys: ['cswin_unet.output.weight']
[01:38:52.163] Loaded 462 backbone layers from pretrained model.
[01:38:52.163] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[01:38:52.200] Successfully adapted model for continual learning.
[01:38:52.200] Pretrained weights loaded and adapted for continual learning
[01:38:52.211] Using 23805/95221 samples (25.00% of dataset)
[01:38:52.214] Surgical fine-tuning enabled for continual learning
[01:38:52.216] 744 iterations per epoch. 3720 max iterations
[01:38:52.217] Updating surgical weights at epoch 0
[01:39:17.246] Surgical weights range: [0.0002, 1.0000]
[01:40:01.844] Epoch [0/5] Iteration [0/744]: Loss: 0.5991, CE: 0.1218
[01:40:06.310] Epoch [0/5] Iteration [10/744]: Loss: 0.6131, CE: 0.1637
[01:40:10.807] Epoch [0/5] Iteration [20/744]: Loss: 0.5387, CE: 0.1051
[01:40:15.278] Epoch [0/5] Iteration [30/744]: Loss: 0.3729, CE: 0.0904
[01:40:19.781] Epoch [0/5] Iteration [40/744]: Loss: 0.2826, CE: 0.1514
[01:40:24.300] Epoch [0/5] Iteration [50/744]: Loss: 0.2414, CE: 0.0987
[01:40:28.785] Epoch [0/5] Iteration [60/744]: Loss: 0.2756, CE: 0.2093
[01:40:33.276] Epoch [0/5] Iteration [70/744]: Loss: 0.2235, CE: 0.1250
[01:40:37.751] Epoch [0/5] Iteration [80/744]: Loss: 0.2198, CE: 0.1505
[01:40:42.235] Epoch [0/5] Iteration [90/744]: Loss: 0.2099, CE: 0.1367
[01:40:46.717] Epoch [0/5] Iteration [100/744]: Loss: 0.1981, CE: 0.1109
[01:40:51.203] Epoch [0/5] Iteration [110/744]: Loss: 0.2396, CE: 0.2145
[01:40:55.702] Epoch [0/5] Iteration [120/744]: Loss: 0.1530, CE: 0.1240
[01:41:00.201] Epoch [0/5] Iteration [130/744]: Loss: 0.0729, CE: 0.0501
[01:41:04.697] Epoch [0/5] Iteration [140/744]: Loss: 0.1981, CE: 0.1117
[01:41:09.174] Epoch [0/5] Iteration [150/744]: Loss: 0.1872, CE: 0.0844
[01:41:13.654] Epoch [0/5] Iteration [160/744]: Loss: 0.1896, CE: 0.0922
[01:41:18.167] Epoch [0/5] Iteration [170/744]: Loss: 0.1987, CE: 0.1142
[01:41:22.663] Epoch [0/5] Iteration [180/744]: Loss: 0.2004, CE: 0.1159
[01:41:27.137] Epoch [0/5] Iteration [190/744]: Loss: 0.2702, CE: 0.2923
[01:41:31.628] Epoch [0/5] Iteration [200/744]: Loss: 0.2109, CE: 0.1436
[01:41:36.109] Epoch [0/5] Iteration [210/744]: Loss: 0.2059, CE: 0.1327
[01:41:40.639] Epoch [0/5] Iteration [220/744]: Loss: 0.2459, CE: 0.2318
[01:41:45.141] Epoch [0/5] Iteration [230/744]: Loss: 0.1616, CE: 0.1447
[01:41:49.642] Epoch [0/5] Iteration [240/744]: Loss: 0.1488, CE: 0.1147
[01:41:54.148] Epoch [0/5] Iteration [250/744]: Loss: 0.2096, CE: 0.1411
[01:41:58.655] Epoch [0/5] Iteration [260/744]: Loss: 0.1419, CE: 0.0971
[01:42:03.149] Epoch [0/5] Iteration [270/744]: Loss: 0.2107, CE: 0.1419
[01:42:07.641] Epoch [0/5] Iteration [280/744]: Loss: 0.1489, CE: 0.1148
[01:42:12.135] Epoch [0/5] Iteration [290/744]: Loss: 0.2186, CE: 0.1644
[01:42:16.628] Epoch [0/5] Iteration [300/744]: Loss: 0.2169, CE: 0.1601
[01:42:21.113] Epoch [0/5] Iteration [310/744]: Loss: 0.1959, CE: 0.1062
[01:42:25.617] Epoch [0/5] Iteration [320/744]: Loss: 0.2063, CE: 0.1327
[01:42:30.119] Epoch [0/5] Iteration [330/744]: Loss: 0.2327, CE: 0.1984
[01:42:34.635] Epoch [0/5] Iteration [340/744]: Loss: 0.1449, CE: 0.1047
[01:42:39.143] Epoch [0/5] Iteration [350/744]: Loss: 0.2192, CE: 0.1649
[01:42:43.634] Epoch [0/5] Iteration [360/744]: Loss: 0.2253, CE: 0.1812
[01:42:48.155] Epoch [0/5] Iteration [370/744]: Loss: 0.2371, CE: 0.2093
[01:42:52.661] Epoch [0/5] Iteration [380/744]: Loss: 0.2038, CE: 0.1266
[01:42:57.166] Epoch [0/5] Iteration [390/744]: Loss: 0.1495, CE: 0.1161
[01:43:01.677] Epoch [0/5] Iteration [400/744]: Loss: 0.2058, CE: 0.1284
[01:43:06.179] Epoch [0/5] Iteration [410/744]: Loss: 0.2265, CE: 0.1818
[01:43:10.688] Epoch [0/5] Iteration [420/744]: Loss: 0.1871, CE: 0.0797
[01:43:15.187] Epoch [0/5] Iteration [430/744]: Loss: 0.2125, CE: 0.1484
[01:43:19.701] Epoch [0/5] Iteration [440/744]: Loss: 0.1930, CE: 0.1006
[01:43:24.197] Epoch [0/5] Iteration [450/744]: Loss: 0.1822, CE: 0.0735
[01:43:28.714] Epoch [0/5] Iteration [460/744]: Loss: 0.2338, CE: 0.2022
[01:43:33.221] Epoch [0/5] Iteration [470/744]: Loss: 0.2131, CE: 0.1507
[01:43:37.746] Epoch [0/5] Iteration [480/744]: Loss: 0.1975, CE: 0.1108
[01:43:42.256] Epoch [0/5] Iteration [490/744]: Loss: 0.1302, CE: 0.0655
[01:43:46.770] Epoch [0/5] Iteration [500/744]: Loss: 0.2036, CE: 0.1274
[01:43:51.261] Epoch [0/5] Iteration [510/744]: Loss: 0.1867, CE: 0.0831
[01:43:55.776] Epoch [0/5] Iteration [520/744]: Loss: 0.2206, CE: 0.1693
[01:44:00.360] Epoch [0/5] Iteration [530/744]: Loss: 0.1542, CE: 0.1287
[01:44:04.869] Epoch [0/5] Iteration [540/744]: Loss: 0.2069, CE: 0.1318
[01:44:09.379] Epoch [0/5] Iteration [550/744]: Loss: 0.2174, CE: 0.1593
[01:44:13.884] Epoch [0/5] Iteration [560/744]: Loss: 0.1796, CE: 0.0675
[01:44:18.370] Epoch [0/5] Iteration [570/744]: Loss: 0.1953, CE: 0.1035
[01:44:22.880] Epoch [0/5] Iteration [580/744]: Loss: 0.1849, CE: 0.0804
[01:44:27.386] Epoch [0/5] Iteration [590/744]: Loss: 0.2202, CE: 0.1683
[01:44:31.897] Epoch [0/5] Iteration [600/744]: Loss: 0.2106, CE: 0.1419
[01:44:36.389] Epoch [0/5] Iteration [610/744]: Loss: 0.2136, CE: 0.1502
[01:44:40.898] Epoch [0/5] Iteration [620/744]: Loss: 0.2163, CE: 0.1584
[01:44:45.400] Epoch [0/5] Iteration [630/744]: Loss: 0.2450, CE: 0.2280
[01:44:49.924] Epoch [0/5] Iteration [640/744]: Loss: 0.1223, CE: 0.0478
[01:44:54.426] Epoch [0/5] Iteration [650/744]: Loss: 0.1726, CE: 0.0486
[01:44:58.936] Epoch [0/5] Iteration [660/744]: Loss: 0.2151, CE: 0.1559
[01:45:03.442] Epoch [0/5] Iteration [670/744]: Loss: 0.1876, CE: 0.0874
[01:45:07.966] Epoch [0/5] Iteration [680/744]: Loss: 0.1759, CE: 0.0573
[01:45:12.485] Epoch [0/5] Iteration [690/744]: Loss: 0.2162, CE: 0.1574
[01:45:16.986] Epoch [0/5] Iteration [700/744]: Loss: 0.2201, CE: 0.1676
[01:45:21.488] Epoch [0/5] Iteration [710/744]: Loss: 0.1450, CE: 0.1030
[01:45:26.002] Epoch [0/5] Iteration [720/744]: Loss: 0.2296, CE: 0.1905
[01:45:30.519] Epoch [0/5] Iteration [730/744]: Loss: 0.2031, CE: 0.1263
[01:45:35.042] Epoch [0/5] Iteration [740/744]: Loss: 0.2174, CE: 0.1599
[01:45:37.081] Epoch [0/5] Average Loss: 0.2167, CE: 0.1397, Dice: 0.2679
[01:46:19.217] Epoch [1/5] Iteration [0/744]: Loss: 0.2008, CE: 0.1172
[01:46:23.664] Epoch [1/5] Iteration [10/744]: Loss: 0.2145, CE: 0.1531
[01:46:28.121] Epoch [1/5] Iteration [20/744]: Loss: 0.2158, CE: 0.1572
[01:46:38.160] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=5, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', dataset_fraction=0.25, enable_surgical=True, surgical_mode='RGN', surgical_update_freq=50, enable_tpgm=False, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[01:46:38.160] Continual Learning: 9 -> 12 classes
[01:46:38.160] Task offset: 9, New classes: 4
[01:46:38.160] Dataset fraction: 0.25
[01:46:38.160] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[01:46:38.160] Old task classes: 9, New task classes: 4
[01:46:38.207] Identified output layer keys: ['cswin_unet.output.weight']
[01:46:38.208] Loaded 462 backbone layers from pretrained model.
[01:46:38.208] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[01:46:38.241] Successfully adapted model for continual learning.
[01:46:38.242] Pretrained weights loaded and adapted for continual learning
[01:46:38.252] Using 23805/95221 samples (25.00% of dataset)
[01:46:38.255] Surgical fine-tuning enabled for continual learning
[01:46:38.257] 744 iterations per epoch. 3720 max iterations
[01:46:38.257] Updating surgical weights at epoch 0
[01:47:39.450] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=5, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', dataset_fraction=0.25, enable_surgical=True, surgical_mode='RGN', surgical_update_freq=1, enable_tpgm=False, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[01:47:39.450] Continual Learning: 9 -> 12 classes
[01:47:39.450] Task offset: 9, New classes: 4
[01:47:39.450] Dataset fraction: 0.25
[01:47:39.450] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[01:47:39.450] Old task classes: 9, New task classes: 4
[01:47:39.497] Identified output layer keys: ['cswin_unet.output.weight']
[01:47:39.498] Loaded 462 backbone layers from pretrained model.
[01:47:39.498] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[01:47:39.531] Successfully adapted model for continual learning.
[01:47:39.531] Pretrained weights loaded and adapted for continual learning
[01:47:39.540] Using 23805/95221 samples (25.00% of dataset)
[01:47:39.543] Surgical fine-tuning enabled for continual learning
[01:47:39.545] 744 iterations per epoch. 3720 max iterations
[01:47:39.546] Updating surgical weights at epoch 0
[01:48:03.100] Surgical weights range: [0.0002, 1.0000]
[01:48:46.643] Epoch [0/5] Iteration [0/744]: Loss: 0.5991, CE: 0.1218
[01:48:51.066] Epoch [0/5] Iteration [10/744]: Loss: 0.6131, CE: 0.1637
[01:48:55.542] Epoch [0/5] Iteration [20/744]: Loss: 0.5387, CE: 0.1051
[01:48:59.978] Epoch [0/5] Iteration [30/744]: Loss: 0.3729, CE: 0.0904
[01:49:04.440] Epoch [0/5] Iteration [40/744]: Loss: 0.2826, CE: 0.1514
[01:49:08.921] Epoch [0/5] Iteration [50/744]: Loss: 0.2414, CE: 0.0987
[01:49:13.376] Epoch [0/5] Iteration [60/744]: Loss: 0.2756, CE: 0.2093
[01:49:17.827] Epoch [0/5] Iteration [70/744]: Loss: 0.2235, CE: 0.1250
[01:49:22.297] Epoch [0/5] Iteration [80/744]: Loss: 0.2198, CE: 0.1505
[01:49:26.769] Epoch [0/5] Iteration [90/744]: Loss: 0.2099, CE: 0.1367
[01:49:31.253] Epoch [0/5] Iteration [100/744]: Loss: 0.1981, CE: 0.1109
[01:49:35.729] Epoch [0/5] Iteration [110/744]: Loss: 0.2396, CE: 0.2145
[01:49:40.218] Epoch [0/5] Iteration [120/744]: Loss: 0.1530, CE: 0.1240
[01:49:44.709] Epoch [0/5] Iteration [130/744]: Loss: 0.0729, CE: 0.0501
[01:49:49.177] Epoch [0/5] Iteration [140/744]: Loss: 0.1981, CE: 0.1117
[01:49:53.676] Epoch [0/5] Iteration [150/744]: Loss: 0.1872, CE: 0.0844
[01:49:58.159] Epoch [0/5] Iteration [160/744]: Loss: 0.1896, CE: 0.0922
[01:50:02.648] Epoch [0/5] Iteration [170/744]: Loss: 0.1987, CE: 0.1142
[01:50:24.976] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, output_dir='./universal/synapse_to_kits23_tpgm', max_iterations=30000, max_epochs=10, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, continual_learning=True, pretrained_ckpt='./pretrain/epoch_149.pth', old_num_classes=9, old_dataset='Synapse', dataset_fraction=0.5, enable_surgical=False, surgical_mode='RGN', surgical_update_freq=10, enable_tpgm=False, tpgm_proj_freq=5, tpgm_max_iters=50, tpgm_proj_lr=0.01, tpgm_norm_mode='mars', tpgm_batch_size=8)
[01:50:24.976] Continual Learning: 9 -> 12 classes
[01:50:24.976] Task offset: 9, New classes: 4
[01:50:24.976] Dataset fraction: 0.5
[01:50:24.976] Loading pretrained model from ./pretrain/epoch_149.pth for continual learning.
[01:50:24.976] Old task classes: 9, New task classes: 4
[01:50:25.023] Identified output layer keys: ['cswin_unet.output.weight']
[01:50:25.025] Loaded 462 backbone layers from pretrained model.
[01:50:25.025] Copied pretrained weights for 'cswin_unet.output.weight' for the first 9 classes.
[01:50:25.060] Successfully adapted model for continual learning.
[01:50:25.061] Pretrained weights loaded and adapted for continual learning
[01:50:25.073] Using 47610/95221 samples (50.00% of dataset)
[01:50:25.077] 1488 iterations per epoch. 14880 max iterations
[01:51:07.674] Epoch [0/10] Iteration [0/1488]: Loss: 0.5848, CE: 0.0870
[01:51:11.961] Epoch [0/10] Iteration [10/1488]: Loss: 0.3075, CE: 0.1060
[01:51:16.277] Epoch [0/10] Iteration [20/1488]: Loss: 0.2474, CE: 0.2419
[01:51:20.571] Epoch [0/10] Iteration [30/1488]: Loss: 0.2140, CE: 0.1595
[01:51:24.875] Epoch [0/10] Iteration [40/1488]: Loss: 0.1959, CE: 0.1141
[01:51:29.173] Epoch [0/10] Iteration [50/1488]: Loss: 0.1896, CE: 0.0910
[01:51:33.486] Epoch [0/10] Iteration [60/1488]: Loss: 0.1893, CE: 0.0953
[01:51:37.792] Epoch [0/10] Iteration [70/1488]: Loss: 0.1871, CE: 0.0902
[01:51:42.104] Epoch [0/10] Iteration [80/1488]: Loss: 0.1902, CE: 0.0973
[01:51:46.401] Epoch [0/10] Iteration [90/1488]: Loss: 0.1744, CE: 0.0579
[01:51:50.712] Epoch [0/10] Iteration [100/1488]: Loss: 0.1795, CE: 0.0687
[01:51:55.012] Epoch [0/10] Iteration [110/1488]: Loss: 0.1977, CE: 0.1154
[01:51:59.323] Epoch [0/10] Iteration [120/1488]: Loss: 0.1883, CE: 0.0923
[01:52:03.628] Epoch [0/10] Iteration [130/1488]: Loss: 0.1734, CE: 0.0559
[01:52:07.950] Epoch [0/10] Iteration [140/1488]: Loss: 0.2030, CE: 0.1297
[01:52:12.262] Epoch [0/10] Iteration [150/1488]: Loss: 0.1784, CE: 0.0692
[01:52:16.582] Epoch [0/10] Iteration [160/1488]: Loss: 0.1742, CE: 0.0593
[01:52:20.889] Epoch [0/10] Iteration [170/1488]: Loss: 0.1806, CE: 0.0760
[01:52:25.215] Epoch [0/10] Iteration [180/1488]: Loss: 0.1801, CE: 0.0770
[01:52:29.525] Epoch [0/10] Iteration [190/1488]: Loss: 0.1743, CE: 0.0636
[01:52:33.854] Epoch [0/10] Iteration [200/1488]: Loss: 0.1600, CE: 0.0314
[01:52:38.163] Epoch [0/10] Iteration [210/1488]: Loss: 0.1141, CE: 0.0428
[01:52:42.488] Epoch [0/10] Iteration [220/1488]: Loss: 0.1632, CE: 0.0408
[01:52:46.804] Epoch [0/10] Iteration [230/1488]: Loss: 0.1681, CE: 0.0563
[01:52:51.141] Epoch [0/10] Iteration [240/1488]: Loss: 0.1646, CE: 0.0460
[01:52:55.462] Epoch [0/10] Iteration [250/1488]: Loss: 0.1641, CE: 0.0460
[01:52:59.794] Epoch [0/10] Iteration [260/1488]: Loss: 0.1119, CE: 0.0420
[01:53:04.115] Epoch [0/10] Iteration [270/1488]: Loss: 0.1167, CE: 0.0557
[01:53:08.444] Epoch [0/10] Iteration [280/1488]: Loss: 0.1604, CE: 0.0389
[01:53:12.766] Epoch [0/10] Iteration [290/1488]: Loss: 0.1775, CE: 0.0826
[01:53:17.100] Epoch [0/10] Iteration [300/1488]: Loss: 0.1642, CE: 0.0503
[01:53:21.425] Epoch [0/10] Iteration [310/1488]: Loss: 0.1608, CE: 0.0419
[01:53:25.753] Epoch [0/10] Iteration [320/1488]: Loss: 0.1224, CE: 0.0712
[01:53:30.081] Epoch [0/10] Iteration [330/1488]: Loss: 0.1576, CE: 0.0345
[01:53:34.411] Epoch [0/10] Iteration [340/1488]: Loss: 0.1096, CE: 0.0374
[01:53:38.735] Epoch [0/10] Iteration [350/1488]: Loss: 0.1597, CE: 0.0380
[01:53:43.079] Epoch [0/10] Iteration [360/1488]: Loss: 0.1616, CE: 0.0438
[01:53:47.405] Epoch [0/10] Iteration [370/1488]: Loss: 0.1088, CE: 0.0378
[01:53:51.744] Epoch [0/10] Iteration [380/1488]: Loss: 0.1692, CE: 0.0635
[01:53:56.074] Epoch [0/10] Iteration [390/1488]: Loss: 0.1680, CE: 0.0606
[01:54:00.416] Epoch [0/10] Iteration [400/1488]: Loss: 0.1672, CE: 0.0592
[01:54:04.744] Epoch [0/10] Iteration [410/1488]: Loss: 0.1765, CE: 0.0798
[01:54:09.086] Epoch [0/10] Iteration [420/1488]: Loss: 0.1555, CE: 0.0313
[01:54:13.420] Epoch [0/10] Iteration [430/1488]: Loss: 0.1550, CE: 0.0230
[01:54:17.767] Epoch [0/10] Iteration [440/1488]: Loss: 0.1625, CE: 0.0490
[01:54:22.102] Epoch [0/10] Iteration [450/1488]: Loss: 0.1081, CE: 0.0395
[01:54:26.445] Epoch [0/10] Iteration [460/1488]: Loss: 0.1596, CE: 0.0395
[01:54:30.787] Epoch [0/10] Iteration [470/1488]: Loss: 0.1718, CE: 0.0733
[01:54:35.136] Epoch [0/10] Iteration [480/1488]: Loss: 0.1643, CE: 0.0529
[01:54:39.478] Epoch [0/10] Iteration [490/1488]: Loss: 0.1721, CE: 0.0713
[01:54:43.826] Epoch [0/10] Iteration [500/1488]: Loss: 0.1750, CE: 0.0818
[01:54:48.169] Epoch [0/10] Iteration [510/1488]: Loss: 0.1666, CE: 0.0591
[01:54:52.519] Epoch [0/10] Iteration [520/1488]: Loss: 0.1559, CE: 0.0321
[01:54:56.861] Epoch [0/10] Iteration [530/1488]: Loss: 0.1650, CE: 0.0541
[01:55:01.210] Epoch [0/10] Iteration [540/1488]: Loss: 0.1604, CE: 0.0437
[01:55:05.552] Epoch [0/10] Iteration [550/1488]: Loss: 0.1148, CE: 0.0537
[01:55:09.913] Epoch [0/10] Iteration [560/1488]: Loss: 0.1670, CE: 0.0623
[01:55:14.259] Epoch [0/10] Iteration [570/1488]: Loss: 0.1872, CE: 0.1118
[01:55:18.615] Epoch [0/10] Iteration [580/1488]: Loss: 0.1567, CE: 0.0376
[01:55:22.963] Epoch [0/10] Iteration [590/1488]: Loss: 0.1054, CE: 0.0300
[01:55:27.319] Epoch [0/10] Iteration [600/1488]: Loss: 0.1140, CE: 0.0558
[01:55:31.673] Epoch [0/10] Iteration [610/1488]: Loss: 0.1104, CE: 0.0423
[01:55:36.028] Epoch [0/10] Iteration [620/1488]: Loss: 0.1587, CE: 0.0433
[01:55:40.372] Epoch [0/10] Iteration [630/1488]: Loss: 0.1110, CE: 0.0445
[01:55:44.727] Epoch [0/10] Iteration [640/1488]: Loss: 0.1077, CE: 0.0369
[01:55:49.071] Epoch [0/10] Iteration [650/1488]: Loss: 0.1646, CE: 0.0544
[01:55:53.434] Epoch [0/10] Iteration [660/1488]: Loss: 0.1584, CE: 0.0462
[01:55:57.782] Epoch [0/10] Iteration [670/1488]: Loss: 0.1533, CE: 0.0288
[01:56:02.151] Epoch [0/10] Iteration [680/1488]: Loss: 0.1655, CE: 0.0573
[01:56:06.502] Epoch [0/10] Iteration [690/1488]: Loss: 0.1585, CE: 0.0439
[01:56:10.860] Epoch [0/10] Iteration [700/1488]: Loss: 0.1592, CE: 0.0441
[01:56:15.212] Epoch [0/10] Iteration [710/1488]: Loss: 0.1580, CE: 0.0419
[01:56:19.576] Epoch [0/10] Iteration [720/1488]: Loss: 0.1520, CE: 0.0369
[01:56:23.929] Epoch [0/10] Iteration [730/1488]: Loss: 0.1669, CE: 0.0747
[01:56:28.295] Epoch [0/10] Iteration [740/1488]: Loss: 0.1527, CE: 0.0407
[01:56:32.650] Epoch [0/10] Iteration [750/1488]: Loss: 0.1585, CE: 0.0591
[01:56:37.014] Epoch [0/10] Iteration [760/1488]: Loss: 0.1629, CE: 0.0630
[01:56:41.372] Epoch [0/10] Iteration [770/1488]: Loss: 0.1426, CE: 0.0315
[01:56:45.737] Epoch [0/10] Iteration [780/1488]: Loss: 0.1536, CE: 0.0594
[01:56:50.103] Epoch [0/10] Iteration [790/1488]: Loss: 0.0966, CE: 0.0347
[01:56:54.465] Epoch [0/10] Iteration [800/1488]: Loss: 0.1328, CE: 0.0254
[01:56:58.827] Epoch [0/10] Iteration [810/1488]: Loss: 0.1295, CE: 0.0316
[01:57:03.198] Epoch [0/10] Iteration [820/1488]: Loss: 0.1345, CE: 0.0390
[01:57:07.574] Epoch [0/10] Iteration [830/1488]: Loss: 0.1319, CE: 0.0214
[01:57:11.931] Epoch [0/10] Iteration [840/1488]: Loss: 0.0734, CE: 0.0281
[01:57:16.291] Epoch [0/10] Iteration [850/1488]: Loss: 0.1197, CE: 0.0205
[01:57:20.661] Epoch [0/10] Iteration [860/1488]: Loss: 0.1314, CE: 0.0371
[01:57:25.020] Epoch [0/10] Iteration [870/1488]: Loss: 0.1197, CE: 0.0346
[01:57:29.387] Epoch [0/10] Iteration [880/1488]: Loss: 0.1156, CE: 0.0150
[01:57:33.756] Epoch [0/10] Iteration [890/1488]: Loss: 0.1128, CE: 0.0155
[01:57:38.145] Epoch [0/10] Iteration [900/1488]: Loss: 0.1186, CE: 0.0170
[01:57:42.507] Epoch [0/10] Iteration [910/1488]: Loss: 0.0644, CE: 0.0108
[01:57:46.883] Epoch [0/10] Iteration [920/1488]: Loss: 0.0530, CE: 0.0145
[01:57:51.244] Epoch [0/10] Iteration [930/1488]: Loss: 0.0986, CE: 0.0151
[01:57:55.623] Epoch [0/10] Iteration [940/1488]: Loss: 0.0995, CE: 0.0151
[01:57:59.994] Epoch [0/10] Iteration [950/1488]: Loss: 0.1148, CE: 0.0165
[01:58:04.371] Epoch [0/10] Iteration [960/1488]: Loss: 0.1061, CE: 0.0124
[01:58:08.745] Epoch [0/10] Iteration [970/1488]: Loss: 0.0959, CE: 0.0169
[01:58:13.119] Epoch [0/10] Iteration [980/1488]: Loss: 0.1145, CE: 0.0271
[01:58:17.481] Epoch [0/10] Iteration [990/1488]: Loss: 0.1059, CE: 0.0177
[01:58:21.864] Epoch [0/10] Iteration [1000/1488]: Loss: 0.1157, CE: 0.0190
[01:58:26.239] Epoch [0/10] Iteration [1010/1488]: Loss: 0.1031, CE: 0.0111
[01:58:30.617] Epoch [0/10] Iteration [1020/1488]: Loss: 0.1114, CE: 0.0161
[01:58:34.987] Epoch [0/10] Iteration [1030/1488]: Loss: 0.0566, CE: 0.0126
[01:58:39.360] Epoch [0/10] Iteration [1040/1488]: Loss: 0.0985, CE: 0.0159
[01:58:43.730] Epoch [0/10] Iteration [1050/1488]: Loss: 0.0495, CE: 0.0108
[01:58:48.133] Epoch [0/10] Iteration [1060/1488]: Loss: 0.0940, CE: 0.0211
[01:58:52.496] Epoch [0/10] Iteration [1070/1488]: Loss: 0.1042, CE: 0.0263
[01:58:56.875] Epoch [0/10] Iteration [1080/1488]: Loss: 0.1064, CE: 0.0491
[01:59:01.248] Epoch [0/10] Iteration [1090/1488]: Loss: 0.1055, CE: 0.0135
[01:59:05.634] Epoch [0/10] Iteration [1100/1488]: Loss: 0.1120, CE: 0.0294
[01:59:10.010] Epoch [0/10] Iteration [1110/1488]: Loss: 0.1100, CE: 0.0094
[01:59:14.396] Epoch [0/10] Iteration [1120/1488]: Loss: 0.0993, CE: 0.0143
[01:59:18.762] Epoch [0/10] Iteration [1130/1488]: Loss: 0.0895, CE: 0.0134
[01:59:23.142] Epoch [0/10] Iteration [1140/1488]: Loss: 0.0357, CE: 0.0071
[01:59:27.519] Epoch [0/10] Iteration [1150/1488]: Loss: 0.0980, CE: 0.0094
[01:59:31.895] Epoch [0/10] Iteration [1160/1488]: Loss: 0.0835, CE: 0.0150
[01:59:36.258] Epoch [0/10] Iteration [1170/1488]: Loss: 0.0774, CE: 0.0104
[01:59:40.638] Epoch [0/10] Iteration [1180/1488]: Loss: 0.0754, CE: 0.0086
[01:59:45.011] Epoch [0/10] Iteration [1190/1488]: Loss: 0.1113, CE: 0.0081
[01:59:49.399] Epoch [0/10] Iteration [1200/1488]: Loss: 0.0489, CE: 0.0034
[01:59:53.785] Epoch [0/10] Iteration [1210/1488]: Loss: 0.0592, CE: 0.0130
[01:59:58.178] Epoch [0/10] Iteration [1220/1488]: Loss: 0.0472, CE: 0.0091
[02:00:02.536] Epoch [0/10] Iteration [1230/1488]: Loss: 0.1218, CE: 0.0364
[02:00:06.916] Epoch [0/10] Iteration [1240/1488]: Loss: 0.1086, CE: 0.0207
[02:00:11.299] Epoch [0/10] Iteration [1250/1488]: Loss: 0.0291, CE: 0.0081
[02:00:15.688] Epoch [0/10] Iteration [1260/1488]: Loss: 0.0779, CE: 0.0048
[02:00:20.054] Epoch [0/10] Iteration [1270/1488]: Loss: 0.0850, CE: 0.0130
[02:00:24.437] Epoch [0/10] Iteration [1280/1488]: Loss: 0.0750, CE: 0.0167
[02:00:28.813] Epoch [0/10] Iteration [1290/1488]: Loss: 0.0814, CE: 0.0163
[02:00:33.190] Epoch [0/10] Iteration [1300/1488]: Loss: 0.0116, CE: 0.0061
[02:00:37.575] Epoch [0/10] Iteration [1310/1488]: Loss: 0.0715, CE: 0.0074
[02:00:41.966] Epoch [0/10] Iteration [1320/1488]: Loss: 0.0694, CE: 0.0075
[02:00:46.343] Epoch [0/10] Iteration [1330/1488]: Loss: 0.0719, CE: 0.0100
[02:00:50.720] Epoch [0/10] Iteration [1340/1488]: Loss: 0.0830, CE: 0.0109
[02:00:55.089] Epoch [0/10] Iteration [1350/1488]: Loss: 0.1055, CE: 0.0055
[02:00:59.468] Epoch [0/10] Iteration [1360/1488]: Loss: 0.0860, CE: 0.0209
[02:01:03.850] Epoch [0/10] Iteration [1370/1488]: Loss: 0.0850, CE: 0.0119
[02:01:08.239] Epoch [0/10] Iteration [1380/1488]: Loss: 0.0886, CE: 0.0064
[02:01:12.606] Epoch [0/10] Iteration [1390/1488]: Loss: 0.0487, CE: 0.0068
[02:01:16.988] Epoch [0/10] Iteration [1400/1488]: Loss: 0.1049, CE: 0.0140
[02:01:21.366] Epoch [0/10] Iteration [1410/1488]: Loss: 0.1053, CE: 0.0108
[02:01:25.763] Epoch [0/10] Iteration [1420/1488]: Loss: 0.0833, CE: 0.0042
[02:01:30.143] Epoch [0/10] Iteration [1430/1488]: Loss: 0.0904, CE: 0.0064
[02:01:34.527] Epoch [0/10] Iteration [1440/1488]: Loss: 0.1070, CE: 0.0104
[02:01:38.903] Epoch [0/10] Iteration [1450/1488]: Loss: 0.0419, CE: 0.0076
[02:01:43.280] Epoch [0/10] Iteration [1460/1488]: Loss: 0.1035, CE: 0.0201
[02:01:47.672] Epoch [0/10] Iteration [1470/1488]: Loss: 0.0800, CE: 0.0120
[02:01:52.055] Epoch [0/10] Iteration [1480/1488]: Loss: 0.0877, CE: 0.0156
[02:01:55.786] Epoch [0/10] Average Loss: 0.1294, CE: 0.0413, Dice: 0.1882
[02:02:38.957] Epoch [1/10] Iteration [0/1488]: Loss: 0.0927, CE: 0.0089
[02:02:43.248] Epoch [1/10] Iteration [10/1488]: Loss: 0.1043, CE: 0.0154
[02:02:47.554] Epoch [1/10] Iteration [20/1488]: Loss: 0.0869, CE: 0.0162
[02:02:51.851] Epoch [1/10] Iteration [30/1488]: Loss: 0.0779, CE: 0.0097
[02:02:56.159] Epoch [1/10] Iteration [40/1488]: Loss: 0.0907, CE: 0.0241
[02:03:00.456] Epoch [1/10] Iteration [50/1488]: Loss: 0.0254, CE: 0.0098
[02:03:04.767] Epoch [1/10] Iteration [60/1488]: Loss: 0.0735, CE: 0.0129
[02:03:09.071] Epoch [1/10] Iteration [70/1488]: Loss: 0.0537, CE: 0.0037
[02:03:13.391] Epoch [1/10] Iteration [80/1488]: Loss: 0.0655, CE: 0.0076
[02:03:17.697] Epoch [1/10] Iteration [90/1488]: Loss: 0.0246, CE: 0.0098
[02:03:22.019] Epoch [1/10] Iteration [100/1488]: Loss: 0.0380, CE: 0.0044
[02:03:26.327] Epoch [1/10] Iteration [110/1488]: Loss: 0.0671, CE: 0.0096
[02:03:30.646] Epoch [1/10] Iteration [120/1488]: Loss: 0.0700, CE: 0.0081
[02:03:34.956] Epoch [1/10] Iteration [130/1488]: Loss: 0.0502, CE: 0.0056
[02:03:39.286] Epoch [1/10] Iteration [140/1488]: Loss: 0.1071, CE: 0.0125
[02:03:43.603] Epoch [1/10] Iteration [150/1488]: Loss: 0.0915, CE: 0.0138
[02:03:47.932] Epoch [1/10] Iteration [160/1488]: Loss: 0.0194, CE: 0.0023
[02:03:52.249] Epoch [1/10] Iteration [170/1488]: Loss: 0.0588, CE: 0.0041
[02:03:56.582] Epoch [1/10] Iteration [180/1488]: Loss: 0.0389, CE: 0.0041
[02:04:00.910] Epoch [1/10] Iteration [190/1488]: Loss: 0.0725, CE: 0.0087
[02:04:05.245] Epoch [1/10] Iteration [200/1488]: Loss: 0.0214, CE: 0.0037
[02:04:09.573] Epoch [1/10] Iteration [210/1488]: Loss: 0.1098, CE: 0.0101
[02:04:13.916] Epoch [1/10] Iteration [220/1488]: Loss: 0.0951, CE: 0.0088
[02:04:18.246] Epoch [1/10] Iteration [230/1488]: Loss: 0.0566, CE: 0.0048
[02:04:22.591] Epoch [1/10] Iteration [240/1488]: Loss: 0.0852, CE: 0.0177
[02:04:26.931] Epoch [1/10] Iteration [250/1488]: Loss: 0.0751, CE: 0.0056
[02:04:31.275] Epoch [1/10] Iteration [260/1488]: Loss: 0.0727, CE: 0.0136
[02:04:35.610] Epoch [1/10] Iteration [270/1488]: Loss: 0.0211, CE: 0.0061
[02:04:39.956] Epoch [1/10] Iteration [280/1488]: Loss: 0.0929, CE: 0.0061
[02:04:44.296] Epoch [1/10] Iteration [290/1488]: Loss: 0.0731, CE: 0.0107
[02:04:48.646] Epoch [1/10] Iteration [300/1488]: Loss: 0.0738, CE: 0.0178
[02:04:52.985] Epoch [1/10] Iteration [310/1488]: Loss: 0.0224, CE: 0.0052
[02:04:57.337] Epoch [1/10] Iteration [320/1488]: Loss: 0.0730, CE: 0.0057
[02:05:01.680] Epoch [1/10] Iteration [330/1488]: Loss: 0.0617, CE: 0.0071
[02:05:06.032] Epoch [1/10] Iteration [340/1488]: Loss: 0.0163, CE: 0.0043
[02:05:10.377] Epoch [1/10] Iteration [350/1488]: Loss: 0.0781, CE: 0.0076
[02:05:14.734] Epoch [1/10] Iteration [360/1488]: Loss: 0.0819, CE: 0.0246
[02:05:19.082] Epoch [1/10] Iteration [370/1488]: Loss: 0.0626, CE: 0.0050
[02:05:23.439] Epoch [1/10] Iteration [380/1488]: Loss: 0.0764, CE: 0.0127
[02:05:27.789] Epoch [1/10] Iteration [390/1488]: Loss: 0.0635, CE: 0.0072
[02:05:32.146] Epoch [1/10] Iteration [400/1488]: Loss: 0.0823, CE: 0.0122
[02:05:36.495] Epoch [1/10] Iteration [410/1488]: Loss: 0.1006, CE: 0.0287
[02:05:40.864] Epoch [1/10] Iteration [420/1488]: Loss: 0.0236, CE: 0.0069
[02:05:45.216] Epoch [1/10] Iteration [430/1488]: Loss: 0.0734, CE: 0.0105
[02:05:49.579] Epoch [1/10] Iteration [440/1488]: Loss: 0.0982, CE: 0.0098
[02:05:53.932] Epoch [1/10] Iteration [450/1488]: Loss: 0.0989, CE: 0.0067
[02:05:58.294] Epoch [1/10] Iteration [460/1488]: Loss: 0.0686, CE: 0.0102
[02:06:02.650] Epoch [1/10] Iteration [470/1488]: Loss: 0.0720, CE: 0.0091
[02:06:07.018] Epoch [1/10] Iteration [480/1488]: Loss: 0.0903, CE: 0.0068
[02:06:11.369] Epoch [1/10] Iteration [490/1488]: Loss: 0.0852, CE: 0.0087
[02:06:15.741] Epoch [1/10] Iteration [500/1488]: Loss: 0.0268, CE: 0.0038
[02:06:20.113] Epoch [1/10] Iteration [510/1488]: Loss: 0.0171, CE: 0.0058
[02:06:24.477] Epoch [1/10] Iteration [520/1488]: Loss: 0.0675, CE: 0.0127
[02:06:28.832] Epoch [1/10] Iteration [530/1488]: Loss: 0.0723, CE: 0.0201
[02:06:33.203] Epoch [1/10] Iteration [540/1488]: Loss: 0.0616, CE: 0.0052
[02:06:37.562] Epoch [1/10] Iteration [550/1488]: Loss: 0.0168, CE: 0.0074
[02:06:41.930] Epoch [1/10] Iteration [560/1488]: Loss: 0.0540, CE: 0.0030
[02:06:46.294] Epoch [1/10] Iteration [570/1488]: Loss: 0.0240, CE: 0.0063
[02:06:50.668] Epoch [1/10] Iteration [580/1488]: Loss: 0.0331, CE: 0.0087
[02:06:55.023] Epoch [1/10] Iteration [590/1488]: Loss: 0.1106, CE: 0.0260
[02:06:59.401] Epoch [1/10] Iteration [600/1488]: Loss: 0.0662, CE: 0.0103
[02:07:03.771] Epoch [1/10] Iteration [610/1488]: Loss: 0.0782, CE: 0.0101
[02:07:08.141] Epoch [1/10] Iteration [620/1488]: Loss: 0.0694, CE: 0.0073
[02:07:12.503] Epoch [1/10] Iteration [630/1488]: Loss: 0.0828, CE: 0.0056
[02:07:16.879] Epoch [1/10] Iteration [640/1488]: Loss: 0.0894, CE: 0.0339
[02:07:21.231] Epoch [1/10] Iteration [650/1488]: Loss: 0.0749, CE: 0.0067
[02:07:25.602] Epoch [1/10] Iteration [660/1488]: Loss: 0.0779, CE: 0.0170
[02:07:29.974] Epoch [1/10] Iteration [670/1488]: Loss: 0.0623, CE: 0.0113
[02:07:34.346] Epoch [1/10] Iteration [680/1488]: Loss: 0.0687, CE: 0.0034
[02:07:38.714] Epoch [1/10] Iteration [690/1488]: Loss: 0.0616, CE: 0.0081
[02:07:43.086] Epoch [1/10] Iteration [700/1488]: Loss: 0.0200, CE: 0.0080
[02:07:47.453] Epoch [1/10] Iteration [710/1488]: Loss: 0.0806, CE: 0.0079
[02:07:51.831] Epoch [1/10] Iteration [720/1488]: Loss: 0.0680, CE: 0.0067
[02:07:56.200] Epoch [1/10] Iteration [730/1488]: Loss: 0.0688, CE: 0.0071
[02:08:00.582] Epoch [1/10] Iteration [740/1488]: Loss: 0.0704, CE: 0.0095
[02:08:04.945] Epoch [1/10] Iteration [750/1488]: Loss: 0.0750, CE: 0.0089
[02:08:09.334] Epoch [1/10] Iteration [760/1488]: Loss: 0.0887, CE: 0.0186
[02:08:13.702] Epoch [1/10] Iteration [770/1488]: Loss: 0.0592, CE: 0.0039
[02:08:18.081] Epoch [1/10] Iteration [780/1488]: Loss: 0.0678, CE: 0.0050
[02:08:22.442] Epoch [1/10] Iteration [790/1488]: Loss: 0.1072, CE: 0.0103
[02:08:26.817] Epoch [1/10] Iteration [800/1488]: Loss: 0.0794, CE: 0.0073
[02:08:31.183] Epoch [1/10] Iteration [810/1488]: Loss: 0.0653, CE: 0.0102
[02:08:35.556] Epoch [1/10] Iteration [820/1488]: Loss: 0.0928, CE: 0.0112
[02:08:39.927] Epoch [1/10] Iteration [830/1488]: Loss: 0.0807, CE: 0.0046
[02:08:44.312] Epoch [1/10] Iteration [840/1488]: Loss: 0.0934, CE: 0.0046
[02:08:48.680] Epoch [1/10] Iteration [850/1488]: Loss: 0.0799, CE: 0.0050
[02:08:53.061] Epoch [1/10] Iteration [860/1488]: Loss: 0.0724, CE: 0.0087
[02:08:57.420] Epoch [1/10] Iteration [870/1488]: Loss: 0.0583, CE: 0.0060
[02:09:01.794] Epoch [1/10] Iteration [880/1488]: Loss: 0.0925, CE: 0.0064
[02:09:06.168] Epoch [1/10] Iteration [890/1488]: Loss: 0.0682, CE: 0.0120
[02:09:10.553] Epoch [1/10] Iteration [900/1488]: Loss: 0.0778, CE: 0.0080
[02:09:14.916] Epoch [1/10] Iteration [910/1488]: Loss: 0.0607, CE: 0.0074
[02:09:19.294] Epoch [1/10] Iteration [920/1488]: Loss: 0.0706, CE: 0.0076
[02:09:23.661] Epoch [1/10] Iteration [930/1488]: Loss: 0.0622, CE: 0.0074
[02:09:28.036] Epoch [1/10] Iteration [940/1488]: Loss: 0.0660, CE: 0.0109
[02:09:32.411] Epoch [1/10] Iteration [950/1488]: Loss: 0.1037, CE: 0.0074
[02:09:36.793] Epoch [1/10] Iteration [960/1488]: Loss: 0.0778, CE: 0.0058
[02:09:41.166] Epoch [1/10] Iteration [970/1488]: Loss: 0.0881, CE: 0.0191
[02:09:45.543] Epoch [1/10] Iteration [980/1488]: Loss: 0.0774, CE: 0.0067
[02:09:49.914] Epoch [1/10] Iteration [990/1488]: Loss: 0.0838, CE: 0.0076
[02:09:54.302] Epoch [1/10] Iteration [1000/1488]: Loss: 0.0120, CE: 0.0050
[02:09:58.679] Epoch [1/10] Iteration [1010/1488]: Loss: 0.1028, CE: 0.0267
[02:10:03.054] Epoch [1/10] Iteration [1020/1488]: Loss: 0.0744, CE: 0.0183
[02:10:07.418] Epoch [1/10] Iteration [1030/1488]: Loss: 0.0639, CE: 0.0056
[02:10:11.796] Epoch [1/10] Iteration [1040/1488]: Loss: 0.0648, CE: 0.0099
[02:10:16.176] Epoch [1/10] Iteration [1050/1488]: Loss: 0.0702, CE: 0.0065
[02:10:20.555] Epoch [1/10] Iteration [1060/1488]: Loss: 0.0340, CE: 0.0050
[02:10:24.917] Epoch [1/10] Iteration [1070/1488]: Loss: 0.0697, CE: 0.0078
[02:10:29.298] Epoch [1/10] Iteration [1080/1488]: Loss: 0.0557, CE: 0.0047
[02:10:33.674] Epoch [1/10] Iteration [1090/1488]: Loss: 0.0749, CE: 0.0172
[02:10:38.053] Epoch [1/10] Iteration [1100/1488]: Loss: 0.0729, CE: 0.0090
[02:10:42.432] Epoch [1/10] Iteration [1110/1488]: Loss: 0.0826, CE: 0.0151
[02:10:46.819] Epoch [1/10] Iteration [1120/1488]: Loss: 0.0895, CE: 0.0103
[02:10:51.187] Epoch [1/10] Iteration [1130/1488]: Loss: 0.0662, CE: 0.0121
[02:10:55.570] Epoch [1/10] Iteration [1140/1488]: Loss: 0.0662, CE: 0.0055
[02:10:59.954] Epoch [1/10] Iteration [1150/1488]: Loss: 0.0657, CE: 0.0047
[02:11:04.353] Epoch [1/10] Iteration [1160/1488]: Loss: 0.0642, CE: 0.0075
[02:11:08.729] Epoch [1/10] Iteration [1170/1488]: Loss: 0.0677, CE: 0.0064
[02:11:13.103] Epoch [1/10] Iteration [1180/1488]: Loss: 0.0634, CE: 0.0069
[02:11:17.470] Epoch [1/10] Iteration [1190/1488]: Loss: 0.0638, CE: 0.0115
[02:11:21.860] Epoch [1/10] Iteration [1200/1488]: Loss: 0.0914, CE: 0.0063
[02:11:26.233] Epoch [1/10] Iteration [1210/1488]: Loss: 0.0689, CE: 0.0128
[02:11:30.612] Epoch [1/10] Iteration [1220/1488]: Loss: 0.1005, CE: 0.0039
[02:11:34.984] Epoch [1/10] Iteration [1230/1488]: Loss: 0.0618, CE: 0.0038
[02:11:39.376] Epoch [1/10] Iteration [1240/1488]: Loss: 0.0823, CE: 0.0058
[02:11:43.744] Epoch [1/10] Iteration [1250/1488]: Loss: 0.0726, CE: 0.0106
[02:11:48.136] Epoch [1/10] Iteration [1260/1488]: Loss: 0.0578, CE: 0.0044
[02:11:52.506] Epoch [1/10] Iteration [1270/1488]: Loss: 0.0107, CE: 0.0035
[02:11:56.874] Epoch [1/10] Iteration [1280/1488]: Loss: 0.0660, CE: 0.0076
[02:12:01.250] Epoch [1/10] Iteration [1290/1488]: Loss: 0.0235, CE: 0.0069
[02:12:05.627] Epoch [1/10] Iteration [1300/1488]: Loss: 0.1006, CE: 0.0063
[02:12:10.003] Epoch [1/10] Iteration [1310/1488]: Loss: 0.0652, CE: 0.0045
[02:12:14.390] Epoch [1/10] Iteration [1320/1488]: Loss: 0.0119, CE: 0.0027
[02:12:18.767] Epoch [1/10] Iteration [1330/1488]: Loss: 0.0666, CE: 0.0049
[02:12:23.142] Epoch [1/10] Iteration [1340/1488]: Loss: 0.0806, CE: 0.0193
[02:12:27.513] Epoch [1/10] Iteration [1350/1488]: Loss: 0.0620, CE: 0.0051
[02:12:31.898] Epoch [1/10] Iteration [1360/1488]: Loss: 0.0663, CE: 0.0061
[02:12:36.266] Epoch [1/10] Iteration [1370/1488]: Loss: 0.0192, CE: 0.0054
[02:12:40.646] Epoch [1/10] Iteration [1380/1488]: Loss: 0.0572, CE: 0.0119
[02:12:45.022] Epoch [1/10] Iteration [1390/1488]: Loss: 0.0619, CE: 0.0074
[02:12:49.416] Epoch [1/10] Iteration [1400/1488]: Loss: 0.1073, CE: 0.0080
[02:12:53.797] Epoch [1/10] Iteration [1410/1488]: Loss: 0.1011, CE: 0.0033
[02:12:58.188] Epoch [1/10] Iteration [1420/1488]: Loss: 0.0194, CE: 0.0103
[02:13:02.564] Epoch [1/10] Iteration [1430/1488]: Loss: 0.0650, CE: 0.0068
[02:13:06.944] Epoch [1/10] Iteration [1440/1488]: Loss: 0.0392, CE: 0.0050
[02:13:11.329] Epoch [1/10] Iteration [1450/1488]: Loss: 0.0770, CE: 0.0046
[02:13:15.710] Epoch [1/10] Iteration [1460/1488]: Loss: 0.0643, CE: 0.0028
[02:13:20.091] Epoch [1/10] Iteration [1470/1488]: Loss: 0.0730, CE: 0.0082
[02:13:24.480] Epoch [1/10] Iteration [1480/1488]: Loss: 0.0185, CE: 0.0040
[02:13:28.198] Epoch [1/10] Average Loss: 0.0675, CE: 0.0093, Dice: 0.1063
[02:14:09.981] Epoch [2/10] Iteration [0/1488]: Loss: 0.0647, CE: 0.0073
[02:14:14.283] Epoch [2/10] Iteration [10/1488]: Loss: 0.0091, CE: 0.0037
[02:14:18.575] Epoch [2/10] Iteration [20/1488]: Loss: 0.0871, CE: 0.0081
[02:14:22.879] Epoch [2/10] Iteration [30/1488]: Loss: 0.0741, CE: 0.0101
[02:14:27.176] Epoch [2/10] Iteration [40/1488]: Loss: 0.0724, CE: 0.0067
[02:14:31.482] Epoch [2/10] Iteration [50/1488]: Loss: 0.0652, CE: 0.0083
[02:14:35.782] Epoch [2/10] Iteration [60/1488]: Loss: 0.0756, CE: 0.0063
[02:14:40.098] Epoch [2/10] Iteration [70/1488]: Loss: 0.0702, CE: 0.0104
[02:14:44.403] Epoch [2/10] Iteration [80/1488]: Loss: 0.0781, CE: 0.0079
[02:14:48.721] Epoch [2/10] Iteration [90/1488]: Loss: 0.0728, CE: 0.0084
[02:14:53.026] Epoch [2/10] Iteration [100/1488]: Loss: 0.1053, CE: 0.0061
[02:14:57.346] Epoch [2/10] Iteration [110/1488]: Loss: 0.1050, CE: 0.0072
[02:15:01.655] Epoch [2/10] Iteration [120/1488]: Loss: 0.0677, CE: 0.0095
[02:15:05.980] Epoch [2/10] Iteration [130/1488]: Loss: 0.0438, CE: 0.0047
[02:15:10.297] Epoch [2/10] Iteration [140/1488]: Loss: 0.0667, CE: 0.0037
[02:15:14.624] Epoch [2/10] Iteration [150/1488]: Loss: 0.0647, CE: 0.0061
[02:15:18.940] Epoch [2/10] Iteration [160/1488]: Loss: 0.0773, CE: 0.0145
[02:15:23.267] Epoch [2/10] Iteration [170/1488]: Loss: 0.0728, CE: 0.0117
[02:15:27.586] Epoch [2/10] Iteration [180/1488]: Loss: 0.0656, CE: 0.0106
[02:15:31.925] Epoch [2/10] Iteration [190/1488]: Loss: 0.0711, CE: 0.0081
[02:15:36.253] Epoch [2/10] Iteration [200/1488]: Loss: 0.0723, CE: 0.0144
[02:15:40.597] Epoch [2/10] Iteration [210/1488]: Loss: 0.0112, CE: 0.0029
[02:15:44.933] Epoch [2/10] Iteration [220/1488]: Loss: 0.0649, CE: 0.0093
[02:15:49.276] Epoch [2/10] Iteration [230/1488]: Loss: 0.0254, CE: 0.0050
[02:15:53.609] Epoch [2/10] Iteration [240/1488]: Loss: 0.0755, CE: 0.0087
[02:15:57.952] Epoch [2/10] Iteration [250/1488]: Loss: 0.0595, CE: 0.0050
[02:16:02.285] Epoch [2/10] Iteration [260/1488]: Loss: 0.0646, CE: 0.0066
[02:16:06.633] Epoch [2/10] Iteration [270/1488]: Loss: 0.0769, CE: 0.0129
[02:16:10.969] Epoch [2/10] Iteration [280/1488]: Loss: 0.0564, CE: 0.0055
[02:16:15.325] Epoch [2/10] Iteration [290/1488]: Loss: 0.0654, CE: 0.0079
[02:16:19.664] Epoch [2/10] Iteration [300/1488]: Loss: 0.0880, CE: 0.0045
[02:16:24.021] Epoch [2/10] Iteration [310/1488]: Loss: 0.0751, CE: 0.0064
[02:16:28.366] Epoch [2/10] Iteration [320/1488]: Loss: 0.0903, CE: 0.0105
[02:16:32.722] Epoch [2/10] Iteration [330/1488]: Loss: 0.0796, CE: 0.0163
[02:16:37.067] Epoch [2/10] Iteration [340/1488]: Loss: 0.1033, CE: 0.0030
[02:16:41.427] Epoch [2/10] Iteration [350/1488]: Loss: 0.0737, CE: 0.0107
[02:16:45.769] Epoch [2/10] Iteration [360/1488]: Loss: 0.0611, CE: 0.0044
[02:16:50.128] Epoch [2/10] Iteration [370/1488]: Loss: 0.0124, CE: 0.0071
[02:16:54.479] Epoch [2/10] Iteration [380/1488]: Loss: 0.0227, CE: 0.0041
[02:16:58.847] Epoch [2/10] Iteration [390/1488]: Loss: 0.0829, CE: 0.0039
[02:17:03.204] Epoch [2/10] Iteration [400/1488]: Loss: 0.0977, CE: 0.0354
[02:17:07.563] Epoch [2/10] Iteration [410/1488]: Loss: 0.0626, CE: 0.0075
[02:17:11.917] Epoch [2/10] Iteration [420/1488]: Loss: 0.1046, CE: 0.0057
[02:17:16.281] Epoch [2/10] Iteration [430/1488]: Loss: 0.0152, CE: 0.0061
[02:17:20.641] Epoch [2/10] Iteration [440/1488]: Loss: 0.0625, CE: 0.0070
[02:17:24.997] Epoch [2/10] Iteration [450/1488]: Loss: 0.0603, CE: 0.0063
[02:17:29.358] Epoch [2/10] Iteration [460/1488]: Loss: 0.0722, CE: 0.0160
[02:17:33.724] Epoch [2/10] Iteration [470/1488]: Loss: 0.0629, CE: 0.0061
[02:17:38.077] Epoch [2/10] Iteration [480/1488]: Loss: 0.0751, CE: 0.0067
[02:17:42.449] Epoch [2/10] Iteration [490/1488]: Loss: 0.0571, CE: 0.0027
[02:17:46.817] Epoch [2/10] Iteration [500/1488]: Loss: 0.0192, CE: 0.0043
[02:17:51.188] Epoch [2/10] Iteration [510/1488]: Loss: 0.0686, CE: 0.0038
[02:17:55.545] Epoch [2/10] Iteration [520/1488]: Loss: 0.0776, CE: 0.0086
[02:17:59.913] Epoch [2/10] Iteration [530/1488]: Loss: 0.0664, CE: 0.0107
[02:18:04.287] Epoch [2/10] Iteration [540/1488]: Loss: 0.0307, CE: 0.0043
[02:18:08.657] Epoch [2/10] Iteration [550/1488]: Loss: 0.0870, CE: 0.0056
[02:18:13.019] Epoch [2/10] Iteration [560/1488]: Loss: 0.0734, CE: 0.0043
[02:18:17.396] Epoch [2/10] Iteration [570/1488]: Loss: 0.0647, CE: 0.0035
[02:18:21.761] Epoch [2/10] Iteration [580/1488]: Loss: 0.0925, CE: 0.0057
[02:18:26.131] Epoch [2/10] Iteration [590/1488]: Loss: 0.0977, CE: 0.0069
[02:18:30.490] Epoch [2/10] Iteration [600/1488]: Loss: 0.0890, CE: 0.0094
[02:18:34.858] Epoch [2/10] Iteration [610/1488]: Loss: 0.0683, CE: 0.0169
[02:18:39.219] Epoch [2/10] Iteration [620/1488]: Loss: 0.0395, CE: 0.0065
[02:18:43.609] Epoch [2/10] Iteration [630/1488]: Loss: 0.0719, CE: 0.0053
[02:18:47.975] Epoch [2/10] Iteration [640/1488]: Loss: 0.0881, CE: 0.0060
[02:18:52.351] Epoch [2/10] Iteration [650/1488]: Loss: 0.1046, CE: 0.0059
[02:18:56.717] Epoch [2/10] Iteration [660/1488]: Loss: 0.0740, CE: 0.0201
[02:19:01.098] Epoch [2/10] Iteration [670/1488]: Loss: 0.0587, CE: 0.0046
[02:19:05.463] Epoch [2/10] Iteration [680/1488]: Loss: 0.0142, CE: 0.0029
[02:19:09.839] Epoch [2/10] Iteration [690/1488]: Loss: 0.0805, CE: 0.0262
[02:19:14.208] Epoch [2/10] Iteration [700/1488]: Loss: 0.0823, CE: 0.0026
[02:19:18.573] Epoch [2/10] Iteration [710/1488]: Loss: 0.1060, CE: 0.0064
[02:19:22.940] Epoch [2/10] Iteration [720/1488]: Loss: 0.0784, CE: 0.0062
[02:19:27.319] Epoch [2/10] Iteration [730/1488]: Loss: 0.0234, CE: 0.0059
[02:19:31.684] Epoch [2/10] Iteration [740/1488]: Loss: 0.0907, CE: 0.0187
[02:19:36.063] Epoch [2/10] Iteration [750/1488]: Loss: 0.0406, CE: 0.0040
[02:19:40.431] Epoch [2/10] Iteration [760/1488]: Loss: 0.0249, CE: 0.0066
[02:19:44.809] Epoch [2/10] Iteration [770/1488]: Loss: 0.1034, CE: 0.0038
[02:19:49.174] Epoch [2/10] Iteration [780/1488]: Loss: 0.0741, CE: 0.0162
[02:19:53.558] Epoch [2/10] Iteration [790/1488]: Loss: 0.0764, CE: 0.0199
[02:19:57.926] Epoch [2/10] Iteration [800/1488]: Loss: 0.0794, CE: 0.0196
[02:20:02.321] Epoch [2/10] Iteration [810/1488]: Loss: 0.0605, CE: 0.0042
[02:20:06.693] Epoch [2/10] Iteration [820/1488]: Loss: 0.0620, CE: 0.0057
[02:20:11.076] Epoch [2/10] Iteration [830/1488]: Loss: 0.0623, CE: 0.0147
[02:20:15.444] Epoch [2/10] Iteration [840/1488]: Loss: 0.0589, CE: 0.0064
[02:20:19.816] Epoch [2/10] Iteration [850/1488]: Loss: 0.0759, CE: 0.0147
[02:20:24.194] Epoch [2/10] Iteration [860/1488]: Loss: 0.1043, CE: 0.0057
[02:20:28.569] Epoch [2/10] Iteration [870/1488]: Loss: 0.0766, CE: 0.0027
[02:20:32.952] Epoch [2/10] Iteration [880/1488]: Loss: 0.0157, CE: 0.0052
[02:20:37.335] Epoch [2/10] Iteration [890/1488]: Loss: 0.0612, CE: 0.0053
[02:20:41.705] Epoch [2/10] Iteration [900/1488]: Loss: 0.0190, CE: 0.0031
[02:20:46.094] Epoch [2/10] Iteration [910/1488]: Loss: 0.0842, CE: 0.0071
[02:20:50.469] Epoch [2/10] Iteration [920/1488]: Loss: 0.0289, CE: 0.0057
[02:20:54.852] Epoch [2/10] Iteration [930/1488]: Loss: 0.1032, CE: 0.0076
[02:20:59.213] Epoch [2/10] Iteration [940/1488]: Loss: 0.0671, CE: 0.0046
[02:21:03.599] Epoch [2/10] Iteration [950/1488]: Loss: 0.0702, CE: 0.0062
[02:21:07.971] Epoch [2/10] Iteration [960/1488]: Loss: 0.0219, CE: 0.0071
[02:21:12.359] Epoch [2/10] Iteration [970/1488]: Loss: 0.0611, CE: 0.0047
[02:21:16.745] Epoch [2/10] Iteration [980/1488]: Loss: 0.0274, CE: 0.0042
[02:21:21.122] Epoch [2/10] Iteration [990/1488]: Loss: 0.0589, CE: 0.0046
[02:21:25.491] Epoch [2/10] Iteration [1000/1488]: Loss: 0.0681, CE: 0.0062
[02:21:29.864] Epoch [2/10] Iteration [1010/1488]: Loss: 0.0226, CE: 0.0092
[02:21:34.249] Epoch [2/10] Iteration [1020/1488]: Loss: 0.0648, CE: 0.0062
[02:21:38.632] Epoch [2/10] Iteration [1030/1488]: Loss: 0.0660, CE: 0.0052
[02:21:42.999] Epoch [2/10] Iteration [1040/1488]: Loss: 0.0737, CE: 0.0079
[02:21:47.380] Epoch [2/10] Iteration [1050/1488]: Loss: 0.0689, CE: 0.0100
[02:21:51.749] Epoch [2/10] Iteration [1060/1488]: Loss: 0.0723, CE: 0.0125
[02:21:56.132] Epoch [2/10] Iteration [1070/1488]: Loss: 0.0647, CE: 0.0055
[02:22:00.506] Epoch [2/10] Iteration [1080/1488]: Loss: 0.0672, CE: 0.0090
[02:22:04.908] Epoch [2/10] Iteration [1090/1488]: Loss: 0.0612, CE: 0.0082
[02:22:09.265] Epoch [2/10] Iteration [1100/1488]: Loss: 0.0204, CE: 0.0084
[02:22:13.650] Epoch [2/10] Iteration [1110/1488]: Loss: 0.0153, CE: 0.0064
[02:22:18.033] Epoch [2/10] Iteration [1120/1488]: Loss: 0.0605, CE: 0.0073
[02:22:22.424] Epoch [2/10] Iteration [1130/1488]: Loss: 0.0137, CE: 0.0075
[02:22:26.804] Epoch [2/10] Iteration [1140/1488]: Loss: 0.0910, CE: 0.0340
[02:22:31.173] Epoch [2/10] Iteration [1150/1488]: Loss: 0.0638, CE: 0.0049
[02:22:35.541] Epoch [2/10] Iteration [1160/1488]: Loss: 0.0928, CE: 0.0107
[02:22:39.923] Epoch [2/10] Iteration [1170/1488]: Loss: 0.0714, CE: 0.0147
[02:22:44.306] Epoch [2/10] Iteration [1180/1488]: Loss: 0.0607, CE: 0.0058
[02:22:48.679] Epoch [2/10] Iteration [1190/1488]: Loss: 0.0675, CE: 0.0069
[02:22:53.055] Epoch [2/10] Iteration [1200/1488]: Loss: 0.0612, CE: 0.0062
[02:22:57.440] Epoch [2/10] Iteration [1210/1488]: Loss: 0.0624, CE: 0.0075
[02:23:01.816] Epoch [2/10] Iteration [1220/1488]: Loss: 0.0579, CE: 0.0036
[02:23:06.201] Epoch [2/10] Iteration [1230/1488]: Loss: 0.0930, CE: 0.0268
[02:23:10.568] Epoch [2/10] Iteration [1240/1488]: Loss: 0.0784, CE: 0.0049
[02:23:14.958] Epoch [2/10] Iteration [1250/1488]: Loss: 0.0662, CE: 0.0019
[02:23:19.321] Epoch [2/10] Iteration [1260/1488]: Loss: 0.0680, CE: 0.0077
[02:23:23.705] Epoch [2/10] Iteration [1270/1488]: Loss: 0.0809, CE: 0.0051
[02:23:28.090] Epoch [2/10] Iteration [1280/1488]: Loss: 0.0821, CE: 0.0062
[02:23:32.483] Epoch [2/10] Iteration [1290/1488]: Loss: 0.0521, CE: 0.0023
[02:23:36.866] Epoch [2/10] Iteration [1300/1488]: Loss: 0.0718, CE: 0.0070
[02:23:41.246] Epoch [2/10] Iteration [1310/1488]: Loss: 0.0601, CE: 0.0089
[02:23:45.619] Epoch [2/10] Iteration [1320/1488]: Loss: 0.0744, CE: 0.0054
[02:23:50.003] Epoch [2/10] Iteration [1330/1488]: Loss: 0.1035, CE: 0.0028
[02:23:54.387] Epoch [2/10] Iteration [1340/1488]: Loss: 0.0638, CE: 0.0052
[02:23:58.782] Epoch [2/10] Iteration [1350/1488]: Loss: 0.0136, CE: 0.0032
[02:24:03.147] Epoch [2/10] Iteration [1360/1488]: Loss: 0.0172, CE: 0.0059
[02:24:07.521] Epoch [2/10] Iteration [1370/1488]: Loss: 0.0677, CE: 0.0084
[02:24:11.896] Epoch [2/10] Iteration [1380/1488]: Loss: 0.0620, CE: 0.0037
[02:24:16.282] Epoch [2/10] Iteration [1390/1488]: Loss: 0.0717, CE: 0.0056
[02:24:20.660] Epoch [2/10] Iteration [1400/1488]: Loss: 0.0684, CE: 0.0095
[02:24:25.049] Epoch [2/10] Iteration [1410/1488]: Loss: 0.1068, CE: 0.0115
[02:24:29.420] Epoch [2/10] Iteration [1420/1488]: Loss: 0.0987, CE: 0.0029
[02:24:33.803] Epoch [2/10] Iteration [1430/1488]: Loss: 0.0699, CE: 0.0033
[02:24:38.168] Epoch [2/10] Iteration [1440/1488]: Loss: 0.0699, CE: 0.0130
[02:24:42.567] Epoch [2/10] Iteration [1450/1488]: Loss: 0.0836, CE: 0.0097
[02:24:46.938] Epoch [2/10] Iteration [1460/1488]: Loss: 0.0704, CE: 0.0083
[02:24:51.327] Epoch [2/10] Iteration [1470/1488]: Loss: 0.0089, CE: 0.0033
[02:24:55.703] Epoch [2/10] Iteration [1480/1488]: Loss: 0.0719, CE: 0.0289
[02:24:59.428] Epoch [2/10] Average Loss: 0.0639, CE: 0.0078, Dice: 0.1013
[02:25:42.150] Epoch [3/10] Iteration [0/1488]: Loss: 0.0667, CE: 0.0088
[02:25:46.440] Epoch [3/10] Iteration [10/1488]: Loss: 0.0239, CE: 0.0060
[02:25:50.744] Epoch [3/10] Iteration [20/1488]: Loss: 0.0642, CE: 0.0042
[02:25:55.038] Epoch [3/10] Iteration [30/1488]: Loss: 0.0710, CE: 0.0171
[02:25:59.343] Epoch [3/10] Iteration [40/1488]: Loss: 0.0735, CE: 0.0089
[02:26:03.640] Epoch [3/10] Iteration [50/1488]: Loss: 0.1068, CE: 0.0056
[02:26:07.952] Epoch [3/10] Iteration [60/1488]: Loss: 0.0627, CE: 0.0117
[02:26:12.254] Epoch [3/10] Iteration [70/1488]: Loss: 0.0724, CE: 0.0120
[02:26:16.570] Epoch [3/10] Iteration [80/1488]: Loss: 0.0791, CE: 0.0048
[02:26:20.877] Epoch [3/10] Iteration [90/1488]: Loss: 0.0624, CE: 0.0063
[02:26:25.193] Epoch [3/10] Iteration [100/1488]: Loss: 0.0749, CE: 0.0175
[02:26:29.501] Epoch [3/10] Iteration [110/1488]: Loss: 0.0092, CE: 0.0031
[02:26:33.820] Epoch [3/10] Iteration [120/1488]: Loss: 0.0647, CE: 0.0042
[02:26:38.136] Epoch [3/10] Iteration [130/1488]: Loss: 0.0667, CE: 0.0129
[02:26:42.458] Epoch [3/10] Iteration [140/1488]: Loss: 0.0604, CE: 0.0065
[02:26:46.770] Epoch [3/10] Iteration [150/1488]: Loss: 0.0147, CE: 0.0079
[02:26:51.096] Epoch [3/10] Iteration [160/1488]: Loss: 0.0691, CE: 0.0056
[02:26:55.418] Epoch [3/10] Iteration [170/1488]: Loss: 0.0684, CE: 0.0131
[02:26:59.751] Epoch [3/10] Iteration [180/1488]: Loss: 0.0710, CE: 0.0161
[02:27:04.074] Epoch [3/10] Iteration [190/1488]: Loss: 0.0669, CE: 0.0048
[02:27:08.409] Epoch [3/10] Iteration [200/1488]: Loss: 0.0576, CE: 0.0046
[02:27:12.731] Epoch [3/10] Iteration [210/1488]: Loss: 0.0609, CE: 0.0067
[02:27:17.070] Epoch [3/10] Iteration [220/1488]: Loss: 0.0612, CE: 0.0038
[02:27:21.401] Epoch [3/10] Iteration [230/1488]: Loss: 0.0587, CE: 0.0049
[02:27:25.745] Epoch [3/10] Iteration [240/1488]: Loss: 0.0101, CE: 0.0046
[02:27:30.077] Epoch [3/10] Iteration [250/1488]: Loss: 0.0756, CE: 0.0064
[02:27:34.433] Epoch [3/10] Iteration [260/1488]: Loss: 0.0744, CE: 0.0054
[02:27:38.770] Epoch [3/10] Iteration [270/1488]: Loss: 0.0143, CE: 0.0025
[02:27:43.130] Epoch [3/10] Iteration [280/1488]: Loss: 0.0801, CE: 0.0081
[02:27:47.465] Epoch [3/10] Iteration [290/1488]: Loss: 0.0086, CE: 0.0026
[02:27:51.813] Epoch [3/10] Iteration [300/1488]: Loss: 0.0939, CE: 0.0152
[02:27:56.163] Epoch [3/10] Iteration [310/1488]: Loss: 0.0604, CE: 0.0063
[02:28:00.515] Epoch [3/10] Iteration [320/1488]: Loss: 0.0635, CE: 0.0063
[02:28:04.855] Epoch [3/10] Iteration [330/1488]: Loss: 0.0794, CE: 0.0143
[02:28:09.215] Epoch [3/10] Iteration [340/1488]: Loss: 0.0731, CE: 0.0138
[02:28:13.566] Epoch [3/10] Iteration [350/1488]: Loss: 0.0693, CE: 0.0110
[02:28:17.923] Epoch [3/10] Iteration [360/1488]: Loss: 0.0759, CE: 0.0096
[02:28:22.273] Epoch [3/10] Iteration [370/1488]: Loss: 0.0096, CE: 0.0022
[02:28:26.633] Epoch [3/10] Iteration [380/1488]: Loss: 0.0780, CE: 0.0077
[02:28:30.977] Epoch [3/10] Iteration [390/1488]: Loss: 0.0587, CE: 0.0040
[02:28:35.338] Epoch [3/10] Iteration [400/1488]: Loss: 0.0694, CE: 0.0180
[02:28:39.689] Epoch [3/10] Iteration [410/1488]: Loss: 0.0091, CE: 0.0055
[02:28:44.058] Epoch [3/10] Iteration [420/1488]: Loss: 0.0154, CE: 0.0066
[02:28:48.402] Epoch [3/10] Iteration [430/1488]: Loss: 0.0692, CE: 0.0066
[02:28:52.766] Epoch [3/10] Iteration [440/1488]: Loss: 0.0627, CE: 0.0094
[02:28:57.126] Epoch [3/10] Iteration [450/1488]: Loss: 0.0628, CE: 0.0090
[02:29:01.488] Epoch [3/10] Iteration [460/1488]: Loss: 0.0603, CE: 0.0042
[02:29:05.841] Epoch [3/10] Iteration [470/1488]: Loss: 0.0645, CE: 0.0118
[02:29:10.204] Epoch [3/10] Iteration [480/1488]: Loss: 0.0663, CE: 0.0056
[02:29:14.571] Epoch [3/10] Iteration [490/1488]: Loss: 0.0789, CE: 0.0092
[02:29:18.932] Epoch [3/10] Iteration [500/1488]: Loss: 0.0654, CE: 0.0099
[02:29:23.288] Epoch [3/10] Iteration [510/1488]: Loss: 0.0568, CE: 0.0019
[02:29:27.660] Epoch [3/10] Iteration [520/1488]: Loss: 0.0634, CE: 0.0035
[02:29:32.017] Epoch [3/10] Iteration [530/1488]: Loss: 0.0625, CE: 0.0055
[02:29:36.380] Epoch [3/10] Iteration [540/1488]: Loss: 0.0222, CE: 0.0053
[02:29:40.747] Epoch [3/10] Iteration [550/1488]: Loss: 0.0697, CE: 0.0094
[02:29:45.123] Epoch [3/10] Iteration [560/1488]: Loss: 0.0075, CE: 0.0025
[02:29:49.485] Epoch [3/10] Iteration [570/1488]: Loss: 0.0823, CE: 0.0204
[02:29:53.857] Epoch [3/10] Iteration [580/1488]: Loss: 0.0111, CE: 0.0033
[02:29:58.216] Epoch [3/10] Iteration [590/1488]: Loss: 0.0727, CE: 0.0118
[02:30:02.586] Epoch [3/10] Iteration [600/1488]: Loss: 0.0760, CE: 0.0047
[02:30:06.945] Epoch [3/10] Iteration [610/1488]: Loss: 0.0636, CE: 0.0136
[02:30:11.313] Epoch [3/10] Iteration [620/1488]: Loss: 0.0635, CE: 0.0056
[02:30:15.671] Epoch [3/10] Iteration [630/1488]: Loss: 0.0758, CE: 0.0066
[02:30:20.044] Epoch [3/10] Iteration [640/1488]: Loss: 0.0814, CE: 0.0074
[02:30:24.418] Epoch [3/10] Iteration [650/1488]: Loss: 0.0716, CE: 0.0058
[02:30:28.781] Epoch [3/10] Iteration [660/1488]: Loss: 0.0788, CE: 0.0133
[02:30:33.145] Epoch [3/10] Iteration [670/1488]: Loss: 0.0792, CE: 0.0085
[02:30:37.525] Epoch [3/10] Iteration [680/1488]: Loss: 0.0671, CE: 0.0024
[02:30:41.896] Epoch [3/10] Iteration [690/1488]: Loss: 0.1029, CE: 0.0018
[02:30:46.277] Epoch [3/10] Iteration [700/1488]: Loss: 0.0738, CE: 0.0061
[02:30:50.645] Epoch [3/10] Iteration [710/1488]: Loss: 0.0119, CE: 0.0061
[02:30:55.010] Epoch [3/10] Iteration [720/1488]: Loss: 0.0643, CE: 0.0084
[02:30:59.384] Epoch [3/10] Iteration [730/1488]: Loss: 0.0680, CE: 0.0080
[02:31:03.773] Epoch [3/10] Iteration [740/1488]: Loss: 0.0685, CE: 0.0058
[02:31:08.130] Epoch [3/10] Iteration [750/1488]: Loss: 0.0703, CE: 0.0024
[02:31:12.515] Epoch [3/10] Iteration [760/1488]: Loss: 0.0656, CE: 0.0037
[02:31:16.879] Epoch [3/10] Iteration [770/1488]: Loss: 0.1013, CE: 0.0277
[02:31:21.260] Epoch [3/10] Iteration [780/1488]: Loss: 0.0601, CE: 0.0039
[02:31:25.625] Epoch [3/10] Iteration [790/1488]: Loss: 0.0704, CE: 0.0083
[02:31:30.002] Epoch [3/10] Iteration [800/1488]: Loss: 0.0676, CE: 0.0073
[02:31:34.377] Epoch [3/10] Iteration [810/1488]: Loss: 0.0886, CE: 0.0168
[02:31:38.770] Epoch [3/10] Iteration [820/1488]: Loss: 0.0666, CE: 0.0033
[02:31:43.142] Epoch [3/10] Iteration [830/1488]: Loss: 0.1053, CE: 0.0079
[02:31:47.513] Epoch [3/10] Iteration [840/1488]: Loss: 0.0107, CE: 0.0028
[02:31:51.887] Epoch [3/10] Iteration [850/1488]: Loss: 0.0148, CE: 0.0076
[02:31:56.263] Epoch [3/10] Iteration [860/1488]: Loss: 0.0847, CE: 0.0032
[02:32:00.633] Epoch [3/10] Iteration [870/1488]: Loss: 0.0137, CE: 0.0036
[02:32:05.023] Epoch [3/10] Iteration [880/1488]: Loss: 0.0716, CE: 0.0064
[02:32:09.391] Epoch [3/10] Iteration [890/1488]: Loss: 0.0858, CE: 0.0069
[02:32:13.767] Epoch [3/10] Iteration [900/1488]: Loss: 0.0640, CE: 0.0055
[02:32:18.143] Epoch [3/10] Iteration [910/1488]: Loss: 0.0643, CE: 0.0061
[02:32:22.519] Epoch [3/10] Iteration [920/1488]: Loss: 0.0777, CE: 0.0156
[02:32:26.894] Epoch [3/10] Iteration [930/1488]: Loss: 0.0727, CE: 0.0099
[02:32:31.273] Epoch [3/10] Iteration [940/1488]: Loss: 0.0792, CE: 0.0109
[02:32:35.645] Epoch [3/10] Iteration [950/1488]: Loss: 0.0607, CE: 0.0084
[02:32:40.026] Epoch [3/10] Iteration [960/1488]: Loss: 0.0737, CE: 0.0079
[02:32:44.393] Epoch [3/10] Iteration [970/1488]: Loss: 0.0683, CE: 0.0299
[02:32:48.791] Epoch [3/10] Iteration [980/1488]: Loss: 0.0684, CE: 0.0081
[02:32:53.165] Epoch [3/10] Iteration [990/1488]: Loss: 0.0743, CE: 0.0101
[02:32:57.544] Epoch [3/10] Iteration [1000/1488]: Loss: 0.0857, CE: 0.0052
[02:33:01.920] Epoch [3/10] Iteration [1010/1488]: Loss: 0.0621, CE: 0.0059
[02:33:06.297] Epoch [3/10] Iteration [1020/1488]: Loss: 0.0626, CE: 0.0065
[02:33:10.665] Epoch [3/10] Iteration [1030/1488]: Loss: 0.1046, CE: 0.0037
[02:33:15.044] Epoch [3/10] Iteration [1040/1488]: Loss: 0.0160, CE: 0.0076
[02:33:19.406] Epoch [3/10] Iteration [1050/1488]: Loss: 0.0606, CE: 0.0032
[02:33:23.781] Epoch [3/10] Iteration [1060/1488]: Loss: 0.0623, CE: 0.0056
[02:33:28.155] Epoch [3/10] Iteration [1070/1488]: Loss: 0.0635, CE: 0.0083
[02:33:32.546] Epoch [3/10] Iteration [1080/1488]: Loss: 0.0659, CE: 0.0051
[02:33:36.917] Epoch [3/10] Iteration [1090/1488]: Loss: 0.0735, CE: 0.0107
[02:33:41.291] Epoch [3/10] Iteration [1100/1488]: Loss: 0.0733, CE: 0.0042
[02:33:45.657] Epoch [3/10] Iteration [1110/1488]: Loss: 0.0727, CE: 0.0059
[02:33:50.050] Epoch [3/10] Iteration [1120/1488]: Loss: 0.0973, CE: 0.0096
[02:33:54.413] Epoch [3/10] Iteration [1130/1488]: Loss: 0.0645, CE: 0.0043
[02:33:58.806] Epoch [3/10] Iteration [1140/1488]: Loss: 0.0640, CE: 0.0047
[02:34:03.184] Epoch [3/10] Iteration [1150/1488]: Loss: 0.0966, CE: 0.0035
[02:34:07.565] Epoch [3/10] Iteration [1160/1488]: Loss: 0.0719, CE: 0.0095
[02:34:11.936] Epoch [3/10] Iteration [1170/1488]: Loss: 0.0683, CE: 0.0058
[02:34:16.324] Epoch [3/10] Iteration [1180/1488]: Loss: 0.0620, CE: 0.0085
[02:34:20.701] Epoch [3/10] Iteration [1190/1488]: Loss: 0.0971, CE: 0.0083
[02:34:25.075] Epoch [3/10] Iteration [1200/1488]: Loss: 0.0594, CE: 0.0073
[02:34:29.439] Epoch [3/10] Iteration [1210/1488]: Loss: 0.0672, CE: 0.0087
[02:34:33.822] Epoch [3/10] Iteration [1220/1488]: Loss: 0.1044, CE: 0.0070
[02:34:38.200] Epoch [3/10] Iteration [1230/1488]: Loss: 0.0691, CE: 0.0051
[02:34:42.580] Epoch [3/10] Iteration [1240/1488]: Loss: 0.0621, CE: 0.0096
[02:34:46.947] Epoch [3/10] Iteration [1250/1488]: Loss: 0.0842, CE: 0.0102
[02:34:51.336] Epoch [3/10] Iteration [1260/1488]: Loss: 0.0661, CE: 0.0091
[02:34:55.706] Epoch [3/10] Iteration [1270/1488]: Loss: 0.0858, CE: 0.0367
[02:35:00.103] Epoch [3/10] Iteration [1280/1488]: Loss: 0.0200, CE: 0.0083
[02:35:04.490] Epoch [3/10] Iteration [1290/1488]: Loss: 0.0669, CE: 0.0054
[02:35:08.869] Epoch [3/10] Iteration [1300/1488]: Loss: 0.0625, CE: 0.0074
[02:35:13.232] Epoch [3/10] Iteration [1310/1488]: Loss: 0.0608, CE: 0.0055
[02:35:17.618] Epoch [3/10] Iteration [1320/1488]: Loss: 0.0633, CE: 0.0075
[02:35:21.987] Epoch [3/10] Iteration [1330/1488]: Loss: 0.0155, CE: 0.0050
[02:35:26.362] Epoch [3/10] Iteration [1340/1488]: Loss: 0.0889, CE: 0.0109
[02:35:30.745] Epoch [3/10] Iteration [1350/1488]: Loss: 0.0648, CE: 0.0051
[02:35:35.126] Epoch [3/10] Iteration [1360/1488]: Loss: 0.0321, CE: 0.0048
[02:35:39.503] Epoch [3/10] Iteration [1370/1488]: Loss: 0.0792, CE: 0.0132
[02:35:43.893] Epoch [3/10] Iteration [1380/1488]: Loss: 0.0716, CE: 0.0058
[02:35:48.265] Epoch [3/10] Iteration [1390/1488]: Loss: 0.0632, CE: 0.0046
[02:35:52.656] Epoch [3/10] Iteration [1400/1488]: Loss: 0.0765, CE: 0.0135
[02:35:57.025] Epoch [3/10] Iteration [1410/1488]: Loss: 0.0558, CE: 0.0023
[02:36:01.404] Epoch [3/10] Iteration [1420/1488]: Loss: 0.0651, CE: 0.0117
[02:36:05.770] Epoch [3/10] Iteration [1430/1488]: Loss: 0.0602, CE: 0.0018
[02:36:10.158] Epoch [3/10] Iteration [1440/1488]: Loss: 0.0617, CE: 0.0040
[02:36:14.540] Epoch [3/10] Iteration [1450/1488]: Loss: 0.0798, CE: 0.0132
[02:36:18.923] Epoch [3/10] Iteration [1460/1488]: Loss: 0.0105, CE: 0.0035
[02:36:23.295] Epoch [3/10] Iteration [1470/1488]: Loss: 0.0804, CE: 0.0071
[02:36:27.694] Epoch [3/10] Iteration [1480/1488]: Loss: 0.0366, CE: 0.0038
[02:36:31.410] Epoch [3/10] Average Loss: 0.0617, CE: 0.0072, Dice: 0.0981
[02:37:14.137] Epoch [4/10] Iteration [0/1488]: Loss: 0.0236, CE: 0.0133
[02:37:18.439] Epoch [4/10] Iteration [10/1488]: Loss: 0.0670, CE: 0.0047
[02:37:22.734] Epoch [4/10] Iteration [20/1488]: Loss: 0.0580, CE: 0.0041
[02:37:27.037] Epoch [4/10] Iteration [30/1488]: Loss: 0.0904, CE: 0.0048
[02:37:31.331] Epoch [4/10] Iteration [40/1488]: Loss: 0.0578, CE: 0.0047
[02:37:35.641] Epoch [4/10] Iteration [50/1488]: Loss: 0.0589, CE: 0.0061
[02:37:39.941] Epoch [4/10] Iteration [60/1488]: Loss: 0.0601, CE: 0.0030
[02:37:44.260] Epoch [4/10] Iteration [70/1488]: Loss: 0.0575, CE: 0.0063
[02:37:48.563] Epoch [4/10] Iteration [80/1488]: Loss: 0.0557, CE: 0.0039
[02:37:52.882] Epoch [4/10] Iteration [90/1488]: Loss: 0.0633, CE: 0.0047
[02:37:57.190] Epoch [4/10] Iteration [100/1488]: Loss: 0.0730, CE: 0.0071
[02:38:01.509] Epoch [4/10] Iteration [110/1488]: Loss: 0.0577, CE: 0.0022
[02:38:05.818] Epoch [4/10] Iteration [120/1488]: Loss: 0.0747, CE: 0.0082
[02:38:10.140] Epoch [4/10] Iteration [130/1488]: Loss: 0.0609, CE: 0.0092
[02:38:14.456] Epoch [4/10] Iteration [140/1488]: Loss: 0.0619, CE: 0.0052
[02:38:18.787] Epoch [4/10] Iteration [150/1488]: Loss: 0.0326, CE: 0.0047
[02:38:23.102] Epoch [4/10] Iteration [160/1488]: Loss: 0.0594, CE: 0.0032
[02:38:27.433] Epoch [4/10] Iteration [170/1488]: Loss: 0.0689, CE: 0.0077
[02:38:31.763] Epoch [4/10] Iteration [180/1488]: Loss: 0.0622, CE: 0.0045
[02:38:36.096] Epoch [4/10] Iteration [190/1488]: Loss: 0.0753, CE: 0.0042
[02:38:40.423] Epoch [4/10] Iteration [200/1488]: Loss: 0.0819, CE: 0.0086
[02:38:44.764] Epoch [4/10] Iteration [210/1488]: Loss: 0.0672, CE: 0.0083
[02:38:49.090] Epoch [4/10] Iteration [220/1488]: Loss: 0.0597, CE: 0.0039
[02:38:53.433] Epoch [4/10] Iteration [230/1488]: Loss: 0.0657, CE: 0.0143
[02:38:57.759] Epoch [4/10] Iteration [240/1488]: Loss: 0.0148, CE: 0.0028
[02:39:02.101] Epoch [4/10] Iteration [250/1488]: Loss: 0.0603, CE: 0.0087
[02:39:06.434] Epoch [4/10] Iteration [260/1488]: Loss: 0.0582, CE: 0.0035
[02:39:10.785] Epoch [4/10] Iteration [270/1488]: Loss: 0.0622, CE: 0.0101
[02:39:15.125] Epoch [4/10] Iteration [280/1488]: Loss: 0.0713, CE: 0.0130
[02:39:19.473] Epoch [4/10] Iteration [290/1488]: Loss: 0.0106, CE: 0.0022
[02:39:23.813] Epoch [4/10] Iteration [300/1488]: Loss: 0.0718, CE: 0.0096
[02:39:28.174] Epoch [4/10] Iteration [310/1488]: Loss: 0.0742, CE: 0.0101
[02:39:32.519] Epoch [4/10] Iteration [320/1488]: Loss: 0.0684, CE: 0.0092
[02:39:36.871] Epoch [4/10] Iteration [330/1488]: Loss: 0.0853, CE: 0.0197
[02:39:41.219] Epoch [4/10] Iteration [340/1488]: Loss: 0.0695, CE: 0.0067
[02:39:45.579] Epoch [4/10] Iteration [350/1488]: Loss: 0.0738, CE: 0.0053
[02:39:49.920] Epoch [4/10] Iteration [360/1488]: Loss: 0.1011, CE: 0.0088
[02:39:54.284] Epoch [4/10] Iteration [370/1488]: Loss: 0.0595, CE: 0.0078
[02:39:58.632] Epoch [4/10] Iteration [380/1488]: Loss: 0.0247, CE: 0.0045
[02:40:02.994] Epoch [4/10] Iteration [390/1488]: Loss: 0.0678, CE: 0.0114
[02:40:07.347] Epoch [4/10] Iteration [400/1488]: Loss: 0.0610, CE: 0.0076
[02:40:11.713] Epoch [4/10] Iteration [410/1488]: Loss: 0.0564, CE: 0.0042
[02:40:16.074] Epoch [4/10] Iteration [420/1488]: Loss: 0.0691, CE: 0.0073
[02:40:20.435] Epoch [4/10] Iteration [430/1488]: Loss: 0.0556, CE: 0.0047
[02:40:24.794] Epoch [4/10] Iteration [440/1488]: Loss: 0.1061, CE: 0.0039
[02:40:29.155] Epoch [4/10] Iteration [450/1488]: Loss: 0.0602, CE: 0.0077
[02:40:33.513] Epoch [4/10] Iteration [460/1488]: Loss: 0.0658, CE: 0.0082
[02:40:37.875] Epoch [4/10] Iteration [470/1488]: Loss: 0.0696, CE: 0.0082
[02:40:42.237] Epoch [4/10] Iteration [480/1488]: Loss: 0.0816, CE: 0.0058
[02:40:46.607] Epoch [4/10] Iteration [490/1488]: Loss: 0.0616, CE: 0.0069
[02:40:50.955] Epoch [4/10] Iteration [500/1488]: Loss: 0.0575, CE: 0.0047
[02:40:55.320] Epoch [4/10] Iteration [510/1488]: Loss: 0.0599, CE: 0.0042
[02:40:59.683] Epoch [4/10] Iteration [520/1488]: Loss: 0.0789, CE: 0.0060
[02:41:04.058] Epoch [4/10] Iteration [530/1488]: Loss: 0.0608, CE: 0.0068
[02:41:08.413] Epoch [4/10] Iteration [540/1488]: Loss: 0.0590, CE: 0.0050
[02:41:12.790] Epoch [4/10] Iteration [550/1488]: Loss: 0.0663, CE: 0.0060
[02:41:17.152] Epoch [4/10] Iteration [560/1488]: Loss: 0.1045, CE: 0.0043
[02:41:21.520] Epoch [4/10] Iteration [570/1488]: Loss: 0.1092, CE: 0.0194
[02:41:25.878] Epoch [4/10] Iteration [580/1488]: Loss: 0.0723, CE: 0.0049
[02:41:30.244] Epoch [4/10] Iteration [590/1488]: Loss: 0.0949, CE: 0.0010
[02:41:34.600] Epoch [4/10] Iteration [600/1488]: Loss: 0.0661, CE: 0.0073
[02:41:38.989] Epoch [4/10] Iteration [610/1488]: Loss: 0.0076, CE: 0.0025
[02:41:43.359] Epoch [4/10] Iteration [620/1488]: Loss: 0.0668, CE: 0.0095
[02:41:47.733] Epoch [4/10] Iteration [630/1488]: Loss: 0.0622, CE: 0.0088
[02:41:52.096] Epoch [4/10] Iteration [640/1488]: Loss: 0.0376, CE: 0.0037
[02:41:56.478] Epoch [4/10] Iteration [650/1488]: Loss: 0.0654, CE: 0.0066
[02:42:00.836] Epoch [4/10] Iteration [660/1488]: Loss: 0.0708, CE: 0.0036
[02:42:05.206] Epoch [4/10] Iteration [670/1488]: Loss: 0.0113, CE: 0.0013
[02:42:09.569] Epoch [4/10] Iteration [680/1488]: Loss: 0.0619, CE: 0.0038
[02:42:13.941] Epoch [4/10] Iteration [690/1488]: Loss: 0.0653, CE: 0.0057
[02:42:18.303] Epoch [4/10] Iteration [700/1488]: Loss: 0.0618, CE: 0.0063
[02:42:22.683] Epoch [4/10] Iteration [710/1488]: Loss: 0.0324, CE: 0.0037
[02:42:27.050] Epoch [4/10] Iteration [720/1488]: Loss: 0.0819, CE: 0.0285
[02:42:31.427] Epoch [4/10] Iteration [730/1488]: Loss: 0.0907, CE: 0.0041
[02:42:35.804] Epoch [4/10] Iteration [740/1488]: Loss: 0.0352, CE: 0.0115
[02:42:40.176] Epoch [4/10] Iteration [750/1488]: Loss: 0.1023, CE: 0.0032
[02:42:44.551] Epoch [4/10] Iteration [760/1488]: Loss: 0.0628, CE: 0.0080
[02:42:48.933] Epoch [4/10] Iteration [770/1488]: Loss: 0.0115, CE: 0.0015
[02:42:53.299] Epoch [4/10] Iteration [780/1488]: Loss: 0.0613, CE: 0.0043
[02:42:57.680] Epoch [4/10] Iteration [790/1488]: Loss: 0.0973, CE: 0.0165
[02:43:02.047] Epoch [4/10] Iteration [800/1488]: Loss: 0.1080, CE: 0.0048
[02:43:06.419] Epoch [4/10] Iteration [810/1488]: Loss: 0.0727, CE: 0.0275
[02:43:10.783] Epoch [4/10] Iteration [820/1488]: Loss: 0.0761, CE: 0.0035
[02:43:15.162] Epoch [4/10] Iteration [830/1488]: Loss: 0.0695, CE: 0.0041
[02:43:19.530] Epoch [4/10] Iteration [840/1488]: Loss: 0.1022, CE: 0.0035
[02:43:23.903] Epoch [4/10] Iteration [850/1488]: Loss: 0.0860, CE: 0.0030
[02:43:28.263] Epoch [4/10] Iteration [860/1488]: Loss: 0.0688, CE: 0.0112
[02:43:32.658] Epoch [4/10] Iteration [870/1488]: Loss: 0.0971, CE: 0.0057
[02:43:37.031] Epoch [4/10] Iteration [880/1488]: Loss: 0.0795, CE: 0.0030
[02:43:41.412] Epoch [4/10] Iteration [890/1488]: Loss: 0.0750, CE: 0.0047
[02:43:45.790] Epoch [4/10] Iteration [900/1488]: Loss: 0.0659, CE: 0.0043
[02:43:50.166] Epoch [4/10] Iteration [910/1488]: Loss: 0.0725, CE: 0.0092
[02:43:54.536] Epoch [4/10] Iteration [920/1488]: Loss: 0.0614, CE: 0.0036
[02:43:58.927] Epoch [4/10] Iteration [930/1488]: Loss: 0.0579, CE: 0.0045
[02:44:03.298] Epoch [4/10] Iteration [940/1488]: Loss: 0.0683, CE: 0.0076
[02:44:07.680] Epoch [4/10] Iteration [950/1488]: Loss: 0.0669, CE: 0.0045
[02:44:12.044] Epoch [4/10] Iteration [960/1488]: Loss: 0.0698, CE: 0.0055
[02:44:16.423] Epoch [4/10] Iteration [970/1488]: Loss: 0.0664, CE: 0.0081
[02:44:20.797] Epoch [4/10] Iteration [980/1488]: Loss: 0.0753, CE: 0.0066
[02:44:25.173] Epoch [4/10] Iteration [990/1488]: Loss: 0.0725, CE: 0.0106
[02:44:29.555] Epoch [4/10] Iteration [1000/1488]: Loss: 0.0303, CE: 0.0033
[02:44:33.922] Epoch [4/10] Iteration [1010/1488]: Loss: 0.0746, CE: 0.0056
[02:44:38.295] Epoch [4/10] Iteration [1020/1488]: Loss: 0.0587, CE: 0.0050
[02:44:42.684] Epoch [4/10] Iteration [1030/1488]: Loss: 0.0586, CE: 0.0026
[02:44:47.066] Epoch [4/10] Iteration [1040/1488]: Loss: 0.0873, CE: 0.0139
[02:44:51.458] Epoch [4/10] Iteration [1050/1488]: Loss: 0.0613, CE: 0.0062
[02:44:55.821] Epoch [4/10] Iteration [1060/1488]: Loss: 0.0671, CE: 0.0115
[02:45:00.196] Epoch [4/10] Iteration [1070/1488]: Loss: 0.1016, CE: 0.0101
[02:45:04.570] Epoch [4/10] Iteration [1080/1488]: Loss: 0.0643, CE: 0.0102
[02:45:08.960] Epoch [4/10] Iteration [1090/1488]: Loss: 0.0585, CE: 0.0037
[02:45:13.323] Epoch [4/10] Iteration [1100/1488]: Loss: 0.0577, CE: 0.0023
[02:45:17.702] Epoch [4/10] Iteration [1110/1488]: Loss: 0.0634, CE: 0.0078
[02:45:22.076] Epoch [4/10] Iteration [1120/1488]: Loss: 0.1012, CE: 0.0028
[02:45:26.451] Epoch [4/10] Iteration [1130/1488]: Loss: 0.1027, CE: 0.0022
[02:45:30.825] Epoch [4/10] Iteration [1140/1488]: Loss: 0.0562, CE: 0.0039
[02:45:35.207] Epoch [4/10] Iteration [1150/1488]: Loss: 0.0595, CE: 0.0095
[02:45:39.569] Epoch [4/10] Iteration [1160/1488]: Loss: 0.0665, CE: 0.0040
[02:45:43.959] Epoch [4/10] Iteration [1170/1488]: Loss: 0.0675, CE: 0.0275
[02:45:48.330] Epoch [4/10] Iteration [1180/1488]: Loss: 0.0761, CE: 0.0143
[02:45:52.716] Epoch [4/10] Iteration [1190/1488]: Loss: 0.1109, CE: 0.0103
[02:45:57.083] Epoch [4/10] Iteration [1200/1488]: Loss: 0.1032, CE: 0.0051
[02:46:01.457] Epoch [4/10] Iteration [1210/1488]: Loss: 0.0610, CE: 0.0047
[02:46:05.831] Epoch [4/10] Iteration [1220/1488]: Loss: 0.0631, CE: 0.0039
[02:46:10.219] Epoch [4/10] Iteration [1230/1488]: Loss: 0.0647, CE: 0.0084
[02:46:14.590] Epoch [4/10] Iteration [1240/1488]: Loss: 0.0320, CE: 0.0052
[02:46:18.961] Epoch [4/10] Iteration [1250/1488]: Loss: 0.0654, CE: 0.0075
[02:46:23.334] Epoch [4/10] Iteration [1260/1488]: Loss: 0.0358, CE: 0.0043
[02:46:27.714] Epoch [4/10] Iteration [1270/1488]: Loss: 0.0625, CE: 0.0060
[02:46:32.086] Epoch [4/10] Iteration [1280/1488]: Loss: 0.0614, CE: 0.0046
[02:46:36.475] Epoch [4/10] Iteration [1290/1488]: Loss: 0.0749, CE: 0.0091
[02:46:40.845] Epoch [4/10] Iteration [1300/1488]: Loss: 0.0098, CE: 0.0033
[02:46:45.230] Epoch [4/10] Iteration [1310/1488]: Loss: 0.0370, CE: 0.0028
[02:46:49.602] Epoch [4/10] Iteration [1320/1488]: Loss: 0.0728, CE: 0.0145
[02:46:53.986] Epoch [4/10] Iteration [1330/1488]: Loss: 0.0902, CE: 0.0029
[02:46:58.363] Epoch [4/10] Iteration [1340/1488]: Loss: 0.0893, CE: 0.0113
[02:47:02.749] Epoch [4/10] Iteration [1350/1488]: Loss: 0.0670, CE: 0.0180
[02:47:07.118] Epoch [4/10] Iteration [1360/1488]: Loss: 0.0805, CE: 0.0163
[02:47:11.496] Epoch [4/10] Iteration [1370/1488]: Loss: 0.0174, CE: 0.0038
[02:47:15.880] Epoch [4/10] Iteration [1380/1488]: Loss: 0.0626, CE: 0.0089
[02:47:20.267] Epoch [4/10] Iteration [1390/1488]: Loss: 0.0727, CE: 0.0056
[02:47:24.635] Epoch [4/10] Iteration [1400/1488]: Loss: 0.0583, CE: 0.0046
[02:47:29.011] Epoch [4/10] Iteration [1410/1488]: Loss: 0.0595, CE: 0.0063
[02:47:33.388] Epoch [4/10] Iteration [1420/1488]: Loss: 0.0111, CE: 0.0037
[02:47:37.778] Epoch [4/10] Iteration [1430/1488]: Loss: 0.0590, CE: 0.0052
[02:47:42.146] Epoch [4/10] Iteration [1440/1488]: Loss: 0.0655, CE: 0.0038
[02:47:46.543] Epoch [4/10] Iteration [1450/1488]: Loss: 0.0636, CE: 0.0072
[02:47:50.905] Epoch [4/10] Iteration [1460/1488]: Loss: 0.0833, CE: 0.0245
[02:47:55.289] Epoch [4/10] Iteration [1470/1488]: Loss: 0.0588, CE: 0.0054
[02:47:59.656] Epoch [4/10] Iteration [1480/1488]: Loss: 0.0547, CE: 0.0021
[02:48:03.388] Epoch [4/10] Average Loss: 0.0609, CE: 0.0067, Dice: 0.0970
[02:48:46.119] Epoch [5/10] Iteration [0/1488]: Loss: 0.0759, CE: 0.0132
[02:48:50.409] Epoch [5/10] Iteration [10/1488]: Loss: 0.0144, CE: 0.0064
[02:48:54.711] Epoch [5/10] Iteration [20/1488]: Loss: 0.0642, CE: 0.0105
[02:48:59.009] Epoch [5/10] Iteration [30/1488]: Loss: 0.0670, CE: 0.0105
[02:49:03.313] Epoch [5/10] Iteration [40/1488]: Loss: 0.0606, CE: 0.0017
[02:49:07.613] Epoch [5/10] Iteration [50/1488]: Loss: 0.0654, CE: 0.0110
[02:49:11.930] Epoch [5/10] Iteration [60/1488]: Loss: 0.0287, CE: 0.0032
[02:49:16.234] Epoch [5/10] Iteration [70/1488]: Loss: 0.0780, CE: 0.0091
[02:49:20.548] Epoch [5/10] Iteration [80/1488]: Loss: 0.0712, CE: 0.0091
[02:49:24.858] Epoch [5/10] Iteration [90/1488]: Loss: 0.0593, CE: 0.0083
[02:49:29.178] Epoch [5/10] Iteration [100/1488]: Loss: 0.0727, CE: 0.0068
[02:49:33.486] Epoch [5/10] Iteration [110/1488]: Loss: 0.0631, CE: 0.0072
[02:49:37.806] Epoch [5/10] Iteration [120/1488]: Loss: 0.0265, CE: 0.0043
[02:49:42.121] Epoch [5/10] Iteration [130/1488]: Loss: 0.0145, CE: 0.0036
[02:49:46.448] Epoch [5/10] Iteration [140/1488]: Loss: 0.0653, CE: 0.0088
[02:49:50.765] Epoch [5/10] Iteration [150/1488]: Loss: 0.0232, CE: 0.0063
[02:49:55.093] Epoch [5/10] Iteration [160/1488]: Loss: 0.0626, CE: 0.0054
[02:49:59.414] Epoch [5/10] Iteration [170/1488]: Loss: 0.0126, CE: 0.0022
[02:50:03.750] Epoch [5/10] Iteration [180/1488]: Loss: 0.0633, CE: 0.0062
[02:50:08.070] Epoch [5/10] Iteration [190/1488]: Loss: 0.0191, CE: 0.0030
[02:50:12.407] Epoch [5/10] Iteration [200/1488]: Loss: 0.0114, CE: 0.0036
[02:50:16.736] Epoch [5/10] Iteration [210/1488]: Loss: 0.0597, CE: 0.0031
[02:50:21.074] Epoch [5/10] Iteration [220/1488]: Loss: 0.1081, CE: 0.0057
[02:50:25.407] Epoch [5/10] Iteration [230/1488]: Loss: 0.0525, CE: 0.0018
[02:50:29.746] Epoch [5/10] Iteration [240/1488]: Loss: 0.0613, CE: 0.0095
[02:50:34.082] Epoch [5/10] Iteration [250/1488]: Loss: 0.0658, CE: 0.0038
[02:50:38.430] Epoch [5/10] Iteration [260/1488]: Loss: 0.0583, CE: 0.0036
[02:50:42.764] Epoch [5/10] Iteration [270/1488]: Loss: 0.0672, CE: 0.0055
[02:50:47.118] Epoch [5/10] Iteration [280/1488]: Loss: 0.0604, CE: 0.0069
[02:50:51.458] Epoch [5/10] Iteration [290/1488]: Loss: 0.0775, CE: 0.0083
[02:50:55.813] Epoch [5/10] Iteration [300/1488]: Loss: 0.0619, CE: 0.0080
[02:51:00.157] Epoch [5/10] Iteration [310/1488]: Loss: 0.0122, CE: 0.0038
[02:51:04.509] Epoch [5/10] Iteration [320/1488]: Loss: 0.0668, CE: 0.0053
[02:51:08.861] Epoch [5/10] Iteration [330/1488]: Loss: 0.1037, CE: 0.0032
[02:51:13.214] Epoch [5/10] Iteration [340/1488]: Loss: 0.0639, CE: 0.0033
[02:51:17.556] Epoch [5/10] Iteration [350/1488]: Loss: 0.0589, CE: 0.0040
[02:51:21.911] Epoch [5/10] Iteration [360/1488]: Loss: 0.1090, CE: 0.0141
[02:51:26.256] Epoch [5/10] Iteration [370/1488]: Loss: 0.0769, CE: 0.0118
[02:51:30.616] Epoch [5/10] Iteration [380/1488]: Loss: 0.0638, CE: 0.0039
[02:51:34.965] Epoch [5/10] Iteration [390/1488]: Loss: 0.0610, CE: 0.0095
[02:51:39.335] Epoch [5/10] Iteration [400/1488]: Loss: 0.0693, CE: 0.0039
[02:51:43.688] Epoch [5/10] Iteration [410/1488]: Loss: 0.0509, CE: 0.0012
[02:51:48.058] Epoch [5/10] Iteration [420/1488]: Loss: 0.1060, CE: 0.0051
[02:51:52.408] Epoch [5/10] Iteration [430/1488]: Loss: 0.0459, CE: 0.0054
[02:51:56.772] Epoch [5/10] Iteration [440/1488]: Loss: 0.0510, CE: 0.0037
[02:52:01.139] Epoch [5/10] Iteration [450/1488]: Loss: 0.1019, CE: 0.0034
[02:52:05.518] Epoch [5/10] Iteration [460/1488]: Loss: 0.0624, CE: 0.0058
[02:52:09.868] Epoch [5/10] Iteration [470/1488]: Loss: 0.0694, CE: 0.0031
[02:52:14.232] Epoch [5/10] Iteration [480/1488]: Loss: 0.0103, CE: 0.0021
[02:52:18.594] Epoch [5/10] Iteration [490/1488]: Loss: 0.0984, CE: 0.0300
[02:52:22.963] Epoch [5/10] Iteration [500/1488]: Loss: 0.0145, CE: 0.0030
[02:52:27.313] Epoch [5/10] Iteration [510/1488]: Loss: 0.0654, CE: 0.0118
[02:52:31.686] Epoch [5/10] Iteration [520/1488]: Loss: 0.0619, CE: 0.0046
[02:52:36.053] Epoch [5/10] Iteration [530/1488]: Loss: 0.0680, CE: 0.0121
[02:52:40.417] Epoch [5/10] Iteration [540/1488]: Loss: 0.0396, CE: 0.0054
[02:52:44.778] Epoch [5/10] Iteration [550/1488]: Loss: 0.0127, CE: 0.0044
[02:52:49.152] Epoch [5/10] Iteration [560/1488]: Loss: 0.0703, CE: 0.0059
[02:52:53.511] Epoch [5/10] Iteration [570/1488]: Loss: 0.0677, CE: 0.0079
[02:52:57.889] Epoch [5/10] Iteration [580/1488]: Loss: 0.0756, CE: 0.0070
[02:53:02.251] Epoch [5/10] Iteration [590/1488]: Loss: 0.0694, CE: 0.0066
[02:53:06.623] Epoch [5/10] Iteration [600/1488]: Loss: 0.0695, CE: 0.0035
[02:53:10.994] Epoch [5/10] Iteration [610/1488]: Loss: 0.0607, CE: 0.0038
[02:53:15.361] Epoch [5/10] Iteration [620/1488]: Loss: 0.0599, CE: 0.0070
[02:53:19.717] Epoch [5/10] Iteration [630/1488]: Loss: 0.0605, CE: 0.0052
[02:53:24.090] Epoch [5/10] Iteration [640/1488]: Loss: 0.0233, CE: 0.0084
[02:53:28.456] Epoch [5/10] Iteration [650/1488]: Loss: 0.0658, CE: 0.0047
[02:53:32.830] Epoch [5/10] Iteration [660/1488]: Loss: 0.0614, CE: 0.0059
[02:53:37.192] Epoch [5/10] Iteration [670/1488]: Loss: 0.0701, CE: 0.0060
[02:53:41.566] Epoch [5/10] Iteration [680/1488]: Loss: 0.0652, CE: 0.0076
[02:53:45.936] Epoch [5/10] Iteration [690/1488]: Loss: 0.0563, CE: 0.0035
[02:53:50.315] Epoch [5/10] Iteration [700/1488]: Loss: 0.0627, CE: 0.0061
[02:53:54.673] Epoch [5/10] Iteration [710/1488]: Loss: 0.0152, CE: 0.0066
[02:53:59.046] Epoch [5/10] Iteration [720/1488]: Loss: 0.0616, CE: 0.0069
[02:54:03.418] Epoch [5/10] Iteration [730/1488]: Loss: 0.0605, CE: 0.0072
[02:54:07.796] Epoch [5/10] Iteration [740/1488]: Loss: 0.0732, CE: 0.0082
[02:54:12.167] Epoch [5/10] Iteration [750/1488]: Loss: 0.0203, CE: 0.0028
[02:54:16.553] Epoch [5/10] Iteration [760/1488]: Loss: 0.0085, CE: 0.0023
[02:54:20.929] Epoch [5/10] Iteration [770/1488]: Loss: 0.0806, CE: 0.0052
[02:54:25.302] Epoch [5/10] Iteration [780/1488]: Loss: 0.0727, CE: 0.0057
[02:54:29.674] Epoch [5/10] Iteration [790/1488]: Loss: 0.0789, CE: 0.0114
[02:54:34.055] Epoch [5/10] Iteration [800/1488]: Loss: 0.0607, CE: 0.0053
[02:54:38.429] Epoch [5/10] Iteration [810/1488]: Loss: 0.0617, CE: 0.0020
[02:54:42.808] Epoch [5/10] Iteration [820/1488]: Loss: 0.0567, CE: 0.0041
[02:54:47.171] Epoch [5/10] Iteration [830/1488]: Loss: 0.0706, CE: 0.0042
[02:54:51.547] Epoch [5/10] Iteration [840/1488]: Loss: 0.0399, CE: 0.0043
[02:54:55.914] Epoch [5/10] Iteration [850/1488]: Loss: 0.0698, CE: 0.0271
[02:55:00.302] Epoch [5/10] Iteration [860/1488]: Loss: 0.0589, CE: 0.0053
[02:55:04.678] Epoch [5/10] Iteration [870/1488]: Loss: 0.0620, CE: 0.0040
[02:55:09.062] Epoch [5/10] Iteration [880/1488]: Loss: 0.0617, CE: 0.0060
[02:55:13.443] Epoch [5/10] Iteration [890/1488]: Loss: 0.0631, CE: 0.0072
[02:55:17.822] Epoch [5/10] Iteration [900/1488]: Loss: 0.0634, CE: 0.0020
[02:55:22.184] Epoch [5/10] Iteration [910/1488]: Loss: 0.0240, CE: 0.0045
[02:55:26.552] Epoch [5/10] Iteration [920/1488]: Loss: 0.0609, CE: 0.0057
[02:55:30.917] Epoch [5/10] Iteration [930/1488]: Loss: 0.1031, CE: 0.0029
[02:55:35.303] Epoch [5/10] Iteration [940/1488]: Loss: 0.0066, CE: 0.0042
[02:55:39.680] Epoch [5/10] Iteration [950/1488]: Loss: 0.0149, CE: 0.0053
[02:55:44.065] Epoch [5/10] Iteration [960/1488]: Loss: 0.0709, CE: 0.0066
[02:55:48.430] Epoch [5/10] Iteration [970/1488]: Loss: 0.0071, CE: 0.0038
[02:55:52.828] Epoch [5/10] Iteration [980/1488]: Loss: 0.0619, CE: 0.0047
[02:55:57.204] Epoch [5/10] Iteration [990/1488]: Loss: 0.0646, CE: 0.0096
[02:56:01.586] Epoch [5/10] Iteration [1000/1488]: Loss: 0.0681, CE: 0.0259
[02:56:05.955] Epoch [5/10] Iteration [1010/1488]: Loss: 0.0711, CE: 0.0262
[02:56:10.337] Epoch [5/10] Iteration [1020/1488]: Loss: 0.0650, CE: 0.0067
[02:56:14.708] Epoch [5/10] Iteration [1030/1488]: Loss: 0.0081, CE: 0.0033
[02:56:19.091] Epoch [5/10] Iteration [1040/1488]: Loss: 0.0084, CE: 0.0021
[02:56:23.472] Epoch [5/10] Iteration [1050/1488]: Loss: 0.0643, CE: 0.0087
[02:56:27.865] Epoch [5/10] Iteration [1060/1488]: Loss: 0.0574, CE: 0.0026
[02:56:32.226] Epoch [5/10] Iteration [1070/1488]: Loss: 0.0619, CE: 0.0079
[02:56:36.602] Epoch [5/10] Iteration [1080/1488]: Loss: 0.0086, CE: 0.0020
[02:56:40.978] Epoch [5/10] Iteration [1090/1488]: Loss: 0.0614, CE: 0.0061
[02:56:45.354] Epoch [5/10] Iteration [1100/1488]: Loss: 0.0695, CE: 0.0029
[02:56:49.730] Epoch [5/10] Iteration [1110/1488]: Loss: 0.0564, CE: 0.0030
[02:56:54.110] Epoch [5/10] Iteration [1120/1488]: Loss: 0.0615, CE: 0.0053
[02:56:58.479] Epoch [5/10] Iteration [1130/1488]: Loss: 0.0618, CE: 0.0067
[02:57:02.868] Epoch [5/10] Iteration [1140/1488]: Loss: 0.0707, CE: 0.0047
[02:57:07.245] Epoch [5/10] Iteration [1150/1488]: Loss: 0.0591, CE: 0.0033
[02:57:11.627] Epoch [5/10] Iteration [1160/1488]: Loss: 0.0653, CE: 0.0043
[02:57:16.001] Epoch [5/10] Iteration [1170/1488]: Loss: 0.0658, CE: 0.0030
[02:57:20.378] Epoch [5/10] Iteration [1180/1488]: Loss: 0.0237, CE: 0.0015
[02:57:24.759] Epoch [5/10] Iteration [1190/1488]: Loss: 0.0616, CE: 0.0059
[02:57:29.145] Epoch [5/10] Iteration [1200/1488]: Loss: 0.1043, CE: 0.0144
[02:57:33.509] Epoch [5/10] Iteration [1210/1488]: Loss: 0.0586, CE: 0.0030
[02:57:37.891] Epoch [5/10] Iteration [1220/1488]: Loss: 0.0632, CE: 0.0097
[02:57:42.277] Epoch [5/10] Iteration [1230/1488]: Loss: 0.0664, CE: 0.0077
[02:57:46.661] Epoch [5/10] Iteration [1240/1488]: Loss: 0.1039, CE: 0.0125
[02:57:51.036] Epoch [5/10] Iteration [1250/1488]: Loss: 0.0651, CE: 0.0040
[02:57:55.424] Epoch [5/10] Iteration [1260/1488]: Loss: 0.0730, CE: 0.0062
[02:57:59.802] Epoch [5/10] Iteration [1270/1488]: Loss: 0.0632, CE: 0.0060
[02:58:04.189] Epoch [5/10] Iteration [1280/1488]: Loss: 0.0573, CE: 0.0024
[02:58:08.567] Epoch [5/10] Iteration [1290/1488]: Loss: 0.0187, CE: 0.0090
[02:58:12.947] Epoch [5/10] Iteration [1300/1488]: Loss: 0.0311, CE: 0.0034
[02:58:17.329] Epoch [5/10] Iteration [1310/1488]: Loss: 0.0835, CE: 0.0301
[02:58:21.716] Epoch [5/10] Iteration [1320/1488]: Loss: 0.0603, CE: 0.0068
[02:58:26.094] Epoch [5/10] Iteration [1330/1488]: Loss: 0.0928, CE: 0.0060
[02:58:30.482] Epoch [5/10] Iteration [1340/1488]: Loss: 0.0082, CE: 0.0013
[02:58:34.868] Epoch [5/10] Iteration [1350/1488]: Loss: 0.0669, CE: 0.0077
[02:58:39.240] Epoch [5/10] Iteration [1360/1488]: Loss: 0.0714, CE: 0.0088
[02:58:43.621] Epoch [5/10] Iteration [1370/1488]: Loss: 0.1037, CE: 0.0188
[02:58:48.011] Epoch [5/10] Iteration [1380/1488]: Loss: 0.0629, CE: 0.0044
[02:58:52.395] Epoch [5/10] Iteration [1390/1488]: Loss: 0.0744, CE: 0.0056
[02:58:56.782] Epoch [5/10] Iteration [1400/1488]: Loss: 0.0510, CE: 0.0004
[02:59:01.151] Epoch [5/10] Iteration [1410/1488]: Loss: 0.0567, CE: 0.0035
[02:59:05.554] Epoch [5/10] Iteration [1420/1488]: Loss: 0.0799, CE: 0.0080
[02:59:09.940] Epoch [5/10] Iteration [1430/1488]: Loss: 0.0075, CE: 0.0026
[02:59:14.339] Epoch [5/10] Iteration [1440/1488]: Loss: 0.0620, CE: 0.0058
[02:59:18.710] Epoch [5/10] Iteration [1450/1488]: Loss: 0.0869, CE: 0.0043
[02:59:23.083] Epoch [5/10] Iteration [1460/1488]: Loss: 0.0619, CE: 0.0032
[02:59:27.457] Epoch [5/10] Iteration [1470/1488]: Loss: 0.0330, CE: 0.0053
[02:59:31.845] Epoch [5/10] Iteration [1480/1488]: Loss: 0.0604, CE: 0.0025
[02:59:35.574] Epoch [5/10] Average Loss: 0.0591, CE: 0.0064, Dice: 0.0942
[03:00:18.713] Epoch [6/10] Iteration [0/1488]: Loss: 0.0112, CE: 0.0056
[03:00:23.004] Epoch [6/10] Iteration [10/1488]: Loss: 0.0102, CE: 0.0064
[03:00:27.308] Epoch [6/10] Iteration [20/1488]: Loss: 0.0600, CE: 0.0050
[03:00:31.601] Epoch [6/10] Iteration [30/1488]: Loss: 0.0616, CE: 0.0063
[03:00:35.906] Epoch [6/10] Iteration [40/1488]: Loss: 0.0594, CE: 0.0043
[03:00:40.205] Epoch [6/10] Iteration [50/1488]: Loss: 0.0662, CE: 0.0076
[03:00:44.516] Epoch [6/10] Iteration [60/1488]: Loss: 0.0663, CE: 0.0069
[03:00:48.815] Epoch [6/10] Iteration [70/1488]: Loss: 0.0099, CE: 0.0055
[03:00:53.129] Epoch [6/10] Iteration [80/1488]: Loss: 0.0636, CE: 0.0058
[03:00:57.436] Epoch [6/10] Iteration [90/1488]: Loss: 0.0638, CE: 0.0041
[03:01:01.752] Epoch [6/10] Iteration [100/1488]: Loss: 0.0678, CE: 0.0077
[03:01:06.058] Epoch [6/10] Iteration [110/1488]: Loss: 0.0146, CE: 0.0022
[03:01:10.381] Epoch [6/10] Iteration [120/1488]: Loss: 0.0139, CE: 0.0029
[03:01:14.696] Epoch [6/10] Iteration [130/1488]: Loss: 0.0575, CE: 0.0063
[03:01:19.024] Epoch [6/10] Iteration [140/1488]: Loss: 0.0727, CE: 0.0049
[03:01:23.336] Epoch [6/10] Iteration [150/1488]: Loss: 0.1039, CE: 0.0047
[03:01:27.662] Epoch [6/10] Iteration [160/1488]: Loss: 0.0634, CE: 0.0041
[03:01:31.994] Epoch [6/10] Iteration [170/1488]: Loss: 0.0080, CE: 0.0043
[03:01:36.323] Epoch [6/10] Iteration [180/1488]: Loss: 0.0682, CE: 0.0034
[03:01:40.649] Epoch [6/10] Iteration [190/1488]: Loss: 0.0057, CE: 0.0020
[03:01:44.990] Epoch [6/10] Iteration [200/1488]: Loss: 0.0603, CE: 0.0102
[03:01:49.326] Epoch [6/10] Iteration [210/1488]: Loss: 0.0141, CE: 0.0050
[03:01:53.671] Epoch [6/10] Iteration [220/1488]: Loss: 0.0756, CE: 0.0084
[03:01:57.999] Epoch [6/10] Iteration [230/1488]: Loss: 0.0069, CE: 0.0025
[03:02:02.344] Epoch [6/10] Iteration [240/1488]: Loss: 0.0727, CE: 0.0119
[03:02:06.677] Epoch [6/10] Iteration [250/1488]: Loss: 0.0667, CE: 0.0024
[03:02:11.019] Epoch [6/10] Iteration [260/1488]: Loss: 0.0699, CE: 0.0040
[03:02:15.356] Epoch [6/10] Iteration [270/1488]: Loss: 0.0654, CE: 0.0069
[03:02:19.701] Epoch [6/10] Iteration [280/1488]: Loss: 0.0717, CE: 0.0055
[03:02:24.037] Epoch [6/10] Iteration [290/1488]: Loss: 0.0688, CE: 0.0116
[03:02:28.389] Epoch [6/10] Iteration [300/1488]: Loss: 0.0098, CE: 0.0010
[03:02:32.727] Epoch [6/10] Iteration [310/1488]: Loss: 0.0647, CE: 0.0021
[03:02:37.078] Epoch [6/10] Iteration [320/1488]: Loss: 0.0635, CE: 0.0073
[03:02:41.424] Epoch [6/10] Iteration [330/1488]: Loss: 0.0588, CE: 0.0035
[03:02:45.776] Epoch [6/10] Iteration [340/1488]: Loss: 0.0900, CE: 0.0022
[03:02:50.119] Epoch [6/10] Iteration [350/1488]: Loss: 0.0613, CE: 0.0038
[03:02:54.478] Epoch [6/10] Iteration [360/1488]: Loss: 0.0837, CE: 0.0080
[03:02:58.819] Epoch [6/10] Iteration [370/1488]: Loss: 0.0658, CE: 0.0059
[03:03:03.178] Epoch [6/10] Iteration [380/1488]: Loss: 0.0060, CE: 0.0021
[03:03:07.531] Epoch [6/10] Iteration [390/1488]: Loss: 0.0687, CE: 0.0078
[03:03:11.886] Epoch [6/10] Iteration [400/1488]: Loss: 0.0652, CE: 0.0045
[03:03:16.235] Epoch [6/10] Iteration [410/1488]: Loss: 0.0704, CE: 0.0086
[03:03:20.594] Epoch [6/10] Iteration [420/1488]: Loss: 0.0225, CE: 0.0079
[03:03:24.942] Epoch [6/10] Iteration [430/1488]: Loss: 0.0633, CE: 0.0122
[03:03:29.307] Epoch [6/10] Iteration [440/1488]: Loss: 0.0809, CE: 0.0022
[03:03:33.659] Epoch [6/10] Iteration [450/1488]: Loss: 0.0614, CE: 0.0038
[03:03:38.028] Epoch [6/10] Iteration [460/1488]: Loss: 0.0099, CE: 0.0024
[03:03:42.384] Epoch [6/10] Iteration [470/1488]: Loss: 0.0753, CE: 0.0140
[03:03:46.748] Epoch [6/10] Iteration [480/1488]: Loss: 0.0677, CE: 0.0073
[03:03:51.108] Epoch [6/10] Iteration [490/1488]: Loss: 0.0609, CE: 0.0069
[03:03:55.480] Epoch [6/10] Iteration [500/1488]: Loss: 0.0666, CE: 0.0058
[03:03:59.829] Epoch [6/10] Iteration [510/1488]: Loss: 0.0649, CE: 0.0038
[03:04:04.203] Epoch [6/10] Iteration [520/1488]: Loss: 0.0204, CE: 0.0037
[03:04:08.563] Epoch [6/10] Iteration [530/1488]: Loss: 0.0976, CE: 0.0083
[03:04:12.926] Epoch [6/10] Iteration [540/1488]: Loss: 0.0683, CE: 0.0098
[03:04:17.283] Epoch [6/10] Iteration [550/1488]: Loss: 0.0616, CE: 0.0102
[03:04:21.652] Epoch [6/10] Iteration [560/1488]: Loss: 0.0163, CE: 0.0070
[03:04:26.013] Epoch [6/10] Iteration [570/1488]: Loss: 0.0188, CE: 0.0036
[03:04:30.383] Epoch [6/10] Iteration [580/1488]: Loss: 0.0786, CE: 0.0243
[03:04:34.743] Epoch [6/10] Iteration [590/1488]: Loss: 0.0574, CE: 0.0042
[03:04:39.116] Epoch [6/10] Iteration [600/1488]: Loss: 0.0620, CE: 0.0034
[03:04:43.475] Epoch [6/10] Iteration [610/1488]: Loss: 0.0693, CE: 0.0071
[03:04:47.840] Epoch [6/10] Iteration [620/1488]: Loss: 0.0691, CE: 0.0143
[03:04:52.197] Epoch [6/10] Iteration [630/1488]: Loss: 0.0486, CE: 0.0031
[03:04:56.566] Epoch [6/10] Iteration [640/1488]: Loss: 0.0580, CE: 0.0025
[03:05:00.921] Epoch [6/10] Iteration [650/1488]: Loss: 0.0641, CE: 0.0064
[03:05:05.306] Epoch [6/10] Iteration [660/1488]: Loss: 0.1043, CE: 0.0027
[03:05:09.670] Epoch [6/10] Iteration [670/1488]: Loss: 0.0689, CE: 0.0140
[03:05:14.045] Epoch [6/10] Iteration [680/1488]: Loss: 0.0607, CE: 0.0066
[03:05:18.405] Epoch [6/10] Iteration [690/1488]: Loss: 0.0566, CE: 0.0031
[03:05:22.774] Epoch [6/10] Iteration [700/1488]: Loss: 0.0639, CE: 0.0034
[03:05:27.145] Epoch [6/10] Iteration [710/1488]: Loss: 0.0121, CE: 0.0055
[03:05:31.523] Epoch [6/10] Iteration [720/1488]: Loss: 0.0633, CE: 0.0041
[03:05:35.881] Epoch [6/10] Iteration [730/1488]: Loss: 0.0308, CE: 0.0057
[03:05:40.249] Epoch [6/10] Iteration [740/1488]: Loss: 0.0661, CE: 0.0067
[03:05:44.607] Epoch [6/10] Iteration [750/1488]: Loss: 0.0625, CE: 0.0036
[03:05:48.987] Epoch [6/10] Iteration [760/1488]: Loss: 0.0145, CE: 0.0047
[03:05:53.353] Epoch [6/10] Iteration [770/1488]: Loss: 0.0594, CE: 0.0040
[03:05:57.733] Epoch [6/10] Iteration [780/1488]: Loss: 0.0784, CE: 0.0094
[03:06:02.101] Epoch [6/10] Iteration [790/1488]: Loss: 0.0177, CE: 0.0026
[03:06:06.487] Epoch [6/10] Iteration [800/1488]: Loss: 0.0692, CE: 0.0022
[03:06:10.849] Epoch [6/10] Iteration [810/1488]: Loss: 0.0718, CE: 0.0086
[03:06:15.228] Epoch [6/10] Iteration [820/1488]: Loss: 0.0621, CE: 0.0050
[03:06:19.607] Epoch [6/10] Iteration [830/1488]: Loss: 0.0568, CE: 0.0017
[03:06:23.980] Epoch [6/10] Iteration [840/1488]: Loss: 0.0668, CE: 0.0071
[03:06:28.348] Epoch [6/10] Iteration [850/1488]: Loss: 0.0500, CE: 0.0015
[03:06:32.724] Epoch [6/10] Iteration [860/1488]: Loss: 0.0674, CE: 0.0090
[03:06:37.090] Epoch [6/10] Iteration [870/1488]: Loss: 0.0635, CE: 0.0065
[03:06:41.471] Epoch [6/10] Iteration [880/1488]: Loss: 0.0196, CE: 0.0048
[03:06:45.846] Epoch [6/10] Iteration [890/1488]: Loss: 0.0634, CE: 0.0053
[03:06:50.229] Epoch [6/10] Iteration [900/1488]: Loss: 0.0795, CE: 0.0062
[03:06:54.601] Epoch [6/10] Iteration [910/1488]: Loss: 0.0630, CE: 0.0128
[03:06:58.978] Epoch [6/10] Iteration [920/1488]: Loss: 0.0682, CE: 0.0129
[03:07:03.350] Epoch [6/10] Iteration [930/1488]: Loss: 0.0194, CE: 0.0061
[03:07:07.730] Epoch [6/10] Iteration [940/1488]: Loss: 0.0626, CE: 0.0045
[03:07:12.095] Epoch [6/10] Iteration [950/1488]: Loss: 0.0240, CE: 0.0062
[03:07:16.483] Epoch [6/10] Iteration [960/1488]: Loss: 0.0704, CE: 0.0122
[03:07:20.851] Epoch [6/10] Iteration [970/1488]: Loss: 0.0582, CE: 0.0058
[03:07:25.229] Epoch [6/10] Iteration [980/1488]: Loss: 0.0647, CE: 0.0040
[03:07:29.594] Epoch [6/10] Iteration [990/1488]: Loss: 0.0751, CE: 0.0107
[03:07:33.969] Epoch [6/10] Iteration [1000/1488]: Loss: 0.0748, CE: 0.0152
[03:07:38.335] Epoch [6/10] Iteration [1010/1488]: Loss: 0.0642, CE: 0.0079
[03:07:42.720] Epoch [6/10] Iteration [1020/1488]: Loss: 0.0551, CE: 0.0023
[03:07:47.097] Epoch [6/10] Iteration [1030/1488]: Loss: 0.0810, CE: 0.0104
[03:07:51.479] Epoch [6/10] Iteration [1040/1488]: Loss: 0.0127, CE: 0.0031
[03:07:55.855] Epoch [6/10] Iteration [1050/1488]: Loss: 0.0619, CE: 0.0086
[03:08:00.233] Epoch [6/10] Iteration [1060/1488]: Loss: 0.0633, CE: 0.0063
[03:08:04.599] Epoch [6/10] Iteration [1070/1488]: Loss: 0.0165, CE: 0.0024
[03:08:08.986] Epoch [6/10] Iteration [1080/1488]: Loss: 0.0571, CE: 0.0032
[03:08:13.353] Epoch [6/10] Iteration [1090/1488]: Loss: 0.0605, CE: 0.0074
[03:08:17.748] Epoch [6/10] Iteration [1100/1488]: Loss: 0.0752, CE: 0.0085
[03:08:22.116] Epoch [6/10] Iteration [1110/1488]: Loss: 0.0615, CE: 0.0065
[03:08:26.499] Epoch [6/10] Iteration [1120/1488]: Loss: 0.0780, CE: 0.0122
[03:08:30.872] Epoch [6/10] Iteration [1130/1488]: Loss: 0.0757, CE: 0.0145
[03:08:35.248] Epoch [6/10] Iteration [1140/1488]: Loss: 0.0624, CE: 0.0078
[03:08:39.623] Epoch [6/10] Iteration [1150/1488]: Loss: 0.0577, CE: 0.0034
[03:08:43.995] Epoch [6/10] Iteration [1160/1488]: Loss: 0.0099, CE: 0.0023
[03:08:48.370] Epoch [6/10] Iteration [1170/1488]: Loss: 0.0623, CE: 0.0064
[03:08:52.751] Epoch [6/10] Iteration [1180/1488]: Loss: 0.0593, CE: 0.0057
[03:08:57.125] Epoch [6/10] Iteration [1190/1488]: Loss: 0.0411, CE: 0.0019
[03:09:01.515] Epoch [6/10] Iteration [1200/1488]: Loss: 0.0448, CE: 0.0036
[03:09:05.891] Epoch [6/10] Iteration [1210/1488]: Loss: 0.0696, CE: 0.0052
[03:09:10.271] Epoch [6/10] Iteration [1220/1488]: Loss: 0.0763, CE: 0.0144
[03:09:14.640] Epoch [6/10] Iteration [1230/1488]: Loss: 0.0791, CE: 0.0044
[03:09:19.030] Epoch [6/10] Iteration [1240/1488]: Loss: 0.0752, CE: 0.0133
[03:09:23.403] Epoch [6/10] Iteration [1250/1488]: Loss: 0.0618, CE: 0.0066
[03:09:27.793] Epoch [6/10] Iteration [1260/1488]: Loss: 0.0656, CE: 0.0108
[03:09:32.160] Epoch [6/10] Iteration [1270/1488]: Loss: 0.0629, CE: 0.0064
[03:09:36.545] Epoch [6/10] Iteration [1280/1488]: Loss: 0.0642, CE: 0.0027
[03:09:40.933] Epoch [6/10] Iteration [1290/1488]: Loss: 0.0583, CE: 0.0049
[03:09:45.310] Epoch [6/10] Iteration [1300/1488]: Loss: 0.0609, CE: 0.0085
[03:09:49.695] Epoch [6/10] Iteration [1310/1488]: Loss: 0.0685, CE: 0.0049
[03:09:54.083] Epoch [6/10] Iteration [1320/1488]: Loss: 0.0389, CE: 0.0036
[03:09:58.451] Epoch [6/10] Iteration [1330/1488]: Loss: 0.0588, CE: 0.0036
[03:10:02.843] Epoch [6/10] Iteration [1340/1488]: Loss: 0.0721, CE: 0.0043
[03:10:07.229] Epoch [6/10] Iteration [1350/1488]: Loss: 0.0631, CE: 0.0088
[03:10:11.617] Epoch [6/10] Iteration [1360/1488]: Loss: 0.0624, CE: 0.0054
[03:10:15.985] Epoch [6/10] Iteration [1370/1488]: Loss: 0.0607, CE: 0.0059
[03:10:20.370] Epoch [6/10] Iteration [1380/1488]: Loss: 0.0620, CE: 0.0065
[03:10:24.749] Epoch [6/10] Iteration [1390/1488]: Loss: 0.0614, CE: 0.0048
[03:10:29.137] Epoch [6/10] Iteration [1400/1488]: Loss: 0.0088, CE: 0.0020
[03:10:33.507] Epoch [6/10] Iteration [1410/1488]: Loss: 0.0639, CE: 0.0101
[03:10:37.892] Epoch [6/10] Iteration [1420/1488]: Loss: 0.0618, CE: 0.0040
[03:10:42.267] Epoch [6/10] Iteration [1430/1488]: Loss: 0.0633, CE: 0.0044
[03:10:46.654] Epoch [6/10] Iteration [1440/1488]: Loss: 0.0226, CE: 0.0038
[03:10:51.021] Epoch [6/10] Iteration [1450/1488]: Loss: 0.0852, CE: 0.0030
[03:10:55.419] Epoch [6/10] Iteration [1460/1488]: Loss: 0.0594, CE: 0.0024
[03:10:59.793] Epoch [6/10] Iteration [1470/1488]: Loss: 0.0081, CE: 0.0029
[03:11:04.172] Epoch [6/10] Iteration [1480/1488]: Loss: 0.0632, CE: 0.0113
[03:11:07.886] Epoch [6/10] Average Loss: 0.0579, CE: 0.0062, Dice: 0.0923
[03:11:50.719] Epoch [7/10] Iteration [0/1488]: Loss: 0.0714, CE: 0.0032
[03:11:55.020] Epoch [7/10] Iteration [10/1488]: Loss: 0.0693, CE: 0.0083
[03:11:59.316] Epoch [7/10] Iteration [20/1488]: Loss: 0.0740, CE: 0.0179
[03:12:03.625] Epoch [7/10] Iteration [30/1488]: Loss: 0.0702, CE: 0.0192
[03:12:07.922] Epoch [7/10] Iteration [40/1488]: Loss: 0.0640, CE: 0.0107
[03:12:12.230] Epoch [7/10] Iteration [50/1488]: Loss: 0.0622, CE: 0.0069
[03:12:16.526] Epoch [7/10] Iteration [60/1488]: Loss: 0.0196, CE: 0.0044
[03:12:20.840] Epoch [7/10] Iteration [70/1488]: Loss: 0.0932, CE: 0.0071
[03:12:25.148] Epoch [7/10] Iteration [80/1488]: Loss: 0.0653, CE: 0.0168
[03:12:29.463] Epoch [7/10] Iteration [90/1488]: Loss: 0.0738, CE: 0.0164
[03:12:33.770] Epoch [7/10] Iteration [100/1488]: Loss: 0.0531, CE: 0.0022
[03:12:38.090] Epoch [7/10] Iteration [110/1488]: Loss: 0.0670, CE: 0.0048
[03:12:42.400] Epoch [7/10] Iteration [120/1488]: Loss: 0.0695, CE: 0.0071
[03:12:46.716] Epoch [7/10] Iteration [130/1488]: Loss: 0.0590, CE: 0.0073
[03:12:51.030] Epoch [7/10] Iteration [140/1488]: Loss: 0.0533, CE: 0.0019
[03:12:55.355] Epoch [7/10] Iteration [150/1488]: Loss: 0.0772, CE: 0.0024
[03:12:59.669] Epoch [7/10] Iteration [160/1488]: Loss: 0.0755, CE: 0.0031
[03:13:04.002] Epoch [7/10] Iteration [170/1488]: Loss: 0.0620, CE: 0.0036
[03:13:08.322] Epoch [7/10] Iteration [180/1488]: Loss: 0.0076, CE: 0.0026
[03:13:12.660] Epoch [7/10] Iteration [190/1488]: Loss: 0.0178, CE: 0.0054
[03:13:16.992] Epoch [7/10] Iteration [200/1488]: Loss: 0.0582, CE: 0.0033
[03:13:21.331] Epoch [7/10] Iteration [210/1488]: Loss: 0.0747, CE: 0.0111
[03:13:25.667] Epoch [7/10] Iteration [220/1488]: Loss: 0.0083, CE: 0.0039
[03:13:30.012] Epoch [7/10] Iteration [230/1488]: Loss: 0.0910, CE: 0.0026
[03:13:34.346] Epoch [7/10] Iteration [240/1488]: Loss: 0.0732, CE: 0.0309
[03:13:38.688] Epoch [7/10] Iteration [250/1488]: Loss: 0.0566, CE: 0.0027
[03:13:43.023] Epoch [7/10] Iteration [260/1488]: Loss: 0.0613, CE: 0.0029
[03:13:47.368] Epoch [7/10] Iteration [270/1488]: Loss: 0.0927, CE: 0.0180
[03:13:51.702] Epoch [7/10] Iteration [280/1488]: Loss: 0.0599, CE: 0.0036
[03:13:56.051] Epoch [7/10] Iteration [290/1488]: Loss: 0.0710, CE: 0.0160
[03:14:00.393] Epoch [7/10] Iteration [300/1488]: Loss: 0.0667, CE: 0.0056
[03:14:04.748] Epoch [7/10] Iteration [310/1488]: Loss: 0.0552, CE: 0.0024
[03:14:09.090] Epoch [7/10] Iteration [320/1488]: Loss: 0.0105, CE: 0.0038
[03:14:13.453] Epoch [7/10] Iteration [330/1488]: Loss: 0.0713, CE: 0.0037
[03:14:17.796] Epoch [7/10] Iteration [340/1488]: Loss: 0.0059, CE: 0.0027
[03:14:22.146] Epoch [7/10] Iteration [350/1488]: Loss: 0.1023, CE: 0.0016
[03:14:26.498] Epoch [7/10] Iteration [360/1488]: Loss: 0.0966, CE: 0.0070
[03:14:30.863] Epoch [7/10] Iteration [370/1488]: Loss: 0.0741, CE: 0.0055
[03:14:35.216] Epoch [7/10] Iteration [380/1488]: Loss: 0.0657, CE: 0.0097
[03:14:39.584] Epoch [7/10] Iteration [390/1488]: Loss: 0.0626, CE: 0.0056
[03:14:43.934] Epoch [7/10] Iteration [400/1488]: Loss: 0.0751, CE: 0.0129
[03:14:48.304] Epoch [7/10] Iteration [410/1488]: Loss: 0.0140, CE: 0.0044
[03:14:52.643] Epoch [7/10] Iteration [420/1488]: Loss: 0.0887, CE: 0.0051
[03:14:57.013] Epoch [7/10] Iteration [430/1488]: Loss: 0.0782, CE: 0.0085
[03:15:01.367] Epoch [7/10] Iteration [440/1488]: Loss: 0.0584, CE: 0.0060
[03:15:05.722] Epoch [7/10] Iteration [450/1488]: Loss: 0.1036, CE: 0.0039
[03:15:10.072] Epoch [7/10] Iteration [460/1488]: Loss: 0.0609, CE: 0.0048
[03:15:14.443] Epoch [7/10] Iteration [470/1488]: Loss: 0.0551, CE: 0.0032
[03:15:18.806] Epoch [7/10] Iteration [480/1488]: Loss: 0.0097, CE: 0.0034
[03:15:23.166] Epoch [7/10] Iteration [490/1488]: Loss: 0.0549, CE: 0.0033
[03:15:27.528] Epoch [7/10] Iteration [500/1488]: Loss: 0.0676, CE: 0.0032
[03:15:31.897] Epoch [7/10] Iteration [510/1488]: Loss: 0.0616, CE: 0.0041
[03:15:36.255] Epoch [7/10] Iteration [520/1488]: Loss: 0.0239, CE: 0.0035
[03:15:40.630] Epoch [7/10] Iteration [530/1488]: Loss: 0.0644, CE: 0.0031
[03:15:44.987] Epoch [7/10] Iteration [540/1488]: Loss: 0.0637, CE: 0.0051
[03:15:49.363] Epoch [7/10] Iteration [550/1488]: Loss: 0.0212, CE: 0.0026
[03:15:53.721] Epoch [7/10] Iteration [560/1488]: Loss: 0.0658, CE: 0.0021
[03:15:58.094] Epoch [7/10] Iteration [570/1488]: Loss: 0.0645, CE: 0.0089
[03:16:02.464] Epoch [7/10] Iteration [580/1488]: Loss: 0.0622, CE: 0.0076
[03:16:06.837] Epoch [7/10] Iteration [590/1488]: Loss: 0.0645, CE: 0.0062
[03:16:11.206] Epoch [7/10] Iteration [600/1488]: Loss: 0.0694, CE: 0.0087
[03:16:15.580] Epoch [7/10] Iteration [610/1488]: Loss: 0.0595, CE: 0.0042
[03:16:19.938] Epoch [7/10] Iteration [620/1488]: Loss: 0.0978, CE: 0.0046
[03:16:24.310] Epoch [7/10] Iteration [630/1488]: Loss: 0.0688, CE: 0.0046
[03:16:28.684] Epoch [7/10] Iteration [640/1488]: Loss: 0.0597, CE: 0.0041
[03:16:33.051] Epoch [7/10] Iteration [650/1488]: Loss: 0.0790, CE: 0.0258
[03:16:37.410] Epoch [7/10] Iteration [660/1488]: Loss: 0.0606, CE: 0.0054
[03:16:41.795] Epoch [7/10] Iteration [670/1488]: Loss: 0.0602, CE: 0.0041
[03:16:46.163] Epoch [7/10] Iteration [680/1488]: Loss: 0.0617, CE: 0.0037
[03:16:50.537] Epoch [7/10] Iteration [690/1488]: Loss: 0.0115, CE: 0.0044
[03:16:54.901] Epoch [7/10] Iteration [700/1488]: Loss: 0.0607, CE: 0.0056
[03:16:59.288] Epoch [7/10] Iteration [710/1488]: Loss: 0.0496, CE: 0.0026
[03:17:03.655] Epoch [7/10] Iteration [720/1488]: Loss: 0.0554, CE: 0.0025
[03:17:08.036] Epoch [7/10] Iteration [730/1488]: Loss: 0.0900, CE: 0.0229
[03:17:12.393] Epoch [7/10] Iteration [740/1488]: Loss: 0.0686, CE: 0.0077
[03:17:16.768] Epoch [7/10] Iteration [750/1488]: Loss: 0.0697, CE: 0.0354
[03:17:21.137] Epoch [7/10] Iteration [760/1488]: Loss: 0.0990, CE: 0.0098
[03:17:25.513] Epoch [7/10] Iteration [770/1488]: Loss: 0.0584, CE: 0.0058
[03:17:29.883] Epoch [7/10] Iteration [780/1488]: Loss: 0.0601, CE: 0.0047
[03:17:34.257] Epoch [7/10] Iteration [790/1488]: Loss: 0.0594, CE: 0.0025
[03:17:38.633] Epoch [7/10] Iteration [800/1488]: Loss: 0.0549, CE: 0.0020
[03:17:43.015] Epoch [7/10] Iteration [810/1488]: Loss: 0.0065, CE: 0.0029
[03:17:47.398] Epoch [7/10] Iteration [820/1488]: Loss: 0.0469, CE: 0.0033
[03:17:51.797] Epoch [7/10] Iteration [830/1488]: Loss: 0.0324, CE: 0.0017
[03:17:56.171] Epoch [7/10] Iteration [840/1488]: Loss: 0.0556, CE: 0.0034
[03:18:00.549] Epoch [7/10] Iteration [850/1488]: Loss: 0.0741, CE: 0.0271
[03:18:04.917] Epoch [7/10] Iteration [860/1488]: Loss: 0.0595, CE: 0.0035
[03:18:09.293] Epoch [7/10] Iteration [870/1488]: Loss: 0.0583, CE: 0.0063
[03:18:13.660] Epoch [7/10] Iteration [880/1488]: Loss: 0.0660, CE: 0.0035
[03:18:18.050] Epoch [7/10] Iteration [890/1488]: Loss: 0.0324, CE: 0.0030
[03:18:22.411] Epoch [7/10] Iteration [900/1488]: Loss: 0.1021, CE: 0.0012
[03:18:26.791] Epoch [7/10] Iteration [910/1488]: Loss: 0.0097, CE: 0.0024
[03:18:31.165] Epoch [7/10] Iteration [920/1488]: Loss: 0.0565, CE: 0.0021
[03:18:35.557] Epoch [7/10] Iteration [930/1488]: Loss: 0.0119, CE: 0.0037
[03:18:39.923] Epoch [7/10] Iteration [940/1488]: Loss: 0.0110, CE: 0.0043
[03:18:44.294] Epoch [7/10] Iteration [950/1488]: Loss: 0.0692, CE: 0.0055
[03:18:48.660] Epoch [7/10] Iteration [960/1488]: Loss: 0.0781, CE: 0.0029
[03:18:53.045] Epoch [7/10] Iteration [970/1488]: Loss: 0.0652, CE: 0.0114
[03:18:57.410] Epoch [7/10] Iteration [980/1488]: Loss: 0.0587, CE: 0.0041
[03:19:01.793] Epoch [7/10] Iteration [990/1488]: Loss: 0.0639, CE: 0.0056
[03:19:06.161] Epoch [7/10] Iteration [1000/1488]: Loss: 0.0597, CE: 0.0043
[03:19:10.537] Epoch [7/10] Iteration [1010/1488]: Loss: 0.0589, CE: 0.0077
[03:19:14.913] Epoch [7/10] Iteration [1020/1488]: Loss: 0.0631, CE: 0.0067
[03:19:19.289] Epoch [7/10] Iteration [1030/1488]: Loss: 0.0222, CE: 0.0037
[03:19:23.666] Epoch [7/10] Iteration [1040/1488]: Loss: 0.0650, CE: 0.0040
[03:19:28.043] Epoch [7/10] Iteration [1050/1488]: Loss: 0.0563, CE: 0.0022
[03:19:32.414] Epoch [7/10] Iteration [1060/1488]: Loss: 0.0612, CE: 0.0047
[03:19:36.794] Epoch [7/10] Iteration [1070/1488]: Loss: 0.0054, CE: 0.0022
[03:19:41.165] Epoch [7/10] Iteration [1080/1488]: Loss: 0.1016, CE: 0.0051
[03:19:45.552] Epoch [7/10] Iteration [1090/1488]: Loss: 0.0924, CE: 0.0030
[03:19:49.918] Epoch [7/10] Iteration [1100/1488]: Loss: 0.0692, CE: 0.0057
[03:19:54.309] Epoch [7/10] Iteration [1110/1488]: Loss: 0.0155, CE: 0.0030
[03:19:58.683] Epoch [7/10] Iteration [1120/1488]: Loss: 0.0067, CE: 0.0028
[03:20:03.074] Epoch [7/10] Iteration [1130/1488]: Loss: 0.0777, CE: 0.0104
[03:20:07.447] Epoch [7/10] Iteration [1140/1488]: Loss: 0.0096, CE: 0.0043
[03:20:11.838] Epoch [7/10] Iteration [1150/1488]: Loss: 0.0660, CE: 0.0074
[03:20:16.204] Epoch [7/10] Iteration [1160/1488]: Loss: 0.0691, CE: 0.0037
[03:20:20.587] Epoch [7/10] Iteration [1170/1488]: Loss: 0.0778, CE: 0.0191
[03:20:24.954] Epoch [7/10] Iteration [1180/1488]: Loss: 0.1128, CE: 0.0211
[03:20:29.335] Epoch [7/10] Iteration [1190/1488]: Loss: 0.0113, CE: 0.0044
[03:20:33.711] Epoch [7/10] Iteration [1200/1488]: Loss: 0.0659, CE: 0.0062
[03:20:38.085] Epoch [7/10] Iteration [1210/1488]: Loss: 0.0599, CE: 0.0068
[03:20:42.459] Epoch [7/10] Iteration [1220/1488]: Loss: 0.0101, CE: 0.0029
[03:20:46.848] Epoch [7/10] Iteration [1230/1488]: Loss: 0.0142, CE: 0.0044
[03:20:51.229] Epoch [7/10] Iteration [1240/1488]: Loss: 0.0278, CE: 0.0048
[03:20:55.612] Epoch [7/10] Iteration [1250/1488]: Loss: 0.0666, CE: 0.0094
[03:20:59.974] Epoch [7/10] Iteration [1260/1488]: Loss: 0.0178, CE: 0.0057
[03:21:04.351] Epoch [7/10] Iteration [1270/1488]: Loss: 0.0114, CE: 0.0036
[03:21:08.731] Epoch [7/10] Iteration [1280/1488]: Loss: 0.0672, CE: 0.0042
[03:21:13.124] Epoch [7/10] Iteration [1290/1488]: Loss: 0.0532, CE: 0.0011
[03:21:17.487] Epoch [7/10] Iteration [1300/1488]: Loss: 0.0703, CE: 0.0046
[03:21:21.871] Epoch [7/10] Iteration [1310/1488]: Loss: 0.0678, CE: 0.0096
[03:21:26.247] Epoch [7/10] Iteration [1320/1488]: Loss: 0.0104, CE: 0.0054
[03:21:30.631] Epoch [7/10] Iteration [1330/1488]: Loss: 0.0597, CE: 0.0067
[03:21:35.015] Epoch [7/10] Iteration [1340/1488]: Loss: 0.0093, CE: 0.0043
[03:21:39.393] Epoch [7/10] Iteration [1350/1488]: Loss: 0.0607, CE: 0.0058
[03:21:43.755] Epoch [7/10] Iteration [1360/1488]: Loss: 0.0162, CE: 0.0049
[03:21:48.132] Epoch [7/10] Iteration [1370/1488]: Loss: 0.0095, CE: 0.0032
[03:21:52.512] Epoch [7/10] Iteration [1380/1488]: Loss: 0.0638, CE: 0.0024
[03:21:56.902] Epoch [7/10] Iteration [1390/1488]: Loss: 0.0166, CE: 0.0019
[03:22:01.276] Epoch [7/10] Iteration [1400/1488]: Loss: 0.0556, CE: 0.0028
[03:22:05.651] Epoch [7/10] Iteration [1410/1488]: Loss: 0.0582, CE: 0.0037
[03:22:10.025] Epoch [7/10] Iteration [1420/1488]: Loss: 0.0306, CE: 0.0039
[03:22:14.411] Epoch [7/10] Iteration [1430/1488]: Loss: 0.0625, CE: 0.0051
[03:22:18.794] Epoch [7/10] Iteration [1440/1488]: Loss: 0.0781, CE: 0.0019
[03:22:23.183] Epoch [7/10] Iteration [1450/1488]: Loss: 0.0296, CE: 0.0034
[03:22:27.540] Epoch [7/10] Iteration [1460/1488]: Loss: 0.0580, CE: 0.0043
[03:22:31.915] Epoch [7/10] Iteration [1470/1488]: Loss: 0.0594, CE: 0.0073
[03:22:36.306] Epoch [7/10] Iteration [1480/1488]: Loss: 0.0752, CE: 0.0072
[03:22:40.040] Epoch [7/10] Average Loss: 0.0575, CE: 0.0060, Dice: 0.0918
[03:23:22.690] Epoch [8/10] Iteration [0/1488]: Loss: 0.0631, CE: 0.0063
[03:23:26.975] Epoch [8/10] Iteration [10/1488]: Loss: 0.0587, CE: 0.0051
[03:23:31.279] Epoch [8/10] Iteration [20/1488]: Loss: 0.0813, CE: 0.0042
[03:23:35.576] Epoch [8/10] Iteration [30/1488]: Loss: 0.0138, CE: 0.0060
[03:23:39.883] Epoch [8/10] Iteration [40/1488]: Loss: 0.0694, CE: 0.0020
[03:23:44.177] Epoch [8/10] Iteration [50/1488]: Loss: 0.0595, CE: 0.0042
[03:23:48.490] Epoch [8/10] Iteration [60/1488]: Loss: 0.0637, CE: 0.0058
[03:23:52.796] Epoch [8/10] Iteration [70/1488]: Loss: 0.0648, CE: 0.0108
[03:23:57.110] Epoch [8/10] Iteration [80/1488]: Loss: 0.0626, CE: 0.0037
[03:24:01.412] Epoch [8/10] Iteration [90/1488]: Loss: 0.0758, CE: 0.0053
[03:24:05.726] Epoch [8/10] Iteration [100/1488]: Loss: 0.0730, CE: 0.0035
[03:24:10.037] Epoch [8/10] Iteration [110/1488]: Loss: 0.1128, CE: 0.0146
[03:24:14.356] Epoch [8/10] Iteration [120/1488]: Loss: 0.0644, CE: 0.0049
[03:24:18.662] Epoch [8/10] Iteration [130/1488]: Loss: 0.0801, CE: 0.0046
[03:24:22.984] Epoch [8/10] Iteration [140/1488]: Loss: 0.0646, CE: 0.0080
[03:24:27.305] Epoch [8/10] Iteration [150/1488]: Loss: 0.0641, CE: 0.0028
[03:24:31.632] Epoch [8/10] Iteration [160/1488]: Loss: 0.0566, CE: 0.0025
[03:24:35.949] Epoch [8/10] Iteration [170/1488]: Loss: 0.0673, CE: 0.0055
[03:24:40.275] Epoch [8/10] Iteration [180/1488]: Loss: 0.0591, CE: 0.0072
[03:24:44.601] Epoch [8/10] Iteration [190/1488]: Loss: 0.0201, CE: 0.0048
[03:24:48.938] Epoch [8/10] Iteration [200/1488]: Loss: 0.0682, CE: 0.0058
[03:24:53.262] Epoch [8/10] Iteration [210/1488]: Loss: 0.0614, CE: 0.0072
[03:24:57.597] Epoch [8/10] Iteration [220/1488]: Loss: 0.0588, CE: 0.0043
[03:25:01.930] Epoch [8/10] Iteration [230/1488]: Loss: 0.0119, CE: 0.0040
[03:25:06.265] Epoch [8/10] Iteration [240/1488]: Loss: 0.0640, CE: 0.0040
[03:25:10.600] Epoch [8/10] Iteration [250/1488]: Loss: 0.0679, CE: 0.0179
[03:25:14.941] Epoch [8/10] Iteration [260/1488]: Loss: 0.0867, CE: 0.0100
[03:25:19.279] Epoch [8/10] Iteration [270/1488]: Loss: 0.0689, CE: 0.0024
[03:25:23.624] Epoch [8/10] Iteration [280/1488]: Loss: 0.0621, CE: 0.0071
[03:25:27.957] Epoch [8/10] Iteration [290/1488]: Loss: 0.0639, CE: 0.0129
[03:25:32.304] Epoch [8/10] Iteration [300/1488]: Loss: 0.0584, CE: 0.0052
[03:25:36.645] Epoch [8/10] Iteration [310/1488]: Loss: 0.0089, CE: 0.0025
[03:25:40.995] Epoch [8/10] Iteration [320/1488]: Loss: 0.0640, CE: 0.0059
[03:25:45.335] Epoch [8/10] Iteration [330/1488]: Loss: 0.0611, CE: 0.0074
[03:25:49.685] Epoch [8/10] Iteration [340/1488]: Loss: 0.0604, CE: 0.0038
[03:25:54.034] Epoch [8/10] Iteration [350/1488]: Loss: 0.0047, CE: 0.0012
[03:25:58.389] Epoch [8/10] Iteration [360/1488]: Loss: 0.0602, CE: 0.0058
[03:26:02.733] Epoch [8/10] Iteration [370/1488]: Loss: 0.0624, CE: 0.0114
[03:26:07.088] Epoch [8/10] Iteration [380/1488]: Loss: 0.0678, CE: 0.0076
[03:26:11.447] Epoch [8/10] Iteration [390/1488]: Loss: 0.0632, CE: 0.0043
[03:26:15.806] Epoch [8/10] Iteration [400/1488]: Loss: 0.0586, CE: 0.0036
[03:26:20.162] Epoch [8/10] Iteration [410/1488]: Loss: 0.0629, CE: 0.0066
[03:26:24.523] Epoch [8/10] Iteration [420/1488]: Loss: 0.0648, CE: 0.0065
[03:26:28.880] Epoch [8/10] Iteration [430/1488]: Loss: 0.0392, CE: 0.0026
[03:26:33.241] Epoch [8/10] Iteration [440/1488]: Loss: 0.0622, CE: 0.0046
[03:26:37.590] Epoch [8/10] Iteration [450/1488]: Loss: 0.0593, CE: 0.0040
[03:26:41.951] Epoch [8/10] Iteration [460/1488]: Loss: 0.0072, CE: 0.0017
[03:26:46.308] Epoch [8/10] Iteration [470/1488]: Loss: 0.0660, CE: 0.0115
[03:26:50.668] Epoch [8/10] Iteration [480/1488]: Loss: 0.0647, CE: 0.0064
[03:26:55.015] Epoch [8/10] Iteration [490/1488]: Loss: 0.0191, CE: 0.0023
[03:26:59.388] Epoch [8/10] Iteration [500/1488]: Loss: 0.0671, CE: 0.0096
[03:27:03.740] Epoch [8/10] Iteration [510/1488]: Loss: 0.0662, CE: 0.0197
[03:27:08.108] Epoch [8/10] Iteration [520/1488]: Loss: 0.0075, CE: 0.0026
[03:27:12.463] Epoch [8/10] Iteration [530/1488]: Loss: 0.0713, CE: 0.0048
[03:27:16.828] Epoch [8/10] Iteration [540/1488]: Loss: 0.0749, CE: 0.0056
[03:27:21.194] Epoch [8/10] Iteration [550/1488]: Loss: 0.0413, CE: 0.0031
[03:27:25.560] Epoch [8/10] Iteration [560/1488]: Loss: 0.0188, CE: 0.0023
[03:27:29.909] Epoch [8/10] Iteration [570/1488]: Loss: 0.0608, CE: 0.0103
[03:27:34.284] Epoch [8/10] Iteration [580/1488]: Loss: 0.0695, CE: 0.0039
[03:27:38.644] Epoch [8/10] Iteration [590/1488]: Loss: 0.0606, CE: 0.0051
[03:27:43.011] Epoch [8/10] Iteration [600/1488]: Loss: 0.0695, CE: 0.0188
[03:27:47.374] Epoch [8/10] Iteration [610/1488]: Loss: 0.0647, CE: 0.0078
[03:27:51.740] Epoch [8/10] Iteration [620/1488]: Loss: 0.0894, CE: 0.0184
[03:27:56.107] Epoch [8/10] Iteration [630/1488]: Loss: 0.0642, CE: 0.0055
[03:28:00.485] Epoch [8/10] Iteration [640/1488]: Loss: 0.0638, CE: 0.0055
[03:28:04.841] Epoch [8/10] Iteration [650/1488]: Loss: 0.0716, CE: 0.0061
[03:28:09.216] Epoch [8/10] Iteration [660/1488]: Loss: 0.0644, CE: 0.0103
[03:28:13.577] Epoch [8/10] Iteration [670/1488]: Loss: 0.0614, CE: 0.0090
[03:28:17.952] Epoch [8/10] Iteration [680/1488]: Loss: 0.0598, CE: 0.0047
[03:28:22.321] Epoch [8/10] Iteration [690/1488]: Loss: 0.0633, CE: 0.0033
[03:28:26.685] Epoch [8/10] Iteration [700/1488]: Loss: 0.0248, CE: 0.0014
[03:28:31.056] Epoch [8/10] Iteration [710/1488]: Loss: 0.0612, CE: 0.0062
[03:28:35.439] Epoch [8/10] Iteration [720/1488]: Loss: 0.0677, CE: 0.0067
[03:28:39.805] Epoch [8/10] Iteration [730/1488]: Loss: 0.0617, CE: 0.0073
[03:28:44.169] Epoch [8/10] Iteration [740/1488]: Loss: 0.0583, CE: 0.0024
[03:28:48.534] Epoch [8/10] Iteration [750/1488]: Loss: 0.0144, CE: 0.0026
[03:28:52.914] Epoch [8/10] Iteration [760/1488]: Loss: 0.0060, CE: 0.0019
[03:28:57.272] Epoch [8/10] Iteration [770/1488]: Loss: 0.0140, CE: 0.0028
[03:29:01.656] Epoch [8/10] Iteration [780/1488]: Loss: 0.0741, CE: 0.0061
[03:29:06.028] Epoch [8/10] Iteration [790/1488]: Loss: 0.0123, CE: 0.0053
[03:29:10.404] Epoch [8/10] Iteration [800/1488]: Loss: 0.0637, CE: 0.0070
[03:29:14.774] Epoch [8/10] Iteration [810/1488]: Loss: 0.0569, CE: 0.0047
[03:29:19.165] Epoch [8/10] Iteration [820/1488]: Loss: 0.0117, CE: 0.0045
[03:29:23.530] Epoch [8/10] Iteration [830/1488]: Loss: 0.0611, CE: 0.0050
[03:29:27.911] Epoch [8/10] Iteration [840/1488]: Loss: 0.0564, CE: 0.0046
[03:29:32.273] Epoch [8/10] Iteration [850/1488]: Loss: 0.0609, CE: 0.0021
[03:29:36.646] Epoch [8/10] Iteration [860/1488]: Loss: 0.0190, CE: 0.0027
[03:29:41.021] Epoch [8/10] Iteration [870/1488]: Loss: 0.0699, CE: 0.0022
[03:29:45.400] Epoch [8/10] Iteration [880/1488]: Loss: 0.0108, CE: 0.0015
[03:29:49.766] Epoch [8/10] Iteration [890/1488]: Loss: 0.0624, CE: 0.0060
[03:29:54.153] Epoch [8/10] Iteration [900/1488]: Loss: 0.0608, CE: 0.0073
[03:29:58.516] Epoch [8/10] Iteration [910/1488]: Loss: 0.0576, CE: 0.0039
[03:30:02.898] Epoch [8/10] Iteration [920/1488]: Loss: 0.0647, CE: 0.0034
[03:30:07.267] Epoch [8/10] Iteration [930/1488]: Loss: 0.0092, CE: 0.0021
[03:30:11.635] Epoch [8/10] Iteration [940/1488]: Loss: 0.0671, CE: 0.0036
[03:30:16.006] Epoch [8/10] Iteration [950/1488]: Loss: 0.0615, CE: 0.0058
[03:30:20.388] Epoch [8/10] Iteration [960/1488]: Loss: 0.0641, CE: 0.0110
[03:30:24.748] Epoch [8/10] Iteration [970/1488]: Loss: 0.0600, CE: 0.0113
[03:30:29.126] Epoch [8/10] Iteration [980/1488]: Loss: 0.0577, CE: 0.0037
[03:30:33.501] Epoch [8/10] Iteration [990/1488]: Loss: 0.0656, CE: 0.0081
[03:30:37.881] Epoch [8/10] Iteration [1000/1488]: Loss: 0.0688, CE: 0.0117
[03:30:42.252] Epoch [8/10] Iteration [1010/1488]: Loss: 0.0599, CE: 0.0038
[03:30:46.630] Epoch [8/10] Iteration [1020/1488]: Loss: 0.0609, CE: 0.0052
[03:30:50.999] Epoch [8/10] Iteration [1030/1488]: Loss: 0.1063, CE: 0.0042
[03:30:55.391] Epoch [8/10] Iteration [1040/1488]: Loss: 0.0367, CE: 0.0021
[03:30:59.753] Epoch [8/10] Iteration [1050/1488]: Loss: 0.0704, CE: 0.0056
[03:31:04.136] Epoch [8/10] Iteration [1060/1488]: Loss: 0.0629, CE: 0.0147
[03:31:08.511] Epoch [8/10] Iteration [1070/1488]: Loss: 0.0799, CE: 0.0083
[03:31:12.887] Epoch [8/10] Iteration [1080/1488]: Loss: 0.0772, CE: 0.0036
[03:31:17.257] Epoch [8/10] Iteration [1090/1488]: Loss: 0.0137, CE: 0.0034
[03:31:21.635] Epoch [8/10] Iteration [1100/1488]: Loss: 0.0592, CE: 0.0067
[03:31:26.021] Epoch [8/10] Iteration [1110/1488]: Loss: 0.0607, CE: 0.0048
[03:31:30.403] Epoch [8/10] Iteration [1120/1488]: Loss: 0.0653, CE: 0.0093
[03:31:34.757] Epoch [8/10] Iteration [1130/1488]: Loss: 0.0952, CE: 0.0025
[03:31:39.131] Epoch [8/10] Iteration [1140/1488]: Loss: 0.0893, CE: 0.0058
[03:31:43.503] Epoch [8/10] Iteration [1150/1488]: Loss: 0.0665, CE: 0.0064
[03:31:47.892] Epoch [8/10] Iteration [1160/1488]: Loss: 0.0099, CE: 0.0023
[03:31:52.253] Epoch [8/10] Iteration [1170/1488]: Loss: 0.0074, CE: 0.0019
[03:31:56.630] Epoch [8/10] Iteration [1180/1488]: Loss: 0.0572, CE: 0.0037
[03:32:00.991] Epoch [8/10] Iteration [1190/1488]: Loss: 0.0653, CE: 0.0064
[03:32:05.381] Epoch [8/10] Iteration [1200/1488]: Loss: 0.0681, CE: 0.0060
[03:32:09.756] Epoch [8/10] Iteration [1210/1488]: Loss: 0.0586, CE: 0.0041
[03:32:14.132] Epoch [8/10] Iteration [1220/1488]: Loss: 0.0618, CE: 0.0045
[03:32:18.498] Epoch [8/10] Iteration [1230/1488]: Loss: 0.0694, CE: 0.0059
[03:32:22.881] Epoch [8/10] Iteration [1240/1488]: Loss: 0.0062, CE: 0.0027
[03:32:27.247] Epoch [8/10] Iteration [1250/1488]: Loss: 0.1023, CE: 0.0061
[03:32:31.631] Epoch [8/10] Iteration [1260/1488]: Loss: 0.1010, CE: 0.0030
[03:32:35.995] Epoch [8/10] Iteration [1270/1488]: Loss: 0.0582, CE: 0.0048
[03:32:40.371] Epoch [8/10] Iteration [1280/1488]: Loss: 0.0334, CE: 0.0020
[03:32:44.740] Epoch [8/10] Iteration [1290/1488]: Loss: 0.0090, CE: 0.0027
[03:32:49.129] Epoch [8/10] Iteration [1300/1488]: Loss: 0.0612, CE: 0.0110
[03:32:53.504] Epoch [8/10] Iteration [1310/1488]: Loss: 0.0598, CE: 0.0042
[03:32:57.886] Epoch [8/10] Iteration [1320/1488]: Loss: 0.0615, CE: 0.0030
[03:33:02.250] Epoch [8/10] Iteration [1330/1488]: Loss: 0.0632, CE: 0.0105
[03:33:06.630] Epoch [8/10] Iteration [1340/1488]: Loss: 0.0659, CE: 0.0040
[03:33:11.013] Epoch [8/10] Iteration [1350/1488]: Loss: 0.0109, CE: 0.0049
[03:33:15.399] Epoch [8/10] Iteration [1360/1488]: Loss: 0.0166, CE: 0.0021
[03:33:19.765] Epoch [8/10] Iteration [1370/1488]: Loss: 0.0082, CE: 0.0019
[03:33:24.135] Epoch [8/10] Iteration [1380/1488]: Loss: 0.0563, CE: 0.0025
[03:33:28.513] Epoch [8/10] Iteration [1390/1488]: Loss: 0.0813, CE: 0.0028
[03:33:32.912] Epoch [8/10] Iteration [1400/1488]: Loss: 0.0966, CE: 0.0043
[03:33:37.294] Epoch [8/10] Iteration [1410/1488]: Loss: 0.0353, CE: 0.0028
[03:33:41.685] Epoch [8/10] Iteration [1420/1488]: Loss: 0.0645, CE: 0.0085
[03:33:46.043] Epoch [8/10] Iteration [1430/1488]: Loss: 0.0614, CE: 0.0055
[03:33:50.418] Epoch [8/10] Iteration [1440/1488]: Loss: 0.0591, CE: 0.0064
[03:33:54.786] Epoch [8/10] Iteration [1450/1488]: Loss: 0.0647, CE: 0.0037
[03:33:59.182] Epoch [8/10] Iteration [1460/1488]: Loss: 0.0597, CE: 0.0042
[03:34:03.552] Epoch [8/10] Iteration [1470/1488]: Loss: 0.0577, CE: 0.0072
[03:34:07.935] Epoch [8/10] Iteration [1480/1488]: Loss: 0.0721, CE: 0.0198
[03:34:11.659] Epoch [8/10] Average Loss: 0.0575, CE: 0.0059, Dice: 0.0919
[03:34:54.481] Epoch [9/10] Iteration [0/1488]: Loss: 0.0691, CE: 0.0140
[03:34:58.781] Epoch [9/10] Iteration [10/1488]: Loss: 0.0657, CE: 0.0080
[03:35:03.076] Epoch [9/10] Iteration [20/1488]: Loss: 0.0575, CE: 0.0057
[03:35:07.385] Epoch [9/10] Iteration [30/1488]: Loss: 0.0659, CE: 0.0061
[03:35:11.675] Epoch [9/10] Iteration [40/1488]: Loss: 0.0569, CE: 0.0041
[03:35:15.981] Epoch [9/10] Iteration [50/1488]: Loss: 0.0649, CE: 0.0051
[03:35:20.287] Epoch [9/10] Iteration [60/1488]: Loss: 0.0582, CE: 0.0043
[03:35:24.603] Epoch [9/10] Iteration [70/1488]: Loss: 0.0588, CE: 0.0041
[03:35:28.905] Epoch [9/10] Iteration [80/1488]: Loss: 0.0605, CE: 0.0034
[03:35:33.219] Epoch [9/10] Iteration [90/1488]: Loss: 0.0632, CE: 0.0069
[03:35:37.529] Epoch [9/10] Iteration [100/1488]: Loss: 0.0667, CE: 0.0101
[03:35:41.853] Epoch [9/10] Iteration [110/1488]: Loss: 0.0626, CE: 0.0055
[03:35:46.165] Epoch [9/10] Iteration [120/1488]: Loss: 0.0103, CE: 0.0034
[03:35:50.484] Epoch [9/10] Iteration [130/1488]: Loss: 0.0594, CE: 0.0035
[03:35:54.798] Epoch [9/10] Iteration [140/1488]: Loss: 0.0582, CE: 0.0032
[03:35:59.127] Epoch [9/10] Iteration [150/1488]: Loss: 0.0645, CE: 0.0070
[03:36:03.444] Epoch [9/10] Iteration [160/1488]: Loss: 0.0611, CE: 0.0057
[03:36:07.770] Epoch [9/10] Iteration [170/1488]: Loss: 0.0063, CE: 0.0010
[03:36:12.098] Epoch [9/10] Iteration [180/1488]: Loss: 0.0555, CE: 0.0028
[03:36:16.432] Epoch [9/10] Iteration [190/1488]: Loss: 0.0637, CE: 0.0095
[03:36:20.752] Epoch [9/10] Iteration [200/1488]: Loss: 0.0584, CE: 0.0045
[03:36:25.089] Epoch [9/10] Iteration [210/1488]: Loss: 0.0778, CE: 0.0054
[03:36:29.423] Epoch [9/10] Iteration [220/1488]: Loss: 0.0106, CE: 0.0044
[03:36:33.764] Epoch [9/10] Iteration [230/1488]: Loss: 0.0609, CE: 0.0056
[03:36:38.095] Epoch [9/10] Iteration [240/1488]: Loss: 0.0650, CE: 0.0081
[03:36:42.438] Epoch [9/10] Iteration [250/1488]: Loss: 0.0603, CE: 0.0046
[03:36:46.772] Epoch [9/10] Iteration [260/1488]: Loss: 0.0593, CE: 0.0072
[03:36:51.120] Epoch [9/10] Iteration [270/1488]: Loss: 0.0702, CE: 0.0038
[03:36:55.445] Epoch [9/10] Iteration [280/1488]: Loss: 0.0587, CE: 0.0064
[03:36:59.791] Epoch [9/10] Iteration [290/1488]: Loss: 0.0582, CE: 0.0044
[03:37:04.132] Epoch [9/10] Iteration [300/1488]: Loss: 0.0614, CE: 0.0051
[03:37:08.478] Epoch [9/10] Iteration [310/1488]: Loss: 0.0611, CE: 0.0083
[03:37:12.816] Epoch [9/10] Iteration [320/1488]: Loss: 0.1019, CE: 0.0065
[03:37:17.166] Epoch [9/10] Iteration [330/1488]: Loss: 0.0650, CE: 0.0098
[03:37:21.509] Epoch [9/10] Iteration [340/1488]: Loss: 0.0970, CE: 0.0017
[03:37:25.865] Epoch [9/10] Iteration [350/1488]: Loss: 0.0618, CE: 0.0053
[03:37:30.211] Epoch [9/10] Iteration [360/1488]: Loss: 0.0639, CE: 0.0027
[03:37:34.559] Epoch [9/10] Iteration [370/1488]: Loss: 0.0602, CE: 0.0042
[03:37:38.908] Epoch [9/10] Iteration [380/1488]: Loss: 0.0107, CE: 0.0008
[03:37:43.267] Epoch [9/10] Iteration [390/1488]: Loss: 0.0644, CE: 0.0050
[03:37:47.613] Epoch [9/10] Iteration [400/1488]: Loss: 0.0559, CE: 0.0032
[03:37:51.970] Epoch [9/10] Iteration [410/1488]: Loss: 0.0569, CE: 0.0039
[03:37:56.326] Epoch [9/10] Iteration [420/1488]: Loss: 0.0584, CE: 0.0041
[03:38:00.693] Epoch [9/10] Iteration [430/1488]: Loss: 0.0677, CE: 0.0093
[03:38:05.038] Epoch [9/10] Iteration [440/1488]: Loss: 0.0616, CE: 0.0048
[03:38:09.393] Epoch [9/10] Iteration [450/1488]: Loss: 0.0129, CE: 0.0076
[03:38:13.749] Epoch [9/10] Iteration [460/1488]: Loss: 0.0654, CE: 0.0059
[03:38:18.108] Epoch [9/10] Iteration [470/1488]: Loss: 0.0788, CE: 0.0142
[03:38:22.467] Epoch [9/10] Iteration [480/1488]: Loss: 0.0864, CE: 0.0032
[03:38:26.839] Epoch [9/10] Iteration [490/1488]: Loss: 0.0680, CE: 0.0038
[03:38:31.192] Epoch [9/10] Iteration [500/1488]: Loss: 0.0646, CE: 0.0108
[03:38:35.557] Epoch [9/10] Iteration [510/1488]: Loss: 0.0127, CE: 0.0026
[03:38:39.915] Epoch [9/10] Iteration [520/1488]: Loss: 0.0626, CE: 0.0057
[03:38:44.289] Epoch [9/10] Iteration [530/1488]: Loss: 0.0585, CE: 0.0037
[03:38:48.646] Epoch [9/10] Iteration [540/1488]: Loss: 0.0577, CE: 0.0039
[03:38:53.012] Epoch [9/10] Iteration [550/1488]: Loss: 0.0639, CE: 0.0079
[03:38:57.359] Epoch [9/10] Iteration [560/1488]: Loss: 0.0591, CE: 0.0051
[03:39:01.737] Epoch [9/10] Iteration [570/1488]: Loss: 0.0720, CE: 0.0047
[03:39:06.097] Epoch [9/10] Iteration [580/1488]: Loss: 0.0576, CE: 0.0024
[03:39:10.464] Epoch [9/10] Iteration [590/1488]: Loss: 0.1015, CE: 0.0010
[03:39:14.820] Epoch [9/10] Iteration [600/1488]: Loss: 0.0608, CE: 0.0032
[03:39:19.196] Epoch [9/10] Iteration [610/1488]: Loss: 0.0673, CE: 0.0049
[03:39:23.559] Epoch [9/10] Iteration [620/1488]: Loss: 0.0079, CE: 0.0020
[03:39:27.928] Epoch [9/10] Iteration [630/1488]: Loss: 0.0619, CE: 0.0057
[03:39:32.289] Epoch [9/10] Iteration [640/1488]: Loss: 0.0652, CE: 0.0116
[03:39:36.668] Epoch [9/10] Iteration [650/1488]: Loss: 0.0603, CE: 0.0010
[03:39:41.029] Epoch [9/10] Iteration [660/1488]: Loss: 0.0120, CE: 0.0027
[03:39:45.400] Epoch [9/10] Iteration [670/1488]: Loss: 0.0919, CE: 0.0058
[03:39:49.752] Epoch [9/10] Iteration [680/1488]: Loss: 0.0087, CE: 0.0048
[03:39:54.134] Epoch [9/10] Iteration [690/1488]: Loss: 0.0579, CE: 0.0056
[03:39:58.498] Epoch [9/10] Iteration [700/1488]: Loss: 0.0666, CE: 0.0026
[03:40:02.881] Epoch [9/10] Iteration [710/1488]: Loss: 0.0616, CE: 0.0059
[03:40:07.240] Epoch [9/10] Iteration [720/1488]: Loss: 0.0619, CE: 0.0038
[03:40:11.615] Epoch [9/10] Iteration [730/1488]: Loss: 0.0597, CE: 0.0042
[03:40:15.988] Epoch [9/10] Iteration [740/1488]: Loss: 0.0819, CE: 0.0102
[03:40:20.372] Epoch [9/10] Iteration [750/1488]: Loss: 0.0590, CE: 0.0054
[03:40:24.727] Epoch [9/10] Iteration [760/1488]: Loss: 0.0489, CE: 0.0017
[03:40:29.106] Epoch [9/10] Iteration [770/1488]: Loss: 0.0779, CE: 0.0032
[03:40:33.478] Epoch [9/10] Iteration [780/1488]: Loss: 0.0623, CE: 0.0096
[03:40:37.860] Epoch [9/10] Iteration [790/1488]: Loss: 0.0674, CE: 0.0052
[03:40:42.215] Epoch [9/10] Iteration [800/1488]: Loss: 0.0069, CE: 0.0036
[03:40:46.600] Epoch [9/10] Iteration [810/1488]: Loss: 0.0062, CE: 0.0033
[03:40:50.970] Epoch [9/10] Iteration [820/1488]: Loss: 0.1001, CE: 0.0043
[03:40:55.353] Epoch [9/10] Iteration [830/1488]: Loss: 0.0578, CE: 0.0053
[03:40:59.708] Epoch [9/10] Iteration [840/1488]: Loss: 0.0610, CE: 0.0045
[03:41:04.083] Epoch [9/10] Iteration [850/1488]: Loss: 0.0553, CE: 0.0031
[03:41:08.451] Epoch [9/10] Iteration [860/1488]: Loss: 0.0709, CE: 0.0042
[03:41:12.833] Epoch [9/10] Iteration [870/1488]: Loss: 0.0584, CE: 0.0046
[03:41:17.200] Epoch [9/10] Iteration [880/1488]: Loss: 0.0633, CE: 0.0043
[03:41:21.585] Epoch [9/10] Iteration [890/1488]: Loss: 0.0667, CE: 0.0161
[03:41:25.951] Epoch [9/10] Iteration [900/1488]: Loss: 0.0600, CE: 0.0041
[03:41:30.322] Epoch [9/10] Iteration [910/1488]: Loss: 0.0096, CE: 0.0047
[03:41:34.697] Epoch [9/10] Iteration [920/1488]: Loss: 0.0099, CE: 0.0020
[03:41:39.072] Epoch [9/10] Iteration [930/1488]: Loss: 0.0101, CE: 0.0065
[03:41:43.436] Epoch [9/10] Iteration [940/1488]: Loss: 0.0098, CE: 0.0032
[03:41:47.818] Epoch [9/10] Iteration [950/1488]: Loss: 0.0640, CE: 0.0055
[03:41:52.179] Epoch [9/10] Iteration [960/1488]: Loss: 0.1051, CE: 0.0214
[03:41:56.550] Epoch [9/10] Iteration [970/1488]: Loss: 0.0613, CE: 0.0081
[03:42:00.931] Epoch [9/10] Iteration [980/1488]: Loss: 0.0239, CE: 0.0024
[03:42:05.306] Epoch [9/10] Iteration [990/1488]: Loss: 0.0134, CE: 0.0023
[03:42:09.668] Epoch [9/10] Iteration [1000/1488]: Loss: 0.0646, CE: 0.0066
[03:42:14.057] Epoch [9/10] Iteration [1010/1488]: Loss: 0.0648, CE: 0.0052
[03:42:18.435] Epoch [9/10] Iteration [1020/1488]: Loss: 0.0719, CE: 0.0050
[03:42:22.816] Epoch [9/10] Iteration [1030/1488]: Loss: 0.0618, CE: 0.0064
[03:42:27.173] Epoch [9/10] Iteration [1040/1488]: Loss: 0.0090, CE: 0.0040
[03:42:31.552] Epoch [9/10] Iteration [1050/1488]: Loss: 0.0609, CE: 0.0048
[03:42:35.933] Epoch [9/10] Iteration [1060/1488]: Loss: 0.0630, CE: 0.0018
[03:42:40.322] Epoch [9/10] Iteration [1070/1488]: Loss: 0.0600, CE: 0.0058
[03:42:44.694] Epoch [9/10] Iteration [1080/1488]: Loss: 0.0121, CE: 0.0029
[03:42:49.072] Epoch [9/10] Iteration [1090/1488]: Loss: 0.0621, CE: 0.0076
[03:42:53.446] Epoch [9/10] Iteration [1100/1488]: Loss: 0.0551, CE: 0.0015
[03:42:57.835] Epoch [9/10] Iteration [1110/1488]: Loss: 0.0738, CE: 0.0044
[03:43:02.204] Epoch [9/10] Iteration [1120/1488]: Loss: 0.0166, CE: 0.0018
[03:43:06.578] Epoch [9/10] Iteration [1130/1488]: Loss: 0.0571, CE: 0.0026
[03:43:10.947] Epoch [9/10] Iteration [1140/1488]: Loss: 0.0253, CE: 0.0029
[03:43:15.321] Epoch [9/10] Iteration [1150/1488]: Loss: 0.0609, CE: 0.0062
[03:43:19.689] Epoch [9/10] Iteration [1160/1488]: Loss: 0.0635, CE: 0.0051
[03:43:24.065] Epoch [9/10] Iteration [1170/1488]: Loss: 0.0050, CE: 0.0021
[03:43:28.451] Epoch [9/10] Iteration [1180/1488]: Loss: 0.0162, CE: 0.0040
[03:43:32.823] Epoch [9/10] Iteration [1190/1488]: Loss: 0.0075, CE: 0.0041
[03:43:37.190] Epoch [9/10] Iteration [1200/1488]: Loss: 0.0690, CE: 0.0043
[03:43:41.579] Epoch [9/10] Iteration [1210/1488]: Loss: 0.0668, CE: 0.0140
[03:43:45.960] Epoch [9/10] Iteration [1220/1488]: Loss: 0.0599, CE: 0.0055
[03:43:50.346] Epoch [9/10] Iteration [1230/1488]: Loss: 0.0250, CE: 0.0040
[03:43:54.702] Epoch [9/10] Iteration [1240/1488]: Loss: 0.0573, CE: 0.0025
[03:43:59.088] Epoch [9/10] Iteration [1250/1488]: Loss: 0.0123, CE: 0.0037
[03:44:03.461] Epoch [9/10] Iteration [1260/1488]: Loss: 0.0962, CE: 0.0145
[03:44:07.859] Epoch [9/10] Iteration [1270/1488]: Loss: 0.0700, CE: 0.0027
[03:44:12.218] Epoch [9/10] Iteration [1280/1488]: Loss: 0.0612, CE: 0.0044
[03:44:16.595] Epoch [9/10] Iteration [1290/1488]: Loss: 0.0606, CE: 0.0051
[03:44:20.968] Epoch [9/10] Iteration [1300/1488]: Loss: 0.0596, CE: 0.0055
[03:44:25.358] Epoch [9/10] Iteration [1310/1488]: Loss: 0.0056, CE: 0.0026
[03:44:29.733] Epoch [9/10] Iteration [1320/1488]: Loss: 0.0582, CE: 0.0051
[03:44:34.114] Epoch [9/10] Iteration [1330/1488]: Loss: 0.0630, CE: 0.0044
[03:44:38.494] Epoch [9/10] Iteration [1340/1488]: Loss: 0.0636, CE: 0.0051
[03:44:42.864] Epoch [9/10] Iteration [1350/1488]: Loss: 0.0595, CE: 0.0054
[03:44:47.229] Epoch [9/10] Iteration [1360/1488]: Loss: 0.0615, CE: 0.0030
[03:44:51.619] Epoch [9/10] Iteration [1370/1488]: Loss: 0.0637, CE: 0.0060
[03:44:55.990] Epoch [9/10] Iteration [1380/1488]: Loss: 0.0900, CE: 0.0049
[03:45:00.376] Epoch [9/10] Iteration [1390/1488]: Loss: 0.0624, CE: 0.0075
[03:45:04.740] Epoch [9/10] Iteration [1400/1488]: Loss: 0.0590, CE: 0.0048
[03:45:09.128] Epoch [9/10] Iteration [1410/1488]: Loss: 0.0629, CE: 0.0039
[03:45:13.507] Epoch [9/10] Iteration [1420/1488]: Loss: 0.0622, CE: 0.0047
[03:45:17.888] Epoch [9/10] Iteration [1430/1488]: Loss: 0.0418, CE: 0.0028
[03:45:22.253] Epoch [9/10] Iteration [1440/1488]: Loss: 0.0569, CE: 0.0018
[03:45:26.629] Epoch [9/10] Iteration [1450/1488]: Loss: 0.0588, CE: 0.0065
[03:45:31.000] Epoch [9/10] Iteration [1460/1488]: Loss: 0.0104, CE: 0.0036
[03:45:35.381] Epoch [9/10] Iteration [1470/1488]: Loss: 0.0135, CE: 0.0012
[03:45:39.764] Epoch [9/10] Iteration [1480/1488]: Loss: 0.0667, CE: 0.0055
[03:45:43.502] Epoch [9/10] Average Loss: 0.0568, CE: 0.0057, Dice: 0.0909
[03:45:43.705] Saved continual learning checkpoint to ./universal/synapse_to_kits23_tpgm\epoch_9_continual.pth
[03:45:43.886] Saved final continual model to ./universal/synapse_to_kits23_tpgm\final_continual_model.pth
