[13:37:16.572] Namespace(root_path='./datasets/lits17/train_npz', dataset='lits17', list_dir='./lists/lits17', output_dir='./finetune_continual_lits17', num_classes_new=3, num_classes_old=13, max_iterations=10000, max_epochs=30, batch_size=24, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', pretrained_path='./finetune_continual_kits23/continual_epoch_29.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, num_classes=16)
[13:37:16.576] Using 1484/14843 samples (10.0%) for continual learning.
[13:37:16.581] 62 iterations per epoch.
[13:37:34.150] iteration 20 : loss : 1.1060, loss_ce: 1.3218
[13:37:40.899] iteration 40 : loss : 0.7007, loss_ce: 0.3515
[13:37:47.589] iteration 60 : loss : 0.5895, loss_ce: 0.1621
[13:38:05.165] iteration 80 : loss : 0.5955, loss_ce: 0.1795
[13:38:11.853] iteration 100 : loss : 0.5601, loss_ce: 0.1777
[13:38:18.541] iteration 120 : loss : 0.4965, loss_ce: 0.1183
[13:38:36.075] iteration 140 : loss : 0.4971, loss_ce: 0.1219
[13:38:42.760] iteration 160 : loss : 0.4952, loss_ce: 0.1200
[13:38:49.455] iteration 180 : loss : 0.4805, loss_ce: 0.0845
[13:39:07.211] iteration 200 : loss : 0.4628, loss_ce: 0.0676
[13:39:13.898] iteration 220 : loss : 0.4328, loss_ce: 0.0578
[13:39:20.591] iteration 240 : loss : 0.3602, loss_ce: 0.1382
[13:39:38.128] iteration 260 : loss : 0.3352, loss_ce: 0.0963
[13:39:44.820] iteration 280 : loss : 0.2913, loss_ce: 0.1090
[13:39:51.529] iteration 300 : loss : 0.2685, loss_ce: 0.0556
[13:40:09.059] iteration 320 : loss : 0.2532, loss_ce: 0.0356
[13:40:15.745] iteration 340 : loss : 0.2510, loss_ce: 0.0448
[13:40:22.442] iteration 360 : loss : 0.2489, loss_ce: 0.0384
[13:40:40.181] iteration 380 : loss : 0.2294, loss_ce: 0.0200
[13:40:46.874] iteration 400 : loss : 0.2071, loss_ce: 0.0319
[13:40:53.571] iteration 420 : loss : 0.2072, loss_ce: 0.0296
[13:41:11.290] iteration 440 : loss : 0.2269, loss_ce: 0.0198
[13:41:17.974] iteration 460 : loss : 0.2066, loss_ce: 0.0300
[13:41:24.680] iteration 480 : loss : 0.1745, loss_ce: 0.0379
[13:41:42.413] iteration 500 : loss : 0.1675, loss_ce: 0.0349
[13:41:49.102] iteration 520 : loss : 0.1314, loss_ce: 0.0395
[13:41:55.800] iteration 540 : loss : 0.0958, loss_ce: 0.0306
[13:42:13.394] iteration 560 : loss : 0.0943, loss_ce: 0.0302
[13:42:20.080] iteration 580 : loss : 0.0969, loss_ce: 0.0263
[13:42:26.792] iteration 600 : loss : 0.0906, loss_ce: 0.0278
[13:42:33.434] iteration 620 : loss : 0.0974, loss_ce: 0.0367
[13:42:34.198] Saved model to ./finetune_continual_lits17\continual_epoch_9.pth
[13:42:51.349] iteration 640 : loss : 0.0969, loss_ce: 0.0340
[13:42:58.053] iteration 660 : loss : 0.0889, loss_ce: 0.0195
[13:43:04.754] iteration 680 : loss : 0.0865, loss_ce: 0.0160
[13:43:22.268] iteration 700 : loss : 0.0865, loss_ce: 0.0155
[13:43:28.989] iteration 720 : loss : 0.0857, loss_ce: 0.0177
[13:43:35.685] iteration 740 : loss : 0.0879, loss_ce: 0.0271
[13:43:53.399] iteration 760 : loss : 0.0908, loss_ce: 0.0294
[13:44:00.101] iteration 780 : loss : 0.0887, loss_ce: 0.0248
[13:44:06.807] iteration 800 : loss : 0.0798, loss_ce: 0.0132
[13:44:24.520] iteration 820 : loss : 0.0831, loss_ce: 0.0151
[13:44:31.228] iteration 840 : loss : 0.0929, loss_ce: 0.0304
[13:44:37.925] iteration 860 : loss : 0.0889, loss_ce: 0.0242
[13:44:55.642] iteration 880 : loss : 0.0845, loss_ce: 0.0133
[13:45:02.341] iteration 900 : loss : 0.0817, loss_ce: 0.0193
[13:45:09.041] iteration 920 : loss : 0.0889, loss_ce: 0.0265
[13:45:26.754] iteration 940 : loss : 0.0881, loss_ce: 0.0261
[13:45:33.457] iteration 960 : loss : 0.0695, loss_ce: 0.0095
[13:45:40.158] iteration 980 : loss : 0.0843, loss_ce: 0.0163
[13:45:57.688] iteration 1000 : loss : 0.0752, loss_ce: 0.0112
[13:46:04.389] iteration 1020 : loss : 0.0838, loss_ce: 0.0047
[13:46:11.090] iteration 1040 : loss : 0.0765, loss_ce: 0.0179
[13:46:28.801] iteration 1060 : loss : 0.0871, loss_ce: 0.0163
[13:46:35.502] iteration 1080 : loss : 0.0643, loss_ce: 0.0180
[13:46:42.201] iteration 1100 : loss : 0.0885, loss_ce: 0.0237
[13:46:59.730] iteration 1120 : loss : 0.0691, loss_ce: 0.0114
[13:47:06.425] iteration 1140 : loss : 0.0726, loss_ce: 0.0143
[13:47:13.125] iteration 1160 : loss : 0.0601, loss_ce: 0.0131
[13:47:31.045] iteration 1180 : loss : 0.0642, loss_ce: 0.0135
[13:47:37.739] iteration 1200 : loss : 0.0569, loss_ce: 0.0159
[13:47:44.439] iteration 1220 : loss : 0.0836, loss_ce: 0.0112
[13:47:51.089] iteration 1240 : loss : 0.0512, loss_ce: 0.0101
[13:47:51.824] Saved model to ./finetune_continual_lits17\continual_epoch_19.pth
[13:48:08.853] iteration 1260 : loss : 0.0651, loss_ce: 0.0119
[13:48:15.550] iteration 1280 : loss : 0.0567, loss_ce: 0.0155
[13:48:22.263] iteration 1300 : loss : 0.0556, loss_ce: 0.0075
[13:48:39.987] iteration 1320 : loss : 0.1022, loss_ce: 0.0892
[13:48:46.696] iteration 1340 : loss : 0.0624, loss_ce: 0.0320
[13:48:53.412] iteration 1360 : loss : 0.0526, loss_ce: 0.0279
[13:49:20.338] Namespace(root_path='./datasets/lits17/train_npz', dataset='lits17', list_dir='./lists/lits17', output_dir='./finetune_continual_lits17', num_classes_new=3, num_classes_old=13, max_iterations=10000, max_epochs=30, batch_size=24, n_gpu=1, deterministic=1, base_lr=0.0005, img_size=224, seed=1234, cfg='configs/pretrain_kits23.yaml', pretrained_path='./finetune_continual_kits23/continual_epoch_29.pth', data_fraction=0.3, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, num_classes=16)
[13:49:20.341] Using 4452/14843 samples (30.0%) for continual learning.
[13:49:20.344] 186 iterations per epoch.
[13:49:37.796] iteration 20 : loss : 1.0791, loss_ce: 1.2528
[13:49:44.516] iteration 40 : loss : 0.7090, loss_ce: 0.3926
[13:49:51.232] iteration 60 : loss : 0.6081, loss_ce: 0.2079
[13:49:57.941] iteration 80 : loss : 0.5872, loss_ce: 0.1579
[13:50:04.644] iteration 100 : loss : 0.5570, loss_ce: 0.1478
[13:50:11.357] iteration 120 : loss : 0.5030, loss_ce: 0.1335
[13:50:18.092] iteration 140 : loss : 0.4918, loss_ce: 0.1076
[13:50:24.818] iteration 160 : loss : 0.5132, loss_ce: 0.1609
[13:50:31.539] iteration 180 : loss : 0.4734, loss_ce: 0.0662
[13:50:49.208] iteration 200 : loss : 0.4321, loss_ce: 0.0814
[13:50:55.914] iteration 220 : loss : 0.3903, loss_ce: 0.0491
[13:51:02.619] iteration 240 : loss : 0.3666, loss_ce: 0.0699
[13:51:09.329] iteration 260 : loss : 0.3286, loss_ce: 0.0832
[13:51:16.047] iteration 280 : loss : 0.3166, loss_ce: 0.1394
[13:51:22.757] iteration 300 : loss : 0.2903, loss_ce: 0.0763
[13:51:29.473] iteration 320 : loss : 0.2651, loss_ce: 0.0938
[13:51:36.189] iteration 340 : loss : 0.1977, loss_ce: 0.0277
[13:51:42.899] iteration 360 : loss : 0.2190, loss_ce: 0.0987
[13:52:00.566] iteration 380 : loss : 0.1973, loss_ce: 0.0472
[13:52:07.289] iteration 400 : loss : 0.1877, loss_ce: 0.0553
[13:52:14.017] iteration 420 : loss : 0.1780, loss_ce: 0.0350
[13:52:20.725] iteration 440 : loss : 0.1835, loss_ce: 0.0491
[13:52:27.441] iteration 460 : loss : 0.1366, loss_ce: 0.0307
[13:52:34.151] iteration 480 : loss : 0.1276, loss_ce: 0.0167
[13:52:40.864] iteration 500 : loss : 0.1184, loss_ce: 0.0353
[13:52:47.595] iteration 520 : loss : 0.0958, loss_ce: 0.0373
[13:52:54.318] iteration 540 : loss : 0.0870, loss_ce: 0.0168
[13:53:11.941] iteration 560 : loss : 0.0955, loss_ce: 0.0297
[13:53:18.669] iteration 580 : loss : 0.0883, loss_ce: 0.0250
[13:53:25.386] iteration 600 : loss : 0.0905, loss_ce: 0.0229
[13:53:32.144] iteration 620 : loss : 0.0989, loss_ce: 0.0355
[13:53:38.859] iteration 640 : loss : 0.0915, loss_ce: 0.0284
[13:53:45.581] iteration 660 : loss : 0.0968, loss_ce: 0.0483
[13:53:52.302] iteration 680 : loss : 0.0872, loss_ce: 0.0272
[13:53:59.032] iteration 700 : loss : 0.1021, loss_ce: 0.0444
[13:54:05.772] iteration 720 : loss : 0.1019, loss_ce: 0.0487
[13:54:12.505] iteration 740 : loss : 0.0847, loss_ce: 0.0119
[13:54:29.605] iteration 760 : loss : 0.0944, loss_ce: 0.0378
[13:54:36.315] iteration 780 : loss : 0.0763, loss_ce: 0.0120
[13:54:43.027] iteration 800 : loss : 0.0900, loss_ce: 0.0240
[13:54:49.741] iteration 820 : loss : 0.0870, loss_ce: 0.0229
[13:54:56.461] iteration 840 : loss : 0.0866, loss_ce: 0.0236
[13:55:03.183] iteration 860 : loss : 0.0925, loss_ce: 0.0270
[13:55:09.905] iteration 880 : loss : 0.0977, loss_ce: 0.0342
[13:55:16.629] iteration 900 : loss : 0.0867, loss_ce: 0.0207
[13:55:23.359] iteration 920 : loss : 0.0978, loss_ce: 0.0292
[13:55:40.583] iteration 940 : loss : 0.0934, loss_ce: 0.0440
[13:55:47.303] iteration 960 : loss : 0.0858, loss_ce: 0.0202
[13:55:54.017] iteration 980 : loss : 0.0856, loss_ce: 0.0219
[13:56:00.738] iteration 1000 : loss : 0.0812, loss_ce: 0.0227
[13:56:07.454] iteration 1020 : loss : 0.0888, loss_ce: 0.0264
[13:56:14.178] iteration 1040 : loss : 0.0789, loss_ce: 0.0202
[13:56:20.911] iteration 1060 : loss : 0.0855, loss_ce: 0.0157
[13:56:27.644] iteration 1080 : loss : 0.0877, loss_ce: 0.0222
[13:56:34.394] iteration 1100 : loss : 0.0914, loss_ce: 0.0186
[13:56:51.960] iteration 1120 : loss : 0.0918, loss_ce: 0.0251
[13:56:58.673] iteration 1140 : loss : 0.0682, loss_ce: 0.0238
[13:57:05.389] iteration 1160 : loss : 0.0839, loss_ce: 0.0190
[13:57:12.107] iteration 1180 : loss : 0.0805, loss_ce: 0.0312
[13:57:18.837] iteration 1200 : loss : 0.0721, loss_ce: 0.0343
[13:57:25.565] iteration 1220 : loss : 0.0821, loss_ce: 0.0177
[13:57:32.324] iteration 1240 : loss : 0.0531, loss_ce: 0.0218
[13:57:39.053] iteration 1260 : loss : 0.0502, loss_ce: 0.0187
[13:57:45.786] iteration 1280 : loss : 0.0508, loss_ce: 0.0217
[13:57:52.518] iteration 1300 : loss : 0.0461, loss_ce: 0.0254
[13:58:10.412] iteration 1320 : loss : 0.0253, loss_ce: 0.0136
[13:58:17.147] iteration 1340 : loss : 0.0258, loss_ce: 0.0171
[13:58:23.884] iteration 1360 : loss : 0.0553, loss_ce: 0.0276
[13:58:30.628] iteration 1380 : loss : 0.0510, loss_ce: 0.0134
[13:58:37.364] iteration 1400 : loss : 0.0303, loss_ce: 0.0182
[13:58:44.098] iteration 1420 : loss : 0.0167, loss_ce: 0.0131
[13:58:50.841] iteration 1440 : loss : 0.0483, loss_ce: 0.0286
[13:58:57.598] iteration 1460 : loss : 0.0303, loss_ce: 0.0272
[13:59:04.344] iteration 1480 : loss : 0.0367, loss_ce: 0.0208
[13:59:21.984] iteration 1500 : loss : 0.0280, loss_ce: 0.0195
[13:59:28.712] iteration 1520 : loss : 0.0474, loss_ce: 0.0267
[13:59:35.443] iteration 1540 : loss : 0.0519, loss_ce: 0.0245
[13:59:42.173] iteration 1560 : loss : 0.0264, loss_ce: 0.0133
[13:59:48.905] iteration 1580 : loss : 0.0457, loss_ce: 0.0238
[13:59:55.635] iteration 1600 : loss : 0.0646, loss_ce: 0.0518
[14:00:02.377] iteration 1620 : loss : 0.0550, loss_ce: 0.0168
[14:00:09.134] iteration 1640 : loss : 0.0500, loss_ce: 0.0237
[14:00:15.875] iteration 1660 : loss : 0.0404, loss_ce: 0.0283
[14:00:33.573] iteration 1680 : loss : 0.0413, loss_ce: 0.0264
[14:00:40.291] iteration 1700 : loss : 0.0218, loss_ce: 0.0170
[14:00:47.011] iteration 1720 : loss : 0.0389, loss_ce: 0.0241
[14:00:53.733] iteration 1740 : loss : 0.0434, loss_ce: 0.0141
[14:01:00.463] iteration 1760 : loss : 0.0220, loss_ce: 0.0131
[14:01:07.217] iteration 1780 : loss : 0.0306, loss_ce: 0.0302
[14:01:13.940] iteration 1800 : loss : 0.0344, loss_ce: 0.0186
[14:01:20.667] iteration 1820 : loss : 0.0277, loss_ce: 0.0147
[14:01:27.397] iteration 1840 : loss : 0.0563, loss_ce: 0.0182
[14:01:33.977] iteration 1860 : loss : 0.0452, loss_ce: 0.0093
[14:01:34.710] Saved model to ./finetune_continual_lits17\continual_epoch_9.pth
[14:01:52.033] iteration 1880 : loss : 0.0450, loss_ce: 0.0094
[14:01:58.768] iteration 1900 : loss : 0.0280, loss_ce: 0.0146
[14:02:05.486] iteration 1920 : loss : 0.0099, loss_ce: 0.0089
[14:02:12.216] iteration 1940 : loss : 0.0237, loss_ce: 0.0250
[14:02:18.945] iteration 1960 : loss : 0.0213, loss_ce: 0.0186
[14:02:25.682] iteration 1980 : loss : 0.0218, loss_ce: 0.0165
[14:02:32.422] iteration 2000 : loss : 0.0184, loss_ce: 0.0141
[14:02:39.164] iteration 2020 : loss : 0.0503, loss_ce: 0.0190
[14:02:45.895] iteration 2040 : loss : 0.0233, loss_ce: 0.0182
[14:03:03.389] iteration 2060 : loss : 0.0284, loss_ce: 0.0172
[14:03:10.119] iteration 2080 : loss : 0.0499, loss_ce: 0.0171
[14:03:16.851] iteration 2100 : loss : 0.0120, loss_ce: 0.0119
[14:03:23.583] iteration 2120 : loss : 0.0160, loss_ce: 0.0113
[14:03:30.316] iteration 2140 : loss : 0.0425, loss_ce: 0.0119
[14:03:37.048] iteration 2160 : loss : 0.0393, loss_ce: 0.0166
[14:03:43.791] iteration 2180 : loss : 0.0117, loss_ce: 0.0055
[14:03:50.529] iteration 2200 : loss : 0.0269, loss_ce: 0.0248
[14:03:57.274] iteration 2220 : loss : 0.0451, loss_ce: 0.0075
[14:04:15.558] iteration 2240 : loss : 0.0318, loss_ce: 0.0263
[14:04:22.290] iteration 2260 : loss : 0.0147, loss_ce: 0.0165
[14:04:29.040] iteration 2280 : loss : 0.0445, loss_ce: 0.0085
[14:04:35.782] iteration 2300 : loss : 0.0387, loss_ce: 0.0165
[14:04:42.525] iteration 2320 : loss : 0.0430, loss_ce: 0.0091
[14:04:49.268] iteration 2340 : loss : 0.0463, loss_ce: 0.0134
[14:04:56.010] iteration 2360 : loss : 0.0463, loss_ce: 0.0135
[14:05:02.751] iteration 2380 : loss : 0.0210, loss_ce: 0.0097
[14:05:09.480] iteration 2400 : loss : 0.0367, loss_ce: 0.0305
[14:05:26.732] iteration 2420 : loss : 0.0211, loss_ce: 0.0140
[14:05:33.462] iteration 2440 : loss : 0.0189, loss_ce: 0.0060
[14:05:40.195] iteration 2460 : loss : 0.0524, loss_ce: 0.0190
[14:05:46.925] iteration 2480 : loss : 0.0122, loss_ce: 0.0052
[14:05:53.656] iteration 2500 : loss : 0.0181, loss_ce: 0.0139
[14:06:00.387] iteration 2520 : loss : 0.0448, loss_ce: 0.0064
[14:06:07.125] iteration 2540 : loss : 0.0456, loss_ce: 0.0160
[14:06:13.862] iteration 2560 : loss : 0.0182, loss_ce: 0.0189
[14:06:20.601] iteration 2580 : loss : 0.0273, loss_ce: 0.0117
[14:06:27.337] iteration 2600 : loss : 0.0129, loss_ce: 0.0105
[14:06:44.794] iteration 2620 : loss : 0.0262, loss_ce: 0.0226
[14:06:51.520] iteration 2640 : loss : 0.0334, loss_ce: 0.0210
[14:06:58.263] iteration 2660 : loss : 0.0093, loss_ce: 0.0108
[14:07:05.005] iteration 2680 : loss : 0.0109, loss_ce: 0.0070
[14:07:11.750] iteration 2700 : loss : 0.0167, loss_ce: 0.0067
[14:07:18.500] iteration 2720 : loss : 0.0453, loss_ce: 0.0139
[14:07:25.236] iteration 2740 : loss : 0.0116, loss_ce: 0.0099
[14:07:31.986] iteration 2760 : loss : 0.0278, loss_ce: 0.0147
[14:07:38.720] iteration 2780 : loss : 0.0306, loss_ce: 0.0141
[14:07:56.174] iteration 2800 : loss : 0.0139, loss_ce: 0.0098
[14:08:02.899] iteration 2820 : loss : 0.0125, loss_ce: 0.0087
[14:08:09.633] iteration 2840 : loss : 0.0457, loss_ce: 0.0139
[14:08:16.361] iteration 2860 : loss : 0.0127, loss_ce: 0.0080
[14:08:23.090] iteration 2880 : loss : 0.0086, loss_ce: 0.0050
[14:08:29.826] iteration 2900 : loss : 0.0248, loss_ce: 0.0182
[14:08:36.587] iteration 2920 : loss : 0.0074, loss_ce: 0.0069
[14:08:43.317] iteration 2940 : loss : 0.0456, loss_ce: 0.0129
[14:08:50.050] iteration 2960 : loss : 0.0164, loss_ce: 0.0100
[14:09:08.136] iteration 2980 : loss : 0.0161, loss_ce: 0.0122
[14:09:14.864] iteration 3000 : loss : 0.0104, loss_ce: 0.0080
[14:09:21.605] iteration 3020 : loss : 0.0087, loss_ce: 0.0066
[14:09:28.330] iteration 3040 : loss : 0.0095, loss_ce: 0.0071
[14:09:35.072] iteration 3060 : loss : 0.0102, loss_ce: 0.0117
[14:09:41.819] iteration 3080 : loss : 0.0085, loss_ce: 0.0030
[14:09:48.559] iteration 3100 : loss : 0.0487, loss_ce: 0.0171
[14:09:55.296] iteration 3120 : loss : 0.0280, loss_ce: 0.0141
[14:10:02.039] iteration 3140 : loss : 0.0124, loss_ce: 0.0059
[14:10:08.776] iteration 3160 : loss : 0.0206, loss_ce: 0.0063
[14:10:26.000] iteration 3180 : loss : 0.0415, loss_ce: 0.0140
[14:10:32.725] iteration 3200 : loss : 0.0101, loss_ce: 0.0077
[14:10:39.458] iteration 3220 : loss : 0.0154, loss_ce: 0.0061
[14:10:46.186] iteration 3240 : loss : 0.0371, loss_ce: 0.0088
[14:10:52.917] iteration 3260 : loss : 0.0180, loss_ce: 0.0103
[14:10:59.649] iteration 3280 : loss : 0.0081, loss_ce: 0.0088
[14:11:06.388] iteration 3300 : loss : 0.0426, loss_ce: 0.0067
[14:11:13.124] iteration 3320 : loss : 0.0180, loss_ce: 0.0073
[14:11:19.857] iteration 3340 : loss : 0.0075, loss_ce: 0.0054
[14:11:37.159] iteration 3360 : loss : 0.0089, loss_ce: 0.0081
[14:11:43.872] iteration 3380 : loss : 0.0436, loss_ce: 0.0063
[14:11:50.591] iteration 3400 : loss : 0.0336, loss_ce: 0.0123
[14:11:57.319] iteration 3420 : loss : 0.0115, loss_ce: 0.0080
[14:12:04.047] iteration 3440 : loss : 0.0190, loss_ce: 0.0072
[14:12:10.774] iteration 3460 : loss : 0.0121, loss_ce: 0.0085
[14:12:17.504] iteration 3480 : loss : 0.0119, loss_ce: 0.0087
[14:12:24.236] iteration 3500 : loss : 0.0104, loss_ce: 0.0093
[14:12:30.973] iteration 3520 : loss : 0.0149, loss_ce: 0.0180
[14:12:48.141] iteration 3540 : loss : 0.0159, loss_ce: 0.0067
[14:12:54.858] iteration 3560 : loss : 0.0200, loss_ce: 0.0077
[14:13:01.577] iteration 3580 : loss : 0.0228, loss_ce: 0.0191
[14:13:08.307] iteration 3600 : loss : 0.0399, loss_ce: 0.0058
[14:13:15.036] iteration 3620 : loss : 0.0102, loss_ce: 0.0101
[14:13:21.768] iteration 3640 : loss : 0.0125, loss_ce: 0.0067
[14:13:28.502] iteration 3660 : loss : 0.0088, loss_ce: 0.0088
[14:13:35.236] iteration 3680 : loss : 0.0118, loss_ce: 0.0123
[14:13:41.970] iteration 3700 : loss : 0.0108, loss_ce: 0.0078
[14:13:48.552] iteration 3720 : loss : 0.0377, loss_ce: 0.0055
[14:13:49.230] Saved model to ./finetune_continual_lits17\continual_epoch_19.pth
[14:14:05.999] iteration 3740 : loss : 0.0442, loss_ce: 0.0088
[14:14:12.723] iteration 3760 : loss : 0.0111, loss_ce: 0.0120
[14:14:19.451] iteration 3780 : loss : 0.0185, loss_ce: 0.0104
[14:14:26.182] iteration 3800 : loss : 0.0206, loss_ce: 0.0125
[14:14:32.917] iteration 3820 : loss : 0.0422, loss_ce: 0.0068
[14:14:39.650] iteration 3840 : loss : 0.0174, loss_ce: 0.0191
[14:14:46.386] iteration 3860 : loss : 0.0104, loss_ce: 0.0056
[14:14:53.125] iteration 3880 : loss : 0.0073, loss_ce: 0.0056
[14:14:59.862] iteration 3900 : loss : 0.0165, loss_ce: 0.0078
[14:15:17.157] iteration 3920 : loss : 0.0220, loss_ce: 0.0116
[14:15:23.886] iteration 3940 : loss : 0.0097, loss_ce: 0.0110
[14:15:30.606] iteration 3960 : loss : 0.0144, loss_ce: 0.0112
[14:15:37.332] iteration 3980 : loss : 0.0182, loss_ce: 0.0041
[14:15:44.069] iteration 4000 : loss : 0.0124, loss_ce: 0.0107
[14:15:50.804] iteration 4020 : loss : 0.0109, loss_ce: 0.0102
[14:15:57.543] iteration 4040 : loss : 0.0143, loss_ce: 0.0095
[14:16:04.274] iteration 4060 : loss : 0.0105, loss_ce: 0.0124
[14:16:11.041] iteration 4080 : loss : 0.0157, loss_ce: 0.0110
[14:16:28.135] iteration 4100 : loss : 0.0066, loss_ce: 0.0045
[14:16:34.855] iteration 4120 : loss : 0.0085, loss_ce: 0.0045
[14:16:41.577] iteration 4140 : loss : 0.0064, loss_ce: 0.0047
[14:16:48.300] iteration 4160 : loss : 0.0106, loss_ce: 0.0084
[14:16:55.030] iteration 4180 : loss : 0.0097, loss_ce: 0.0045
[14:17:01.761] iteration 4200 : loss : 0.0109, loss_ce: 0.0098
[14:17:08.493] iteration 4220 : loss : 0.0096, loss_ce: 0.0030
[14:17:15.229] iteration 4240 : loss : 0.0117, loss_ce: 0.0048
[14:17:21.969] iteration 4260 : loss : 0.0158, loss_ce: 0.0072
[14:17:39.108] iteration 4280 : loss : 0.0078, loss_ce: 0.0052
[14:17:45.827] iteration 4300 : loss : 0.0121, loss_ce: 0.0077
[14:17:52.547] iteration 4320 : loss : 0.0404, loss_ce: 0.0035
[14:17:59.271] iteration 4340 : loss : 0.0302, loss_ce: 0.0092
[14:18:05.998] iteration 4360 : loss : 0.0194, loss_ce: 0.0144
[14:18:12.737] iteration 4380 : loss : 0.0127, loss_ce: 0.0072
[14:18:19.471] iteration 4400 : loss : 0.0152, loss_ce: 0.0033
[14:18:26.200] iteration 4420 : loss : 0.0123, loss_ce: 0.0081
[14:18:32.937] iteration 4440 : loss : 0.0195, loss_ce: 0.0076
[14:18:39.676] iteration 4460 : loss : 0.0148, loss_ce: 0.0114
[14:18:56.784] iteration 4480 : loss : 0.0129, loss_ce: 0.0072
[14:19:03.510] iteration 4500 : loss : 0.0411, loss_ce: 0.0053
[14:19:10.246] iteration 4520 : loss : 0.0085, loss_ce: 0.0042
[14:19:16.980] iteration 4540 : loss : 0.0144, loss_ce: 0.0058
[14:19:23.711] iteration 4560 : loss : 0.0406, loss_ce: 0.0037
[14:19:30.455] iteration 4580 : loss : 0.0416, loss_ce: 0.0062
[14:19:37.195] iteration 4600 : loss : 0.0068, loss_ce: 0.0063
[14:19:43.944] iteration 4620 : loss : 0.0114, loss_ce: 0.0107
[14:19:50.684] iteration 4640 : loss : 0.0081, loss_ce: 0.0075
[14:20:08.534] iteration 4660 : loss : 0.0114, loss_ce: 0.0060
[14:20:15.256] iteration 4680 : loss : 0.0258, loss_ce: 0.0164
[14:20:21.985] iteration 4700 : loss : 0.0412, loss_ce: 0.0045
[14:20:28.729] iteration 4720 : loss : 0.0306, loss_ce: 0.0049
[14:20:35.468] iteration 4740 : loss : 0.0123, loss_ce: 0.0127
[14:20:42.203] iteration 4760 : loss : 0.0088, loss_ce: 0.0085
[14:20:48.939] iteration 4780 : loss : 0.0080, loss_ce: 0.0064
[14:20:55.680] iteration 4800 : loss : 0.0093, loss_ce: 0.0093
[14:21:02.409] iteration 4820 : loss : 0.0085, loss_ce: 0.0072
[14:21:19.902] iteration 4840 : loss : 0.0418, loss_ce: 0.0056
[14:21:26.627] iteration 4860 : loss : 0.0076, loss_ce: 0.0054
[14:21:33.362] iteration 4880 : loss : 0.0096, loss_ce: 0.0085
[14:21:40.085] iteration 4900 : loss : 0.0147, loss_ce: 0.0118
[14:21:46.830] iteration 4920 : loss : 0.0263, loss_ce: 0.0039
[14:21:53.564] iteration 4940 : loss : 0.0095, loss_ce: 0.0028
[14:22:00.326] iteration 4960 : loss : 0.0416, loss_ce: 0.0064
[14:22:07.069] iteration 4980 : loss : 0.0119, loss_ce: 0.0070
[14:22:13.814] iteration 5000 : loss : 0.0079, loss_ce: 0.0018
[14:22:20.550] iteration 5020 : loss : 0.0176, loss_ce: 0.0048
[14:22:37.981] iteration 5040 : loss : 0.0428, loss_ce: 0.0078
[14:22:44.709] iteration 5060 : loss : 0.0109, loss_ce: 0.0108
[14:22:51.443] iteration 5080 : loss : 0.0090, loss_ce: 0.0049
[14:22:58.180] iteration 5100 : loss : 0.0088, loss_ce: 0.0048
[14:23:04.923] iteration 5120 : loss : 0.0140, loss_ce: 0.0040
[14:23:11.655] iteration 5140 : loss : 0.0084, loss_ce: 0.0079
[14:23:18.400] iteration 5160 : loss : 0.0158, loss_ce: 0.0070
[14:23:25.140] iteration 5180 : loss : 0.0126, loss_ce: 0.0079
[14:23:31.891] iteration 5200 : loss : 0.0403, loss_ce: 0.0031
[14:23:49.359] iteration 5220 : loss : 0.0407, loss_ce: 0.0049
[14:23:56.120] iteration 5240 : loss : 0.0441, loss_ce: 0.0086
[14:24:02.844] iteration 5260 : loss : 0.0174, loss_ce: 0.0049
[14:24:09.568] iteration 5280 : loss : 0.0103, loss_ce: 0.0053
[14:24:16.304] iteration 5300 : loss : 0.0068, loss_ce: 0.0056
[14:24:23.035] iteration 5320 : loss : 0.0092, loss_ce: 0.0084
[14:24:29.764] iteration 5340 : loss : 0.0188, loss_ce: 0.0079
[14:24:36.492] iteration 5360 : loss : 0.0115, loss_ce: 0.0078
[14:24:43.230] iteration 5380 : loss : 0.0257, loss_ce: 0.0060
[14:25:01.115] iteration 5400 : loss : 0.0135, loss_ce: 0.0119
[14:25:07.836] iteration 5420 : loss : 0.0219, loss_ce: 0.0077
[14:25:14.571] iteration 5440 : loss : 0.0078, loss_ce: 0.0058
[14:25:21.300] iteration 5460 : loss : 0.0073, loss_ce: 0.0035
[14:25:28.038] iteration 5480 : loss : 0.0069, loss_ce: 0.0067
[14:25:34.778] iteration 5500 : loss : 0.0158, loss_ce: 0.0109
[14:25:41.516] iteration 5520 : loss : 0.0097, loss_ce: 0.0057
[14:25:48.251] iteration 5540 : loss : 0.0155, loss_ce: 0.0059
[14:25:54.986] iteration 5560 : loss : 0.0065, loss_ce: 0.0063
[14:26:01.566] iteration 5580 : loss : 0.0387, loss_ce: 0.0032
[14:26:02.259] Saved model to ./finetune_continual_lits17\continual_epoch_29.pth
