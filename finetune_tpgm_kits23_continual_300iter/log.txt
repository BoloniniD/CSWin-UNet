[01:43:41.365] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual_300iter', max_iterations=10000, max_epochs=50, batch_size=32, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=300, tpgm_exclude=[])
[01:43:41.391] Using 8569/95221 samples for finetuning
[01:43:41.391] Using 953/95221 samples for TPGM
[01:43:41.391] Model has 9 total classes, training on 4 classes
[01:43:50.946] 268 iterations per epoch. 13400 max iterations 
[01:44:05.128] iteration 10 : loss : 0.461870, loss_ce: 0.081058
[01:44:09.193] iteration 20 : loss : 0.447126, loss_ce: 0.067454
[01:44:13.283] iteration 30 : loss : 0.454064, loss_ce: 0.061675
[01:44:17.350] iteration 40 : loss : 0.438523, loss_ce: 0.037797
[01:44:21.427] iteration 50 : loss : 0.427871, loss_ce: 0.060777
[01:44:25.511] iteration 60 : loss : 0.289397, loss_ce: 0.043874
[01:44:29.605] iteration 70 : loss : 0.430093, loss_ce: 0.078371
[01:44:33.716] iteration 80 : loss : 0.432416, loss_ce: 0.045691
[01:44:37.865] iteration 90 : loss : 0.438128, loss_ce: 0.059353
[01:44:41.953] iteration 100 : loss : 0.424840, loss_ce: 0.065352
[01:44:46.073] iteration 110 : loss : 0.289580, loss_ce: 0.027083
[01:44:50.153] iteration 120 : loss : 0.420143, loss_ce: 0.054165
[01:44:54.242] iteration 130 : loss : 0.428500, loss_ce: 0.046873
[01:44:58.327] iteration 140 : loss : 0.401049, loss_ce: 0.039351
[01:45:02.424] iteration 150 : loss : 0.411577, loss_ce: 0.030802
[01:45:06.522] iteration 160 : loss : 0.382576, loss_ce: 0.048536
[01:45:10.632] iteration 170 : loss : 0.386560, loss_ce: 0.035044
[01:45:14.754] iteration 180 : loss : 0.363701, loss_ce: 0.024090
[01:45:18.875] iteration 190 : loss : 0.240404, loss_ce: 0.031053
[01:45:22.962] iteration 200 : loss : 0.373939, loss_ce: 0.052010
[01:45:27.064] iteration 210 : loss : 0.388509, loss_ce: 0.043876
[01:45:31.148] iteration 220 : loss : 0.379827, loss_ce: 0.024280
[01:45:35.295] iteration 230 : loss : 0.358815, loss_ce: 0.026818
[01:45:39.413] iteration 240 : loss : 0.366219, loss_ce: 0.032203
[01:45:43.811] iteration 250 : loss : 0.391504, loss_ce: 0.052399
[01:45:48.197] iteration 260 : loss : 0.350837, loss_ce: 0.040769
[01:50:19.828] iteration 270 : loss : 0.459218, loss_ce: 0.227408
[01:50:23.898] iteration 280 : loss : 0.459283, loss_ce: 0.035740
[01:50:27.974] iteration 290 : loss : 0.471173, loss_ce: 0.054368
[01:50:32.046] iteration 300 : loss : 0.447334, loss_ce: 0.037229
[01:50:36.121] iteration 310 : loss : 0.435651, loss_ce: 0.083077
[01:50:40.187] iteration 320 : loss : 0.442000, loss_ce: 0.066265
[01:50:44.269] iteration 330 : loss : 0.423197, loss_ce: 0.068675
[01:50:48.347] iteration 340 : loss : 0.424317, loss_ce: 0.050888
[01:50:52.424] iteration 350 : loss : 0.430558, loss_ce: 0.052924
[01:50:56.491] iteration 360 : loss : 0.269150, loss_ce: 0.042137
[01:51:00.574] iteration 370 : loss : 0.278416, loss_ce: 0.043453
[01:51:04.643] iteration 380 : loss : 0.423859, loss_ce: 0.050456
[01:51:08.724] iteration 390 : loss : 0.247927, loss_ce: 0.027668
[01:51:12.813] iteration 400 : loss : 0.245580, loss_ce: 0.033289
[01:51:16.894] iteration 410 : loss : 0.384478, loss_ce: 0.031326
[01:51:21.008] iteration 420 : loss : 0.431573, loss_ce: 0.045305
[01:51:25.124] iteration 430 : loss : 0.367814, loss_ce: 0.047655
[01:51:29.202] iteration 440 : loss : 0.208412, loss_ce: 0.034109
[01:51:33.289] iteration 450 : loss : 0.194552, loss_ce: 0.039468
[01:51:37.384] iteration 460 : loss : 0.311431, loss_ce: 0.039979
[01:51:41.481] iteration 470 : loss : 0.346136, loss_ce: 0.034614
[01:51:45.588] iteration 480 : loss : 0.227577, loss_ce: 0.008165
[01:51:49.798] iteration 490 : loss : 0.232933, loss_ce: 0.015659
[01:51:54.107] iteration 500 : loss : 0.180621, loss_ce: 0.014570
[01:51:58.226] iteration 510 : loss : 0.256396, loss_ce: 0.011684
[01:52:02.403] iteration 520 : loss : 0.328464, loss_ce: 0.036958
[01:52:06.524] iteration 530 : loss : 0.341455, loss_ce: 0.017458
[01:57:33.312] iteration 540 : loss : 0.474544, loss_ce: 0.067630
[01:57:37.502] iteration 550 : loss : 0.494947, loss_ce: 0.110658
[01:57:41.552] iteration 560 : loss : 0.464011, loss_ce: 0.045036
[01:57:45.617] iteration 570 : loss : 0.465941, loss_ce: 0.051074
[01:57:49.671] iteration 580 : loss : 0.310460, loss_ce: 0.053030
[01:57:53.744] iteration 590 : loss : 0.463679, loss_ce: 0.061907
[01:57:57.803] iteration 600 : loss : 0.454389, loss_ce: 0.039944
[01:58:01.872] iteration 610 : loss : 0.457157, loss_ce: 0.054193
[01:58:05.937] iteration 620 : loss : 0.454565, loss_ce: 0.070785
[01:58:10.012] iteration 630 : loss : 0.429823, loss_ce: 0.072638
[01:58:14.076] iteration 640 : loss : 0.434220, loss_ce: 0.050017
[01:58:18.158] iteration 650 : loss : 0.293497, loss_ce: 0.056212
[01:58:22.227] iteration 660 : loss : 0.443983, loss_ce: 0.058498
[01:58:26.311] iteration 670 : loss : 0.452053, loss_ce: 0.038336
[01:58:30.379] iteration 680 : loss : 0.437833, loss_ce: 0.073427
[01:58:34.461] iteration 690 : loss : 0.432913, loss_ce: 0.059378
[01:58:38.533] iteration 700 : loss : 0.452668, loss_ce: 0.080245
[01:58:42.615] iteration 710 : loss : 0.435261, loss_ce: 0.051572
[01:58:46.686] iteration 720 : loss : 0.430524, loss_ce: 0.050469
[01:58:50.774] iteration 730 : loss : 0.294109, loss_ce: 0.051786
[01:58:54.844] iteration 740 : loss : 0.427478, loss_ce: 0.048424
[01:58:58.938] iteration 750 : loss : 0.443036, loss_ce: 0.047448
[01:59:03.008] iteration 760 : loss : 0.259802, loss_ce: 0.027712
[01:59:07.093] iteration 770 : loss : 0.268743, loss_ce: 0.030034
[01:59:11.166] iteration 780 : loss : 0.423161, loss_ce: 0.033834
[01:59:15.251] iteration 790 : loss : 0.423830, loss_ce: 0.050578
[01:59:19.324] iteration 800 : loss : 0.414510, loss_ce: 0.074562
[02:04:44.706] iteration 810 : loss : 0.491035, loss_ce: 0.100879
[02:04:48.782] iteration 820 : loss : 0.467061, loss_ce: 0.082201
[02:04:52.841] iteration 830 : loss : 0.453124, loss_ce: 0.032058
[02:04:56.892] iteration 840 : loss : 0.470514, loss_ce: 0.107318
[02:05:00.959] iteration 850 : loss : 0.307797, loss_ce: 0.078679
[02:05:05.014] iteration 860 : loss : 0.292363, loss_ce: 0.054623
[02:05:09.081] iteration 870 : loss : 0.285738, loss_ce: 0.040990
[02:05:13.146] iteration 880 : loss : 0.446806, loss_ce: 0.063117
[02:05:17.223] iteration 890 : loss : 0.439316, loss_ce: 0.046852
[02:05:21.289] iteration 900 : loss : 0.432760, loss_ce: 0.061892
[02:05:25.367] iteration 910 : loss : 0.426378, loss_ce: 0.054448
[02:05:29.438] iteration 920 : loss : 0.453224, loss_ce: 0.051678
[02:05:33.515] iteration 930 : loss : 0.433847, loss_ce: 0.067661
[02:05:37.581] iteration 940 : loss : 0.459433, loss_ce: 0.070442
[02:05:41.661] iteration 950 : loss : 0.432021, loss_ce: 0.083954
[02:05:45.731] iteration 960 : loss : 0.436113, loss_ce: 0.055029
[02:05:49.814] iteration 970 : loss : 0.437215, loss_ce: 0.041458
[02:05:53.885] iteration 980 : loss : 0.285998, loss_ce: 0.043689
[02:05:57.968] iteration 990 : loss : 0.285703, loss_ce: 0.046893
[02:06:02.042] iteration 1000 : loss : 0.429019, loss_ce: 0.059358
[02:06:06.124] iteration 1010 : loss : 0.430979, loss_ce: 0.052689
[02:06:10.196] iteration 1020 : loss : 0.435853, loss_ce: 0.030256
[02:06:14.278] iteration 1030 : loss : 0.429786, loss_ce: 0.038342
[02:06:18.353] iteration 1040 : loss : 0.271555, loss_ce: 0.035312
[02:06:22.441] iteration 1050 : loss : 0.402739, loss_ce: 0.052794
[02:06:26.512] iteration 1060 : loss : 0.402378, loss_ce: 0.063503
[02:06:30.599] iteration 1070 : loss : 0.378689, loss_ce: 0.044238
[02:11:55.572] iteration 1080 : loss : 0.500740, loss_ce: 0.125217
[02:11:59.635] iteration 1090 : loss : 0.470008, loss_ce: 0.065146
[02:12:03.689] iteration 1100 : loss : 0.457404, loss_ce: 0.046015
[02:12:07.755] iteration 1110 : loss : 0.460219, loss_ce: 0.078054
[02:12:11.811] iteration 1120 : loss : 0.460378, loss_ce: 0.087136
[02:12:15.884] iteration 1130 : loss : 0.455218, loss_ce: 0.028508
[02:12:19.945] iteration 1140 : loss : 0.447442, loss_ce: 0.046045
[02:12:24.021] iteration 1150 : loss : 0.442209, loss_ce: 0.075342
[02:12:28.088] iteration 1160 : loss : 0.448418, loss_ce: 0.040372
[02:12:32.164] iteration 1170 : loss : 0.441560, loss_ce: 0.067059
[02:12:36.232] iteration 1180 : loss : 0.442643, loss_ce: 0.060395
[02:12:40.313] iteration 1190 : loss : 0.395229, loss_ce: 0.051703
[02:12:44.383] iteration 1200 : loss : 0.442842, loss_ce: 0.063150
[02:12:48.466] iteration 1210 : loss : 0.427587, loss_ce: 0.049636
[02:12:52.538] iteration 1220 : loss : 0.396520, loss_ce: 0.034146
[02:12:56.622] iteration 1230 : loss : 0.425141, loss_ce: 0.054059
[02:13:00.695] iteration 1240 : loss : 0.419614, loss_ce: 0.033225
[02:13:04.781] iteration 1250 : loss : 0.418247, loss_ce: 0.040077
[02:13:08.853] iteration 1260 : loss : 0.452586, loss_ce: 0.078814
[02:13:12.937] iteration 1270 : loss : 0.594944, loss_ce: 0.360880
[02:13:17.006] iteration 1280 : loss : 0.468756, loss_ce: 0.046166
[02:13:21.086] iteration 1290 : loss : 0.349463, loss_ce: 0.121830
[02:13:25.158] iteration 1300 : loss : 0.486533, loss_ce: 0.089993
[02:13:29.239] iteration 1310 : loss : 0.482484, loss_ce: 0.079714
[02:13:33.312] iteration 1320 : loss : 0.461462, loss_ce: 0.033241
[02:13:37.400] iteration 1330 : loss : 0.320423, loss_ce: 0.077685
[02:13:41.372] iteration 1340 : loss : 0.460873, loss_ce: 0.056868
[02:19:06.662] iteration 1350 : loss : 0.459209, loss_ce: 0.026270
[02:19:10.716] iteration 1360 : loss : 0.466710, loss_ce: 0.047975
[02:19:14.780] iteration 1370 : loss : 0.467873, loss_ce: 0.061956
[02:19:18.832] iteration 1380 : loss : 0.465348, loss_ce: 0.053900
[02:19:22.899] iteration 1390 : loss : 0.458726, loss_ce: 0.051843
[02:19:26.958] iteration 1400 : loss : 0.461654, loss_ce: 0.056573
[02:19:31.033] iteration 1410 : loss : 0.456215, loss_ce: 0.072555
[02:19:35.090] iteration 1420 : loss : 0.458135, loss_ce: 0.065915
[02:19:39.166] iteration 1430 : loss : 0.443824, loss_ce: 0.048539
[02:19:43.228] iteration 1440 : loss : 0.440045, loss_ce: 0.036780
[02:19:47.305] iteration 1450 : loss : 0.446933, loss_ce: 0.026760
[02:19:51.368] iteration 1460 : loss : 0.316471, loss_ce: 0.054100
[02:19:55.448] iteration 1470 : loss : 0.477394, loss_ce: 0.067375
[02:19:59.513] iteration 1480 : loss : 0.458307, loss_ce: 0.067389
[02:20:03.595] iteration 1490 : loss : 0.296190, loss_ce: 0.050340
[02:20:07.665] iteration 1500 : loss : 0.441772, loss_ce: 0.055139
[02:20:11.751] iteration 1510 : loss : 0.438581, loss_ce: 0.045947
[02:20:15.821] iteration 1520 : loss : 0.435755, loss_ce: 0.046242
[02:20:19.902] iteration 1530 : loss : 0.441730, loss_ce: 0.045648
[02:20:23.972] iteration 1540 : loss : 0.429911, loss_ce: 0.065233
[02:20:28.054] iteration 1550 : loss : 0.445016, loss_ce: 0.064797
[02:20:32.124] iteration 1560 : loss : 0.432387, loss_ce: 0.057391
[02:20:36.205] iteration 1570 : loss : 0.429763, loss_ce: 0.062313
[02:20:40.275] iteration 1580 : loss : 0.284890, loss_ce: 0.040069
[02:20:44.360] iteration 1590 : loss : 0.422931, loss_ce: 0.068054
[02:20:48.432] iteration 1600 : loss : 0.449213, loss_ce: 0.055609
[02:26:13.849] iteration 1610 : loss : 0.431882, loss_ce: 0.089265
[02:26:18.160] iteration 1620 : loss : 0.480592, loss_ce: 0.076631
[02:26:22.225] iteration 1630 : loss : 0.488591, loss_ce: 0.095255
[02:26:26.277] iteration 1640 : loss : 0.490437, loss_ce: 0.099315
[02:26:30.342] iteration 1650 : loss : 0.327894, loss_ce: 0.068560
[02:26:34.398] iteration 1660 : loss : 0.492170, loss_ce: 0.103575
[02:26:38.470] iteration 1670 : loss : 0.466133, loss_ce: 0.057731
[02:26:42.530] iteration 1680 : loss : 0.458643, loss_ce: 0.072778
[02:26:46.604] iteration 1690 : loss : 0.455104, loss_ce: 0.057614
[02:26:50.669] iteration 1700 : loss : 0.297671, loss_ce: 0.066558
[02:26:54.748] iteration 1710 : loss : 0.435917, loss_ce: 0.073678
[02:26:58.819] iteration 1720 : loss : 0.435009, loss_ce: 0.045638
[02:27:02.899] iteration 1730 : loss : 0.443650, loss_ce: 0.073320
[02:27:06.970] iteration 1740 : loss : 0.281587, loss_ce: 0.041207
[02:27:11.059] iteration 1750 : loss : 0.430038, loss_ce: 0.051929
[02:27:15.125] iteration 1760 : loss : 0.452343, loss_ce: 0.065106
[02:27:19.205] iteration 1770 : loss : 0.429992, loss_ce: 0.040890
[02:27:23.278] iteration 1780 : loss : 0.436915, loss_ce: 0.054070
[02:27:27.358] iteration 1790 : loss : 0.297010, loss_ce: 0.066973
[02:27:31.428] iteration 1800 : loss : 0.441197, loss_ce: 0.062961
[02:27:35.514] iteration 1810 : loss : 0.442230, loss_ce: 0.049572
[02:27:39.585] iteration 1820 : loss : 0.432070, loss_ce: 0.048207
[02:27:43.670] iteration 1830 : loss : 0.431873, loss_ce: 0.086234
[02:27:47.739] iteration 1840 : loss : 0.432048, loss_ce: 0.057581
[02:27:51.828] iteration 1850 : loss : 0.280769, loss_ce: 0.043606
[02:27:55.902] iteration 1860 : loss : 0.428907, loss_ce: 0.052786
[02:27:59.988] iteration 1870 : loss : 0.435753, loss_ce: 0.076939
[02:33:26.291] iteration 1880 : loss : 0.509556, loss_ce: 0.147299
[02:33:30.490] iteration 1890 : loss : 0.497004, loss_ce: 0.129168
[02:33:34.539] iteration 1900 : loss : 0.473072, loss_ce: 0.061539
[02:33:38.606] iteration 1910 : loss : 0.465109, loss_ce: 0.063404
[02:33:42.660] iteration 1920 : loss : 0.434119, loss_ce: 0.084347
[02:33:46.733] iteration 1930 : loss : 0.438481, loss_ce: 0.042386
[02:33:50.795] iteration 1940 : loss : 0.445001, loss_ce: 0.059925
[02:33:54.865] iteration 1950 : loss : 0.450638, loss_ce: 0.031825
[02:33:58.930] iteration 1960 : loss : 0.455593, loss_ce: 0.060431
[02:34:03.007] iteration 1970 : loss : 0.447662, loss_ce: 0.049761
[02:34:07.075] iteration 1980 : loss : 0.424708, loss_ce: 0.057805
[02:34:11.156] iteration 1990 : loss : 0.439053, loss_ce: 0.051846
[02:34:15.224] iteration 2000 : loss : 0.431588, loss_ce: 0.075920
[02:34:19.311] iteration 2010 : loss : 0.426669, loss_ce: 0.047762
[02:34:23.382] iteration 2020 : loss : 0.427169, loss_ce: 0.039358
[02:34:27.468] iteration 2030 : loss : 0.426436, loss_ce: 0.053079
[02:34:31.545] iteration 2040 : loss : 0.410312, loss_ce: 0.045842
[02:34:35.628] iteration 2050 : loss : 0.404162, loss_ce: 0.036823
[02:34:39.702] iteration 2060 : loss : 0.401356, loss_ce: 0.054514
[02:34:43.781] iteration 2070 : loss : 0.364169, loss_ce: 0.045003
[02:34:47.854] iteration 2080 : loss : 0.353930, loss_ce: 0.019693
[02:34:51.943] iteration 2090 : loss : 0.403636, loss_ce: 0.079850
[02:34:56.015] iteration 2100 : loss : 0.349274, loss_ce: 0.040413
[02:35:00.107] iteration 2110 : loss : 0.373543, loss_ce: 0.021512
[02:35:04.179] iteration 2120 : loss : 0.268522, loss_ce: 0.009144
[02:35:08.265] iteration 2130 : loss : 0.295601, loss_ce: 0.019745
[02:35:12.336] iteration 2140 : loss : 0.368360, loss_ce: 0.028431
[02:40:38.050] iteration 2150 : loss : 0.502506, loss_ce: 0.155623
[02:40:42.122] iteration 2160 : loss : 0.502114, loss_ce: 0.128006
[02:40:46.181] iteration 2170 : loss : 0.461000, loss_ce: 0.041490
[02:40:50.234] iteration 2180 : loss : 0.455665, loss_ce: 0.067349
[02:40:54.301] iteration 2190 : loss : 0.452581, loss_ce: 0.036739
[02:40:58.359] iteration 2200 : loss : 0.452810, loss_ce: 0.063562
[02:41:02.433] iteration 2210 : loss : 0.445659, loss_ce: 0.028861
[02:41:06.494] iteration 2220 : loss : 0.457047, loss_ce: 0.051133
[02:41:10.571] iteration 2230 : loss : 0.428767, loss_ce: 0.067677
[02:41:14.635] iteration 2240 : loss : 0.442775, loss_ce: 0.060061
[02:41:18.712] iteration 2250 : loss : 0.435885, loss_ce: 0.079264
[02:41:22.780] iteration 2260 : loss : 0.444175, loss_ce: 0.065442
[02:41:26.857] iteration 2270 : loss : 0.425464, loss_ce: 0.046187
[02:41:30.926] iteration 2280 : loss : 0.409529, loss_ce: 0.046450
[02:41:35.014] iteration 2290 : loss : 0.426774, loss_ce: 0.051428
[02:41:39.086] iteration 2300 : loss : 0.429169, loss_ce: 0.040783
[02:41:43.173] iteration 2310 : loss : 0.418855, loss_ce: 0.042246
[02:41:47.246] iteration 2320 : loss : 0.396022, loss_ce: 0.053926
[02:41:51.332] iteration 2330 : loss : 0.428401, loss_ce: 0.022034
[02:41:55.407] iteration 2340 : loss : 0.409455, loss_ce: 0.061532
[02:41:59.493] iteration 2350 : loss : 0.406576, loss_ce: 0.048217
[02:42:03.571] iteration 2360 : loss : 0.369683, loss_ce: 0.030990
[02:42:07.657] iteration 2370 : loss : 0.339574, loss_ce: 0.036763
[02:42:11.733] iteration 2380 : loss : 0.422442, loss_ce: 0.120042
[02:42:15.823] iteration 2390 : loss : 0.390686, loss_ce: 0.068567
[02:42:19.899] iteration 2400 : loss : 0.371594, loss_ce: 0.031910
[02:42:23.989] iteration 2410 : loss : 0.350213, loss_ce: 0.027934
[02:47:53.091] iteration 2420 : loss : 0.484112, loss_ce: 0.101237
[02:47:57.283] iteration 2430 : loss : 0.465999, loss_ce: 0.053822
[02:48:01.511] iteration 2440 : loss : 0.445760, loss_ce: 0.036435
[02:48:05.708] iteration 2450 : loss : 0.444904, loss_ce: 0.055256
[02:48:09.950] iteration 2460 : loss : 0.452535, loss_ce: 0.058707
[02:48:15.064] iteration 2470 : loss : 0.448981, loss_ce: 0.070895
[02:48:21.262] iteration 2480 : loss : 0.444681, loss_ce: 0.071371
[02:48:25.936] iteration 2490 : loss : 0.446862, loss_ce: 0.056495
[02:48:30.355] iteration 2500 : loss : 0.452678, loss_ce: 0.028967
[02:48:34.824] iteration 2510 : loss : 0.439753, loss_ce: 0.062910
[02:48:39.005] iteration 2520 : loss : 0.438573, loss_ce: 0.051778
[02:48:43.403] iteration 2530 : loss : 0.441649, loss_ce: 0.038300
[02:48:47.614] iteration 2540 : loss : 0.447346, loss_ce: 0.062648
[02:48:51.694] iteration 2550 : loss : 0.440395, loss_ce: 0.041500
[02:48:55.755] iteration 2560 : loss : 0.440698, loss_ce: 0.045120
[02:48:59.838] iteration 2570 : loss : 0.436872, loss_ce: 0.055931
[02:49:03.905] iteration 2580 : loss : 0.416437, loss_ce: 0.052347
[02:49:07.986] iteration 2590 : loss : 0.421226, loss_ce: 0.062819
[02:49:12.056] iteration 2600 : loss : 0.281513, loss_ce: 0.033135
[02:49:16.133] iteration 2610 : loss : 0.417647, loss_ce: 0.041071
[02:49:20.203] iteration 2620 : loss : 0.427680, loss_ce: 0.064765
[02:49:24.283] iteration 2630 : loss : 0.448952, loss_ce: 0.068320
[02:49:28.353] iteration 2640 : loss : 0.404688, loss_ce: 0.049951
[02:49:32.436] iteration 2650 : loss : 0.417681, loss_ce: 0.038649
[02:49:36.503] iteration 2660 : loss : 0.255209, loss_ce: 0.025145
[02:49:40.587] iteration 2670 : loss : 0.378667, loss_ce: 0.032144
[02:49:44.558] iteration 2680 : loss : 0.386563, loss_ce: 0.055021
[02:55:10.579] save model to ./finetune_tpgm_kits23_continual_300iter\finetuned_epoch_9.pth
[02:55:25.297] iteration 2690 : loss : 0.495528, loss_ce: 0.112033
[02:55:29.410] iteration 2700 : loss : 0.467987, loss_ce: 0.048158
[02:55:33.531] iteration 2710 : loss : 0.457081, loss_ce: 0.046827
[02:55:37.641] iteration 2720 : loss : 0.460456, loss_ce: 0.053493
[02:55:41.764] iteration 2730 : loss : 0.454578, loss_ce: 0.067755
[02:55:45.883] iteration 2740 : loss : 0.473664, loss_ce: 0.072887
[02:55:50.015] iteration 2750 : loss : 0.467196, loss_ce: 0.062850
[02:55:54.135] iteration 2760 : loss : 0.462151, loss_ce: 0.055580
[02:55:58.414] iteration 2770 : loss : 0.445686, loss_ce: 0.040126
[02:56:02.515] iteration 2780 : loss : 0.441362, loss_ce: 0.040252
[02:56:06.598] iteration 2790 : loss : 0.423644, loss_ce: 0.062847
[02:56:10.679] iteration 2800 : loss : 0.452426, loss_ce: 0.053462
[02:56:14.775] iteration 2810 : loss : 0.442141, loss_ce: 0.045265
[02:56:19.004] iteration 2820 : loss : 0.438199, loss_ce: 0.053979
[02:56:23.290] iteration 2830 : loss : 0.443916, loss_ce: 0.054986
[02:56:27.418] iteration 2840 : loss : 0.428134, loss_ce: 0.067338
[02:56:31.561] iteration 2850 : loss : 0.433886, loss_ce: 0.068225
[02:56:35.688] iteration 2860 : loss : 0.429228, loss_ce: 0.045834
[02:56:39.828] iteration 2870 : loss : 0.444900, loss_ce: 0.064014
[02:56:43.959] iteration 2880 : loss : 0.433226, loss_ce: 0.063080
[02:56:48.102] iteration 2890 : loss : 0.430641, loss_ce: 0.051097
[02:56:52.232] iteration 2900 : loss : 0.416574, loss_ce: 0.074218
[02:56:56.496] iteration 2910 : loss : 0.422429, loss_ce: 0.037501
[02:57:00.608] iteration 2920 : loss : 0.441438, loss_ce: 0.041604
[02:57:04.723] iteration 2930 : loss : 0.400562, loss_ce: 0.040029
[02:57:09.171] iteration 2940 : loss : 0.410786, loss_ce: 0.039666
[03:02:59.253] iteration 2950 : loss : 0.472492, loss_ce: 0.083288
[03:03:03.927] iteration 2960 : loss : 0.528052, loss_ce: 0.192849
[03:03:08.307] iteration 2970 : loss : 0.485703, loss_ce: 0.087714
[03:03:12.480] iteration 2980 : loss : 0.490322, loss_ce: 0.099241
[03:03:16.663] iteration 2990 : loss : 0.494370, loss_ce: 0.109410
[03:03:20.839] iteration 3000 : loss : 0.315516, loss_ce: 0.038258
[03:03:25.030] iteration 3010 : loss : 0.485214, loss_ce: 0.087910
[03:03:29.205] iteration 3020 : loss : 0.467239, loss_ce: 0.052290
[03:03:33.395] iteration 3030 : loss : 0.487689, loss_ce: 0.099280
[03:03:37.578] iteration 3040 : loss : 0.305517, loss_ce: 0.029582
[03:03:41.774] iteration 3050 : loss : 0.465520, loss_ce: 0.085036
[03:03:45.957] iteration 3060 : loss : 0.460617, loss_ce: 0.062267
[03:03:50.152] iteration 3070 : loss : 0.459387, loss_ce: 0.064716
[03:03:54.365] iteration 3080 : loss : 0.471315, loss_ce: 0.099283
[03:03:58.881] iteration 3090 : loss : 0.453338, loss_ce: 0.042161
[03:04:03.132] iteration 3100 : loss : 0.448289, loss_ce: 0.087018
[03:04:07.216] iteration 3110 : loss : 0.455235, loss_ce: 0.052487
[03:04:11.306] iteration 3120 : loss : 0.451356, loss_ce: 0.087893
[03:04:15.391] iteration 3130 : loss : 0.436442, loss_ce: 0.073566
[03:04:19.461] iteration 3140 : loss : 0.435845, loss_ce: 0.061630
[03:04:23.559] iteration 3150 : loss : 0.444384, loss_ce: 0.055319
[03:04:27.635] iteration 3160 : loss : 0.291895, loss_ce: 0.055291
[03:04:31.721] iteration 3170 : loss : 0.427047, loss_ce: 0.037771
[03:04:35.810] iteration 3180 : loss : 0.433917, loss_ce: 0.045189
[03:04:39.979] iteration 3190 : loss : 0.438563, loss_ce: 0.060886
[03:04:44.558] iteration 3200 : loss : 0.445015, loss_ce: 0.048054
[03:04:49.138] iteration 3210 : loss : 0.437778, loss_ce: 0.052555
[03:10:35.052] iteration 3220 : loss : 0.497422, loss_ce: 0.116652
[03:10:39.298] iteration 3230 : loss : 0.363413, loss_ce: 0.108966
[03:10:43.403] iteration 3240 : loss : 0.313857, loss_ce: 0.036642
[03:10:47.526] iteration 3250 : loss : 0.459057, loss_ce: 0.054911
[03:10:51.636] iteration 3260 : loss : 0.465062, loss_ce: 0.106923
[03:10:55.763] iteration 3270 : loss : 0.456518, loss_ce: 0.082562
[03:10:59.874] iteration 3280 : loss : 0.449645, loss_ce: 0.073359
[03:11:03.998] iteration 3290 : loss : 0.441669, loss_ce: 0.055726
[03:11:08.155] iteration 3300 : loss : 0.444735, loss_ce: 0.065113
[03:11:12.687] iteration 3310 : loss : 0.454232, loss_ce: 0.083884
[03:11:17.010] iteration 3320 : loss : 0.295445, loss_ce: 0.030366
[03:11:21.149] iteration 3330 : loss : 0.433554, loss_ce: 0.057545
[03:11:25.274] iteration 3340 : loss : 0.435054, loss_ce: 0.072620
[03:11:29.409] iteration 3350 : loss : 0.422009, loss_ce: 0.065782
[03:11:33.534] iteration 3360 : loss : 0.441966, loss_ce: 0.065503
[03:11:37.672] iteration 3370 : loss : 0.444363, loss_ce: 0.055777
[03:11:41.798] iteration 3380 : loss : 0.429451, loss_ce: 0.044172
[03:11:45.941] iteration 3390 : loss : 0.428009, loss_ce: 0.086894
[03:11:50.073] iteration 3400 : loss : 0.399693, loss_ce: 0.033448
[03:11:54.218] iteration 3410 : loss : 0.426589, loss_ce: 0.050021
[03:11:58.350] iteration 3420 : loss : 0.402637, loss_ce: 0.061361
[03:12:02.494] iteration 3430 : loss : 0.344011, loss_ce: 0.036695
[03:12:06.624] iteration 3440 : loss : 0.377600, loss_ce: 0.036470
[03:12:10.768] iteration 3450 : loss : 0.371345, loss_ce: 0.045614
[03:12:14.952] iteration 3460 : loss : 0.366336, loss_ce: 0.024869
[03:12:19.395] iteration 3470 : loss : 0.337989, loss_ce: 0.049367
[03:12:23.690] iteration 3480 : loss : 0.361105, loss_ce: 0.070742
[03:17:31.752] iteration 3490 : loss : 0.663099, loss_ce: 0.511822
[03:17:35.826] iteration 3500 : loss : 0.476004, loss_ce: 0.073810
[03:17:39.932] iteration 3510 : loss : 0.470326, loss_ce: 0.060274
[03:17:44.039] iteration 3520 : loss : 0.497022, loss_ce: 0.121154
[03:17:48.159] iteration 3530 : loss : 0.460057, loss_ce: 0.036927
[03:17:52.226] iteration 3540 : loss : 0.443525, loss_ce: 0.065968
[03:17:56.300] iteration 3550 : loss : 0.465167, loss_ce: 0.051036
[03:18:00.371] iteration 3560 : loss : 0.286040, loss_ce: 0.065477
[03:18:04.487] iteration 3570 : loss : 0.435170, loss_ce: 0.077668
[03:18:08.592] iteration 3580 : loss : 0.450539, loss_ce: 0.093845
[03:18:12.687] iteration 3590 : loss : 0.448682, loss_ce: 0.046225
[03:18:16.791] iteration 3600 : loss : 0.298306, loss_ce: 0.042928
[03:18:20.918] iteration 3610 : loss : 0.436329, loss_ce: 0.035941
[03:18:25.018] iteration 3620 : loss : 0.422276, loss_ce: 0.053707
[03:18:29.165] iteration 3630 : loss : 0.281461, loss_ce: 0.050363
[03:18:33.236] iteration 3640 : loss : 0.437640, loss_ce: 0.057531
[03:18:37.325] iteration 3650 : loss : 0.419125, loss_ce: 0.033117
[03:18:41.398] iteration 3660 : loss : 0.252387, loss_ce: 0.041950
[03:18:45.516] iteration 3670 : loss : 0.342226, loss_ce: 0.047033
[03:18:49.615] iteration 3680 : loss : 0.386524, loss_ce: 0.057443
[03:18:53.734] iteration 3690 : loss : 0.381681, loss_ce: 0.028084
[03:18:57.832] iteration 3700 : loss : 0.335238, loss_ce: 0.019763
[03:19:01.958] iteration 3710 : loss : 0.355197, loss_ce: 0.016786
[03:19:06.079] iteration 3720 : loss : 0.307157, loss_ce: 0.021569
[03:19:10.223] iteration 3730 : loss : 0.228090, loss_ce: 0.042367
[03:19:14.336] iteration 3740 : loss : 0.366990, loss_ce: 0.023516
[03:19:18.473] iteration 3750 : loss : 0.318495, loss_ce: 0.014748
[03:24:06.034] iteration 3760 : loss : 0.471494, loss_ce: 0.120148
[03:24:10.146] iteration 3770 : loss : 0.472090, loss_ce: 0.083870
[03:24:14.254] iteration 3780 : loss : 0.459540, loss_ce: 0.066959
[03:24:18.371] iteration 3790 : loss : 0.442026, loss_ce: 0.052291
[03:24:22.481] iteration 3800 : loss : 0.440190, loss_ce: 0.042490
[03:24:26.600] iteration 3810 : loss : 0.291206, loss_ce: 0.046795
[03:24:30.720] iteration 3820 : loss : 0.289519, loss_ce: 0.046693
[03:24:34.843] iteration 3830 : loss : 0.277717, loss_ce: 0.047254
[03:24:39.114] iteration 3840 : loss : 0.431813, loss_ce: 0.062496
[03:24:43.315] iteration 3850 : loss : 0.440380, loss_ce: 0.053515
[03:24:47.433] iteration 3860 : loss : 0.429121, loss_ce: 0.060052
[03:24:51.558] iteration 3870 : loss : 0.421463, loss_ce: 0.073951
[03:24:55.675] iteration 3880 : loss : 0.417480, loss_ce: 0.041154
[03:24:59.963] iteration 3890 : loss : 0.411379, loss_ce: 0.028430
[03:25:04.049] iteration 3900 : loss : 0.418442, loss_ce: 0.044655
[03:25:08.130] iteration 3910 : loss : 0.410490, loss_ce: 0.078978
[03:25:12.254] iteration 3920 : loss : 0.398692, loss_ce: 0.035150
[03:25:16.350] iteration 3930 : loss : 0.202868, loss_ce: 0.028445
[03:25:20.435] iteration 3940 : loss : 0.360188, loss_ce: 0.030500
[03:25:24.531] iteration 3950 : loss : 0.258497, loss_ce: 0.046401
[03:25:28.619] iteration 3960 : loss : 0.335008, loss_ce: 0.020380
[03:25:32.827] iteration 3970 : loss : 0.374943, loss_ce: 0.023760
[03:25:37.180] iteration 3980 : loss : 0.327321, loss_ce: 0.048517
[03:25:41.498] iteration 3990 : loss : 0.359722, loss_ce: 0.040228
[03:25:45.639] iteration 4000 : loss : 0.209006, loss_ce: 0.019953
[03:25:50.569] iteration 4010 : loss : 0.350753, loss_ce: 0.029995
[03:25:54.915] iteration 4020 : loss : 0.348020, loss_ce: 0.017073
[03:30:31.545] iteration 4030 : loss : 0.313578, loss_ce: 0.031466
[03:30:35.661] iteration 4040 : loss : 0.510346, loss_ce: 0.197528
[03:30:39.779] iteration 4050 : loss : 0.313458, loss_ce: 0.039741
[03:30:43.891] iteration 4060 : loss : 0.455307, loss_ce: 0.043208
[03:30:48.013] iteration 4070 : loss : 0.444041, loss_ce: 0.034420
[03:30:52.124] iteration 4080 : loss : 0.436975, loss_ce: 0.045048
[03:30:56.249] iteration 4090 : loss : 0.437166, loss_ce: 0.045832
[03:31:00.360] iteration 4100 : loss : 0.431320, loss_ce: 0.041081
[03:31:04.667] iteration 4110 : loss : 0.437674, loss_ce: 0.047341
[03:31:08.906] iteration 4120 : loss : 0.279478, loss_ce: 0.050010
[03:31:13.671] iteration 4130 : loss : 0.440885, loss_ce: 0.064091
[03:31:18.196] iteration 4140 : loss : 0.433793, loss_ce: 0.050048
[03:31:22.467] iteration 4150 : loss : 0.418485, loss_ce: 0.042298
[03:31:26.582] iteration 4160 : loss : 0.432018, loss_ce: 0.068121
[03:31:30.707] iteration 4170 : loss : 0.410263, loss_ce: 0.055515
[03:31:34.824] iteration 4180 : loss : 0.367515, loss_ce: 0.032975
[03:31:38.952] iteration 4190 : loss : 0.380463, loss_ce: 0.042387
[03:31:43.068] iteration 4200 : loss : 0.209341, loss_ce: 0.026152
[03:31:47.422] iteration 4210 : loss : 0.358865, loss_ce: 0.034683
[03:31:51.746] iteration 4220 : loss : 0.375920, loss_ce: 0.040772
[03:31:55.873] iteration 4230 : loss : 0.367826, loss_ce: 0.070895
[03:31:59.992] iteration 4240 : loss : 0.352932, loss_ce: 0.029551
[03:32:04.120] iteration 4250 : loss : 0.354505, loss_ce: 0.022392
[03:32:08.238] iteration 4260 : loss : 0.115130, loss_ce: 0.027210
[03:32:12.364] iteration 4270 : loss : 0.341351, loss_ce: 0.016969
[03:32:16.483] iteration 4280 : loss : 0.354293, loss_ce: 0.031456
[03:36:59.000] iteration 4290 : loss : 0.430530, loss_ce: 0.132998
[03:37:03.362] iteration 4300 : loss : 0.310538, loss_ce: 0.036991
[03:37:07.735] iteration 4310 : loss : 0.474914, loss_ce: 0.079741
[03:37:12.099] iteration 4320 : loss : 0.465578, loss_ce: 0.101853
[03:37:16.479] iteration 4330 : loss : 0.455346, loss_ce: 0.055637
[03:37:20.858] iteration 4340 : loss : 0.301838, loss_ce: 0.035031
[03:37:25.202] iteration 4350 : loss : 0.445010, loss_ce: 0.040012
[03:37:29.732] iteration 4360 : loss : 0.426560, loss_ce: 0.044780
[03:37:33.830] iteration 4370 : loss : 0.434148, loss_ce: 0.040784
[03:37:38.025] iteration 4380 : loss : 0.285809, loss_ce: 0.040522
[03:37:42.181] iteration 4390 : loss : 0.270686, loss_ce: 0.056773
[03:37:46.482] iteration 4400 : loss : 0.444177, loss_ce: 0.044350
[03:37:51.034] iteration 4410 : loss : 0.425160, loss_ce: 0.062880
[03:37:55.455] iteration 4420 : loss : 0.277047, loss_ce: 0.039564
[03:37:59.782] iteration 4430 : loss : 0.442291, loss_ce: 0.050938
[03:38:04.491] iteration 4440 : loss : 0.276633, loss_ce: 0.046932
[03:38:08.687] iteration 4450 : loss : 0.278275, loss_ce: 0.057466
[03:38:12.765] iteration 4460 : loss : 0.261340, loss_ce: 0.055284
[03:38:16.862] iteration 4470 : loss : 0.423959, loss_ce: 0.044253
[03:38:20.992] iteration 4480 : loss : 0.270396, loss_ce: 0.055619
[03:38:25.273] iteration 4490 : loss : 0.414354, loss_ce: 0.057114
[03:38:29.571] iteration 4500 : loss : 0.412032, loss_ce: 0.051670
[03:38:33.889] iteration 4510 : loss : 0.408369, loss_ce: 0.059069
[03:38:38.133] iteration 4520 : loss : 0.406733, loss_ce: 0.022101
[03:38:43.118] iteration 4530 : loss : 0.384707, loss_ce: 0.046965
[03:38:47.280] iteration 4540 : loss : 0.398323, loss_ce: 0.041086
[03:38:51.341] iteration 4550 : loss : 0.351834, loss_ce: 0.032889
[03:43:17.632] iteration 4560 : loss : 0.896844, loss_ce: 1.082201
[03:43:21.678] iteration 4570 : loss : 0.494488, loss_ce: 0.109532
[03:43:25.713] iteration 4580 : loss : 0.498070, loss_ce: 0.118795
[03:43:29.763] iteration 4590 : loss : 0.335013, loss_ce: 0.086480
[03:43:33.805] iteration 4600 : loss : 0.488358, loss_ce: 0.094212
[03:43:37.855] iteration 4610 : loss : 0.484027, loss_ce: 0.094651
[03:43:41.895] iteration 4620 : loss : 0.461946, loss_ce: 0.044818
[03:43:45.945] iteration 4630 : loss : 0.473994, loss_ce: 0.071359
[03:43:49.989] iteration 4640 : loss : 0.455854, loss_ce: 0.048442
[03:43:54.045] iteration 4650 : loss : 0.468382, loss_ce: 0.065293
[03:43:58.087] iteration 4660 : loss : 0.466640, loss_ce: 0.063831
[03:44:02.146] iteration 4670 : loss : 0.305213, loss_ce: 0.055544
[03:44:06.192] iteration 4680 : loss : 0.459084, loss_ce: 0.061017
[03:44:10.247] iteration 4690 : loss : 0.301246, loss_ce: 0.070966
[03:44:14.293] iteration 4700 : loss : 0.440959, loss_ce: 0.042957
[03:44:18.352] iteration 4710 : loss : 0.285704, loss_ce: 0.036372
[03:44:22.401] iteration 4720 : loss : 0.439199, loss_ce: 0.059070
[03:44:26.460] iteration 4730 : loss : 0.428370, loss_ce: 0.060079
[03:44:30.506] iteration 4740 : loss : 0.441973, loss_ce: 0.044187
[03:44:34.564] iteration 4750 : loss : 0.432358, loss_ce: 0.062603
[03:44:38.614] iteration 4760 : loss : 0.441796, loss_ce: 0.068469
[03:44:42.673] iteration 4770 : loss : 0.431323, loss_ce: 0.066065
[03:44:46.723] iteration 4780 : loss : 0.441173, loss_ce: 0.055330
[03:44:50.781] iteration 4790 : loss : 0.429298, loss_ce: 0.047186
[03:44:54.829] iteration 4800 : loss : 0.444116, loss_ce: 0.049605
[03:44:58.889] iteration 4810 : loss : 0.275267, loss_ce: 0.050570
[03:45:02.937] iteration 4820 : loss : 0.426917, loss_ce: 0.068518
[03:49:28.939] iteration 4830 : loss : 0.368260, loss_ce: 0.051723
[03:49:32.971] iteration 4840 : loss : 0.454822, loss_ce: 0.052519
[03:49:37.015] iteration 4850 : loss : 0.453526, loss_ce: 0.069251
[03:49:41.056] iteration 4860 : loss : 0.445957, loss_ce: 0.050865
[03:49:45.108] iteration 4870 : loss : 0.309685, loss_ce: 0.079501
[03:49:49.149] iteration 4880 : loss : 0.448525, loss_ce: 0.071347
[03:49:53.203] iteration 4890 : loss : 0.449047, loss_ce: 0.037792
[03:49:57.246] iteration 4900 : loss : 0.436710, loss_ce: 0.058400
[03:50:01.303] iteration 4910 : loss : 0.422266, loss_ce: 0.067013
[03:50:05.349] iteration 4920 : loss : 0.276203, loss_ce: 0.045406
[03:50:09.402] iteration 4930 : loss : 0.278596, loss_ce: 0.051063
[03:50:13.450] iteration 4940 : loss : 0.437489, loss_ce: 0.038056
[03:50:17.509] iteration 4950 : loss : 0.435257, loss_ce: 0.067294
[03:50:21.557] iteration 4960 : loss : 0.432793, loss_ce: 0.059271
[03:50:25.616] iteration 4970 : loss : 0.431490, loss_ce: 0.062093
[03:50:29.664] iteration 4980 : loss : 0.284405, loss_ce: 0.047274
[03:50:33.723] iteration 4990 : loss : 0.433928, loss_ce: 0.040857
[03:50:37.770] iteration 5000 : loss : 0.432507, loss_ce: 0.034395
[03:50:41.831] iteration 5010 : loss : 0.400095, loss_ce: 0.057474
[03:50:45.883] iteration 5020 : loss : 0.258059, loss_ce: 0.036777
[03:50:49.944] iteration 5030 : loss : 0.416331, loss_ce: 0.057575
[03:50:53.994] iteration 5040 : loss : 0.265586, loss_ce: 0.032579
[03:50:58.055] iteration 5050 : loss : 0.393275, loss_ce: 0.053770
[03:51:02.106] iteration 5060 : loss : 0.255175, loss_ce: 0.034814
[03:51:06.165] iteration 5070 : loss : 0.403335, loss_ce: 0.027615
[03:51:10.215] iteration 5080 : loss : 0.273513, loss_ce: 0.044269
[03:51:14.276] iteration 5090 : loss : 0.447811, loss_ce: 0.059731
[03:55:40.626] iteration 5100 : loss : 0.510928, loss_ce: 0.167040
[03:55:44.671] iteration 5110 : loss : 0.325620, loss_ce: 0.053108
[03:55:48.708] iteration 5120 : loss : 0.475743, loss_ce: 0.063208
[03:55:52.757] iteration 5130 : loss : 0.461504, loss_ce: 0.056171
[03:55:56.796] iteration 5140 : loss : 0.461747, loss_ce: 0.050911
[03:56:00.847] iteration 5150 : loss : 0.451316, loss_ce: 0.034983
[03:56:04.889] iteration 5160 : loss : 0.444887, loss_ce: 0.070419
[03:56:08.942] iteration 5170 : loss : 0.448102, loss_ce: 0.089293
[03:56:12.986] iteration 5180 : loss : 0.441376, loss_ce: 0.041791
[03:56:17.041] iteration 5190 : loss : 0.291800, loss_ce: 0.029537
[03:56:21.086] iteration 5200 : loss : 0.292903, loss_ce: 0.051328
[03:56:25.142] iteration 5210 : loss : 0.439835, loss_ce: 0.057870
[03:56:29.188] iteration 5220 : loss : 0.292041, loss_ce: 0.048408
[03:56:33.245] iteration 5230 : loss : 0.431918, loss_ce: 0.061889
[03:56:37.292] iteration 5240 : loss : 0.442837, loss_ce: 0.035798
[03:56:41.349] iteration 5250 : loss : 0.429757, loss_ce: 0.038546
[03:56:45.398] iteration 5260 : loss : 0.418843, loss_ce: 0.049540
[03:56:49.456] iteration 5270 : loss : 0.430077, loss_ce: 0.063081
[03:56:53.505] iteration 5280 : loss : 0.431321, loss_ce: 0.068181
[03:56:57.564] iteration 5290 : loss : 0.413352, loss_ce: 0.061921
[03:57:01.614] iteration 5300 : loss : 0.435421, loss_ce: 0.048469
[03:57:05.676] iteration 5310 : loss : 0.425443, loss_ce: 0.068124
[03:57:09.725] iteration 5320 : loss : 0.426408, loss_ce: 0.072184
[03:57:13.784] iteration 5330 : loss : 0.398792, loss_ce: 0.063225
[03:57:17.836] iteration 5340 : loss : 0.411596, loss_ce: 0.049475
[03:57:21.894] iteration 5350 : loss : 0.443973, loss_ce: 0.030824
[03:57:25.843] iteration 5360 : loss : 0.414798, loss_ce: 0.077072
[04:01:38.220] save model to ./finetune_tpgm_kits23_continual_300iter\finetuned_epoch_19.pth
[04:01:51.931] iteration 5370 : loss : 0.453701, loss_ce: 0.063691
[04:01:55.976] iteration 5380 : loss : 0.470161, loss_ce: 0.052379
[04:02:00.026] iteration 5390 : loss : 0.478995, loss_ce: 0.071830
[04:02:04.066] iteration 5400 : loss : 0.468147, loss_ce: 0.055696
[04:02:08.117] iteration 5410 : loss : 0.453077, loss_ce: 0.079690
[04:02:12.160] iteration 5420 : loss : 0.444105, loss_ce: 0.065766
[04:02:16.214] iteration 5430 : loss : 0.447283, loss_ce: 0.070181
[04:02:20.263] iteration 5440 : loss : 0.432197, loss_ce: 0.055261
[04:02:24.320] iteration 5450 : loss : 0.430114, loss_ce: 0.052970
[04:02:28.369] iteration 5460 : loss : 0.430087, loss_ce: 0.036039
[04:02:32.424] iteration 5470 : loss : 0.435735, loss_ce: 0.065245
[04:02:36.471] iteration 5480 : loss : 0.414221, loss_ce: 0.039432
[04:02:40.531] iteration 5490 : loss : 0.258125, loss_ce: 0.048028
[04:02:44.580] iteration 5500 : loss : 0.414580, loss_ce: 0.060823
[04:02:48.639] iteration 5510 : loss : 0.389567, loss_ce: 0.030799
[04:02:52.688] iteration 5520 : loss : 0.377914, loss_ce: 0.028227
[04:02:56.746] iteration 5530 : loss : 0.444524, loss_ce: 0.056686
[04:03:00.795] iteration 5540 : loss : 0.457455, loss_ce: 0.384250
[04:03:04.850] iteration 5550 : loss : 0.474321, loss_ce: 0.059974
[04:03:08.896] iteration 5560 : loss : 0.485557, loss_ce: 0.088735
[04:03:12.950] iteration 5570 : loss : 0.468733, loss_ce: 0.066719
[04:03:16.994] iteration 5580 : loss : 0.467506, loss_ce: 0.059038
[04:03:21.050] iteration 5590 : loss : 0.465952, loss_ce: 0.062909
[04:03:25.096] iteration 5600 : loss : 0.459139, loss_ce: 0.060623
[04:03:29.152] iteration 5610 : loss : 0.452495, loss_ce: 0.073318
[04:03:33.201] iteration 5620 : loss : 0.439684, loss_ce: 0.069742
[04:07:58.783] iteration 5630 : loss : 0.437671, loss_ce: 0.090848
[04:08:02.817] iteration 5640 : loss : 0.458584, loss_ce: 0.063459
[04:08:06.863] iteration 5650 : loss : 0.300524, loss_ce: 0.042571
[04:08:10.900] iteration 5660 : loss : 0.467139, loss_ce: 0.089806
[04:08:14.950] iteration 5670 : loss : 0.456949, loss_ce: 0.030103
[04:08:18.993] iteration 5680 : loss : 0.310250, loss_ce: 0.038670
[04:08:23.044] iteration 5690 : loss : 0.440611, loss_ce: 0.052169
[04:08:27.087] iteration 5700 : loss : 0.438714, loss_ce: 0.047866
[04:08:31.139] iteration 5710 : loss : 0.442805, loss_ce: 0.037421
[04:08:35.182] iteration 5720 : loss : 0.442147, loss_ce: 0.039818
[04:08:39.240] iteration 5730 : loss : 0.437661, loss_ce: 0.070221
[04:08:43.287] iteration 5740 : loss : 0.432968, loss_ce: 0.065037
[04:08:47.345] iteration 5750 : loss : 0.428391, loss_ce: 0.048442
[04:08:51.392] iteration 5760 : loss : 0.438582, loss_ce: 0.070424
[04:08:55.449] iteration 5770 : loss : 0.434792, loss_ce: 0.064720
[04:08:59.495] iteration 5780 : loss : 0.292042, loss_ce: 0.058534
[04:09:03.555] iteration 5790 : loss : 0.297336, loss_ce: 0.041143
[04:09:07.601] iteration 5800 : loss : 0.433031, loss_ce: 0.058892
[04:09:11.654] iteration 5810 : loss : 0.436018, loss_ce: 0.050877
[04:09:15.701] iteration 5820 : loss : 0.447975, loss_ce: 0.057142
[04:09:19.758] iteration 5830 : loss : 0.428375, loss_ce: 0.042381
[04:09:23.807] iteration 5840 : loss : 0.424832, loss_ce: 0.050702
[04:09:27.866] iteration 5850 : loss : 0.424399, loss_ce: 0.065138
[04:09:31.913] iteration 5860 : loss : 0.408872, loss_ce: 0.051477
[04:09:35.972] iteration 5870 : loss : 0.441744, loss_ce: 0.081974
[04:09:40.020] iteration 5880 : loss : 0.423681, loss_ce: 0.063547
[04:09:44.083] iteration 5890 : loss : 0.411514, loss_ce: 0.042697
[04:14:09.694] iteration 5900 : loss : 0.475071, loss_ce: 0.061680
[04:14:13.740] iteration 5910 : loss : 0.476680, loss_ce: 0.085854
[04:14:17.780] iteration 5920 : loss : 0.445565, loss_ce: 0.037834
[04:14:21.831] iteration 5930 : loss : 0.442567, loss_ce: 0.073597
[04:14:25.875] iteration 5940 : loss : 0.438504, loss_ce: 0.046552
[04:14:29.929] iteration 5950 : loss : 0.445322, loss_ce: 0.048642
[04:14:33.973] iteration 5960 : loss : 0.428254, loss_ce: 0.054538
[04:14:38.030] iteration 5970 : loss : 0.286208, loss_ce: 0.048174
[04:14:42.075] iteration 5980 : loss : 0.402022, loss_ce: 0.033235
[04:14:46.132] iteration 5990 : loss : 0.404731, loss_ce: 0.037721
[04:14:50.177] iteration 6000 : loss : 0.399864, loss_ce: 0.027195
[04:14:54.236] iteration 6010 : loss : 0.207556, loss_ce: 0.028531
[04:14:58.286] iteration 6020 : loss : 0.207434, loss_ce: 0.027983
[04:15:02.344] iteration 6030 : loss : 0.371647, loss_ce: 0.073434
[04:15:06.392] iteration 6040 : loss : 0.387683, loss_ce: 0.066260
[04:15:10.455] iteration 6050 : loss : 0.375615, loss_ce: 0.020754
[04:15:14.505] iteration 6060 : loss : 0.348907, loss_ce: 0.067001
[04:15:18.565] iteration 6070 : loss : 0.336340, loss_ce: 0.021251
[04:15:22.614] iteration 6080 : loss : 0.295594, loss_ce: 0.037518
[04:15:26.677] iteration 6090 : loss : 0.314531, loss_ce: 0.018668
[04:15:30.727] iteration 6100 : loss : 0.278429, loss_ce: 0.028046
[04:15:34.786] iteration 6110 : loss : 0.333729, loss_ce: 0.010914
[04:15:38.835] iteration 6120 : loss : 0.220230, loss_ce: 0.013533
[04:15:42.894] iteration 6130 : loss : 0.307646, loss_ce: 0.024361
[04:15:46.947] iteration 6140 : loss : 0.314449, loss_ce: 0.012628
[04:15:51.005] iteration 6150 : loss : 0.179641, loss_ce: 0.045574
[04:15:55.055] iteration 6160 : loss : 0.097528, loss_ce: 0.014327
[04:20:20.806] iteration 6170 : loss : 0.350663, loss_ce: 0.124276
[04:20:24.838] iteration 6180 : loss : 0.463608, loss_ce: 0.038496
[04:20:28.884] iteration 6190 : loss : 0.447214, loss_ce: 0.045658
[04:20:32.924] iteration 6200 : loss : 0.444811, loss_ce: 0.054972
[04:20:36.977] iteration 6210 : loss : 0.436420, loss_ce: 0.085103
[04:20:41.020] iteration 6220 : loss : 0.439747, loss_ce: 0.042035
[04:20:45.072] iteration 6230 : loss : 0.410089, loss_ce: 0.051197
[04:20:49.117] iteration 6240 : loss : 0.291139, loss_ce: 0.055326
[04:20:53.170] iteration 6250 : loss : 0.297111, loss_ce: 0.037319
[04:20:57.220] iteration 6260 : loss : 0.416782, loss_ce: 0.039835
[04:21:01.275] iteration 6270 : loss : 0.426253, loss_ce: 0.068431
[04:21:05.324] iteration 6280 : loss : 0.427257, loss_ce: 0.072293
[04:21:09.387] iteration 6290 : loss : 0.378663, loss_ce: 0.088457
[04:21:13.436] iteration 6300 : loss : 0.430916, loss_ce: 0.045164
[04:21:17.495] iteration 6310 : loss : 0.247216, loss_ce: 0.053925
[04:21:21.542] iteration 6320 : loss : 0.235083, loss_ce: 0.026484
[04:21:25.601] iteration 6330 : loss : 0.197575, loss_ce: 0.015945
[04:21:29.651] iteration 6340 : loss : 0.320027, loss_ce: 0.028119
[04:21:33.707] iteration 6350 : loss : 0.343006, loss_ce: 0.025758
[04:21:37.756] iteration 6360 : loss : 0.341422, loss_ce: 0.016148
[04:21:41.821] iteration 6370 : loss : 0.336446, loss_ce: 0.016097
[04:21:45.869] iteration 6380 : loss : 0.328454, loss_ce: 0.037116
[04:21:49.931] iteration 6390 : loss : 0.270808, loss_ce: 0.019390
[04:21:53.982] iteration 6400 : loss : 0.327143, loss_ce: 0.032506
[04:21:58.041] iteration 6410 : loss : 0.178939, loss_ce: 0.013200
[04:22:02.094] iteration 6420 : loss : 0.342115, loss_ce: 0.026165
[04:22:06.156] iteration 6430 : loss : 0.160126, loss_ce: 0.028565
[04:26:32.480] iteration 6440 : loss : 0.501400, loss_ce: 0.126527
[04:26:36.526] iteration 6450 : loss : 0.463498, loss_ce: 0.036612
[04:26:40.563] iteration 6460 : loss : 0.458670, loss_ce: 0.058049
[04:26:44.609] iteration 6470 : loss : 0.448657, loss_ce: 0.046882
[04:26:48.647] iteration 6480 : loss : 0.446774, loss_ce: 0.058371
[04:26:52.695] iteration 6490 : loss : 0.446835, loss_ce: 0.086830
[04:26:56.737] iteration 6500 : loss : 0.440907, loss_ce: 0.051908
[04:27:00.789] iteration 6510 : loss : 0.436738, loss_ce: 0.064451
[04:27:04.832] iteration 6520 : loss : 0.292961, loss_ce: 0.045485
[04:27:08.886] iteration 6530 : loss : 0.444451, loss_ce: 0.054706
[04:27:12.928] iteration 6540 : loss : 0.444618, loss_ce: 0.052359
[04:27:16.982] iteration 6550 : loss : 0.444611, loss_ce: 0.060950
[04:27:21.027] iteration 6560 : loss : 0.433166, loss_ce: 0.059910
[04:27:25.084] iteration 6570 : loss : 0.296630, loss_ce: 0.065973
[04:27:29.131] iteration 6580 : loss : 0.273456, loss_ce: 0.049169
[04:27:33.187] iteration 6590 : loss : 0.269598, loss_ce: 0.041014
[04:27:37.235] iteration 6600 : loss : 0.277204, loss_ce: 0.039743
[04:27:41.292] iteration 6610 : loss : 0.260375, loss_ce: 0.050471
[04:27:45.341] iteration 6620 : loss : 0.431513, loss_ce: 0.050711
[04:27:49.400] iteration 6630 : loss : 0.413949, loss_ce: 0.040602
[04:27:53.451] iteration 6640 : loss : 0.381386, loss_ce: 0.063484
[04:27:57.511] iteration 6650 : loss : 0.377674, loss_ce: 0.024976
[04:28:01.559] iteration 6660 : loss : 0.369068, loss_ce: 0.037051
[04:28:05.620] iteration 6670 : loss : 0.248424, loss_ce: 0.020944
[04:28:09.669] iteration 6680 : loss : 0.345437, loss_ce: 0.045616
[04:28:13.730] iteration 6690 : loss : 0.322551, loss_ce: 0.010291
[04:28:17.680] iteration 6700 : loss : 0.345050, loss_ce: 0.021588
[04:32:43.983] iteration 6710 : loss : 0.476439, loss_ce: 0.119116
[04:32:48.027] iteration 6720 : loss : 0.479385, loss_ce: 0.072305
[04:32:52.079] iteration 6730 : loss : 0.447696, loss_ce: 0.062423
[04:32:56.122] iteration 6740 : loss : 0.434129, loss_ce: 0.066534
[04:33:00.179] iteration 6750 : loss : 0.451500, loss_ce: 0.048237
[04:33:04.225] iteration 6760 : loss : 0.445174, loss_ce: 0.038991
[04:33:08.282] iteration 6770 : loss : 0.437754, loss_ce: 0.051014
[04:33:12.328] iteration 6780 : loss : 0.423155, loss_ce: 0.060692
[04:33:16.387] iteration 6790 : loss : 0.421576, loss_ce: 0.063668
[04:33:20.434] iteration 6800 : loss : 0.263325, loss_ce: 0.044796
[04:33:24.492] iteration 6810 : loss : 0.393873, loss_ce: 0.028030
[04:33:28.542] iteration 6820 : loss : 0.438023, loss_ce: 0.050794
[04:33:32.600] iteration 6830 : loss : 0.395975, loss_ce: 0.040151
[04:33:36.648] iteration 6840 : loss : 0.341999, loss_ce: 0.024432
[04:33:40.713] iteration 6850 : loss : 0.193558, loss_ce: 0.011789
[04:33:44.764] iteration 6860 : loss : 0.377261, loss_ce: 0.035135
[04:33:48.824] iteration 6870 : loss : 0.328298, loss_ce: 0.014240
[04:33:52.875] iteration 6880 : loss : 0.347129, loss_ce: 0.048939
[04:33:56.938] iteration 6890 : loss : 0.366987, loss_ce: 0.033205
[04:34:00.989] iteration 6900 : loss : 0.287406, loss_ce: 0.017659
[04:34:05.050] iteration 6910 : loss : 0.311456, loss_ce: 0.019339
[04:34:09.100] iteration 6920 : loss : 0.254086, loss_ce: 0.019457
[04:34:13.160] iteration 6930 : loss : 0.317266, loss_ce: 0.025940
[04:34:17.211] iteration 6940 : loss : 0.327344, loss_ce: 0.035625
[04:34:21.271] iteration 6950 : loss : 0.305695, loss_ce: 0.018650
[04:34:25.324] iteration 6960 : loss : 0.319688, loss_ce: 0.020250
[04:38:51.439] iteration 6970 : loss : 0.462184, loss_ce: 0.135798
[04:38:55.475] iteration 6980 : loss : 0.511641, loss_ce: 0.153882
[04:38:59.523] iteration 6990 : loss : 0.476709, loss_ce: 0.084015
[04:39:03.564] iteration 7000 : loss : 0.455178, loss_ce: 0.038405
[04:39:07.615] iteration 7010 : loss : 0.456126, loss_ce: 0.061932
[04:39:11.660] iteration 7020 : loss : 0.446770, loss_ce: 0.047553
[04:39:15.717] iteration 7030 : loss : 0.439748, loss_ce: 0.040918
[04:39:19.760] iteration 7040 : loss : 0.443921, loss_ce: 0.038632
[04:39:23.815] iteration 7050 : loss : 0.437780, loss_ce: 0.062762
[04:39:27.861] iteration 7060 : loss : 0.131350, loss_ce: 0.021576
[04:39:31.917] iteration 7070 : loss : 0.436645, loss_ce: 0.050668
[04:39:35.964] iteration 7080 : loss : 0.417067, loss_ce: 0.072811
[04:39:40.024] iteration 7090 : loss : 0.445888, loss_ce: 0.112271
[04:39:44.073] iteration 7100 : loss : 0.380959, loss_ce: 0.053863
[04:39:48.132] iteration 7110 : loss : 0.216206, loss_ce: 0.030215
[04:39:52.180] iteration 7120 : loss : 0.340106, loss_ce: 0.027063
[04:39:56.239] iteration 7130 : loss : 0.346308, loss_ce: 0.012962
[04:40:00.292] iteration 7140 : loss : 0.161846, loss_ce: 0.013598
[04:40:04.354] iteration 7150 : loss : 0.191011, loss_ce: 0.019826
[04:40:08.405] iteration 7160 : loss : 0.319919, loss_ce: 0.022093
[04:40:12.468] iteration 7170 : loss : 0.337496, loss_ce: 0.024233
[04:40:16.519] iteration 7180 : loss : 0.338664, loss_ce: 0.026066
[04:40:20.581] iteration 7190 : loss : 0.281812, loss_ce: 0.021466
[04:40:24.631] iteration 7200 : loss : 0.363833, loss_ce: 0.037288
[04:40:28.691] iteration 7210 : loss : 0.144560, loss_ce: 0.011355
[04:40:32.741] iteration 7220 : loss : 0.235385, loss_ce: 0.024892
[04:40:36.803] iteration 7230 : loss : 0.340177, loss_ce: 0.024526
[04:45:02.547] iteration 7240 : loss : 0.681561, loss_ce: 0.585772
[04:45:06.592] iteration 7250 : loss : 0.462601, loss_ce: 0.061716
[04:45:10.631] iteration 7260 : loss : 0.293958, loss_ce: 0.034539
[04:45:14.680] iteration 7270 : loss : 0.451203, loss_ce: 0.041762
[04:45:18.727] iteration 7280 : loss : 0.437666, loss_ce: 0.070031
[04:45:22.783] iteration 7290 : loss : 0.444408, loss_ce: 0.104755
[04:45:26.826] iteration 7300 : loss : 0.424507, loss_ce: 0.057022
[04:45:30.882] iteration 7310 : loss : 0.430952, loss_ce: 0.037057
[04:45:34.928] iteration 7320 : loss : 0.448169, loss_ce: 0.038678
[04:45:38.984] iteration 7330 : loss : 0.396982, loss_ce: 0.067331
[04:45:43.031] iteration 7340 : loss : 0.418518, loss_ce: 0.080262
[04:45:47.090] iteration 7350 : loss : 0.264727, loss_ce: 0.025109
[04:45:51.137] iteration 7360 : loss : 0.384354, loss_ce: 0.021991
[04:45:55.195] iteration 7370 : loss : 0.444092, loss_ce: 0.021401
[04:45:59.249] iteration 7380 : loss : 0.408855, loss_ce: 0.047243
[04:46:03.311] iteration 7390 : loss : 0.382528, loss_ce: 0.040423
[04:46:07.360] iteration 7400 : loss : 0.469712, loss_ce: 0.132928
[04:46:11.420] iteration 7410 : loss : 0.457118, loss_ce: 0.041623
[04:46:15.473] iteration 7420 : loss : 0.444381, loss_ce: 0.062736
[04:46:19.530] iteration 7430 : loss : 0.415432, loss_ce: 0.033247
[04:46:23.580] iteration 7440 : loss : 0.384217, loss_ce: 0.047795
[04:46:27.641] iteration 7450 : loss : 0.363279, loss_ce: 0.038583
[04:46:31.689] iteration 7460 : loss : 0.366082, loss_ce: 0.029520
[04:46:35.750] iteration 7470 : loss : 0.344344, loss_ce: 0.028351
[04:46:39.802] iteration 7480 : loss : 0.143550, loss_ce: 0.032661
[04:46:43.865] iteration 7490 : loss : 0.258418, loss_ce: 0.021827
[04:46:47.916] iteration 7500 : loss : 0.358958, loss_ce: 0.054269
[04:51:15.437] iteration 7510 : loss : 0.471532, loss_ce: 0.059876
[04:51:19.477] iteration 7520 : loss : 0.424447, loss_ce: 0.054967
[04:51:23.528] iteration 7530 : loss : 0.397065, loss_ce: 0.039605
[04:51:27.570] iteration 7540 : loss : 0.350033, loss_ce: 0.015229
[04:51:31.621] iteration 7550 : loss : 0.343973, loss_ce: 0.017382
[04:51:35.668] iteration 7560 : loss : 0.343350, loss_ce: 0.025807
[04:51:39.724] iteration 7570 : loss : 0.343986, loss_ce: 0.018451
[04:51:43.779] iteration 7580 : loss : 0.267536, loss_ce: 0.038156
[04:51:47.840] iteration 7590 : loss : 0.335614, loss_ce: 0.025479
[04:51:51.888] iteration 7600 : loss : 0.319020, loss_ce: 0.014368
[04:51:55.945] iteration 7610 : loss : 0.285834, loss_ce: 0.024030
[04:51:59.995] iteration 7620 : loss : 0.333591, loss_ce: 0.015093
[04:52:04.055] iteration 7630 : loss : 0.114267, loss_ce: 0.018698
[04:52:08.105] iteration 7640 : loss : 0.353592, loss_ce: 0.012166
[04:52:12.164] iteration 7650 : loss : 0.249143, loss_ce: 0.014427
[04:52:16.212] iteration 7660 : loss : 0.223744, loss_ce: 0.017311
[04:52:20.277] iteration 7670 : loss : 0.341212, loss_ce: 0.031718
[04:52:24.326] iteration 7680 : loss : 0.275500, loss_ce: 0.037992
[04:52:28.387] iteration 7690 : loss : 0.185899, loss_ce: 0.009686
[04:52:32.436] iteration 7700 : loss : 0.208109, loss_ce: 0.028345
[04:52:36.495] iteration 7710 : loss : 0.264100, loss_ce: 0.020460
[04:52:40.546] iteration 7720 : loss : 0.264239, loss_ce: 0.014769
[04:52:44.606] iteration 7730 : loss : 0.098322, loss_ce: 0.008001
[04:52:48.655] iteration 7740 : loss : 0.311773, loss_ce: 0.018984
[04:52:52.717] iteration 7750 : loss : 0.194868, loss_ce: 0.010929
[04:52:56.769] iteration 7760 : loss : 0.289023, loss_ce: 0.010326
[04:53:00.829] iteration 7770 : loss : 0.360201, loss_ce: 0.030830
[04:57:26.734] iteration 7780 : loss : 0.532021, loss_ce: 0.202105
[04:57:30.781] iteration 7790 : loss : 0.467674, loss_ce: 0.043602
[04:57:34.820] iteration 7800 : loss : 0.456200, loss_ce: 0.026336
[04:57:38.870] iteration 7810 : loss : 0.453116, loss_ce: 0.048347
[04:57:42.911] iteration 7820 : loss : 0.441734, loss_ce: 0.065076
[04:57:46.965] iteration 7830 : loss : 0.283428, loss_ce: 0.032367
[04:57:51.008] iteration 7840 : loss : 0.435348, loss_ce: 0.068256
[04:57:55.065] iteration 7850 : loss : 0.284615, loss_ce: 0.055507
[04:57:59.111] iteration 7860 : loss : 0.402911, loss_ce: 0.044371
[04:58:03.171] iteration 7870 : loss : 0.390077, loss_ce: 0.059337
[04:58:07.218] iteration 7880 : loss : 0.402293, loss_ce: 0.057114
[04:58:11.281] iteration 7890 : loss : 0.319111, loss_ce: 0.043609
[04:58:15.330] iteration 7900 : loss : 0.345133, loss_ce: 0.014749
[04:58:19.390] iteration 7910 : loss : 0.344428, loss_ce: 0.026800
[04:58:23.446] iteration 7920 : loss : 0.323604, loss_ce: 0.013671
[04:58:27.505] iteration 7930 : loss : 0.360752, loss_ce: 0.044103
[04:58:31.555] iteration 7940 : loss : 0.276888, loss_ce: 0.013476
[04:58:35.616] iteration 7950 : loss : 0.091091, loss_ce: 0.015709
[04:58:39.665] iteration 7960 : loss : 0.340021, loss_ce: 0.023193
[04:58:43.728] iteration 7970 : loss : 0.173133, loss_ce: 0.014357
[04:58:47.778] iteration 7980 : loss : 0.176152, loss_ce: 0.021180
[04:58:51.840] iteration 7990 : loss : 0.216707, loss_ce: 0.012748
[04:58:55.891] iteration 8000 : loss : 0.308642, loss_ce: 0.008629
[04:58:59.952] iteration 8010 : loss : 0.281897, loss_ce: 0.012790
[04:59:04.005] iteration 8020 : loss : 0.268602, loss_ce: 0.028867
[04:59:08.074] iteration 8030 : loss : 0.329708, loss_ce: 0.017459
[04:59:12.027] iteration 8040 : loss : 0.324487, loss_ce: 0.010135
[05:03:24.540] save model to ./finetune_tpgm_kits23_continual_300iter\finetuned_epoch_29.pth
[05:03:38.254] iteration 8050 : loss : 0.400966, loss_ce: 0.030028
[05:03:42.300] iteration 8060 : loss : 0.336032, loss_ce: 0.013919
[05:03:46.350] iteration 8070 : loss : 0.189912, loss_ce: 0.041454
[05:03:50.394] iteration 8080 : loss : 0.325430, loss_ce: 0.019176
[05:03:54.447] iteration 8090 : loss : 0.324569, loss_ce: 0.013560
[05:03:58.491] iteration 8100 : loss : 0.358022, loss_ce: 0.056709
[05:04:02.548] iteration 8110 : loss : 0.336460, loss_ce: 0.011970
[05:04:06.595] iteration 8120 : loss : 0.332898, loss_ce: 0.016063
[05:04:10.651] iteration 8130 : loss : 0.298796, loss_ce: 0.013493
[05:04:14.697] iteration 8140 : loss : 0.188506, loss_ce: 0.052230
[05:04:18.754] iteration 8150 : loss : 0.130793, loss_ce: 0.021781
[05:04:22.801] iteration 8160 : loss : 0.245873, loss_ce: 0.017336
[05:04:26.861] iteration 8170 : loss : 0.291387, loss_ce: 0.011397
[05:04:30.909] iteration 8180 : loss : 0.232050, loss_ce: 0.052960
[05:04:34.968] iteration 8190 : loss : 0.258548, loss_ce: 0.028730
[05:04:39.018] iteration 8200 : loss : 0.113980, loss_ce: 0.008776
[05:04:43.077] iteration 8210 : loss : 0.279457, loss_ce: 0.025329
[05:04:47.129] iteration 8220 : loss : 0.243192, loss_ce: 0.021430
[05:04:51.188] iteration 8230 : loss : 0.251362, loss_ce: 0.011733
[05:04:55.239] iteration 8240 : loss : 0.221188, loss_ce: 0.011882
[05:04:59.299] iteration 8250 : loss : 0.313152, loss_ce: 0.024976
[05:05:03.349] iteration 8260 : loss : 0.103804, loss_ce: 0.012195
[05:05:07.410] iteration 8270 : loss : 0.197511, loss_ce: 0.009576
[05:05:11.462] iteration 8280 : loss : 0.303713, loss_ce: 0.027259
[05:05:15.525] iteration 8290 : loss : 0.176615, loss_ce: 0.014390
[05:05:19.575] iteration 8300 : loss : 0.108664, loss_ce: 0.008267
[05:09:47.697] iteration 8310 : loss : 0.376427, loss_ce: 0.026029
[05:09:51.733] iteration 8320 : loss : 0.373974, loss_ce: 0.047248
[05:09:55.784] iteration 8330 : loss : 0.380824, loss_ce: 0.010146
[05:09:59.828] iteration 8340 : loss : 0.196514, loss_ce: 0.029332
[05:10:03.880] iteration 8350 : loss : 0.356002, loss_ce: 0.026465
[05:10:07.926] iteration 8360 : loss : 0.336437, loss_ce: 0.042229
[05:10:11.982] iteration 8370 : loss : 0.302838, loss_ce: 0.048656
[05:10:16.029] iteration 8380 : loss : 0.188187, loss_ce: 0.019745
[05:10:20.089] iteration 8390 : loss : 0.338877, loss_ce: 0.024144
[05:10:24.137] iteration 8400 : loss : 0.336551, loss_ce: 0.029084
[05:10:28.196] iteration 8410 : loss : 0.263519, loss_ce: 0.023023
[05:10:32.246] iteration 8420 : loss : 0.256671, loss_ce: 0.016282
[05:10:36.304] iteration 8430 : loss : 0.297941, loss_ce: 0.052568
[05:10:40.356] iteration 8440 : loss : 0.350396, loss_ce: 0.040540
[05:10:44.414] iteration 8450 : loss : 0.328376, loss_ce: 0.005966
[05:10:48.466] iteration 8460 : loss : 0.232616, loss_ce: 0.016522
[05:10:52.528] iteration 8470 : loss : 0.315683, loss_ce: 0.046246
[05:10:56.579] iteration 8480 : loss : 0.317597, loss_ce: 0.016352
[05:11:00.642] iteration 8490 : loss : 0.336389, loss_ce: 0.027910
[05:11:04.691] iteration 8500 : loss : 0.271220, loss_ce: 0.022868
[05:11:08.752] iteration 8510 : loss : 0.096091, loss_ce: 0.017288
[05:11:12.804] iteration 8520 : loss : 0.233622, loss_ce: 0.005508
[05:11:16.865] iteration 8530 : loss : 0.248202, loss_ce: 0.012382
[05:11:20.921] iteration 8540 : loss : 0.204987, loss_ce: 0.011352
[05:11:24.982] iteration 8550 : loss : 0.062690, loss_ce: 0.005231
[05:11:29.035] iteration 8560 : loss : 0.225119, loss_ce: 0.014453
[05:11:33.095] iteration 8570 : loss : 0.303837, loss_ce: 0.007975
[05:15:59.219] iteration 8580 : loss : 0.510159, loss_ce: 0.229760
[05:16:03.266] iteration 8590 : loss : 0.339255, loss_ce: 0.016297
[05:16:07.305] iteration 8600 : loss : 0.384225, loss_ce: 0.025601
[05:16:11.358] iteration 8610 : loss : 0.327502, loss_ce: 0.023473
[05:16:15.405] iteration 8620 : loss : 0.267365, loss_ce: 0.008082
[05:16:19.460] iteration 8630 : loss : 0.322493, loss_ce: 0.018824
[05:16:23.507] iteration 8640 : loss : 0.311906, loss_ce: 0.008374
[05:16:27.564] iteration 8650 : loss : 0.312770, loss_ce: 0.023438
[05:16:31.612] iteration 8660 : loss : 0.328574, loss_ce: 0.037080
[05:16:35.669] iteration 8670 : loss : 0.257464, loss_ce: 0.017800
[05:16:39.718] iteration 8680 : loss : 0.318415, loss_ce: 0.010755
[05:16:43.778] iteration 8690 : loss : 0.320196, loss_ce: 0.012963
[05:16:47.827] iteration 8700 : loss : 0.257695, loss_ce: 0.013196
[05:16:51.890] iteration 8710 : loss : 0.226966, loss_ce: 0.007250
[05:16:55.939] iteration 8720 : loss : 0.237732, loss_ce: 0.012643
[05:17:00.000] iteration 8730 : loss : 0.336481, loss_ce: 0.012051
[05:17:04.054] iteration 8740 : loss : 0.259908, loss_ce: 0.035119
[05:17:08.114] iteration 8750 : loss : 0.319930, loss_ce: 0.030069
[05:17:12.167] iteration 8760 : loss : 0.227680, loss_ce: 0.017032
[05:17:16.229] iteration 8770 : loss : 0.243749, loss_ce: 0.018766
[05:17:20.281] iteration 8780 : loss : 0.249611, loss_ce: 0.007108
[05:17:24.340] iteration 8790 : loss : 0.200233, loss_ce: 0.008864
[05:17:28.389] iteration 8800 : loss : 0.270200, loss_ce: 0.036100
[05:17:32.452] iteration 8810 : loss : 0.242783, loss_ce: 0.007605
[05:17:36.501] iteration 8820 : loss : 0.238974, loss_ce: 0.019883
[05:17:40.562] iteration 8830 : loss : 0.272385, loss_ce: 0.017569
[05:17:44.613] iteration 8840 : loss : 0.233219, loss_ce: 0.012485
[05:22:12.505] iteration 8850 : loss : 0.436422, loss_ce: 0.054559
[05:22:16.546] iteration 8860 : loss : 0.220647, loss_ce: 0.074484
[05:22:20.598] iteration 8870 : loss : 0.199705, loss_ce: 0.033556
[05:22:24.639] iteration 8880 : loss : 0.349003, loss_ce: 0.033469
[05:22:28.694] iteration 8890 : loss : 0.322585, loss_ce: 0.016082
[05:22:32.740] iteration 8900 : loss : 0.347800, loss_ce: 0.033761
[05:22:36.796] iteration 8910 : loss : 0.332713, loss_ce: 0.030643
[05:22:40.842] iteration 8920 : loss : 0.172329, loss_ce: 0.013613
[05:22:44.898] iteration 8930 : loss : 0.303313, loss_ce: 0.019844
[05:22:48.944] iteration 8940 : loss : 0.289899, loss_ce: 0.021872
[05:22:53.003] iteration 8950 : loss : 0.182017, loss_ce: 0.018611
[05:22:57.050] iteration 8960 : loss : 0.250370, loss_ce: 0.063435
[05:23:01.111] iteration 8970 : loss : 0.334611, loss_ce: 0.017485
[05:23:05.161] iteration 8980 : loss : 0.290257, loss_ce: 0.029981
[05:23:09.220] iteration 8990 : loss : 0.328384, loss_ce: 0.032658
[05:23:13.270] iteration 9000 : loss : 0.152359, loss_ce: 0.026126
[05:23:17.330] iteration 9010 : loss : 0.245968, loss_ce: 0.010750
[05:23:21.382] iteration 9020 : loss : 0.261092, loss_ce: 0.025423
[05:23:25.441] iteration 9030 : loss : 0.150725, loss_ce: 0.029062
[05:23:29.491] iteration 9040 : loss : 0.246970, loss_ce: 0.011948
[05:23:33.550] iteration 9050 : loss : 0.218795, loss_ce: 0.023957
[05:23:37.603] iteration 9060 : loss : 0.300190, loss_ce: 0.012013
[05:23:41.665] iteration 9070 : loss : 0.247996, loss_ce: 0.016433
[05:23:45.716] iteration 9080 : loss : 0.228481, loss_ce: 0.015173
[05:23:49.775] iteration 9090 : loss : 0.246966, loss_ce: 0.008349
[05:23:53.827] iteration 9100 : loss : 0.248538, loss_ce: 0.056810
[05:23:57.889] iteration 9110 : loss : 0.369294, loss_ce: 0.026889
[05:28:23.790] iteration 9120 : loss : 0.331718, loss_ce: 0.010818
[05:28:27.843] iteration 9130 : loss : 0.412968, loss_ce: 0.070354
[05:28:31.885] iteration 9140 : loss : 0.338409, loss_ce: 0.047162
[05:28:35.939] iteration 9150 : loss : 0.343557, loss_ce: 0.026063
[05:28:39.985] iteration 9160 : loss : 0.322613, loss_ce: 0.028252
[05:28:44.042] iteration 9170 : loss : 0.231003, loss_ce: 0.021016
[05:28:48.090] iteration 9180 : loss : 0.296795, loss_ce: 0.025304
[05:28:52.146] iteration 9190 : loss : 0.383843, loss_ce: 0.077494
[05:28:56.193] iteration 9200 : loss : 0.329924, loss_ce: 0.013544
[05:29:00.252] iteration 9210 : loss : 0.327564, loss_ce: 0.025943
[05:29:04.301] iteration 9220 : loss : 0.175591, loss_ce: 0.016249
[05:29:08.360] iteration 9230 : loss : 0.326010, loss_ce: 0.040636
[05:29:12.410] iteration 9240 : loss : 0.325040, loss_ce: 0.034085
[05:29:16.469] iteration 9250 : loss : 0.250682, loss_ce: 0.029489
[05:29:20.520] iteration 9260 : loss : 0.309852, loss_ce: 0.034921
[05:29:24.579] iteration 9270 : loss : 0.229310, loss_ce: 0.090180
[05:29:28.630] iteration 9280 : loss : 0.207895, loss_ce: 0.019534
[05:29:32.692] iteration 9290 : loss : 0.247949, loss_ce: 0.015833
[05:29:36.741] iteration 9300 : loss : 0.194009, loss_ce: 0.007565
[05:29:40.802] iteration 9310 : loss : 0.049745, loss_ce: 0.007794
[05:29:44.855] iteration 9320 : loss : 0.320894, loss_ce: 0.018744
[05:29:48.916] iteration 9330 : loss : 0.256961, loss_ce: 0.014064
[05:29:52.971] iteration 9340 : loss : 0.168497, loss_ce: 0.004082
[05:29:57.032] iteration 9350 : loss : 0.144803, loss_ce: 0.010245
[05:30:01.083] iteration 9360 : loss : 0.083889, loss_ce: 0.006487
[05:30:05.148] iteration 9370 : loss : 0.243011, loss_ce: 0.017266
[05:30:09.101] iteration 9380 : loss : 0.315407, loss_ce: 0.003667
[05:34:35.496] iteration 9390 : loss : 0.459765, loss_ce: 0.138824
[05:34:39.539] iteration 9400 : loss : 0.352104, loss_ce: 0.031855
[05:34:43.590] iteration 9410 : loss : 0.346952, loss_ce: 0.020036
[05:34:47.633] iteration 9420 : loss : 0.355267, loss_ce: 0.028083
[05:34:51.687] iteration 9430 : loss : 0.169826, loss_ce: 0.004722
[05:34:55.734] iteration 9440 : loss : 0.347788, loss_ce: 0.039637
[05:34:59.793] iteration 9450 : loss : 0.317169, loss_ce: 0.013546
[05:35:03.840] iteration 9460 : loss : 0.174271, loss_ce: 0.028990
[05:35:07.900] iteration 9470 : loss : 0.288042, loss_ce: 0.052101
[05:35:11.949] iteration 9480 : loss : 0.361691, loss_ce: 0.073810
[05:35:16.006] iteration 9490 : loss : 0.182198, loss_ce: 0.024715
[05:35:20.055] iteration 9500 : loss : 0.170400, loss_ce: 0.005055
[05:35:24.115] iteration 9510 : loss : 0.277041, loss_ce: 0.015122
[05:35:28.166] iteration 9520 : loss : 0.064218, loss_ce: 0.007782
[05:35:32.226] iteration 9530 : loss : 0.111256, loss_ce: 0.020733
[05:35:36.276] iteration 9540 : loss : 0.336071, loss_ce: 0.076565
[05:35:40.336] iteration 9550 : loss : 0.315716, loss_ce: 0.010992
[05:35:44.388] iteration 9560 : loss : 0.239011, loss_ce: 0.019886
[05:35:48.446] iteration 9570 : loss : 0.238993, loss_ce: 0.018974
[05:35:52.497] iteration 9580 : loss : 0.049156, loss_ce: 0.006400
[05:35:56.556] iteration 9590 : loss : 0.132564, loss_ce: 0.013514
[05:36:00.607] iteration 9600 : loss : 0.303401, loss_ce: 0.032080
[05:36:04.668] iteration 9610 : loss : 0.072870, loss_ce: 0.008190
[05:36:08.718] iteration 9620 : loss : 0.222280, loss_ce: 0.012142
[05:36:12.779] iteration 9630 : loss : 0.077237, loss_ce: 0.010860
[05:36:16.831] iteration 9640 : loss : 0.219397, loss_ce: 0.016378
[05:40:43.151] iteration 9650 : loss : 0.468317, loss_ce: 0.115531
[05:40:47.186] iteration 9660 : loss : 0.356765, loss_ce: 0.048842
[05:40:51.237] iteration 9670 : loss : 0.219473, loss_ce: 0.050616
[05:40:55.280] iteration 9680 : loss : 0.335547, loss_ce: 0.015221
[05:40:59.334] iteration 9690 : loss : 0.334399, loss_ce: 0.032558
[05:41:03.378] iteration 9700 : loss : 0.328460, loss_ce: 0.020808
[05:41:07.439] iteration 9710 : loss : 0.310641, loss_ce: 0.032135
[05:41:11.488] iteration 9720 : loss : 0.252107, loss_ce: 0.013353
[05:41:15.548] iteration 9730 : loss : 0.332393, loss_ce: 0.032728
[05:41:19.598] iteration 9740 : loss : 0.327678, loss_ce: 0.025949
[05:41:23.658] iteration 9750 : loss : 0.237369, loss_ce: 0.012964
[05:41:27.709] iteration 9760 : loss : 0.229614, loss_ce: 0.013378
[05:41:31.767] iteration 9770 : loss : 0.381560, loss_ce: 0.045131
[05:41:35.817] iteration 9780 : loss : 0.178490, loss_ce: 0.038286
[05:41:39.876] iteration 9790 : loss : 0.188869, loss_ce: 0.029268
[05:41:43.924] iteration 9800 : loss : 0.113329, loss_ce: 0.004867
[05:41:47.993] iteration 9810 : loss : 0.230404, loss_ce: 0.010221
[05:41:52.044] iteration 9820 : loss : 0.319055, loss_ce: 0.006503
[05:41:56.104] iteration 9830 : loss : 0.267244, loss_ce: 0.005970
[05:42:00.153] iteration 9840 : loss : 0.265204, loss_ce: 0.012778
[05:42:04.214] iteration 9850 : loss : 0.221117, loss_ce: 0.008267
[05:42:08.268] iteration 9860 : loss : 0.209254, loss_ce: 0.012069
[05:42:12.327] iteration 9870 : loss : 0.280548, loss_ce: 0.014122
[05:42:16.377] iteration 9880 : loss : 0.332803, loss_ce: 0.022713
[05:42:20.437] iteration 9890 : loss : 0.231106, loss_ce: 0.040118
[05:42:24.489] iteration 9900 : loss : 0.077796, loss_ce: 0.007981
[05:42:28.550] iteration 9910 : loss : 0.241139, loss_ce: 0.017382
[05:46:54.855] iteration 9920 : loss : 0.365134, loss_ce: 0.027472
[05:46:58.901] iteration 9930 : loss : 0.337869, loss_ce: 0.017896
[05:47:02.941] iteration 9940 : loss : 0.196005, loss_ce: 0.032430
[05:47:06.993] iteration 9950 : loss : 0.357452, loss_ce: 0.040808
[05:47:11.041] iteration 9960 : loss : 0.339828, loss_ce: 0.021823
[05:47:15.098] iteration 9970 : loss : 0.179251, loss_ce: 0.013785
[05:47:19.149] iteration 9980 : loss : 0.326776, loss_ce: 0.027184
[05:47:23.205] iteration 9990 : loss : 0.167444, loss_ce: 0.006797
[05:47:27.252] iteration 10000 : loss : 0.310664, loss_ce: 0.007863
[05:47:31.310] iteration 10010 : loss : 0.322896, loss_ce: 0.015547
[05:47:35.358] iteration 10020 : loss : 0.352330, loss_ce: 0.043709
[05:47:39.418] iteration 10030 : loss : 0.264350, loss_ce: 0.012774
[05:47:43.468] iteration 10040 : loss : 0.272734, loss_ce: 0.017198
[05:47:47.527] iteration 10050 : loss : 0.296146, loss_ce: 0.019357
[05:47:51.577] iteration 10060 : loss : 0.164235, loss_ce: 0.026429
[05:47:55.638] iteration 10070 : loss : 0.243130, loss_ce: 0.010291
[05:47:59.687] iteration 10080 : loss : 0.256173, loss_ce: 0.018069
[05:48:03.748] iteration 10090 : loss : 0.287517, loss_ce: 0.049421
[05:48:07.797] iteration 10100 : loss : 0.258974, loss_ce: 0.014717
[05:48:11.859] iteration 10110 : loss : 0.300566, loss_ce: 0.028192
[05:48:15.910] iteration 10120 : loss : 0.234806, loss_ce: 0.014365
[05:48:19.974] iteration 10130 : loss : 0.240171, loss_ce: 0.020446
[05:48:24.025] iteration 10140 : loss : 0.239881, loss_ce: 0.019254
[05:48:28.088] iteration 10150 : loss : 0.057936, loss_ce: 0.006266
[05:48:32.138] iteration 10160 : loss : 0.341630, loss_ce: 0.018866
[05:48:36.198] iteration 10170 : loss : 0.321164, loss_ce: 0.013823
[05:48:40.249] iteration 10180 : loss : 0.194556, loss_ce: 0.012967
[05:53:06.150] iteration 10190 : loss : 0.375524, loss_ce: 0.089613
[05:53:10.184] iteration 10200 : loss : 0.327409, loss_ce: 0.022000
[05:53:14.236] iteration 10210 : loss : 0.383720, loss_ce: 0.048776
[05:53:18.279] iteration 10220 : loss : 0.210519, loss_ce: 0.048465
[05:53:22.332] iteration 10230 : loss : 0.349518, loss_ce: 0.053645
[05:53:26.380] iteration 10240 : loss : 0.345411, loss_ce: 0.032055
[05:53:30.437] iteration 10250 : loss : 0.347937, loss_ce: 0.035707
[05:53:34.484] iteration 10260 : loss : 0.189645, loss_ce: 0.010690
[05:53:38.539] iteration 10270 : loss : 0.332802, loss_ce: 0.017849
[05:53:42.589] iteration 10280 : loss : 0.326094, loss_ce: 0.022633
[05:53:46.647] iteration 10290 : loss : 0.320940, loss_ce: 0.034003
[05:53:50.696] iteration 10300 : loss : 0.266243, loss_ce: 0.018835
[05:53:54.752] iteration 10310 : loss : 0.348989, loss_ce: 0.068361
[05:53:58.799] iteration 10320 : loss : 0.347188, loss_ce: 0.050705
[05:54:02.864] iteration 10330 : loss : 0.289228, loss_ce: 0.010406
[05:54:06.914] iteration 10340 : loss : 0.097142, loss_ce: 0.006105
[05:54:10.977] iteration 10350 : loss : 0.287210, loss_ce: 0.033745
[05:54:15.030] iteration 10360 : loss : 0.303780, loss_ce: 0.025078
[05:54:19.089] iteration 10370 : loss : 0.318650, loss_ce: 0.010341
[05:54:23.140] iteration 10380 : loss : 0.313819, loss_ce: 0.006022
[05:54:27.202] iteration 10390 : loss : 0.327162, loss_ce: 0.039738
[05:54:31.253] iteration 10400 : loss : 0.310963, loss_ce: 0.018358
[05:54:35.313] iteration 10410 : loss : 0.312146, loss_ce: 0.014867
[05:54:39.366] iteration 10420 : loss : 0.173760, loss_ce: 0.016752
[05:54:43.426] iteration 10430 : loss : 0.319329, loss_ce: 0.006836
[05:54:47.479] iteration 10440 : loss : 0.218606, loss_ce: 0.020663
[05:54:51.537] iteration 10450 : loss : 0.124554, loss_ce: 0.009462
[05:59:17.245] iteration 10460 : loss : 0.374825, loss_ce: 0.049461
[05:59:21.294] iteration 10470 : loss : 0.355884, loss_ce: 0.046802
[05:59:25.334] iteration 10480 : loss : 0.335040, loss_ce: 0.023718
[05:59:29.387] iteration 10490 : loss : 0.334504, loss_ce: 0.031596
[05:59:33.431] iteration 10500 : loss : 0.366982, loss_ce: 0.052949
[05:59:37.488] iteration 10510 : loss : 0.190492, loss_ce: 0.032736
[05:59:41.536] iteration 10520 : loss : 0.317310, loss_ce: 0.022479
[05:59:45.593] iteration 10530 : loss : 0.256797, loss_ce: 0.019402
[05:59:49.643] iteration 10540 : loss : 0.327341, loss_ce: 0.036242
[05:59:53.701] iteration 10550 : loss : 0.255092, loss_ce: 0.020846
[05:59:57.752] iteration 10560 : loss : 0.063066, loss_ce: 0.006136
[06:00:01.813] iteration 10570 : loss : 0.249131, loss_ce: 0.014774
[06:00:05.865] iteration 10580 : loss : 0.319946, loss_ce: 0.007718
[06:00:09.925] iteration 10590 : loss : 0.315532, loss_ce: 0.007785
[06:00:13.977] iteration 10600 : loss : 0.269264, loss_ce: 0.024517
[06:00:18.039] iteration 10610 : loss : 0.235207, loss_ce: 0.021009
[06:00:22.090] iteration 10620 : loss : 0.296513, loss_ce: 0.017215
[06:00:26.151] iteration 10630 : loss : 0.320407, loss_ce: 0.006928
[06:00:30.202] iteration 10640 : loss : 0.212675, loss_ce: 0.010509
[06:00:34.262] iteration 10650 : loss : 0.144288, loss_ce: 0.016558
[06:00:38.310] iteration 10660 : loss : 0.178100, loss_ce: 0.006599
[06:00:42.372] iteration 10670 : loss : 0.224564, loss_ce: 0.022661
[06:00:46.423] iteration 10680 : loss : 0.245548, loss_ce: 0.013701
[06:00:50.484] iteration 10690 : loss : 0.310577, loss_ce: 0.004426
[06:00:54.534] iteration 10700 : loss : 0.215612, loss_ce: 0.010933
[06:00:58.599] iteration 10710 : loss : 0.235658, loss_ce: 0.021615
[06:01:02.551] iteration 10720 : loss : 0.246619, loss_ce: 0.012926
[06:05:14.633] save model to ./finetune_tpgm_kits23_continual_300iter\finetuned_epoch_39.pth
[06:05:28.366] iteration 10730 : loss : 0.355721, loss_ce: 0.045387
[06:05:32.414] iteration 10740 : loss : 0.372877, loss_ce: 0.093087
[06:05:36.465] iteration 10750 : loss : 0.189964, loss_ce: 0.025695
[06:05:40.509] iteration 10760 : loss : 0.324322, loss_ce: 0.010360
[06:05:44.564] iteration 10770 : loss : 0.323138, loss_ce: 0.008978
[06:05:48.610] iteration 10780 : loss : 0.173799, loss_ce: 0.014575
[06:05:52.668] iteration 10790 : loss : 0.321071, loss_ce: 0.009424
[06:05:56.717] iteration 10800 : loss : 0.196444, loss_ce: 0.058265
[06:06:00.773] iteration 10810 : loss : 0.324094, loss_ce: 0.025360
[06:06:04.823] iteration 10820 : loss : 0.186535, loss_ce: 0.017125
[06:06:08.884] iteration 10830 : loss : 0.322942, loss_ce: 0.022005
[06:06:12.931] iteration 10840 : loss : 0.313501, loss_ce: 0.006378
[06:06:16.991] iteration 10850 : loss : 0.300068, loss_ce: 0.011762
[06:06:21.046] iteration 10860 : loss : 0.289278, loss_ce: 0.009599
[06:06:25.106] iteration 10870 : loss : 0.301357, loss_ce: 0.016104
[06:06:29.156] iteration 10880 : loss : 0.228301, loss_ce: 0.010513
[06:06:33.218] iteration 10890 : loss : 0.326157, loss_ce: 0.030982
[06:06:37.268] iteration 10900 : loss : 0.267071, loss_ce: 0.041868
[06:06:41.328] iteration 10910 : loss : 0.304514, loss_ce: 0.013175
[06:06:45.379] iteration 10920 : loss : 0.299703, loss_ce: 0.014351
[06:06:49.438] iteration 10930 : loss : 0.059112, loss_ce: 0.009408
[06:06:53.490] iteration 10940 : loss : 0.144380, loss_ce: 0.010833
[06:06:57.552] iteration 10950 : loss : 0.132697, loss_ce: 0.013997
[06:07:01.604] iteration 10960 : loss : 0.223253, loss_ce: 0.023395
[06:07:05.665] iteration 10970 : loss : 0.194768, loss_ce: 0.006061
[06:07:09.717] iteration 10980 : loss : 0.089225, loss_ce: 0.010719
[06:11:35.613] iteration 10990 : loss : 0.470202, loss_ce: 0.110068
[06:11:39.651] iteration 11000 : loss : 0.355230, loss_ce: 0.036681
[06:11:43.702] iteration 11010 : loss : 0.227133, loss_ce: 0.016878
[06:11:47.742] iteration 11020 : loss : 0.328389, loss_ce: 0.017012
[06:11:51.799] iteration 11030 : loss : 0.317779, loss_ce: 0.008181
[06:11:55.845] iteration 11040 : loss : 0.331563, loss_ce: 0.030135
[06:11:59.904] iteration 11050 : loss : 0.312263, loss_ce: 0.014216
[06:12:03.950] iteration 11060 : loss : 0.336943, loss_ce: 0.039087
[06:12:08.010] iteration 11070 : loss : 0.349145, loss_ce: 0.043277
[06:12:12.057] iteration 11080 : loss : 0.347457, loss_ce: 0.023361
[06:12:16.115] iteration 11090 : loss : 0.331556, loss_ce: 0.024833
[06:12:20.165] iteration 11100 : loss : 0.342195, loss_ce: 0.059765
[06:12:24.223] iteration 11110 : loss : 0.321906, loss_ce: 0.022814
[06:12:28.270] iteration 11120 : loss : 0.319092, loss_ce: 0.005555
[06:12:32.329] iteration 11130 : loss : 0.311072, loss_ce: 0.007192
[06:12:36.380] iteration 11140 : loss : 0.161240, loss_ce: 0.011264
[06:12:40.439] iteration 11150 : loss : 0.289372, loss_ce: 0.017924
[06:12:44.491] iteration 11160 : loss : 0.247083, loss_ce: 0.012515
[06:12:48.550] iteration 11170 : loss : 0.369804, loss_ce: 0.038747
[06:12:52.599] iteration 11180 : loss : 0.253623, loss_ce: 0.014470
[06:12:56.659] iteration 11190 : loss : 0.151261, loss_ce: 0.020777
[06:13:00.711] iteration 11200 : loss : 0.287829, loss_ce: 0.015716
[06:13:04.775] iteration 11210 : loss : 0.250501, loss_ce: 0.014968
[06:13:08.824] iteration 11220 : loss : 0.313857, loss_ce: 0.007849
[06:13:12.883] iteration 11230 : loss : 0.115931, loss_ce: 0.024105
[06:13:16.932] iteration 11240 : loss : 0.214747, loss_ce: 0.020153
[06:13:20.992] iteration 11250 : loss : 0.154717, loss_ce: 0.004755
[06:17:46.907] iteration 11260 : loss : 0.400211, loss_ce: 0.036978
[06:17:50.956] iteration 11270 : loss : 0.353029, loss_ce: 0.026886
[06:17:54.997] iteration 11280 : loss : 0.331459, loss_ce: 0.029722
[06:17:59.050] iteration 11290 : loss : 0.178832, loss_ce: 0.023391
[06:18:03.092] iteration 11300 : loss : 0.244797, loss_ce: 0.123666
[06:18:07.148] iteration 11310 : loss : 0.328138, loss_ce: 0.017376
[06:18:11.195] iteration 11320 : loss : 0.188121, loss_ce: 0.012057
[06:18:15.253] iteration 11330 : loss : 0.218961, loss_ce: 0.081304
[06:18:19.301] iteration 11340 : loss : 0.320824, loss_ce: 0.013882
[06:18:23.360] iteration 11350 : loss : 0.189636, loss_ce: 0.036432
[06:18:27.408] iteration 11360 : loss : 0.338089, loss_ce: 0.046970
[06:18:31.468] iteration 11370 : loss : 0.326033, loss_ce: 0.015342
[06:18:35.518] iteration 11380 : loss : 0.327122, loss_ce: 0.020301
[06:18:39.576] iteration 11390 : loss : 0.291469, loss_ce: 0.012084
[06:18:43.625] iteration 11400 : loss : 0.279002, loss_ce: 0.041405
[06:18:47.688] iteration 11410 : loss : 0.314402, loss_ce: 0.016569
[06:18:51.737] iteration 11420 : loss : 0.171195, loss_ce: 0.012966
[06:18:55.796] iteration 11430 : loss : 0.339872, loss_ce: 0.028524
[06:18:59.849] iteration 11440 : loss : 0.330132, loss_ce: 0.025276
[06:19:03.910] iteration 11450 : loss : 0.252067, loss_ce: 0.008755
[06:19:07.961] iteration 11460 : loss : 0.042632, loss_ce: 0.005392
[06:19:12.021] iteration 11470 : loss : 0.168251, loss_ce: 0.015395
[06:19:16.074] iteration 11480 : loss : 0.313317, loss_ce: 0.006446
[06:19:20.134] iteration 11490 : loss : 0.319430, loss_ce: 0.009486
[06:19:24.185] iteration 11500 : loss : 0.241573, loss_ce: 0.018919
[06:19:28.247] iteration 11510 : loss : 0.151167, loss_ce: 0.044271
[06:19:32.298] iteration 11520 : loss : 0.221676, loss_ce: 0.016972
[06:23:58.405] iteration 11530 : loss : 0.386600, loss_ce: 0.034391
[06:24:02.441] iteration 11540 : loss : 0.377844, loss_ce: 0.065755
[06:24:06.490] iteration 11550 : loss : 0.339665, loss_ce: 0.034705
[06:24:10.531] iteration 11560 : loss : 0.325646, loss_ce: 0.013681
[06:24:14.584] iteration 11570 : loss : 0.344274, loss_ce: 0.031094
[06:24:18.630] iteration 11580 : loss : 0.343058, loss_ce: 0.044729
[06:24:22.686] iteration 11590 : loss : 0.369743, loss_ce: 0.105939
[06:24:26.734] iteration 11600 : loss : 0.325137, loss_ce: 0.009278
[06:24:30.791] iteration 11610 : loss : 0.337547, loss_ce: 0.042297
[06:24:34.841] iteration 11620 : loss : 0.346128, loss_ce: 0.056654
[06:24:38.899] iteration 11630 : loss : 0.328743, loss_ce: 0.027959
[06:24:42.948] iteration 11640 : loss : 0.340392, loss_ce: 0.061460
[06:24:47.007] iteration 11650 : loss : 0.173121, loss_ce: 0.009499
[06:24:51.058] iteration 11660 : loss : 0.334976, loss_ce: 0.029256
[06:24:55.117] iteration 11670 : loss : 0.346150, loss_ce: 0.044622
[06:24:59.169] iteration 11680 : loss : 0.309968, loss_ce: 0.003672
[06:25:03.231] iteration 11690 : loss : 0.346050, loss_ce: 0.066934
[06:25:07.281] iteration 11700 : loss : 0.172308, loss_ce: 0.010028
[06:25:11.340] iteration 11710 : loss : 0.323686, loss_ce: 0.033683
[06:25:15.392] iteration 11720 : loss : 0.321651, loss_ce: 0.010774
[06:25:19.453] iteration 11730 : loss : 0.329254, loss_ce: 0.030540
[06:25:23.506] iteration 11740 : loss : 0.173690, loss_ce: 0.010881
[06:25:27.568] iteration 11750 : loss : 0.314015, loss_ce: 0.015664
[06:25:31.616] iteration 11760 : loss : 0.192010, loss_ce: 0.045207
[06:25:35.681] iteration 11770 : loss : 0.316220, loss_ce: 0.010335
[06:25:39.731] iteration 11780 : loss : 0.325815, loss_ce: 0.028974
[06:25:43.795] iteration 11790 : loss : 0.362211, loss_ce: 0.059599
[06:30:10.103] iteration 11800 : loss : 0.355788, loss_ce: 0.025095
[06:30:14.148] iteration 11810 : loss : 0.375498, loss_ce: 0.054191
[06:30:18.197] iteration 11820 : loss : 0.283854, loss_ce: 0.041929
[06:30:22.250] iteration 11830 : loss : 0.345973, loss_ce: 0.013871
[06:30:26.297] iteration 11840 : loss : 0.199996, loss_ce: 0.055560
[06:30:30.352] iteration 11850 : loss : 0.341286, loss_ce: 0.065004
[06:30:34.399] iteration 11860 : loss : 0.345535, loss_ce: 0.043163
[06:30:38.453] iteration 11870 : loss : 0.341893, loss_ce: 0.036714
[06:30:42.499] iteration 11880 : loss : 0.323204, loss_ce: 0.015321
[06:30:46.559] iteration 11890 : loss : 0.323321, loss_ce: 0.012936
[06:30:50.606] iteration 11900 : loss : 0.334065, loss_ce: 0.022752
[06:30:54.666] iteration 11910 : loss : 0.327671, loss_ce: 0.034048
[06:30:58.716] iteration 11920 : loss : 0.354739, loss_ce: 0.081704
[06:31:02.776] iteration 11930 : loss : 0.329797, loss_ce: 0.020122
[06:31:06.826] iteration 11940 : loss : 0.323664, loss_ce: 0.016538
[06:31:10.888] iteration 11950 : loss : 0.321117, loss_ce: 0.019137
[06:31:14.937] iteration 11960 : loss : 0.338891, loss_ce: 0.035189
[06:31:18.997] iteration 11970 : loss : 0.323517, loss_ce: 0.017158
[06:31:23.051] iteration 11980 : loss : 0.168616, loss_ce: 0.014059
[06:31:27.109] iteration 11990 : loss : 0.165195, loss_ce: 0.008041
[06:31:31.162] iteration 12000 : loss : 0.339294, loss_ce: 0.043581
[06:31:35.221] iteration 12010 : loss : 0.327721, loss_ce: 0.013426
[06:31:39.272] iteration 12020 : loss : 0.324252, loss_ce: 0.020993
[06:31:43.335] iteration 12030 : loss : 0.326442, loss_ce: 0.013967
[06:31:47.385] iteration 12040 : loss : 0.318462, loss_ce: 0.014541
[06:31:51.445] iteration 12050 : loss : 0.250956, loss_ce: 0.022661
[06:31:55.396] iteration 12060 : loss : 0.319418, loss_ce: 0.008539
[06:36:21.418] iteration 12070 : loss : 0.351458, loss_ce: 0.030187
[06:36:25.468] iteration 12080 : loss : 0.337613, loss_ce: 0.024208
[06:36:29.521] iteration 12090 : loss : 0.324426, loss_ce: 0.015433
[06:36:33.564] iteration 12100 : loss : 0.359286, loss_ce: 0.053412
[06:36:37.622] iteration 12110 : loss : 0.333400, loss_ce: 0.018884
[06:36:41.668] iteration 12120 : loss : 0.330140, loss_ce: 0.015208
[06:36:45.724] iteration 12130 : loss : 0.339412, loss_ce: 0.040445
[06:36:49.770] iteration 12140 : loss : 0.331904, loss_ce: 0.009277
[06:36:53.832] iteration 12150 : loss : 0.342022, loss_ce: 0.047484
[06:36:57.880] iteration 12160 : loss : 0.338062, loss_ce: 0.016102
[06:37:01.940] iteration 12170 : loss : 0.337599, loss_ce: 0.050380
[06:37:05.991] iteration 12180 : loss : 0.325815, loss_ce: 0.017073
[06:37:10.050] iteration 12190 : loss : 0.346555, loss_ce: 0.066451
[06:37:14.099] iteration 12200 : loss : 0.330030, loss_ce: 0.016588
[06:37:18.158] iteration 12210 : loss : 0.318176, loss_ce: 0.016098
[06:37:22.207] iteration 12220 : loss : 0.216095, loss_ce: 0.104713
[06:37:26.266] iteration 12230 : loss : 0.339995, loss_ce: 0.044798
[06:37:30.314] iteration 12240 : loss : 0.342990, loss_ce: 0.042194
[06:37:34.374] iteration 12250 : loss : 0.328975, loss_ce: 0.029591
[06:37:38.422] iteration 12260 : loss : 0.344863, loss_ce: 0.065329
[06:37:42.483] iteration 12270 : loss : 0.334848, loss_ce: 0.031334
[06:37:46.534] iteration 12280 : loss : 0.339005, loss_ce: 0.033306
[06:37:50.595] iteration 12290 : loss : 0.342786, loss_ce: 0.051683
[06:37:54.645] iteration 12300 : loss : 0.336780, loss_ce: 0.045123
[06:37:58.706] iteration 12310 : loss : 0.330901, loss_ce: 0.033215
[06:38:02.760] iteration 12320 : loss : 0.326701, loss_ce: 0.025216
[06:42:29.464] iteration 12330 : loss : 0.446727, loss_ce: 0.026860
[06:42:33.501] iteration 12340 : loss : 0.388716, loss_ce: 0.027966
[06:42:37.552] iteration 12350 : loss : 0.356256, loss_ce: 0.029049
[06:42:41.593] iteration 12360 : loss : 0.343180, loss_ce: 0.037977
[06:42:45.646] iteration 12370 : loss : 0.364968, loss_ce: 0.081080
[06:42:49.689] iteration 12380 : loss : 0.326292, loss_ce: 0.014169
[06:42:53.746] iteration 12390 : loss : 0.356340, loss_ce: 0.086681
[06:42:57.792] iteration 12400 : loss : 0.318434, loss_ce: 0.010988
[06:43:01.851] iteration 12410 : loss : 0.337529, loss_ce: 0.025995
[06:43:05.901] iteration 12420 : loss : 0.327895, loss_ce: 0.018627
[06:43:09.958] iteration 12430 : loss : 0.345862, loss_ce: 0.038367
[06:43:14.009] iteration 12440 : loss : 0.333708, loss_ce: 0.039736
[06:43:18.067] iteration 12450 : loss : 0.341702, loss_ce: 0.034551
[06:43:22.117] iteration 12460 : loss : 0.333450, loss_ce: 0.022462
[06:43:26.180] iteration 12470 : loss : 0.324305, loss_ce: 0.011894
[06:43:30.230] iteration 12480 : loss : 0.336968, loss_ce: 0.048767
[06:43:34.290] iteration 12490 : loss : 0.173494, loss_ce: 0.011377
[06:43:38.340] iteration 12500 : loss : 0.326970, loss_ce: 0.014940
[06:43:42.398] iteration 12510 : loss : 0.330648, loss_ce: 0.020778
[06:43:46.447] iteration 12520 : loss : 0.329449, loss_ce: 0.018068
[06:43:50.508] iteration 12530 : loss : 0.321605, loss_ce: 0.015235
[06:43:54.560] iteration 12540 : loss : 0.323970, loss_ce: 0.018086
[06:43:58.620] iteration 12550 : loss : 0.330561, loss_ce: 0.027746
[06:44:02.673] iteration 12560 : loss : 0.181712, loss_ce: 0.032645
[06:44:06.734] iteration 12570 : loss : 0.349506, loss_ce: 0.073973
[06:44:10.784] iteration 12580 : loss : 0.315776, loss_ce: 0.018597
[06:44:14.845] iteration 12590 : loss : 0.329081, loss_ce: 0.024712
[06:48:40.565] iteration 12600 : loss : 0.461222, loss_ce: 0.056689
[06:48:44.610] iteration 12610 : loss : 0.378449, loss_ce: 0.038900
[06:48:48.649] iteration 12620 : loss : 0.340498, loss_ce: 0.022273
[06:48:52.703] iteration 12630 : loss : 0.346269, loss_ce: 0.026291
[06:48:56.747] iteration 12640 : loss : 0.342205, loss_ce: 0.026861
[06:49:00.804] iteration 12650 : loss : 0.329260, loss_ce: 0.019102
[06:49:04.850] iteration 12660 : loss : 0.318649, loss_ce: 0.003002
[06:49:08.915] iteration 12670 : loss : 0.343256, loss_ce: 0.041044
[06:49:12.962] iteration 12680 : loss : 0.349773, loss_ce: 0.083156
[06:49:17.019] iteration 12690 : loss : 0.337468, loss_ce: 0.035889
[06:49:21.066] iteration 12700 : loss : 0.333385, loss_ce: 0.025720
[06:49:25.125] iteration 12710 : loss : 0.338028, loss_ce: 0.036644
[06:49:29.175] iteration 12720 : loss : 0.329132, loss_ce: 0.031508
[06:49:33.233] iteration 12730 : loss : 0.343465, loss_ce: 0.063931
[06:49:37.285] iteration 12740 : loss : 0.335354, loss_ce: 0.012368
[06:49:41.344] iteration 12750 : loss : 0.332964, loss_ce: 0.056911
[06:49:45.394] iteration 12760 : loss : 0.327600, loss_ce: 0.012553
[06:49:49.457] iteration 12770 : loss : 0.352001, loss_ce: 0.054950
[06:49:53.507] iteration 12780 : loss : 0.323673, loss_ce: 0.014681
[06:49:57.569] iteration 12790 : loss : 0.353940, loss_ce: 0.058491
[06:50:01.620] iteration 12800 : loss : 0.348209, loss_ce: 0.076571
[06:50:05.680] iteration 12810 : loss : 0.334636, loss_ce: 0.023626
[06:50:09.732] iteration 12820 : loss : 0.314840, loss_ce: 0.003814
[06:50:13.795] iteration 12830 : loss : 0.343245, loss_ce: 0.072763
[06:50:17.847] iteration 12840 : loss : 0.357599, loss_ce: 0.070436
[06:50:21.913] iteration 12850 : loss : 0.318957, loss_ce: 0.014172
[06:50:25.964] iteration 12860 : loss : 0.327646, loss_ce: 0.026076
[06:54:52.269] iteration 12870 : loss : 0.446852, loss_ce: 0.050545
[06:54:56.309] iteration 12880 : loss : 0.426004, loss_ce: 0.078334
[06:55:00.366] iteration 12890 : loss : 0.399171, loss_ce: 0.089847
[06:55:04.409] iteration 12900 : loss : 0.347285, loss_ce: 0.052825
[06:55:08.462] iteration 12910 : loss : 0.356029, loss_ce: 0.026567
[06:55:12.512] iteration 12920 : loss : 0.352992, loss_ce: 0.017078
[06:55:16.570] iteration 12930 : loss : 0.357736, loss_ce: 0.084137
[06:55:20.619] iteration 12940 : loss : 0.325211, loss_ce: 0.016923
[06:55:24.678] iteration 12950 : loss : 0.340748, loss_ce: 0.015855
[06:55:28.728] iteration 12960 : loss : 0.320367, loss_ce: 0.009960
[06:55:32.789] iteration 12970 : loss : 0.352444, loss_ce: 0.075387
[06:55:36.838] iteration 12980 : loss : 0.339134, loss_ce: 0.042658
[06:55:40.898] iteration 12990 : loss : 0.354360, loss_ce: 0.072531
[06:55:44.949] iteration 13000 : loss : 0.318058, loss_ce: 0.015675
[06:55:49.009] iteration 13010 : loss : 0.343321, loss_ce: 0.062392
[06:55:53.058] iteration 13020 : loss : 0.330328, loss_ce: 0.029490
[06:55:57.118] iteration 13030 : loss : 0.336530, loss_ce: 0.021352
[06:56:01.167] iteration 13040 : loss : 0.349163, loss_ce: 0.059257
[06:56:05.227] iteration 13050 : loss : 0.371573, loss_ce: 0.137572
[06:56:09.277] iteration 13060 : loss : 0.343353, loss_ce: 0.063871
[06:56:13.336] iteration 13070 : loss : 0.341171, loss_ce: 0.067630
[06:56:17.384] iteration 13080 : loss : 0.331638, loss_ce: 0.027407
[06:56:21.444] iteration 13090 : loss : 0.360491, loss_ce: 0.081392
[06:56:25.495] iteration 13100 : loss : 0.324441, loss_ce: 0.012368
[06:56:29.556] iteration 13110 : loss : 0.339629, loss_ce: 0.037213
[06:56:33.608] iteration 13120 : loss : 0.311774, loss_ce: 0.005086
[06:56:37.668] iteration 13130 : loss : 0.350731, loss_ce: 0.050685
[07:01:03.560] iteration 13140 : loss : 0.466778, loss_ce: 0.078846
[07:01:07.609] iteration 13150 : loss : 0.452083, loss_ce: 0.043217
[07:01:11.651] iteration 13160 : loss : 0.439618, loss_ce: 0.073140
[07:01:15.703] iteration 13170 : loss : 0.421571, loss_ce: 0.047164
[07:01:19.748] iteration 13180 : loss : 0.400935, loss_ce: 0.032543
[07:01:23.804] iteration 13190 : loss : 0.382436, loss_ce: 0.018441
[07:01:27.849] iteration 13200 : loss : 0.433988, loss_ce: 0.189243
[07:01:31.904] iteration 13210 : loss : 0.369607, loss_ce: 0.021102
[07:01:35.953] iteration 13220 : loss : 0.400053, loss_ce: 0.112768
[07:01:40.011] iteration 13230 : loss : 0.358533, loss_ce: 0.034068
[07:01:44.059] iteration 13240 : loss : 0.337972, loss_ce: 0.025329
[07:01:48.118] iteration 13250 : loss : 0.374167, loss_ce: 0.082030
[07:01:52.166] iteration 13260 : loss : 0.362734, loss_ce: 0.024860
[07:01:56.227] iteration 13270 : loss : 0.372576, loss_ce: 0.060019
[07:02:00.276] iteration 13280 : loss : 0.361954, loss_ce: 0.018063
[07:02:04.337] iteration 13290 : loss : 0.367846, loss_ce: 0.072000
[07:02:08.388] iteration 13300 : loss : 0.336000, loss_ce: 0.012093
[07:02:12.448] iteration 13310 : loss : 0.348089, loss_ce: 0.007539
[07:02:16.500] iteration 13320 : loss : 0.363171, loss_ce: 0.048489
[07:02:20.565] iteration 13330 : loss : 0.348683, loss_ce: 0.030835
[07:02:24.616] iteration 13340 : loss : 0.351954, loss_ce: 0.038747
[07:02:28.678] iteration 13350 : loss : 0.356549, loss_ce: 0.080335
[07:02:32.730] iteration 13360 : loss : 0.361221, loss_ce: 0.056857
[07:02:36.792] iteration 13370 : loss : 0.379406, loss_ce: 0.080803
[07:02:40.842] iteration 13380 : loss : 0.377344, loss_ce: 0.078019
[07:02:44.902] iteration 13390 : loss : 0.347804, loss_ce: 0.034651
[07:02:48.853] iteration 13400 : loss : 0.376588, loss_ce: 0.029169
[07:07:01.334] save model to ./finetune_tpgm_kits23_continual_300iter\finetuned_epoch_49.pth
[07:07:01.505] save final model to ./finetune_tpgm_kits23_continual_300iter\finetuned_final.pth
[13:42:00.562] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual_300iter', max_iterations=10000, max_epochs=50, batch_size=40, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=500, tpgm_exclude=[], gpu_id=1)
[13:42:00.590] Using 8569/95221 samples for finetuning
[13:42:00.591] Using 953/95221 samples for TPGM
[13:42:00.591] Model has 9 total classes, training on 4 classes
[13:42:10.684] 215 iterations per epoch. 10750 max iterations 
[13:42:26.458] iteration 10 : loss : 0.456170, loss_ce: 0.050299
[13:42:31.604] iteration 20 : loss : 0.438975, loss_ce: 0.056888
[13:42:36.779] iteration 30 : loss : 0.438346, loss_ce: 0.042798
[13:42:41.934] iteration 40 : loss : 0.435857, loss_ce: 0.049939
[13:42:47.104] iteration 50 : loss : 0.430498, loss_ce: 0.037452
[13:42:52.273] iteration 60 : loss : 0.431522, loss_ce: 0.062120
[13:42:57.459] iteration 70 : loss : 0.432292, loss_ce: 0.063252
[13:43:02.702] iteration 80 : loss : 0.435291, loss_ce: 0.065223
[13:43:07.888] iteration 90 : loss : 0.266989, loss_ce: 0.038618
[13:43:13.062] iteration 100 : loss : 0.444859, loss_ce: 0.050560
[13:43:18.253] iteration 110 : loss : 0.288439, loss_ce: 0.070406
[13:43:23.431] iteration 120 : loss : 0.395793, loss_ce: 0.039557
[13:43:28.622] iteration 130 : loss : 0.400825, loss_ce: 0.062071
[13:43:33.811] iteration 140 : loss : 0.398490, loss_ce: 0.047067
[13:43:39.006] iteration 150 : loss : 0.254200, loss_ce: 0.022944
[13:43:44.188] iteration 160 : loss : 0.370559, loss_ce: 0.042804
[13:43:49.380] iteration 170 : loss : 0.415779, loss_ce: 0.032371
[13:43:54.563] iteration 180 : loss : 0.475309, loss_ce: 0.063158
[13:43:59.753] iteration 190 : loss : 0.487142, loss_ce: 0.091660
[13:44:04.936] iteration 200 : loss : 0.495775, loss_ce: 0.113296
[13:44:10.127] iteration 210 : loss : 0.487324, loss_ce: 0.106846
[13:49:43.324] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual_300iter', max_iterations=10000, max_epochs=50, batch_size=40, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=500, tpgm_exclude=[], gpu_id=1)
[13:49:43.352] Using 8569/95221 samples for finetuning
[13:49:43.352] Using 953/95221 samples for TPGM
[13:49:43.352] Model has 9 total classes, training on 4 classes
[13:49:53.387] 215 iterations per epoch. 10750 max iterations 
[13:50:09.293] iteration 10 : loss : 0.456170, loss_ce: 0.050299
[13:50:14.443] iteration 20 : loss : 0.438975, loss_ce: 0.056888
[13:50:19.624] iteration 30 : loss : 0.438346, loss_ce: 0.042798
[13:50:24.786] iteration 40 : loss : 0.435857, loss_ce: 0.049939
[13:50:29.960] iteration 50 : loss : 0.430498, loss_ce: 0.037452
[13:50:35.128] iteration 60 : loss : 0.431522, loss_ce: 0.062120
[13:50:40.315] iteration 70 : loss : 0.432292, loss_ce: 0.063252
[13:50:45.492] iteration 80 : loss : 0.435291, loss_ce: 0.065223
[13:50:50.684] iteration 90 : loss : 0.266989, loss_ce: 0.038618
[13:50:55.865] iteration 100 : loss : 0.444859, loss_ce: 0.050560
[13:51:01.058] iteration 110 : loss : 0.288439, loss_ce: 0.070406
[13:51:06.242] iteration 120 : loss : 0.395793, loss_ce: 0.039557
[13:51:11.441] iteration 130 : loss : 0.400825, loss_ce: 0.062071
[13:51:16.624] iteration 140 : loss : 0.398490, loss_ce: 0.047067
[13:51:21.822] iteration 150 : loss : 0.254200, loss_ce: 0.022944
[13:51:27.016] iteration 160 : loss : 0.370559, loss_ce: 0.042804
[13:51:32.214] iteration 170 : loss : 0.415779, loss_ce: 0.032371
[13:51:37.399] iteration 180 : loss : 0.475309, loss_ce: 0.063158
[13:51:42.593] iteration 190 : loss : 0.487142, loss_ce: 0.091660
[13:51:47.779] iteration 200 : loss : 0.495775, loss_ce: 0.113296
[13:51:52.981] iteration 210 : loss : 0.487324, loss_ce: 0.106846
[14:03:01.816] Namespace(root_path='./datasets/kits23/train_npz', dataset='kits23', list_dir='./lists/kits23', num_classes=4, model_num_classes=9, output_dir='./finetune_tpgm_kits23_continual_300iter', max_iterations=10000, max_epochs=50, batch_size=40, n_gpu=1, deterministic=1, base_lr=0.001, img_size=224, seed=1234, cfg='configs/finetune.yaml', pretrained_path='./pretrain/epoch_149.pth', data_fraction=0.1, freeze_layers=0, opts=None, zip=False, cache_mode='part', resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level='O1', tag=None, eval=False, throughput=False, tpgm_norm_mode='mars', tpgm_lr=0.001, tpgm_iters=500, tpgm_exclude=[], gpu_id=1)
[14:03:01.842] Using 8569/95221 samples for finetuning
[14:03:01.842] Using 953/95221 samples for TPGM
[14:03:01.842] Model has 9 total classes, training on 4 classes
